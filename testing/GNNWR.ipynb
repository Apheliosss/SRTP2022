{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.images = df.iloc[:,5:].values\n",
    "        self.coef = df.iloc[:,1:5].values\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        coef = self.coef[idx]\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        coef = torch.tensor(coef, dtype=torch.float)\n",
    "\n",
    "        return image, coef, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D://CO2_data1.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(P, C):\n",
    "    A = (P**2).sum(axis=1, keepdims=True)\n",
    " \n",
    "    B = (C**2).sum(axis=1, keepdims=True).T\n",
    " \n",
    "    return np.sqrt(A + B - 2* np.dot(P, C.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "ycor = dataset.lat\n",
    "#ycor = dataset.lon\n",
    "label = dataset.fCO2\n",
    "\n",
    "train_df['label'] = label\n",
    "\n",
    "train_df['beta'] = np.ones(623)\n",
    "train_df['Chl'] = dataset.Chl\n",
    "train_df['Temp'] = dataset.Temp\n",
    "train_df['Salt'] = dataset.Salt\n",
    "\n",
    "alist = dataset.lon\n",
    "temp = []\n",
    "for i in alist:\n",
    "    if i < 0:\n",
    "        i = i+360\n",
    "    temp.append(i)\n",
    "xcor = temp\n",
    "\n",
    "cor_df = pd.DataFrame()\n",
    "cor_df['xcor'] = xcor\n",
    "cor_df['ycor'] = ycor\n",
    "\n",
    "a = [[110.0, 0.0], [290.0,0.0], [110.0, 70.0], [290.0, 70.0]]\n",
    "b = np.array(a)\n",
    "\n",
    "cor_li = cor_df.to_numpy()\n",
    "dis_li = compute_distances(cor_li, b)\n",
    "dis_df = pd.DataFrame(dis_li)\n",
    "train_df = train_df.join(dis_df)\n",
    "\n",
    "\n",
    "\n",
    "train_data = MYDataset(train_df)\n",
    "#test_data = MYDataset(test_df)\n",
    "train_loader = DataLoader(train_data, batch_size=50, shuffle=True, num_workers=0, drop_last=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>beta</th>\n",
       "      <th>Chl</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Salt</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.640840</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.778654</td>\n",
       "      <td>1.087273</td>\n",
       "      <td>0.172380</td>\n",
       "      <td>56.090329</td>\n",
       "      <td>126.554830</td>\n",
       "      <td>77.788977</td>\n",
       "      <td>137.554080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.808094</td>\n",
       "      <td>1.110043</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>57.316010</td>\n",
       "      <td>125.678658</td>\n",
       "      <td>77.782549</td>\n",
       "      <td>136.235550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.403523</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.786881</td>\n",
       "      <td>1.117319</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>56.631484</td>\n",
       "      <td>126.795603</td>\n",
       "      <td>76.368351</td>\n",
       "      <td>136.755713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.786611</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>-0.077296</td>\n",
       "      <td>57.590147</td>\n",
       "      <td>125.803915</td>\n",
       "      <td>77.081937</td>\n",
       "      <td>135.836759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311748</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.828534</td>\n",
       "      <td>1.142110</td>\n",
       "      <td>0.063110</td>\n",
       "      <td>56.926488</td>\n",
       "      <td>126.927637</td>\n",
       "      <td>75.667860</td>\n",
       "      <td>136.365776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>-0.818805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.679605</td>\n",
       "      <td>0.595628</td>\n",
       "      <td>1.005389</td>\n",
       "      <td>31.593116</td>\n",
       "      <td>161.580089</td>\n",
       "      <td>50.031240</td>\n",
       "      <td>166.171974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>-1.223775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.550274</td>\n",
       "      <td>0.499140</td>\n",
       "      <td>1.539144</td>\n",
       "      <td>30.579814</td>\n",
       "      <td>164.697070</td>\n",
       "      <td>47.959618</td>\n",
       "      <td>168.790180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>-1.212453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.621767</td>\n",
       "      <td>0.562813</td>\n",
       "      <td>1.266849</td>\n",
       "      <td>31.154855</td>\n",
       "      <td>163.708964</td>\n",
       "      <td>48.328304</td>\n",
       "      <td>167.826175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>-1.024903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.629206</td>\n",
       "      <td>0.559075</td>\n",
       "      <td>1.113024</td>\n",
       "      <td>31.750984</td>\n",
       "      <td>162.721004</td>\n",
       "      <td>48.714731</td>\n",
       "      <td>166.862593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>-1.114077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.624133</td>\n",
       "      <td>0.553121</td>\n",
       "      <td>0.794493</td>\n",
       "      <td>32.367036</td>\n",
       "      <td>161.733191</td>\n",
       "      <td>49.118479</td>\n",
       "      <td>165.899442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  beta       Chl      Temp      Salt          0           1  \\\n",
       "0    0.640840   1.0 -0.778654  1.087273  0.172380  56.090329  126.554830   \n",
       "1    0.405739   1.0 -0.808094  1.110043  0.021629  57.316010  125.678658   \n",
       "2    0.403523   1.0 -0.786881  1.117319  0.126096  56.631484  126.795603   \n",
       "3    0.225175   1.0 -0.786611  1.085791 -0.077296  57.590147  125.803915   \n",
       "4    0.311748   1.0 -0.828534  1.142110  0.063110  56.926488  126.927637   \n",
       "..        ...   ...       ...       ...       ...        ...         ...   \n",
       "618 -0.818805   1.0 -0.679605  0.595628  1.005389  31.593116  161.580089   \n",
       "619 -1.223775   1.0 -0.550274  0.499140  1.539144  30.579814  164.697070   \n",
       "620 -1.212453   1.0 -0.621767  0.562813  1.266849  31.154855  163.708964   \n",
       "621 -1.024903   1.0 -0.629206  0.559075  1.113024  31.750984  162.721004   \n",
       "622 -1.114077   1.0 -0.624133  0.553121  0.794493  32.367036  161.733191   \n",
       "\n",
       "             2           3  \n",
       "0    77.788977  137.554080  \n",
       "1    77.782549  136.235550  \n",
       "2    76.368351  136.755713  \n",
       "3    77.081937  135.836759  \n",
       "4    75.667860  136.365776  \n",
       "..         ...         ...  \n",
       "618  50.031240  166.171974  \n",
       "619  47.959618  168.790180  \n",
       "620  48.328304  167.826175  \n",
       "621  48.714731  166.862593  \n",
       "622  49.118479  165.899442  \n",
       "\n",
       "[623 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class GNNWR(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(GNNWR, self).__init__()\n",
    "        self.insize = insize\n",
    "        self.outsize = outsize\n",
    "\n",
    "        lastsize = self.insize\n",
    "        thissize = 0\n",
    "        self.fc = nn.Sequential()\n",
    "        i = 2\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(1), nn.Linear(4, 600))\n",
    "        # self.fc.add_module(\"batc\"+str(1), nn.BatchNorm1d(600))\n",
    "        # self.fc.add_module(\"acti\"+str(1), nn.PReLU(init=0.4))\n",
    "        # self.fc.add_module(\"drop\"+str(1), nn.Dropout(0.2))\n",
    "\n",
    "        lastsize = 600\n",
    "        while math.pow(2, int(math.log2(lastsize))) >= max(128, outsize + 1):\n",
    "            if i == 1:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize))))\n",
    "            else:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize)) - 1))\n",
    "            \n",
    "            self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, thissize))\n",
    "            self.fc.add_module(\"batc\"+str(i), nn.BatchNorm1d(thissize))\n",
    "            self.fc.add_module(\"acti\"+str(i), nn.PReLU(init=0.4))\n",
    "            \n",
    "            self.fc.add_module(\"drop\"+str(i), nn.Dropout(0.2))\n",
    "\n",
    "            lastsize = thissize\n",
    "            i = i + 1\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, outsize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = GNNWR(623, 4)\n",
    "criterion = nn.MSELoss(reduce=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 0\n",
    "weightlist = []\n",
    "for i in range(1,2):\n",
    "    temp = []\n",
    "    temp.append(2.463e-16)\n",
    "    temp.append(-0.1239)\n",
    "    temp.append(0.5236)\n",
    "    temp.append(0.0319)\n",
    "    weightlist.append(temp)\n",
    "out = nn.Linear(4, 1, bias = False)\n",
    "out.weight = nn.Parameter(torch.tensor(weightlist), requires_grad=False)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    global r2\n",
    "    global out\n",
    "    for data, coef, label in train_loader:\n",
    "        data = data.view(50, -1)\n",
    "        label = label.view(50, -1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        output = output.mul(coef)\n",
    "        output = out(output)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        a = output.view(-1).detach().numpy()\n",
    "        b = label.view(-1).numpy()\n",
    "        if epoch % 100 == 0:\n",
    "            r2 = r2_score(a, b)\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tR2: {:.6f}'.format(epoch, train_loss, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.671032 \tR2: 0.000000\n",
      "Epoch: 2 \tTraining Loss: 0.564789 \tR2: 0.000000\n",
      "Epoch: 3 \tTraining Loss: 0.560738 \tR2: 0.000000\n",
      "Epoch: 4 \tTraining Loss: 0.530218 \tR2: 0.000000\n",
      "Epoch: 5 \tTraining Loss: 0.518015 \tR2: 0.000000\n",
      "Epoch: 6 \tTraining Loss: 0.549064 \tR2: 0.000000\n",
      "Epoch: 7 \tTraining Loss: 0.517901 \tR2: 0.000000\n",
      "Epoch: 8 \tTraining Loss: 0.510109 \tR2: 0.000000\n",
      "Epoch: 9 \tTraining Loss: 0.535499 \tR2: 0.000000\n",
      "Epoch: 10 \tTraining Loss: 0.516492 \tR2: 0.000000\n",
      "Epoch: 11 \tTraining Loss: 0.517389 \tR2: 0.000000\n",
      "Epoch: 12 \tTraining Loss: 0.520535 \tR2: 0.000000\n",
      "Epoch: 13 \tTraining Loss: 0.512175 \tR2: 0.000000\n",
      "Epoch: 14 \tTraining Loss: 0.520999 \tR2: 0.000000\n",
      "Epoch: 15 \tTraining Loss: 0.498224 \tR2: 0.000000\n",
      "Epoch: 16 \tTraining Loss: 0.502381 \tR2: 0.000000\n",
      "Epoch: 17 \tTraining Loss: 0.493802 \tR2: 0.000000\n",
      "Epoch: 18 \tTraining Loss: 0.508343 \tR2: 0.000000\n",
      "Epoch: 19 \tTraining Loss: 0.515987 \tR2: 0.000000\n",
      "Epoch: 20 \tTraining Loss: 0.500544 \tR2: 0.000000\n",
      "Epoch: 21 \tTraining Loss: 0.518119 \tR2: 0.000000\n",
      "Epoch: 22 \tTraining Loss: 0.502877 \tR2: 0.000000\n",
      "Epoch: 23 \tTraining Loss: 0.477421 \tR2: 0.000000\n",
      "Epoch: 24 \tTraining Loss: 0.513076 \tR2: 0.000000\n",
      "Epoch: 25 \tTraining Loss: 0.508802 \tR2: 0.000000\n",
      "Epoch: 26 \tTraining Loss: 0.497806 \tR2: 0.000000\n",
      "Epoch: 27 \tTraining Loss: 0.492517 \tR2: 0.000000\n",
      "Epoch: 28 \tTraining Loss: 0.503112 \tR2: 0.000000\n",
      "Epoch: 29 \tTraining Loss: 0.493581 \tR2: 0.000000\n",
      "Epoch: 30 \tTraining Loss: 0.472728 \tR2: 0.000000\n",
      "Epoch: 31 \tTraining Loss: 0.469281 \tR2: 0.000000\n",
      "Epoch: 32 \tTraining Loss: 0.469026 \tR2: 0.000000\n",
      "Epoch: 33 \tTraining Loss: 0.474237 \tR2: 0.000000\n",
      "Epoch: 34 \tTraining Loss: 0.496544 \tR2: 0.000000\n",
      "Epoch: 35 \tTraining Loss: 0.499779 \tR2: 0.000000\n",
      "Epoch: 36 \tTraining Loss: 0.481732 \tR2: 0.000000\n",
      "Epoch: 37 \tTraining Loss: 0.496365 \tR2: 0.000000\n",
      "Epoch: 38 \tTraining Loss: 0.495696 \tR2: 0.000000\n",
      "Epoch: 39 \tTraining Loss: 0.489343 \tR2: 0.000000\n",
      "Epoch: 40 \tTraining Loss: 0.469924 \tR2: 0.000000\n",
      "Epoch: 41 \tTraining Loss: 0.466344 \tR2: 0.000000\n",
      "Epoch: 42 \tTraining Loss: 0.492034 \tR2: 0.000000\n",
      "Epoch: 43 \tTraining Loss: 0.474763 \tR2: 0.000000\n",
      "Epoch: 44 \tTraining Loss: 0.456390 \tR2: 0.000000\n",
      "Epoch: 45 \tTraining Loss: 0.497470 \tR2: 0.000000\n",
      "Epoch: 46 \tTraining Loss: 0.485380 \tR2: 0.000000\n",
      "Epoch: 47 \tTraining Loss: 0.483591 \tR2: 0.000000\n",
      "Epoch: 48 \tTraining Loss: 0.485533 \tR2: 0.000000\n",
      "Epoch: 49 \tTraining Loss: 0.494630 \tR2: 0.000000\n",
      "Epoch: 50 \tTraining Loss: 0.472404 \tR2: 0.000000\n",
      "Epoch: 51 \tTraining Loss: 0.475980 \tR2: 0.000000\n",
      "Epoch: 52 \tTraining Loss: 0.475809 \tR2: 0.000000\n",
      "Epoch: 53 \tTraining Loss: 0.460711 \tR2: 0.000000\n",
      "Epoch: 54 \tTraining Loss: 0.458448 \tR2: 0.000000\n",
      "Epoch: 55 \tTraining Loss: 0.463950 \tR2: 0.000000\n",
      "Epoch: 56 \tTraining Loss: 0.461042 \tR2: 0.000000\n",
      "Epoch: 57 \tTraining Loss: 0.467599 \tR2: 0.000000\n",
      "Epoch: 58 \tTraining Loss: 0.463461 \tR2: 0.000000\n",
      "Epoch: 59 \tTraining Loss: 0.458620 \tR2: 0.000000\n",
      "Epoch: 60 \tTraining Loss: 0.472667 \tR2: 0.000000\n",
      "Epoch: 61 \tTraining Loss: 0.491998 \tR2: 0.000000\n",
      "Epoch: 62 \tTraining Loss: 0.466938 \tR2: 0.000000\n",
      "Epoch: 63 \tTraining Loss: 0.467020 \tR2: 0.000000\n",
      "Epoch: 64 \tTraining Loss: 0.453070 \tR2: 0.000000\n",
      "Epoch: 65 \tTraining Loss: 0.455985 \tR2: 0.000000\n",
      "Epoch: 66 \tTraining Loss: 0.434586 \tR2: 0.000000\n",
      "Epoch: 67 \tTraining Loss: 0.467238 \tR2: 0.000000\n",
      "Epoch: 68 \tTraining Loss: 0.452832 \tR2: 0.000000\n",
      "Epoch: 69 \tTraining Loss: 0.452292 \tR2: 0.000000\n",
      "Epoch: 70 \tTraining Loss: 0.462154 \tR2: 0.000000\n",
      "Epoch: 71 \tTraining Loss: 0.420976 \tR2: 0.000000\n",
      "Epoch: 72 \tTraining Loss: 0.441159 \tR2: 0.000000\n",
      "Epoch: 73 \tTraining Loss: 0.478691 \tR2: 0.000000\n",
      "Epoch: 74 \tTraining Loss: 0.473299 \tR2: 0.000000\n",
      "Epoch: 75 \tTraining Loss: 0.470580 \tR2: 0.000000\n",
      "Epoch: 76 \tTraining Loss: 0.448924 \tR2: 0.000000\n",
      "Epoch: 77 \tTraining Loss: 0.478505 \tR2: 0.000000\n",
      "Epoch: 78 \tTraining Loss: 0.466513 \tR2: 0.000000\n",
      "Epoch: 79 \tTraining Loss: 0.449475 \tR2: 0.000000\n",
      "Epoch: 80 \tTraining Loss: 0.449213 \tR2: 0.000000\n",
      "Epoch: 81 \tTraining Loss: 0.448555 \tR2: 0.000000\n",
      "Epoch: 82 \tTraining Loss: 0.451040 \tR2: 0.000000\n",
      "Epoch: 83 \tTraining Loss: 0.454344 \tR2: 0.000000\n",
      "Epoch: 84 \tTraining Loss: 0.436320 \tR2: 0.000000\n",
      "Epoch: 85 \tTraining Loss: 0.440737 \tR2: 0.000000\n",
      "Epoch: 86 \tTraining Loss: 0.443963 \tR2: 0.000000\n",
      "Epoch: 87 \tTraining Loss: 0.411704 \tR2: 0.000000\n",
      "Epoch: 88 \tTraining Loss: 0.437831 \tR2: 0.000000\n",
      "Epoch: 89 \tTraining Loss: 0.444039 \tR2: 0.000000\n",
      "Epoch: 90 \tTraining Loss: 0.420080 \tR2: 0.000000\n",
      "Epoch: 91 \tTraining Loss: 0.404107 \tR2: 0.000000\n",
      "Epoch: 92 \tTraining Loss: 0.419459 \tR2: 0.000000\n",
      "Epoch: 93 \tTraining Loss: 0.430636 \tR2: 0.000000\n",
      "Epoch: 94 \tTraining Loss: 0.438318 \tR2: 0.000000\n",
      "Epoch: 95 \tTraining Loss: 0.438511 \tR2: 0.000000\n",
      "Epoch: 96 \tTraining Loss: 0.419737 \tR2: 0.000000\n",
      "Epoch: 97 \tTraining Loss: 0.460186 \tR2: 0.000000\n",
      "Epoch: 98 \tTraining Loss: 0.406972 \tR2: 0.000000\n",
      "Epoch: 99 \tTraining Loss: 0.437057 \tR2: 0.000000\n",
      "Epoch: 100 \tTraining Loss: 0.406338 \tR2: 0.070701\n",
      "Epoch: 101 \tTraining Loss: 0.424134 \tR2: 0.070701\n",
      "Epoch: 102 \tTraining Loss: 0.423698 \tR2: 0.070701\n",
      "Epoch: 103 \tTraining Loss: 0.400450 \tR2: 0.070701\n",
      "Epoch: 104 \tTraining Loss: 0.446374 \tR2: 0.070701\n",
      "Epoch: 105 \tTraining Loss: 0.427433 \tR2: 0.070701\n",
      "Epoch: 106 \tTraining Loss: 0.428871 \tR2: 0.070701\n",
      "Epoch: 107 \tTraining Loss: 0.435705 \tR2: 0.070701\n",
      "Epoch: 108 \tTraining Loss: 0.430944 \tR2: 0.070701\n",
      "Epoch: 109 \tTraining Loss: 0.438410 \tR2: 0.070701\n",
      "Epoch: 110 \tTraining Loss: 0.421434 \tR2: 0.070701\n",
      "Epoch: 111 \tTraining Loss: 0.413393 \tR2: 0.070701\n",
      "Epoch: 112 \tTraining Loss: 0.428710 \tR2: 0.070701\n",
      "Epoch: 113 \tTraining Loss: 0.401354 \tR2: 0.070701\n",
      "Epoch: 114 \tTraining Loss: 0.419815 \tR2: 0.070701\n",
      "Epoch: 115 \tTraining Loss: 0.436976 \tR2: 0.070701\n",
      "Epoch: 116 \tTraining Loss: 0.425557 \tR2: 0.070701\n",
      "Epoch: 117 \tTraining Loss: 0.431972 \tR2: 0.070701\n",
      "Epoch: 118 \tTraining Loss: 0.432008 \tR2: 0.070701\n",
      "Epoch: 119 \tTraining Loss: 0.416472 \tR2: 0.070701\n",
      "Epoch: 120 \tTraining Loss: 0.420801 \tR2: 0.070701\n",
      "Epoch: 121 \tTraining Loss: 0.411816 \tR2: 0.070701\n",
      "Epoch: 122 \tTraining Loss: 0.425610 \tR2: 0.070701\n",
      "Epoch: 123 \tTraining Loss: 0.419122 \tR2: 0.070701\n",
      "Epoch: 124 \tTraining Loss: 0.415594 \tR2: 0.070701\n",
      "Epoch: 125 \tTraining Loss: 0.407585 \tR2: 0.070701\n",
      "Epoch: 126 \tTraining Loss: 0.400859 \tR2: 0.070701\n",
      "Epoch: 127 \tTraining Loss: 0.389895 \tR2: 0.070701\n",
      "Epoch: 128 \tTraining Loss: 0.407192 \tR2: 0.070701\n",
      "Epoch: 129 \tTraining Loss: 0.402875 \tR2: 0.070701\n",
      "Epoch: 130 \tTraining Loss: 0.416738 \tR2: 0.070701\n",
      "Epoch: 131 \tTraining Loss: 0.384411 \tR2: 0.070701\n",
      "Epoch: 132 \tTraining Loss: 0.404677 \tR2: 0.070701\n",
      "Epoch: 133 \tTraining Loss: 0.390487 \tR2: 0.070701\n",
      "Epoch: 134 \tTraining Loss: 0.426562 \tR2: 0.070701\n",
      "Epoch: 135 \tTraining Loss: 0.443845 \tR2: 0.070701\n",
      "Epoch: 136 \tTraining Loss: 0.403937 \tR2: 0.070701\n",
      "Epoch: 137 \tTraining Loss: 0.395600 \tR2: 0.070701\n",
      "Epoch: 138 \tTraining Loss: 0.377904 \tR2: 0.070701\n",
      "Epoch: 139 \tTraining Loss: 0.378131 \tR2: 0.070701\n",
      "Epoch: 140 \tTraining Loss: 0.404260 \tR2: 0.070701\n",
      "Epoch: 141 \tTraining Loss: 0.408434 \tR2: 0.070701\n",
      "Epoch: 142 \tTraining Loss: 0.403337 \tR2: 0.070701\n",
      "Epoch: 143 \tTraining Loss: 0.472608 \tR2: 0.070701\n",
      "Epoch: 144 \tTraining Loss: 0.378173 \tR2: 0.070701\n",
      "Epoch: 145 \tTraining Loss: 0.402553 \tR2: 0.070701\n",
      "Epoch: 146 \tTraining Loss: 0.381740 \tR2: 0.070701\n",
      "Epoch: 147 \tTraining Loss: 0.415988 \tR2: 0.070701\n",
      "Epoch: 148 \tTraining Loss: 0.398692 \tR2: 0.070701\n",
      "Epoch: 149 \tTraining Loss: 0.402837 \tR2: 0.070701\n",
      "Epoch: 150 \tTraining Loss: 0.380414 \tR2: 0.070701\n",
      "Epoch: 151 \tTraining Loss: 0.410045 \tR2: 0.070701\n",
      "Epoch: 152 \tTraining Loss: 0.419999 \tR2: 0.070701\n",
      "Epoch: 153 \tTraining Loss: 0.376425 \tR2: 0.070701\n",
      "Epoch: 154 \tTraining Loss: 0.390177 \tR2: 0.070701\n",
      "Epoch: 155 \tTraining Loss: 0.394163 \tR2: 0.070701\n",
      "Epoch: 156 \tTraining Loss: 0.386823 \tR2: 0.070701\n",
      "Epoch: 157 \tTraining Loss: 0.403355 \tR2: 0.070701\n",
      "Epoch: 158 \tTraining Loss: 0.374886 \tR2: 0.070701\n",
      "Epoch: 159 \tTraining Loss: 0.399975 \tR2: 0.070701\n",
      "Epoch: 160 \tTraining Loss: 0.403140 \tR2: 0.070701\n",
      "Epoch: 161 \tTraining Loss: 0.373130 \tR2: 0.070701\n",
      "Epoch: 162 \tTraining Loss: 0.378363 \tR2: 0.070701\n",
      "Epoch: 163 \tTraining Loss: 0.376923 \tR2: 0.070701\n",
      "Epoch: 164 \tTraining Loss: 0.377909 \tR2: 0.070701\n",
      "Epoch: 165 \tTraining Loss: 0.378724 \tR2: 0.070701\n",
      "Epoch: 166 \tTraining Loss: 0.383857 \tR2: 0.070701\n",
      "Epoch: 167 \tTraining Loss: 0.367309 \tR2: 0.070701\n",
      "Epoch: 168 \tTraining Loss: 0.365360 \tR2: 0.070701\n",
      "Epoch: 169 \tTraining Loss: 0.388532 \tR2: 0.070701\n",
      "Epoch: 170 \tTraining Loss: 0.367222 \tR2: 0.070701\n",
      "Epoch: 171 \tTraining Loss: 0.378245 \tR2: 0.070701\n",
      "Epoch: 172 \tTraining Loss: 0.386568 \tR2: 0.070701\n",
      "Epoch: 173 \tTraining Loss: 0.413919 \tR2: 0.070701\n",
      "Epoch: 174 \tTraining Loss: 0.388872 \tR2: 0.070701\n",
      "Epoch: 175 \tTraining Loss: 0.386251 \tR2: 0.070701\n",
      "Epoch: 176 \tTraining Loss: 0.345736 \tR2: 0.070701\n",
      "Epoch: 177 \tTraining Loss: 0.357321 \tR2: 0.070701\n",
      "Epoch: 178 \tTraining Loss: 0.379371 \tR2: 0.070701\n",
      "Epoch: 179 \tTraining Loss: 0.387205 \tR2: 0.070701\n",
      "Epoch: 180 \tTraining Loss: 0.374504 \tR2: 0.070701\n",
      "Epoch: 181 \tTraining Loss: 0.390842 \tR2: 0.070701\n",
      "Epoch: 182 \tTraining Loss: 0.362578 \tR2: 0.070701\n",
      "Epoch: 183 \tTraining Loss: 0.363630 \tR2: 0.070701\n",
      "Epoch: 184 \tTraining Loss: 0.371961 \tR2: 0.070701\n",
      "Epoch: 185 \tTraining Loss: 0.343863 \tR2: 0.070701\n",
      "Epoch: 186 \tTraining Loss: 0.371363 \tR2: 0.070701\n",
      "Epoch: 187 \tTraining Loss: 0.354653 \tR2: 0.070701\n",
      "Epoch: 188 \tTraining Loss: 0.410780 \tR2: 0.070701\n",
      "Epoch: 189 \tTraining Loss: 0.374057 \tR2: 0.070701\n",
      "Epoch: 190 \tTraining Loss: 0.375267 \tR2: 0.070701\n",
      "Epoch: 191 \tTraining Loss: 0.366551 \tR2: 0.070701\n",
      "Epoch: 192 \tTraining Loss: 0.403248 \tR2: 0.070701\n",
      "Epoch: 193 \tTraining Loss: 0.387229 \tR2: 0.070701\n",
      "Epoch: 194 \tTraining Loss: 0.357458 \tR2: 0.070701\n",
      "Epoch: 195 \tTraining Loss: 0.360050 \tR2: 0.070701\n",
      "Epoch: 196 \tTraining Loss: 0.369009 \tR2: 0.070701\n",
      "Epoch: 197 \tTraining Loss: 0.398664 \tR2: 0.070701\n",
      "Epoch: 198 \tTraining Loss: 0.380023 \tR2: 0.070701\n",
      "Epoch: 199 \tTraining Loss: 0.376577 \tR2: 0.070701\n",
      "Epoch: 200 \tTraining Loss: 0.371316 \tR2: 0.327387\n",
      "Epoch: 201 \tTraining Loss: 0.354787 \tR2: 0.327387\n",
      "Epoch: 202 \tTraining Loss: 0.360851 \tR2: 0.327387\n",
      "Epoch: 203 \tTraining Loss: 0.344842 \tR2: 0.327387\n",
      "Epoch: 204 \tTraining Loss: 0.366823 \tR2: 0.327387\n",
      "Epoch: 205 \tTraining Loss: 0.353416 \tR2: 0.327387\n",
      "Epoch: 206 \tTraining Loss: 0.327830 \tR2: 0.327387\n",
      "Epoch: 207 \tTraining Loss: 0.346702 \tR2: 0.327387\n",
      "Epoch: 208 \tTraining Loss: 0.376708 \tR2: 0.327387\n",
      "Epoch: 209 \tTraining Loss: 0.367632 \tR2: 0.327387\n",
      "Epoch: 210 \tTraining Loss: 0.399477 \tR2: 0.327387\n",
      "Epoch: 211 \tTraining Loss: 0.386299 \tR2: 0.327387\n",
      "Epoch: 212 \tTraining Loss: 0.367253 \tR2: 0.327387\n",
      "Epoch: 213 \tTraining Loss: 0.360674 \tR2: 0.327387\n",
      "Epoch: 214 \tTraining Loss: 0.355145 \tR2: 0.327387\n",
      "Epoch: 215 \tTraining Loss: 0.379851 \tR2: 0.327387\n",
      "Epoch: 216 \tTraining Loss: 0.329017 \tR2: 0.327387\n",
      "Epoch: 217 \tTraining Loss: 0.360074 \tR2: 0.327387\n",
      "Epoch: 218 \tTraining Loss: 0.358344 \tR2: 0.327387\n",
      "Epoch: 219 \tTraining Loss: 0.352284 \tR2: 0.327387\n",
      "Epoch: 220 \tTraining Loss: 0.367511 \tR2: 0.327387\n",
      "Epoch: 221 \tTraining Loss: 0.365265 \tR2: 0.327387\n",
      "Epoch: 222 \tTraining Loss: 0.335900 \tR2: 0.327387\n",
      "Epoch: 223 \tTraining Loss: 0.340853 \tR2: 0.327387\n",
      "Epoch: 224 \tTraining Loss: 0.390589 \tR2: 0.327387\n",
      "Epoch: 225 \tTraining Loss: 0.356350 \tR2: 0.327387\n",
      "Epoch: 226 \tTraining Loss: 0.372801 \tR2: 0.327387\n",
      "Epoch: 227 \tTraining Loss: 0.340805 \tR2: 0.327387\n",
      "Epoch: 228 \tTraining Loss: 0.337611 \tR2: 0.327387\n",
      "Epoch: 229 \tTraining Loss: 0.349436 \tR2: 0.327387\n",
      "Epoch: 230 \tTraining Loss: 0.347703 \tR2: 0.327387\n",
      "Epoch: 231 \tTraining Loss: 0.358330 \tR2: 0.327387\n",
      "Epoch: 232 \tTraining Loss: 0.329749 \tR2: 0.327387\n",
      "Epoch: 233 \tTraining Loss: 0.343072 \tR2: 0.327387\n",
      "Epoch: 234 \tTraining Loss: 0.306151 \tR2: 0.327387\n",
      "Epoch: 235 \tTraining Loss: 0.335367 \tR2: 0.327387\n",
      "Epoch: 236 \tTraining Loss: 0.347264 \tR2: 0.327387\n",
      "Epoch: 237 \tTraining Loss: 0.357070 \tR2: 0.327387\n",
      "Epoch: 238 \tTraining Loss: 0.332088 \tR2: 0.327387\n",
      "Epoch: 239 \tTraining Loss: 0.328033 \tR2: 0.327387\n",
      "Epoch: 240 \tTraining Loss: 0.331495 \tR2: 0.327387\n",
      "Epoch: 241 \tTraining Loss: 0.337037 \tR2: 0.327387\n",
      "Epoch: 242 \tTraining Loss: 0.393367 \tR2: 0.327387\n",
      "Epoch: 243 \tTraining Loss: 0.358764 \tR2: 0.327387\n",
      "Epoch: 244 \tTraining Loss: 0.313154 \tR2: 0.327387\n",
      "Epoch: 245 \tTraining Loss: 0.362445 \tR2: 0.327387\n",
      "Epoch: 246 \tTraining Loss: 0.312953 \tR2: 0.327387\n",
      "Epoch: 247 \tTraining Loss: 0.329985 \tR2: 0.327387\n",
      "Epoch: 248 \tTraining Loss: 0.350385 \tR2: 0.327387\n",
      "Epoch: 249 \tTraining Loss: 0.372490 \tR2: 0.327387\n",
      "Epoch: 250 \tTraining Loss: 0.343797 \tR2: 0.327387\n",
      "Epoch: 251 \tTraining Loss: 0.339022 \tR2: 0.327387\n",
      "Epoch: 252 \tTraining Loss: 0.345326 \tR2: 0.327387\n",
      "Epoch: 253 \tTraining Loss: 0.328463 \tR2: 0.327387\n",
      "Epoch: 254 \tTraining Loss: 0.336848 \tR2: 0.327387\n",
      "Epoch: 255 \tTraining Loss: 0.337302 \tR2: 0.327387\n",
      "Epoch: 256 \tTraining Loss: 0.336912 \tR2: 0.327387\n",
      "Epoch: 257 \tTraining Loss: 0.330237 \tR2: 0.327387\n",
      "Epoch: 258 \tTraining Loss: 0.342161 \tR2: 0.327387\n",
      "Epoch: 259 \tTraining Loss: 0.333193 \tR2: 0.327387\n",
      "Epoch: 260 \tTraining Loss: 0.348551 \tR2: 0.327387\n",
      "Epoch: 261 \tTraining Loss: 0.324790 \tR2: 0.327387\n",
      "Epoch: 262 \tTraining Loss: 0.324249 \tR2: 0.327387\n",
      "Epoch: 263 \tTraining Loss: 0.344054 \tR2: 0.327387\n",
      "Epoch: 264 \tTraining Loss: 0.329911 \tR2: 0.327387\n",
      "Epoch: 265 \tTraining Loss: 0.322538 \tR2: 0.327387\n",
      "Epoch: 266 \tTraining Loss: 0.337749 \tR2: 0.327387\n",
      "Epoch: 267 \tTraining Loss: 0.380188 \tR2: 0.327387\n",
      "Epoch: 268 \tTraining Loss: 0.368307 \tR2: 0.327387\n",
      "Epoch: 269 \tTraining Loss: 0.367986 \tR2: 0.327387\n",
      "Epoch: 270 \tTraining Loss: 0.330492 \tR2: 0.327387\n",
      "Epoch: 271 \tTraining Loss: 0.341979 \tR2: 0.327387\n",
      "Epoch: 272 \tTraining Loss: 0.314805 \tR2: 0.327387\n",
      "Epoch: 273 \tTraining Loss: 0.307650 \tR2: 0.327387\n",
      "Epoch: 274 \tTraining Loss: 0.364318 \tR2: 0.327387\n",
      "Epoch: 275 \tTraining Loss: 0.327350 \tR2: 0.327387\n",
      "Epoch: 276 \tTraining Loss: 0.314396 \tR2: 0.327387\n",
      "Epoch: 277 \tTraining Loss: 0.343614 \tR2: 0.327387\n",
      "Epoch: 278 \tTraining Loss: 0.328966 \tR2: 0.327387\n",
      "Epoch: 279 \tTraining Loss: 0.334853 \tR2: 0.327387\n",
      "Epoch: 280 \tTraining Loss: 0.307142 \tR2: 0.327387\n",
      "Epoch: 281 \tTraining Loss: 0.327438 \tR2: 0.327387\n",
      "Epoch: 282 \tTraining Loss: 0.321369 \tR2: 0.327387\n",
      "Epoch: 283 \tTraining Loss: 0.380745 \tR2: 0.327387\n",
      "Epoch: 284 \tTraining Loss: 0.368336 \tR2: 0.327387\n",
      "Epoch: 285 \tTraining Loss: 0.333530 \tR2: 0.327387\n",
      "Epoch: 286 \tTraining Loss: 0.336725 \tR2: 0.327387\n",
      "Epoch: 287 \tTraining Loss: 0.351158 \tR2: 0.327387\n",
      "Epoch: 288 \tTraining Loss: 0.348438 \tR2: 0.327387\n",
      "Epoch: 289 \tTraining Loss: 0.318553 \tR2: 0.327387\n",
      "Epoch: 290 \tTraining Loss: 0.352534 \tR2: 0.327387\n",
      "Epoch: 291 \tTraining Loss: 0.317283 \tR2: 0.327387\n",
      "Epoch: 292 \tTraining Loss: 0.326368 \tR2: 0.327387\n",
      "Epoch: 293 \tTraining Loss: 0.312251 \tR2: 0.327387\n",
      "Epoch: 294 \tTraining Loss: 0.328612 \tR2: 0.327387\n",
      "Epoch: 295 \tTraining Loss: 0.330223 \tR2: 0.327387\n",
      "Epoch: 296 \tTraining Loss: 0.365860 \tR2: 0.327387\n",
      "Epoch: 297 \tTraining Loss: 0.333171 \tR2: 0.327387\n",
      "Epoch: 298 \tTraining Loss: 0.320782 \tR2: 0.327387\n",
      "Epoch: 299 \tTraining Loss: 0.322760 \tR2: 0.327387\n",
      "Epoch: 300 \tTraining Loss: 0.325824 \tR2: 0.150210\n",
      "Epoch: 301 \tTraining Loss: 0.319779 \tR2: 0.150210\n",
      "Epoch: 302 \tTraining Loss: 0.324244 \tR2: 0.150210\n",
      "Epoch: 303 \tTraining Loss: 0.378775 \tR2: 0.150210\n",
      "Epoch: 304 \tTraining Loss: 0.382504 \tR2: 0.150210\n",
      "Epoch: 305 \tTraining Loss: 0.333842 \tR2: 0.150210\n",
      "Epoch: 306 \tTraining Loss: 0.335317 \tR2: 0.150210\n",
      "Epoch: 307 \tTraining Loss: 0.342309 \tR2: 0.150210\n",
      "Epoch: 308 \tTraining Loss: 0.318361 \tR2: 0.150210\n",
      "Epoch: 309 \tTraining Loss: 0.316844 \tR2: 0.150210\n",
      "Epoch: 310 \tTraining Loss: 0.299666 \tR2: 0.150210\n",
      "Epoch: 311 \tTraining Loss: 0.326917 \tR2: 0.150210\n",
      "Epoch: 312 \tTraining Loss: 0.304188 \tR2: 0.150210\n",
      "Epoch: 313 \tTraining Loss: 0.309324 \tR2: 0.150210\n",
      "Epoch: 314 \tTraining Loss: 0.325233 \tR2: 0.150210\n",
      "Epoch: 315 \tTraining Loss: 0.325977 \tR2: 0.150210\n",
      "Epoch: 316 \tTraining Loss: 0.336078 \tR2: 0.150210\n",
      "Epoch: 317 \tTraining Loss: 0.310087 \tR2: 0.150210\n",
      "Epoch: 318 \tTraining Loss: 0.355803 \tR2: 0.150210\n",
      "Epoch: 319 \tTraining Loss: 0.340842 \tR2: 0.150210\n",
      "Epoch: 320 \tTraining Loss: 0.317133 \tR2: 0.150210\n",
      "Epoch: 321 \tTraining Loss: 0.308464 \tR2: 0.150210\n",
      "Epoch: 322 \tTraining Loss: 0.313670 \tR2: 0.150210\n",
      "Epoch: 323 \tTraining Loss: 0.314146 \tR2: 0.150210\n",
      "Epoch: 324 \tTraining Loss: 0.298747 \tR2: 0.150210\n",
      "Epoch: 325 \tTraining Loss: 0.314553 \tR2: 0.150210\n",
      "Epoch: 326 \tTraining Loss: 0.323632 \tR2: 0.150210\n",
      "Epoch: 327 \tTraining Loss: 0.335278 \tR2: 0.150210\n",
      "Epoch: 328 \tTraining Loss: 0.336825 \tR2: 0.150210\n",
      "Epoch: 329 \tTraining Loss: 0.349956 \tR2: 0.150210\n",
      "Epoch: 330 \tTraining Loss: 0.322903 \tR2: 0.150210\n",
      "Epoch: 331 \tTraining Loss: 0.331615 \tR2: 0.150210\n",
      "Epoch: 332 \tTraining Loss: 0.320915 \tR2: 0.150210\n",
      "Epoch: 333 \tTraining Loss: 0.299228 \tR2: 0.150210\n",
      "Epoch: 334 \tTraining Loss: 0.308940 \tR2: 0.150210\n",
      "Epoch: 335 \tTraining Loss: 0.315007 \tR2: 0.150210\n",
      "Epoch: 336 \tTraining Loss: 0.334494 \tR2: 0.150210\n",
      "Epoch: 337 \tTraining Loss: 0.344202 \tR2: 0.150210\n",
      "Epoch: 338 \tTraining Loss: 0.310362 \tR2: 0.150210\n",
      "Epoch: 339 \tTraining Loss: 0.371755 \tR2: 0.150210\n",
      "Epoch: 340 \tTraining Loss: 0.302979 \tR2: 0.150210\n",
      "Epoch: 341 \tTraining Loss: 0.322964 \tR2: 0.150210\n",
      "Epoch: 342 \tTraining Loss: 0.337625 \tR2: 0.150210\n",
      "Epoch: 343 \tTraining Loss: 0.327187 \tR2: 0.150210\n",
      "Epoch: 344 \tTraining Loss: 0.306867 \tR2: 0.150210\n",
      "Epoch: 345 \tTraining Loss: 0.306099 \tR2: 0.150210\n",
      "Epoch: 346 \tTraining Loss: 0.326777 \tR2: 0.150210\n",
      "Epoch: 347 \tTraining Loss: 0.309760 \tR2: 0.150210\n",
      "Epoch: 348 \tTraining Loss: 0.304351 \tR2: 0.150210\n",
      "Epoch: 349 \tTraining Loss: 0.312291 \tR2: 0.150210\n",
      "Epoch: 350 \tTraining Loss: 0.318383 \tR2: 0.150210\n",
      "Epoch: 351 \tTraining Loss: 0.323717 \tR2: 0.150210\n",
      "Epoch: 352 \tTraining Loss: 0.319812 \tR2: 0.150210\n",
      "Epoch: 353 \tTraining Loss: 0.315246 \tR2: 0.150210\n",
      "Epoch: 354 \tTraining Loss: 0.332699 \tR2: 0.150210\n",
      "Epoch: 355 \tTraining Loss: 0.314328 \tR2: 0.150210\n",
      "Epoch: 356 \tTraining Loss: 0.334718 \tR2: 0.150210\n",
      "Epoch: 357 \tTraining Loss: 0.304456 \tR2: 0.150210\n",
      "Epoch: 358 \tTraining Loss: 0.366447 \tR2: 0.150210\n",
      "Epoch: 359 \tTraining Loss: 0.320923 \tR2: 0.150210\n",
      "Epoch: 360 \tTraining Loss: 0.319409 \tR2: 0.150210\n",
      "Epoch: 361 \tTraining Loss: 0.319056 \tR2: 0.150210\n",
      "Epoch: 362 \tTraining Loss: 0.409248 \tR2: 0.150210\n",
      "Epoch: 363 \tTraining Loss: 0.374262 \tR2: 0.150210\n",
      "Epoch: 364 \tTraining Loss: 0.335179 \tR2: 0.150210\n",
      "Epoch: 365 \tTraining Loss: 0.353082 \tR2: 0.150210\n",
      "Epoch: 366 \tTraining Loss: 0.324402 \tR2: 0.150210\n",
      "Epoch: 367 \tTraining Loss: 0.339082 \tR2: 0.150210\n",
      "Epoch: 368 \tTraining Loss: 0.318532 \tR2: 0.150210\n",
      "Epoch: 369 \tTraining Loss: 0.320567 \tR2: 0.150210\n",
      "Epoch: 370 \tTraining Loss: 0.311532 \tR2: 0.150210\n",
      "Epoch: 371 \tTraining Loss: 0.315897 \tR2: 0.150210\n",
      "Epoch: 372 \tTraining Loss: 0.352903 \tR2: 0.150210\n",
      "Epoch: 373 \tTraining Loss: 0.359921 \tR2: 0.150210\n",
      "Epoch: 374 \tTraining Loss: 0.338490 \tR2: 0.150210\n",
      "Epoch: 375 \tTraining Loss: 0.321283 \tR2: 0.150210\n",
      "Epoch: 376 \tTraining Loss: 0.295092 \tR2: 0.150210\n",
      "Epoch: 377 \tTraining Loss: 0.296663 \tR2: 0.150210\n",
      "Epoch: 378 \tTraining Loss: 0.297292 \tR2: 0.150210\n",
      "Epoch: 379 \tTraining Loss: 0.316649 \tR2: 0.150210\n",
      "Epoch: 380 \tTraining Loss: 0.277160 \tR2: 0.150210\n",
      "Epoch: 381 \tTraining Loss: 0.324416 \tR2: 0.150210\n",
      "Epoch: 382 \tTraining Loss: 0.321039 \tR2: 0.150210\n",
      "Epoch: 383 \tTraining Loss: 0.335606 \tR2: 0.150210\n",
      "Epoch: 384 \tTraining Loss: 0.307791 \tR2: 0.150210\n",
      "Epoch: 385 \tTraining Loss: 0.303059 \tR2: 0.150210\n",
      "Epoch: 386 \tTraining Loss: 0.321208 \tR2: 0.150210\n",
      "Epoch: 387 \tTraining Loss: 0.304770 \tR2: 0.150210\n",
      "Epoch: 388 \tTraining Loss: 0.302344 \tR2: 0.150210\n",
      "Epoch: 389 \tTraining Loss: 0.307444 \tR2: 0.150210\n",
      "Epoch: 390 \tTraining Loss: 0.300153 \tR2: 0.150210\n",
      "Epoch: 391 \tTraining Loss: 0.300753 \tR2: 0.150210\n",
      "Epoch: 392 \tTraining Loss: 0.297353 \tR2: 0.150210\n",
      "Epoch: 393 \tTraining Loss: 0.312637 \tR2: 0.150210\n",
      "Epoch: 394 \tTraining Loss: 0.289731 \tR2: 0.150210\n",
      "Epoch: 395 \tTraining Loss: 0.294128 \tR2: 0.150210\n",
      "Epoch: 396 \tTraining Loss: 0.301513 \tR2: 0.150210\n",
      "Epoch: 397 \tTraining Loss: 0.324866 \tR2: 0.150210\n",
      "Epoch: 398 \tTraining Loss: 0.327854 \tR2: 0.150210\n",
      "Epoch: 399 \tTraining Loss: 0.311071 \tR2: 0.150210\n",
      "Epoch: 400 \tTraining Loss: 0.292986 \tR2: 0.601788\n",
      "Epoch: 401 \tTraining Loss: 0.316364 \tR2: 0.601788\n",
      "Epoch: 402 \tTraining Loss: 0.292499 \tR2: 0.601788\n",
      "Epoch: 403 \tTraining Loss: 0.298903 \tR2: 0.601788\n",
      "Epoch: 404 \tTraining Loss: 0.324203 \tR2: 0.601788\n",
      "Epoch: 405 \tTraining Loss: 0.288423 \tR2: 0.601788\n",
      "Epoch: 406 \tTraining Loss: 0.278904 \tR2: 0.601788\n",
      "Epoch: 407 \tTraining Loss: 0.300815 \tR2: 0.601788\n",
      "Epoch: 408 \tTraining Loss: 0.314193 \tR2: 0.601788\n",
      "Epoch: 409 \tTraining Loss: 0.313157 \tR2: 0.601788\n",
      "Epoch: 410 \tTraining Loss: 0.310659 \tR2: 0.601788\n",
      "Epoch: 411 \tTraining Loss: 0.329926 \tR2: 0.601788\n",
      "Epoch: 412 \tTraining Loss: 0.305780 \tR2: 0.601788\n",
      "Epoch: 413 \tTraining Loss: 0.319369 \tR2: 0.601788\n",
      "Epoch: 414 \tTraining Loss: 0.312659 \tR2: 0.601788\n",
      "Epoch: 415 \tTraining Loss: 0.289542 \tR2: 0.601788\n",
      "Epoch: 416 \tTraining Loss: 0.316285 \tR2: 0.601788\n",
      "Epoch: 417 \tTraining Loss: 0.342612 \tR2: 0.601788\n",
      "Epoch: 418 \tTraining Loss: 0.295533 \tR2: 0.601788\n",
      "Epoch: 419 \tTraining Loss: 0.288757 \tR2: 0.601788\n",
      "Epoch: 420 \tTraining Loss: 0.297302 \tR2: 0.601788\n",
      "Epoch: 421 \tTraining Loss: 0.303646 \tR2: 0.601788\n",
      "Epoch: 422 \tTraining Loss: 0.290103 \tR2: 0.601788\n",
      "Epoch: 423 \tTraining Loss: 0.318283 \tR2: 0.601788\n",
      "Epoch: 424 \tTraining Loss: 0.298009 \tR2: 0.601788\n",
      "Epoch: 425 \tTraining Loss: 0.313170 \tR2: 0.601788\n",
      "Epoch: 426 \tTraining Loss: 0.397085 \tR2: 0.601788\n",
      "Epoch: 427 \tTraining Loss: 0.308495 \tR2: 0.601788\n",
      "Epoch: 428 \tTraining Loss: 0.318872 \tR2: 0.601788\n",
      "Epoch: 429 \tTraining Loss: 0.315016 \tR2: 0.601788\n",
      "Epoch: 430 \tTraining Loss: 0.288446 \tR2: 0.601788\n",
      "Epoch: 431 \tTraining Loss: 0.322117 \tR2: 0.601788\n",
      "Epoch: 432 \tTraining Loss: 0.300465 \tR2: 0.601788\n",
      "Epoch: 433 \tTraining Loss: 0.288012 \tR2: 0.601788\n",
      "Epoch: 434 \tTraining Loss: 0.296064 \tR2: 0.601788\n",
      "Epoch: 435 \tTraining Loss: 0.315025 \tR2: 0.601788\n",
      "Epoch: 436 \tTraining Loss: 0.307278 \tR2: 0.601788\n",
      "Epoch: 437 \tTraining Loss: 0.289088 \tR2: 0.601788\n",
      "Epoch: 438 \tTraining Loss: 0.321085 \tR2: 0.601788\n",
      "Epoch: 439 \tTraining Loss: 0.319146 \tR2: 0.601788\n",
      "Epoch: 440 \tTraining Loss: 0.282229 \tR2: 0.601788\n",
      "Epoch: 441 \tTraining Loss: 0.276617 \tR2: 0.601788\n",
      "Epoch: 442 \tTraining Loss: 0.334430 \tR2: 0.601788\n",
      "Epoch: 443 \tTraining Loss: 0.300550 \tR2: 0.601788\n",
      "Epoch: 444 \tTraining Loss: 0.289277 \tR2: 0.601788\n",
      "Epoch: 445 \tTraining Loss: 0.299562 \tR2: 0.601788\n",
      "Epoch: 446 \tTraining Loss: 0.284150 \tR2: 0.601788\n",
      "Epoch: 447 \tTraining Loss: 0.295971 \tR2: 0.601788\n",
      "Epoch: 448 \tTraining Loss: 0.332048 \tR2: 0.601788\n",
      "Epoch: 449 \tTraining Loss: 0.308764 \tR2: 0.601788\n",
      "Epoch: 450 \tTraining Loss: 0.319126 \tR2: 0.601788\n",
      "Epoch: 451 \tTraining Loss: 0.324421 \tR2: 0.601788\n",
      "Epoch: 452 \tTraining Loss: 0.297133 \tR2: 0.601788\n",
      "Epoch: 453 \tTraining Loss: 0.292979 \tR2: 0.601788\n",
      "Epoch: 454 \tTraining Loss: 0.311140 \tR2: 0.601788\n",
      "Epoch: 455 \tTraining Loss: 0.308153 \tR2: 0.601788\n",
      "Epoch: 456 \tTraining Loss: 0.294222 \tR2: 0.601788\n",
      "Epoch: 457 \tTraining Loss: 0.310890 \tR2: 0.601788\n",
      "Epoch: 458 \tTraining Loss: 0.303811 \tR2: 0.601788\n",
      "Epoch: 459 \tTraining Loss: 0.312558 \tR2: 0.601788\n",
      "Epoch: 460 \tTraining Loss: 0.300367 \tR2: 0.601788\n",
      "Epoch: 461 \tTraining Loss: 0.331689 \tR2: 0.601788\n",
      "Epoch: 462 \tTraining Loss: 0.313227 \tR2: 0.601788\n",
      "Epoch: 463 \tTraining Loss: 0.329450 \tR2: 0.601788\n",
      "Epoch: 464 \tTraining Loss: 0.323689 \tR2: 0.601788\n",
      "Epoch: 465 \tTraining Loss: 0.314691 \tR2: 0.601788\n",
      "Epoch: 466 \tTraining Loss: 0.286586 \tR2: 0.601788\n",
      "Epoch: 467 \tTraining Loss: 0.294296 \tR2: 0.601788\n",
      "Epoch: 468 \tTraining Loss: 0.301142 \tR2: 0.601788\n",
      "Epoch: 469 \tTraining Loss: 0.305423 \tR2: 0.601788\n",
      "Epoch: 470 \tTraining Loss: 0.298835 \tR2: 0.601788\n",
      "Epoch: 471 \tTraining Loss: 0.355098 \tR2: 0.601788\n",
      "Epoch: 472 \tTraining Loss: 0.354794 \tR2: 0.601788\n",
      "Epoch: 473 \tTraining Loss: 0.304405 \tR2: 0.601788\n",
      "Epoch: 474 \tTraining Loss: 0.295886 \tR2: 0.601788\n",
      "Epoch: 475 \tTraining Loss: 0.295237 \tR2: 0.601788\n",
      "Epoch: 476 \tTraining Loss: 0.307604 \tR2: 0.601788\n",
      "Epoch: 477 \tTraining Loss: 0.297332 \tR2: 0.601788\n",
      "Epoch: 478 \tTraining Loss: 0.298053 \tR2: 0.601788\n",
      "Epoch: 479 \tTraining Loss: 0.283770 \tR2: 0.601788\n",
      "Epoch: 480 \tTraining Loss: 0.273304 \tR2: 0.601788\n",
      "Epoch: 481 \tTraining Loss: 0.283107 \tR2: 0.601788\n",
      "Epoch: 482 \tTraining Loss: 0.287762 \tR2: 0.601788\n",
      "Epoch: 483 \tTraining Loss: 0.319590 \tR2: 0.601788\n",
      "Epoch: 484 \tTraining Loss: 0.309077 \tR2: 0.601788\n",
      "Epoch: 485 \tTraining Loss: 0.275554 \tR2: 0.601788\n",
      "Epoch: 486 \tTraining Loss: 0.287021 \tR2: 0.601788\n",
      "Epoch: 487 \tTraining Loss: 0.325433 \tR2: 0.601788\n",
      "Epoch: 488 \tTraining Loss: 0.289997 \tR2: 0.601788\n",
      "Epoch: 489 \tTraining Loss: 0.294542 \tR2: 0.601788\n",
      "Epoch: 490 \tTraining Loss: 0.292555 \tR2: 0.601788\n",
      "Epoch: 491 \tTraining Loss: 0.281348 \tR2: 0.601788\n",
      "Epoch: 492 \tTraining Loss: 0.272806 \tR2: 0.601788\n",
      "Epoch: 493 \tTraining Loss: 0.309472 \tR2: 0.601788\n",
      "Epoch: 494 \tTraining Loss: 0.287431 \tR2: 0.601788\n",
      "Epoch: 495 \tTraining Loss: 0.328212 \tR2: 0.601788\n",
      "Epoch: 496 \tTraining Loss: 0.293950 \tR2: 0.601788\n",
      "Epoch: 497 \tTraining Loss: 0.287383 \tR2: 0.601788\n",
      "Epoch: 498 \tTraining Loss: 0.335205 \tR2: 0.601788\n",
      "Epoch: 499 \tTraining Loss: 0.286941 \tR2: 0.601788\n",
      "Epoch: 500 \tTraining Loss: 0.278265 \tR2: 0.643969\n",
      "Epoch: 501 \tTraining Loss: 0.286592 \tR2: 0.643969\n",
      "Epoch: 502 \tTraining Loss: 0.299650 \tR2: 0.643969\n",
      "Epoch: 503 \tTraining Loss: 0.287436 \tR2: 0.643969\n",
      "Epoch: 504 \tTraining Loss: 0.292503 \tR2: 0.643969\n",
      "Epoch: 505 \tTraining Loss: 0.287486 \tR2: 0.643969\n",
      "Epoch: 506 \tTraining Loss: 0.308787 \tR2: 0.643969\n",
      "Epoch: 507 \tTraining Loss: 0.287304 \tR2: 0.643969\n",
      "Epoch: 508 \tTraining Loss: 0.296023 \tR2: 0.643969\n",
      "Epoch: 509 \tTraining Loss: 0.303676 \tR2: 0.643969\n",
      "Epoch: 510 \tTraining Loss: 0.294749 \tR2: 0.643969\n",
      "Epoch: 511 \tTraining Loss: 0.297311 \tR2: 0.643969\n",
      "Epoch: 512 \tTraining Loss: 0.273136 \tR2: 0.643969\n",
      "Epoch: 513 \tTraining Loss: 0.274622 \tR2: 0.643969\n",
      "Epoch: 514 \tTraining Loss: 0.311959 \tR2: 0.643969\n",
      "Epoch: 515 \tTraining Loss: 0.295848 \tR2: 0.643969\n",
      "Epoch: 516 \tTraining Loss: 0.285369 \tR2: 0.643969\n",
      "Epoch: 517 \tTraining Loss: 0.279382 \tR2: 0.643969\n",
      "Epoch: 518 \tTraining Loss: 0.285638 \tR2: 0.643969\n",
      "Epoch: 519 \tTraining Loss: 0.281573 \tR2: 0.643969\n",
      "Epoch: 520 \tTraining Loss: 0.320074 \tR2: 0.643969\n",
      "Epoch: 521 \tTraining Loss: 0.301729 \tR2: 0.643969\n",
      "Epoch: 522 \tTraining Loss: 0.292908 \tR2: 0.643969\n",
      "Epoch: 523 \tTraining Loss: 0.300824 \tR2: 0.643969\n",
      "Epoch: 524 \tTraining Loss: 0.286454 \tR2: 0.643969\n",
      "Epoch: 525 \tTraining Loss: 0.294232 \tR2: 0.643969\n",
      "Epoch: 526 \tTraining Loss: 0.287471 \tR2: 0.643969\n",
      "Epoch: 527 \tTraining Loss: 0.313207 \tR2: 0.643969\n",
      "Epoch: 528 \tTraining Loss: 0.297811 \tR2: 0.643969\n",
      "Epoch: 529 \tTraining Loss: 0.288615 \tR2: 0.643969\n",
      "Epoch: 530 \tTraining Loss: 0.320084 \tR2: 0.643969\n",
      "Epoch: 531 \tTraining Loss: 0.289866 \tR2: 0.643969\n",
      "Epoch: 532 \tTraining Loss: 0.304278 \tR2: 0.643969\n",
      "Epoch: 533 \tTraining Loss: 0.294002 \tR2: 0.643969\n",
      "Epoch: 534 \tTraining Loss: 0.292070 \tR2: 0.643969\n",
      "Epoch: 535 \tTraining Loss: 0.308619 \tR2: 0.643969\n",
      "Epoch: 536 \tTraining Loss: 0.291813 \tR2: 0.643969\n",
      "Epoch: 537 \tTraining Loss: 0.295786 \tR2: 0.643969\n",
      "Epoch: 538 \tTraining Loss: 0.296367 \tR2: 0.643969\n",
      "Epoch: 539 \tTraining Loss: 0.282459 \tR2: 0.643969\n",
      "Epoch: 540 \tTraining Loss: 0.281206 \tR2: 0.643969\n",
      "Epoch: 541 \tTraining Loss: 0.277222 \tR2: 0.643969\n",
      "Epoch: 542 \tTraining Loss: 0.288361 \tR2: 0.643969\n",
      "Epoch: 543 \tTraining Loss: 0.276457 \tR2: 0.643969\n",
      "Epoch: 544 \tTraining Loss: 0.271033 \tR2: 0.643969\n",
      "Epoch: 545 \tTraining Loss: 0.298385 \tR2: 0.643969\n",
      "Epoch: 546 \tTraining Loss: 0.317895 \tR2: 0.643969\n",
      "Epoch: 547 \tTraining Loss: 0.271837 \tR2: 0.643969\n",
      "Epoch: 548 \tTraining Loss: 0.269905 \tR2: 0.643969\n",
      "Epoch: 549 \tTraining Loss: 0.289053 \tR2: 0.643969\n",
      "Epoch: 550 \tTraining Loss: 0.289208 \tR2: 0.643969\n",
      "Epoch: 551 \tTraining Loss: 0.320054 \tR2: 0.643969\n",
      "Epoch: 552 \tTraining Loss: 0.296135 \tR2: 0.643969\n",
      "Epoch: 553 \tTraining Loss: 0.291625 \tR2: 0.643969\n",
      "Epoch: 554 \tTraining Loss: 0.261758 \tR2: 0.643969\n",
      "Epoch: 555 \tTraining Loss: 0.306877 \tR2: 0.643969\n",
      "Epoch: 556 \tTraining Loss: 0.299006 \tR2: 0.643969\n",
      "Epoch: 557 \tTraining Loss: 0.281710 \tR2: 0.643969\n",
      "Epoch: 558 \tTraining Loss: 0.292454 \tR2: 0.643969\n",
      "Epoch: 559 \tTraining Loss: 0.266633 \tR2: 0.643969\n",
      "Epoch: 560 \tTraining Loss: 0.281918 \tR2: 0.643969\n",
      "Epoch: 561 \tTraining Loss: 0.330459 \tR2: 0.643969\n",
      "Epoch: 562 \tTraining Loss: 0.306348 \tR2: 0.643969\n",
      "Epoch: 563 \tTraining Loss: 0.274874 \tR2: 0.643969\n",
      "Epoch: 564 \tTraining Loss: 0.270384 \tR2: 0.643969\n",
      "Epoch: 565 \tTraining Loss: 0.270368 \tR2: 0.643969\n",
      "Epoch: 566 \tTraining Loss: 0.285303 \tR2: 0.643969\n",
      "Epoch: 567 \tTraining Loss: 0.254002 \tR2: 0.643969\n",
      "Epoch: 568 \tTraining Loss: 0.289969 \tR2: 0.643969\n",
      "Epoch: 569 \tTraining Loss: 0.271941 \tR2: 0.643969\n",
      "Epoch: 570 \tTraining Loss: 0.284779 \tR2: 0.643969\n",
      "Epoch: 571 \tTraining Loss: 0.277255 \tR2: 0.643969\n",
      "Epoch: 572 \tTraining Loss: 0.303108 \tR2: 0.643969\n",
      "Epoch: 573 \tTraining Loss: 0.282983 \tR2: 0.643969\n",
      "Epoch: 574 \tTraining Loss: 0.303035 \tR2: 0.643969\n",
      "Epoch: 575 \tTraining Loss: 0.286569 \tR2: 0.643969\n",
      "Epoch: 576 \tTraining Loss: 0.313169 \tR2: 0.643969\n",
      "Epoch: 577 \tTraining Loss: 0.281239 \tR2: 0.643969\n",
      "Epoch: 578 \tTraining Loss: 0.294084 \tR2: 0.643969\n",
      "Epoch: 579 \tTraining Loss: 0.270974 \tR2: 0.643969\n",
      "Epoch: 580 \tTraining Loss: 0.269536 \tR2: 0.643969\n",
      "Epoch: 581 \tTraining Loss: 0.298585 \tR2: 0.643969\n",
      "Epoch: 582 \tTraining Loss: 0.267130 \tR2: 0.643969\n",
      "Epoch: 583 \tTraining Loss: 0.317096 \tR2: 0.643969\n",
      "Epoch: 584 \tTraining Loss: 0.302229 \tR2: 0.643969\n",
      "Epoch: 585 \tTraining Loss: 0.304833 \tR2: 0.643969\n",
      "Epoch: 586 \tTraining Loss: 0.276258 \tR2: 0.643969\n",
      "Epoch: 587 \tTraining Loss: 0.297669 \tR2: 0.643969\n",
      "Epoch: 588 \tTraining Loss: 0.281268 \tR2: 0.643969\n",
      "Epoch: 589 \tTraining Loss: 0.301849 \tR2: 0.643969\n",
      "Epoch: 590 \tTraining Loss: 0.298744 \tR2: 0.643969\n",
      "Epoch: 591 \tTraining Loss: 0.256835 \tR2: 0.643969\n",
      "Epoch: 592 \tTraining Loss: 0.282769 \tR2: 0.643969\n",
      "Epoch: 593 \tTraining Loss: 0.287909 \tR2: 0.643969\n",
      "Epoch: 594 \tTraining Loss: 0.268596 \tR2: 0.643969\n",
      "Epoch: 595 \tTraining Loss: 0.270892 \tR2: 0.643969\n",
      "Epoch: 596 \tTraining Loss: 0.290263 \tR2: 0.643969\n",
      "Epoch: 597 \tTraining Loss: 0.289096 \tR2: 0.643969\n",
      "Epoch: 598 \tTraining Loss: 0.271350 \tR2: 0.643969\n",
      "Epoch: 599 \tTraining Loss: 0.259804 \tR2: 0.643969\n",
      "Epoch: 600 \tTraining Loss: 0.270601 \tR2: 0.781525\n",
      "Epoch: 601 \tTraining Loss: 0.272279 \tR2: 0.781525\n",
      "Epoch: 602 \tTraining Loss: 0.291610 \tR2: 0.781525\n",
      "Epoch: 603 \tTraining Loss: 0.287902 \tR2: 0.781525\n",
      "Epoch: 604 \tTraining Loss: 0.297513 \tR2: 0.781525\n",
      "Epoch: 605 \tTraining Loss: 0.308027 \tR2: 0.781525\n",
      "Epoch: 606 \tTraining Loss: 0.280456 \tR2: 0.781525\n",
      "Epoch: 607 \tTraining Loss: 0.315142 \tR2: 0.781525\n",
      "Epoch: 608 \tTraining Loss: 0.311649 \tR2: 0.781525\n",
      "Epoch: 609 \tTraining Loss: 0.286103 \tR2: 0.781525\n",
      "Epoch: 610 \tTraining Loss: 0.325425 \tR2: 0.781525\n",
      "Epoch: 611 \tTraining Loss: 0.295330 \tR2: 0.781525\n",
      "Epoch: 612 \tTraining Loss: 0.278216 \tR2: 0.781525\n",
      "Epoch: 613 \tTraining Loss: 0.274381 \tR2: 0.781525\n",
      "Epoch: 614 \tTraining Loss: 0.268412 \tR2: 0.781525\n",
      "Epoch: 615 \tTraining Loss: 0.280213 \tR2: 0.781525\n",
      "Epoch: 616 \tTraining Loss: 0.298717 \tR2: 0.781525\n",
      "Epoch: 617 \tTraining Loss: 0.255923 \tR2: 0.781525\n",
      "Epoch: 618 \tTraining Loss: 0.294663 \tR2: 0.781525\n",
      "Epoch: 619 \tTraining Loss: 0.271740 \tR2: 0.781525\n",
      "Epoch: 620 \tTraining Loss: 0.278970 \tR2: 0.781525\n",
      "Epoch: 621 \tTraining Loss: 0.281519 \tR2: 0.781525\n",
      "Epoch: 622 \tTraining Loss: 0.280469 \tR2: 0.781525\n",
      "Epoch: 623 \tTraining Loss: 0.280600 \tR2: 0.781525\n",
      "Epoch: 624 \tTraining Loss: 0.311614 \tR2: 0.781525\n",
      "Epoch: 625 \tTraining Loss: 0.296123 \tR2: 0.781525\n",
      "Epoch: 626 \tTraining Loss: 0.294691 \tR2: 0.781525\n",
      "Epoch: 627 \tTraining Loss: 0.291787 \tR2: 0.781525\n",
      "Epoch: 628 \tTraining Loss: 0.278734 \tR2: 0.781525\n",
      "Epoch: 629 \tTraining Loss: 0.281753 \tR2: 0.781525\n",
      "Epoch: 630 \tTraining Loss: 0.285510 \tR2: 0.781525\n",
      "Epoch: 631 \tTraining Loss: 0.285104 \tR2: 0.781525\n",
      "Epoch: 632 \tTraining Loss: 0.299287 \tR2: 0.781525\n",
      "Epoch: 633 \tTraining Loss: 0.293051 \tR2: 0.781525\n",
      "Epoch: 634 \tTraining Loss: 0.292990 \tR2: 0.781525\n",
      "Epoch: 635 \tTraining Loss: 0.281765 \tR2: 0.781525\n",
      "Epoch: 636 \tTraining Loss: 0.280384 \tR2: 0.781525\n",
      "Epoch: 637 \tTraining Loss: 0.292819 \tR2: 0.781525\n",
      "Epoch: 638 \tTraining Loss: 0.275995 \tR2: 0.781525\n",
      "Epoch: 639 \tTraining Loss: 0.286989 \tR2: 0.781525\n",
      "Epoch: 640 \tTraining Loss: 0.291503 \tR2: 0.781525\n",
      "Epoch: 641 \tTraining Loss: 0.288833 \tR2: 0.781525\n",
      "Epoch: 642 \tTraining Loss: 0.254611 \tR2: 0.781525\n",
      "Epoch: 643 \tTraining Loss: 0.277904 \tR2: 0.781525\n",
      "Epoch: 644 \tTraining Loss: 0.276785 \tR2: 0.781525\n",
      "Epoch: 645 \tTraining Loss: 0.274791 \tR2: 0.781525\n",
      "Epoch: 646 \tTraining Loss: 0.273427 \tR2: 0.781525\n",
      "Epoch: 647 \tTraining Loss: 0.254099 \tR2: 0.781525\n",
      "Epoch: 648 \tTraining Loss: 0.284347 \tR2: 0.781525\n",
      "Epoch: 649 \tTraining Loss: 0.267356 \tR2: 0.781525\n",
      "Epoch: 650 \tTraining Loss: 0.274490 \tR2: 0.781525\n",
      "Epoch: 651 \tTraining Loss: 0.263096 \tR2: 0.781525\n",
      "Epoch: 652 \tTraining Loss: 0.281062 \tR2: 0.781525\n",
      "Epoch: 653 \tTraining Loss: 0.296755 \tR2: 0.781525\n",
      "Epoch: 654 \tTraining Loss: 0.258874 \tR2: 0.781525\n",
      "Epoch: 655 \tTraining Loss: 0.274004 \tR2: 0.781525\n",
      "Epoch: 656 \tTraining Loss: 0.283258 \tR2: 0.781525\n",
      "Epoch: 657 \tTraining Loss: 0.278077 \tR2: 0.781525\n",
      "Epoch: 658 \tTraining Loss: 0.278125 \tR2: 0.781525\n",
      "Epoch: 659 \tTraining Loss: 0.277741 \tR2: 0.781525\n",
      "Epoch: 660 \tTraining Loss: 0.282053 \tR2: 0.781525\n",
      "Epoch: 661 \tTraining Loss: 0.294758 \tR2: 0.781525\n",
      "Epoch: 662 \tTraining Loss: 0.289702 \tR2: 0.781525\n",
      "Epoch: 663 \tTraining Loss: 0.280816 \tR2: 0.781525\n",
      "Epoch: 664 \tTraining Loss: 0.268354 \tR2: 0.781525\n",
      "Epoch: 665 \tTraining Loss: 0.300174 \tR2: 0.781525\n",
      "Epoch: 666 \tTraining Loss: 0.280225 \tR2: 0.781525\n",
      "Epoch: 667 \tTraining Loss: 0.292643 \tR2: 0.781525\n",
      "Epoch: 668 \tTraining Loss: 0.274389 \tR2: 0.781525\n",
      "Epoch: 669 \tTraining Loss: 0.275950 \tR2: 0.781525\n",
      "Epoch: 670 \tTraining Loss: 0.289179 \tR2: 0.781525\n",
      "Epoch: 671 \tTraining Loss: 0.275595 \tR2: 0.781525\n",
      "Epoch: 672 \tTraining Loss: 0.260722 \tR2: 0.781525\n",
      "Epoch: 673 \tTraining Loss: 0.261028 \tR2: 0.781525\n",
      "Epoch: 674 \tTraining Loss: 0.275117 \tR2: 0.781525\n",
      "Epoch: 675 \tTraining Loss: 0.296246 \tR2: 0.781525\n",
      "Epoch: 676 \tTraining Loss: 0.261356 \tR2: 0.781525\n",
      "Epoch: 677 \tTraining Loss: 0.258434 \tR2: 0.781525\n",
      "Epoch: 678 \tTraining Loss: 0.289509 \tR2: 0.781525\n",
      "Epoch: 679 \tTraining Loss: 0.274890 \tR2: 0.781525\n",
      "Epoch: 680 \tTraining Loss: 0.305127 \tR2: 0.781525\n",
      "Epoch: 681 \tTraining Loss: 0.259118 \tR2: 0.781525\n",
      "Epoch: 682 \tTraining Loss: 0.283471 \tR2: 0.781525\n",
      "Epoch: 683 \tTraining Loss: 0.277848 \tR2: 0.781525\n",
      "Epoch: 684 \tTraining Loss: 0.255044 \tR2: 0.781525\n",
      "Epoch: 685 \tTraining Loss: 0.262444 \tR2: 0.781525\n",
      "Epoch: 686 \tTraining Loss: 0.304258 \tR2: 0.781525\n",
      "Epoch: 687 \tTraining Loss: 0.309028 \tR2: 0.781525\n",
      "Epoch: 688 \tTraining Loss: 0.263127 \tR2: 0.781525\n",
      "Epoch: 689 \tTraining Loss: 0.259440 \tR2: 0.781525\n",
      "Epoch: 690 \tTraining Loss: 0.277646 \tR2: 0.781525\n",
      "Epoch: 691 \tTraining Loss: 0.257206 \tR2: 0.781525\n",
      "Epoch: 692 \tTraining Loss: 0.263462 \tR2: 0.781525\n",
      "Epoch: 693 \tTraining Loss: 0.298482 \tR2: 0.781525\n",
      "Epoch: 694 \tTraining Loss: 0.286606 \tR2: 0.781525\n",
      "Epoch: 695 \tTraining Loss: 0.276393 \tR2: 0.781525\n",
      "Epoch: 696 \tTraining Loss: 0.287057 \tR2: 0.781525\n",
      "Epoch: 697 \tTraining Loss: 0.287059 \tR2: 0.781525\n",
      "Epoch: 698 \tTraining Loss: 0.273377 \tR2: 0.781525\n",
      "Epoch: 699 \tTraining Loss: 0.258478 \tR2: 0.781525\n",
      "Epoch: 700 \tTraining Loss: 0.276580 \tR2: 0.616664\n",
      "Epoch: 701 \tTraining Loss: 0.285633 \tR2: 0.616664\n",
      "Epoch: 702 \tTraining Loss: 0.276691 \tR2: 0.616664\n",
      "Epoch: 703 \tTraining Loss: 0.276392 \tR2: 0.616664\n",
      "Epoch: 704 \tTraining Loss: 0.269522 \tR2: 0.616664\n",
      "Epoch: 705 \tTraining Loss: 0.244933 \tR2: 0.616664\n",
      "Epoch: 706 \tTraining Loss: 0.284593 \tR2: 0.616664\n",
      "Epoch: 707 \tTraining Loss: 0.294130 \tR2: 0.616664\n",
      "Epoch: 708 \tTraining Loss: 0.282320 \tR2: 0.616664\n",
      "Epoch: 709 \tTraining Loss: 0.285682 \tR2: 0.616664\n",
      "Epoch: 710 \tTraining Loss: 0.312168 \tR2: 0.616664\n",
      "Epoch: 711 \tTraining Loss: 0.284387 \tR2: 0.616664\n",
      "Epoch: 712 \tTraining Loss: 0.286511 \tR2: 0.616664\n",
      "Epoch: 713 \tTraining Loss: 0.277745 \tR2: 0.616664\n",
      "Epoch: 714 \tTraining Loss: 0.265952 \tR2: 0.616664\n",
      "Epoch: 715 \tTraining Loss: 0.276709 \tR2: 0.616664\n",
      "Epoch: 716 \tTraining Loss: 0.285059 \tR2: 0.616664\n",
      "Epoch: 717 \tTraining Loss: 0.296654 \tR2: 0.616664\n",
      "Epoch: 718 \tTraining Loss: 0.286948 \tR2: 0.616664\n",
      "Epoch: 719 \tTraining Loss: 0.279910 \tR2: 0.616664\n",
      "Epoch: 720 \tTraining Loss: 0.273438 \tR2: 0.616664\n",
      "Epoch: 721 \tTraining Loss: 0.263564 \tR2: 0.616664\n",
      "Epoch: 722 \tTraining Loss: 0.297815 \tR2: 0.616664\n",
      "Epoch: 723 \tTraining Loss: 0.258387 \tR2: 0.616664\n",
      "Epoch: 724 \tTraining Loss: 0.254527 \tR2: 0.616664\n",
      "Epoch: 725 \tTraining Loss: 0.284785 \tR2: 0.616664\n",
      "Epoch: 726 \tTraining Loss: 0.284835 \tR2: 0.616664\n",
      "Epoch: 727 \tTraining Loss: 0.288341 \tR2: 0.616664\n",
      "Epoch: 728 \tTraining Loss: 0.297910 \tR2: 0.616664\n",
      "Epoch: 729 \tTraining Loss: 0.292218 \tR2: 0.616664\n",
      "Epoch: 730 \tTraining Loss: 0.264758 \tR2: 0.616664\n",
      "Epoch: 731 \tTraining Loss: 0.321776 \tR2: 0.616664\n",
      "Epoch: 732 \tTraining Loss: 0.283121 \tR2: 0.616664\n",
      "Epoch: 733 \tTraining Loss: 0.277867 \tR2: 0.616664\n",
      "Epoch: 734 \tTraining Loss: 0.276468 \tR2: 0.616664\n",
      "Epoch: 735 \tTraining Loss: 0.265005 \tR2: 0.616664\n",
      "Epoch: 736 \tTraining Loss: 0.283369 \tR2: 0.616664\n",
      "Epoch: 737 \tTraining Loss: 0.277031 \tR2: 0.616664\n",
      "Epoch: 738 \tTraining Loss: 0.267072 \tR2: 0.616664\n",
      "Epoch: 739 \tTraining Loss: 0.290314 \tR2: 0.616664\n",
      "Epoch: 740 \tTraining Loss: 0.294119 \tR2: 0.616664\n",
      "Epoch: 741 \tTraining Loss: 0.281078 \tR2: 0.616664\n",
      "Epoch: 742 \tTraining Loss: 0.298374 \tR2: 0.616664\n",
      "Epoch: 743 \tTraining Loss: 0.261528 \tR2: 0.616664\n",
      "Epoch: 744 \tTraining Loss: 0.264282 \tR2: 0.616664\n",
      "Epoch: 745 \tTraining Loss: 0.262137 \tR2: 0.616664\n",
      "Epoch: 746 \tTraining Loss: 0.264394 \tR2: 0.616664\n",
      "Epoch: 747 \tTraining Loss: 0.248240 \tR2: 0.616664\n",
      "Epoch: 748 \tTraining Loss: 0.273988 \tR2: 0.616664\n",
      "Epoch: 749 \tTraining Loss: 0.277570 \tR2: 0.616664\n",
      "Epoch: 750 \tTraining Loss: 0.279604 \tR2: 0.616664\n",
      "Epoch: 751 \tTraining Loss: 0.296528 \tR2: 0.616664\n",
      "Epoch: 752 \tTraining Loss: 0.277857 \tR2: 0.616664\n",
      "Epoch: 753 \tTraining Loss: 0.261557 \tR2: 0.616664\n",
      "Epoch: 754 \tTraining Loss: 0.272483 \tR2: 0.616664\n",
      "Epoch: 755 \tTraining Loss: 0.266341 \tR2: 0.616664\n",
      "Epoch: 756 \tTraining Loss: 0.258516 \tR2: 0.616664\n",
      "Epoch: 757 \tTraining Loss: 0.293788 \tR2: 0.616664\n",
      "Epoch: 758 \tTraining Loss: 0.251328 \tR2: 0.616664\n",
      "Epoch: 759 \tTraining Loss: 0.294850 \tR2: 0.616664\n",
      "Epoch: 760 \tTraining Loss: 0.277104 \tR2: 0.616664\n",
      "Epoch: 761 \tTraining Loss: 0.259299 \tR2: 0.616664\n",
      "Epoch: 762 \tTraining Loss: 0.288782 \tR2: 0.616664\n",
      "Epoch: 763 \tTraining Loss: 0.292706 \tR2: 0.616664\n",
      "Epoch: 764 \tTraining Loss: 0.300652 \tR2: 0.616664\n",
      "Epoch: 765 \tTraining Loss: 0.314331 \tR2: 0.616664\n",
      "Epoch: 766 \tTraining Loss: 0.299353 \tR2: 0.616664\n",
      "Epoch: 767 \tTraining Loss: 0.287629 \tR2: 0.616664\n",
      "Epoch: 768 \tTraining Loss: 0.298086 \tR2: 0.616664\n",
      "Epoch: 769 \tTraining Loss: 0.282755 \tR2: 0.616664\n",
      "Epoch: 770 \tTraining Loss: 0.266282 \tR2: 0.616664\n",
      "Epoch: 771 \tTraining Loss: 0.259356 \tR2: 0.616664\n",
      "Epoch: 772 \tTraining Loss: 0.248502 \tR2: 0.616664\n",
      "Epoch: 773 \tTraining Loss: 0.254489 \tR2: 0.616664\n",
      "Epoch: 774 \tTraining Loss: 0.266038 \tR2: 0.616664\n",
      "Epoch: 775 \tTraining Loss: 0.259606 \tR2: 0.616664\n",
      "Epoch: 776 \tTraining Loss: 0.270873 \tR2: 0.616664\n",
      "Epoch: 777 \tTraining Loss: 0.278679 \tR2: 0.616664\n",
      "Epoch: 778 \tTraining Loss: 0.255603 \tR2: 0.616664\n",
      "Epoch: 779 \tTraining Loss: 0.287964 \tR2: 0.616664\n",
      "Epoch: 780 \tTraining Loss: 0.254993 \tR2: 0.616664\n",
      "Epoch: 781 \tTraining Loss: 0.323661 \tR2: 0.616664\n",
      "Epoch: 782 \tTraining Loss: 0.280423 \tR2: 0.616664\n",
      "Epoch: 783 \tTraining Loss: 0.273052 \tR2: 0.616664\n",
      "Epoch: 784 \tTraining Loss: 0.261897 \tR2: 0.616664\n",
      "Epoch: 785 \tTraining Loss: 0.286123 \tR2: 0.616664\n",
      "Epoch: 786 \tTraining Loss: 0.289364 \tR2: 0.616664\n",
      "Epoch: 787 \tTraining Loss: 0.265234 \tR2: 0.616664\n",
      "Epoch: 788 \tTraining Loss: 0.296783 \tR2: 0.616664\n",
      "Epoch: 789 \tTraining Loss: 0.295975 \tR2: 0.616664\n",
      "Epoch: 790 \tTraining Loss: 0.249137 \tR2: 0.616664\n",
      "Epoch: 791 \tTraining Loss: 0.254318 \tR2: 0.616664\n",
      "Epoch: 792 \tTraining Loss: 0.260023 \tR2: 0.616664\n",
      "Epoch: 793 \tTraining Loss: 0.276749 \tR2: 0.616664\n",
      "Epoch: 794 \tTraining Loss: 0.260897 \tR2: 0.616664\n",
      "Epoch: 795 \tTraining Loss: 0.245452 \tR2: 0.616664\n",
      "Epoch: 796 \tTraining Loss: 0.282021 \tR2: 0.616664\n",
      "Epoch: 797 \tTraining Loss: 0.270003 \tR2: 0.616664\n",
      "Epoch: 798 \tTraining Loss: 0.294654 \tR2: 0.616664\n",
      "Epoch: 799 \tTraining Loss: 0.276240 \tR2: 0.616664\n",
      "Epoch: 800 \tTraining Loss: 0.239675 \tR2: 0.787152\n",
      "Epoch: 801 \tTraining Loss: 0.284031 \tR2: 0.787152\n",
      "Epoch: 802 \tTraining Loss: 0.243476 \tR2: 0.787152\n",
      "Epoch: 803 \tTraining Loss: 0.283631 \tR2: 0.787152\n",
      "Epoch: 804 \tTraining Loss: 0.273729 \tR2: 0.787152\n",
      "Epoch: 805 \tTraining Loss: 0.251467 \tR2: 0.787152\n",
      "Epoch: 806 \tTraining Loss: 0.261989 \tR2: 0.787152\n",
      "Epoch: 807 \tTraining Loss: 0.305344 \tR2: 0.787152\n",
      "Epoch: 808 \tTraining Loss: 0.375250 \tR2: 0.787152\n",
      "Epoch: 809 \tTraining Loss: 0.263652 \tR2: 0.787152\n",
      "Epoch: 810 \tTraining Loss: 0.289319 \tR2: 0.787152\n",
      "Epoch: 811 \tTraining Loss: 0.298738 \tR2: 0.787152\n",
      "Epoch: 812 \tTraining Loss: 0.265630 \tR2: 0.787152\n",
      "Epoch: 813 \tTraining Loss: 0.268187 \tR2: 0.787152\n",
      "Epoch: 814 \tTraining Loss: 0.273803 \tR2: 0.787152\n",
      "Epoch: 815 \tTraining Loss: 0.288478 \tR2: 0.787152\n",
      "Epoch: 816 \tTraining Loss: 0.301889 \tR2: 0.787152\n",
      "Epoch: 817 \tTraining Loss: 0.274874 \tR2: 0.787152\n",
      "Epoch: 818 \tTraining Loss: 0.274966 \tR2: 0.787152\n",
      "Epoch: 819 \tTraining Loss: 0.284250 \tR2: 0.787152\n",
      "Epoch: 820 \tTraining Loss: 0.285156 \tR2: 0.787152\n",
      "Epoch: 821 \tTraining Loss: 0.264652 \tR2: 0.787152\n",
      "Epoch: 822 \tTraining Loss: 0.293649 \tR2: 0.787152\n",
      "Epoch: 823 \tTraining Loss: 0.283637 \tR2: 0.787152\n",
      "Epoch: 824 \tTraining Loss: 0.282417 \tR2: 0.787152\n",
      "Epoch: 825 \tTraining Loss: 0.266855 \tR2: 0.787152\n",
      "Epoch: 826 \tTraining Loss: 0.282707 \tR2: 0.787152\n",
      "Epoch: 827 \tTraining Loss: 0.274981 \tR2: 0.787152\n",
      "Epoch: 828 \tTraining Loss: 0.253071 \tR2: 0.787152\n",
      "Epoch: 829 \tTraining Loss: 0.262772 \tR2: 0.787152\n",
      "Epoch: 830 \tTraining Loss: 0.258307 \tR2: 0.787152\n",
      "Epoch: 831 \tTraining Loss: 0.311913 \tR2: 0.787152\n",
      "Epoch: 832 \tTraining Loss: 0.286072 \tR2: 0.787152\n",
      "Epoch: 833 \tTraining Loss: 0.288359 \tR2: 0.787152\n",
      "Epoch: 834 \tTraining Loss: 0.256416 \tR2: 0.787152\n",
      "Epoch: 835 \tTraining Loss: 0.330434 \tR2: 0.787152\n",
      "Epoch: 836 \tTraining Loss: 0.306465 \tR2: 0.787152\n",
      "Epoch: 837 \tTraining Loss: 0.268943 \tR2: 0.787152\n",
      "Epoch: 838 \tTraining Loss: 0.270909 \tR2: 0.787152\n",
      "Epoch: 839 \tTraining Loss: 0.263142 \tR2: 0.787152\n",
      "Epoch: 840 \tTraining Loss: 0.289564 \tR2: 0.787152\n",
      "Epoch: 841 \tTraining Loss: 0.276378 \tR2: 0.787152\n",
      "Epoch: 842 \tTraining Loss: 0.256909 \tR2: 0.787152\n",
      "Epoch: 843 \tTraining Loss: 0.292521 \tR2: 0.787152\n",
      "Epoch: 844 \tTraining Loss: 0.254539 \tR2: 0.787152\n",
      "Epoch: 845 \tTraining Loss: 0.258426 \tR2: 0.787152\n",
      "Epoch: 846 \tTraining Loss: 0.261589 \tR2: 0.787152\n",
      "Epoch: 847 \tTraining Loss: 0.288931 \tR2: 0.787152\n",
      "Epoch: 848 \tTraining Loss: 0.254145 \tR2: 0.787152\n",
      "Epoch: 849 \tTraining Loss: 0.289582 \tR2: 0.787152\n",
      "Epoch: 850 \tTraining Loss: 0.274162 \tR2: 0.787152\n",
      "Epoch: 851 \tTraining Loss: 0.271426 \tR2: 0.787152\n",
      "Epoch: 852 \tTraining Loss: 0.250813 \tR2: 0.787152\n",
      "Epoch: 853 \tTraining Loss: 0.248952 \tR2: 0.787152\n",
      "Epoch: 854 \tTraining Loss: 0.260728 \tR2: 0.787152\n",
      "Epoch: 855 \tTraining Loss: 0.242185 \tR2: 0.787152\n",
      "Epoch: 856 \tTraining Loss: 0.279383 \tR2: 0.787152\n",
      "Epoch: 857 \tTraining Loss: 0.268841 \tR2: 0.787152\n",
      "Epoch: 858 \tTraining Loss: 0.268442 \tR2: 0.787152\n",
      "Epoch: 859 \tTraining Loss: 0.262045 \tR2: 0.787152\n",
      "Epoch: 860 \tTraining Loss: 0.256352 \tR2: 0.787152\n",
      "Epoch: 861 \tTraining Loss: 0.287252 \tR2: 0.787152\n",
      "Epoch: 862 \tTraining Loss: 0.298329 \tR2: 0.787152\n",
      "Epoch: 863 \tTraining Loss: 0.268625 \tR2: 0.787152\n",
      "Epoch: 864 \tTraining Loss: 0.269636 \tR2: 0.787152\n",
      "Epoch: 865 \tTraining Loss: 0.288507 \tR2: 0.787152\n",
      "Epoch: 866 \tTraining Loss: 0.250157 \tR2: 0.787152\n",
      "Epoch: 867 \tTraining Loss: 0.277597 \tR2: 0.787152\n",
      "Epoch: 868 \tTraining Loss: 0.287087 \tR2: 0.787152\n",
      "Epoch: 869 \tTraining Loss: 0.272796 \tR2: 0.787152\n",
      "Epoch: 870 \tTraining Loss: 0.262137 \tR2: 0.787152\n",
      "Epoch: 871 \tTraining Loss: 0.270645 \tR2: 0.787152\n",
      "Epoch: 872 \tTraining Loss: 0.251368 \tR2: 0.787152\n",
      "Epoch: 873 \tTraining Loss: 0.295976 \tR2: 0.787152\n",
      "Epoch: 874 \tTraining Loss: 0.275442 \tR2: 0.787152\n",
      "Epoch: 875 \tTraining Loss: 0.277264 \tR2: 0.787152\n",
      "Epoch: 876 \tTraining Loss: 0.259921 \tR2: 0.787152\n",
      "Epoch: 877 \tTraining Loss: 0.261190 \tR2: 0.787152\n",
      "Epoch: 878 \tTraining Loss: 0.278454 \tR2: 0.787152\n",
      "Epoch: 879 \tTraining Loss: 0.249765 \tR2: 0.787152\n",
      "Epoch: 880 \tTraining Loss: 0.272033 \tR2: 0.787152\n",
      "Epoch: 881 \tTraining Loss: 0.259958 \tR2: 0.787152\n",
      "Epoch: 882 \tTraining Loss: 0.279571 \tR2: 0.787152\n",
      "Epoch: 883 \tTraining Loss: 0.293065 \tR2: 0.787152\n",
      "Epoch: 884 \tTraining Loss: 0.251926 \tR2: 0.787152\n",
      "Epoch: 885 \tTraining Loss: 0.296316 \tR2: 0.787152\n",
      "Epoch: 886 \tTraining Loss: 0.257320 \tR2: 0.787152\n",
      "Epoch: 887 \tTraining Loss: 0.275459 \tR2: 0.787152\n",
      "Epoch: 888 \tTraining Loss: 0.287898 \tR2: 0.787152\n",
      "Epoch: 889 \tTraining Loss: 0.277360 \tR2: 0.787152\n",
      "Epoch: 890 \tTraining Loss: 0.270148 \tR2: 0.787152\n",
      "Epoch: 891 \tTraining Loss: 0.279878 \tR2: 0.787152\n",
      "Epoch: 892 \tTraining Loss: 0.272939 \tR2: 0.787152\n",
      "Epoch: 893 \tTraining Loss: 0.265787 \tR2: 0.787152\n",
      "Epoch: 894 \tTraining Loss: 0.271199 \tR2: 0.787152\n",
      "Epoch: 895 \tTraining Loss: 0.256164 \tR2: 0.787152\n",
      "Epoch: 896 \tTraining Loss: 0.276700 \tR2: 0.787152\n",
      "Epoch: 897 \tTraining Loss: 0.259469 \tR2: 0.787152\n",
      "Epoch: 898 \tTraining Loss: 0.274784 \tR2: 0.787152\n",
      "Epoch: 899 \tTraining Loss: 0.257511 \tR2: 0.787152\n",
      "Epoch: 900 \tTraining Loss: 0.279542 \tR2: 0.432038\n",
      "Epoch: 901 \tTraining Loss: 0.284588 \tR2: 0.432038\n",
      "Epoch: 902 \tTraining Loss: 0.250800 \tR2: 0.432038\n",
      "Epoch: 903 \tTraining Loss: 0.269055 \tR2: 0.432038\n",
      "Epoch: 904 \tTraining Loss: 0.263656 \tR2: 0.432038\n",
      "Epoch: 905 \tTraining Loss: 0.257169 \tR2: 0.432038\n",
      "Epoch: 906 \tTraining Loss: 0.268381 \tR2: 0.432038\n",
      "Epoch: 907 \tTraining Loss: 0.265631 \tR2: 0.432038\n",
      "Epoch: 908 \tTraining Loss: 0.268312 \tR2: 0.432038\n",
      "Epoch: 909 \tTraining Loss: 0.240806 \tR2: 0.432038\n",
      "Epoch: 910 \tTraining Loss: 0.277082 \tR2: 0.432038\n",
      "Epoch: 911 \tTraining Loss: 0.247866 \tR2: 0.432038\n",
      "Epoch: 912 \tTraining Loss: 0.247577 \tR2: 0.432038\n",
      "Epoch: 913 \tTraining Loss: 0.268378 \tR2: 0.432038\n",
      "Epoch: 914 \tTraining Loss: 0.274767 \tR2: 0.432038\n",
      "Epoch: 915 \tTraining Loss: 0.257843 \tR2: 0.432038\n",
      "Epoch: 916 \tTraining Loss: 0.259576 \tR2: 0.432038\n",
      "Epoch: 917 \tTraining Loss: 0.249874 \tR2: 0.432038\n",
      "Epoch: 918 \tTraining Loss: 0.254580 \tR2: 0.432038\n",
      "Epoch: 919 \tTraining Loss: 0.298789 \tR2: 0.432038\n",
      "Epoch: 920 \tTraining Loss: 0.257088 \tR2: 0.432038\n",
      "Epoch: 921 \tTraining Loss: 0.245483 \tR2: 0.432038\n",
      "Epoch: 922 \tTraining Loss: 0.268577 \tR2: 0.432038\n",
      "Epoch: 923 \tTraining Loss: 0.260992 \tR2: 0.432038\n",
      "Epoch: 924 \tTraining Loss: 0.246458 \tR2: 0.432038\n",
      "Epoch: 925 \tTraining Loss: 0.266439 \tR2: 0.432038\n",
      "Epoch: 926 \tTraining Loss: 0.270457 \tR2: 0.432038\n",
      "Epoch: 927 \tTraining Loss: 0.270645 \tR2: 0.432038\n",
      "Epoch: 928 \tTraining Loss: 0.282611 \tR2: 0.432038\n",
      "Epoch: 929 \tTraining Loss: 0.254974 \tR2: 0.432038\n",
      "Epoch: 930 \tTraining Loss: 0.245933 \tR2: 0.432038\n",
      "Epoch: 931 \tTraining Loss: 0.260889 \tR2: 0.432038\n",
      "Epoch: 932 \tTraining Loss: 0.278515 \tR2: 0.432038\n",
      "Epoch: 933 \tTraining Loss: 0.255663 \tR2: 0.432038\n",
      "Epoch: 934 \tTraining Loss: 0.267485 \tR2: 0.432038\n",
      "Epoch: 935 \tTraining Loss: 0.268266 \tR2: 0.432038\n",
      "Epoch: 936 \tTraining Loss: 0.246009 \tR2: 0.432038\n",
      "Epoch: 937 \tTraining Loss: 0.263783 \tR2: 0.432038\n",
      "Epoch: 938 \tTraining Loss: 0.286036 \tR2: 0.432038\n",
      "Epoch: 939 \tTraining Loss: 0.263200 \tR2: 0.432038\n",
      "Epoch: 940 \tTraining Loss: 0.250887 \tR2: 0.432038\n",
      "Epoch: 941 \tTraining Loss: 0.257157 \tR2: 0.432038\n",
      "Epoch: 942 \tTraining Loss: 0.253182 \tR2: 0.432038\n",
      "Epoch: 943 \tTraining Loss: 0.277464 \tR2: 0.432038\n",
      "Epoch: 944 \tTraining Loss: 0.266852 \tR2: 0.432038\n",
      "Epoch: 945 \tTraining Loss: 0.283478 \tR2: 0.432038\n",
      "Epoch: 946 \tTraining Loss: 0.262564 \tR2: 0.432038\n",
      "Epoch: 947 \tTraining Loss: 0.261723 \tR2: 0.432038\n",
      "Epoch: 948 \tTraining Loss: 0.278286 \tR2: 0.432038\n",
      "Epoch: 949 \tTraining Loss: 0.261546 \tR2: 0.432038\n",
      "Epoch: 950 \tTraining Loss: 0.270001 \tR2: 0.432038\n",
      "Epoch: 951 \tTraining Loss: 0.246100 \tR2: 0.432038\n",
      "Epoch: 952 \tTraining Loss: 0.269550 \tR2: 0.432038\n",
      "Epoch: 953 \tTraining Loss: 0.283008 \tR2: 0.432038\n",
      "Epoch: 954 \tTraining Loss: 0.271765 \tR2: 0.432038\n",
      "Epoch: 955 \tTraining Loss: 0.290177 \tR2: 0.432038\n",
      "Epoch: 956 \tTraining Loss: 0.307213 \tR2: 0.432038\n",
      "Epoch: 957 \tTraining Loss: 0.297823 \tR2: 0.432038\n",
      "Epoch: 958 \tTraining Loss: 0.263529 \tR2: 0.432038\n",
      "Epoch: 959 \tTraining Loss: 0.261320 \tR2: 0.432038\n",
      "Epoch: 960 \tTraining Loss: 0.254924 \tR2: 0.432038\n",
      "Epoch: 961 \tTraining Loss: 0.254663 \tR2: 0.432038\n",
      "Epoch: 962 \tTraining Loss: 0.289648 \tR2: 0.432038\n",
      "Epoch: 963 \tTraining Loss: 0.251826 \tR2: 0.432038\n",
      "Epoch: 964 \tTraining Loss: 0.279381 \tR2: 0.432038\n",
      "Epoch: 965 \tTraining Loss: 0.234136 \tR2: 0.432038\n",
      "Epoch: 966 \tTraining Loss: 0.257073 \tR2: 0.432038\n",
      "Epoch: 967 \tTraining Loss: 0.246126 \tR2: 0.432038\n",
      "Epoch: 968 \tTraining Loss: 0.276784 \tR2: 0.432038\n",
      "Epoch: 969 \tTraining Loss: 0.270152 \tR2: 0.432038\n",
      "Epoch: 970 \tTraining Loss: 0.252846 \tR2: 0.432038\n",
      "Epoch: 971 \tTraining Loss: 0.266868 \tR2: 0.432038\n",
      "Epoch: 972 \tTraining Loss: 0.266884 \tR2: 0.432038\n",
      "Epoch: 973 \tTraining Loss: 0.273169 \tR2: 0.432038\n",
      "Epoch: 974 \tTraining Loss: 0.247588 \tR2: 0.432038\n",
      "Epoch: 975 \tTraining Loss: 0.256486 \tR2: 0.432038\n",
      "Epoch: 976 \tTraining Loss: 0.246326 \tR2: 0.432038\n",
      "Epoch: 977 \tTraining Loss: 0.252766 \tR2: 0.432038\n",
      "Epoch: 978 \tTraining Loss: 0.242319 \tR2: 0.432038\n",
      "Epoch: 979 \tTraining Loss: 0.278528 \tR2: 0.432038\n",
      "Epoch: 980 \tTraining Loss: 0.241964 \tR2: 0.432038\n",
      "Epoch: 981 \tTraining Loss: 0.262508 \tR2: 0.432038\n",
      "Epoch: 982 \tTraining Loss: 0.270919 \tR2: 0.432038\n",
      "Epoch: 983 \tTraining Loss: 0.247661 \tR2: 0.432038\n",
      "Epoch: 984 \tTraining Loss: 0.260376 \tR2: 0.432038\n",
      "Epoch: 985 \tTraining Loss: 0.236068 \tR2: 0.432038\n",
      "Epoch: 986 \tTraining Loss: 0.303918 \tR2: 0.432038\n",
      "Epoch: 987 \tTraining Loss: 0.249219 \tR2: 0.432038\n",
      "Epoch: 988 \tTraining Loss: 0.241855 \tR2: 0.432038\n",
      "Epoch: 989 \tTraining Loss: 0.292956 \tR2: 0.432038\n",
      "Epoch: 990 \tTraining Loss: 0.249281 \tR2: 0.432038\n",
      "Epoch: 991 \tTraining Loss: 0.260879 \tR2: 0.432038\n",
      "Epoch: 992 \tTraining Loss: 0.286598 \tR2: 0.432038\n",
      "Epoch: 993 \tTraining Loss: 0.266794 \tR2: 0.432038\n",
      "Epoch: 994 \tTraining Loss: 0.247454 \tR2: 0.432038\n",
      "Epoch: 995 \tTraining Loss: 0.250097 \tR2: 0.432038\n",
      "Epoch: 996 \tTraining Loss: 0.265975 \tR2: 0.432038\n",
      "Epoch: 997 \tTraining Loss: 0.269575 \tR2: 0.432038\n",
      "Epoch: 998 \tTraining Loss: 0.263519 \tR2: 0.432038\n",
      "Epoch: 999 \tTraining Loss: 0.290795 \tR2: 0.432038\n",
      "Epoch: 1000 \tTraining Loss: 0.244483 \tR2: 0.721305\n",
      "Epoch: 1001 \tTraining Loss: 0.254156 \tR2: 0.721305\n",
      "Epoch: 1002 \tTraining Loss: 0.266851 \tR2: 0.721305\n",
      "Epoch: 1003 \tTraining Loss: 0.262587 \tR2: 0.721305\n",
      "Epoch: 1004 \tTraining Loss: 0.278554 \tR2: 0.721305\n",
      "Epoch: 1005 \tTraining Loss: 0.260727 \tR2: 0.721305\n",
      "Epoch: 1006 \tTraining Loss: 0.260056 \tR2: 0.721305\n",
      "Epoch: 1007 \tTraining Loss: 0.266772 \tR2: 0.721305\n",
      "Epoch: 1008 \tTraining Loss: 0.273323 \tR2: 0.721305\n",
      "Epoch: 1009 \tTraining Loss: 0.248502 \tR2: 0.721305\n",
      "Epoch: 1010 \tTraining Loss: 0.265142 \tR2: 0.721305\n",
      "Epoch: 1011 \tTraining Loss: 0.266093 \tR2: 0.721305\n",
      "Epoch: 1012 \tTraining Loss: 0.245945 \tR2: 0.721305\n",
      "Epoch: 1013 \tTraining Loss: 0.244366 \tR2: 0.721305\n",
      "Epoch: 1014 \tTraining Loss: 0.255652 \tR2: 0.721305\n",
      "Epoch: 1015 \tTraining Loss: 0.256630 \tR2: 0.721305\n",
      "Epoch: 1016 \tTraining Loss: 0.263732 \tR2: 0.721305\n",
      "Epoch: 1017 \tTraining Loss: 0.251275 \tR2: 0.721305\n",
      "Epoch: 1018 \tTraining Loss: 0.263197 \tR2: 0.721305\n",
      "Epoch: 1019 \tTraining Loss: 0.279281 \tR2: 0.721305\n",
      "Epoch: 1020 \tTraining Loss: 0.260884 \tR2: 0.721305\n",
      "Epoch: 1021 \tTraining Loss: 0.288760 \tR2: 0.721305\n",
      "Epoch: 1022 \tTraining Loss: 0.256557 \tR2: 0.721305\n",
      "Epoch: 1023 \tTraining Loss: 0.257601 \tR2: 0.721305\n",
      "Epoch: 1024 \tTraining Loss: 0.278980 \tR2: 0.721305\n",
      "Epoch: 1025 \tTraining Loss: 0.270014 \tR2: 0.721305\n",
      "Epoch: 1026 \tTraining Loss: 0.247976 \tR2: 0.721305\n",
      "Epoch: 1027 \tTraining Loss: 0.248856 \tR2: 0.721305\n",
      "Epoch: 1028 \tTraining Loss: 0.254280 \tR2: 0.721305\n",
      "Epoch: 1029 \tTraining Loss: 0.272431 \tR2: 0.721305\n",
      "Epoch: 1030 \tTraining Loss: 0.268207 \tR2: 0.721305\n",
      "Epoch: 1031 \tTraining Loss: 0.260677 \tR2: 0.721305\n",
      "Epoch: 1032 \tTraining Loss: 0.262437 \tR2: 0.721305\n",
      "Epoch: 1033 \tTraining Loss: 0.259512 \tR2: 0.721305\n",
      "Epoch: 1034 \tTraining Loss: 0.273764 \tR2: 0.721305\n",
      "Epoch: 1035 \tTraining Loss: 0.271907 \tR2: 0.721305\n",
      "Epoch: 1036 \tTraining Loss: 0.253716 \tR2: 0.721305\n",
      "Epoch: 1037 \tTraining Loss: 0.257913 \tR2: 0.721305\n",
      "Epoch: 1038 \tTraining Loss: 0.252373 \tR2: 0.721305\n",
      "Epoch: 1039 \tTraining Loss: 0.246779 \tR2: 0.721305\n",
      "Epoch: 1040 \tTraining Loss: 0.278895 \tR2: 0.721305\n",
      "Epoch: 1041 \tTraining Loss: 0.276859 \tR2: 0.721305\n",
      "Epoch: 1042 \tTraining Loss: 0.259883 \tR2: 0.721305\n",
      "Epoch: 1043 \tTraining Loss: 0.239542 \tR2: 0.721305\n",
      "Epoch: 1044 \tTraining Loss: 0.264063 \tR2: 0.721305\n",
      "Epoch: 1045 \tTraining Loss: 0.257463 \tR2: 0.721305\n",
      "Epoch: 1046 \tTraining Loss: 0.244714 \tR2: 0.721305\n",
      "Epoch: 1047 \tTraining Loss: 0.272015 \tR2: 0.721305\n",
      "Epoch: 1048 \tTraining Loss: 0.248178 \tR2: 0.721305\n",
      "Epoch: 1049 \tTraining Loss: 0.273183 \tR2: 0.721305\n",
      "Epoch: 1050 \tTraining Loss: 0.240645 \tR2: 0.721305\n",
      "Epoch: 1051 \tTraining Loss: 0.264611 \tR2: 0.721305\n",
      "Epoch: 1052 \tTraining Loss: 0.272999 \tR2: 0.721305\n",
      "Epoch: 1053 \tTraining Loss: 0.254283 \tR2: 0.721305\n",
      "Epoch: 1054 \tTraining Loss: 0.249355 \tR2: 0.721305\n",
      "Epoch: 1055 \tTraining Loss: 0.240229 \tR2: 0.721305\n",
      "Epoch: 1056 \tTraining Loss: 0.292069 \tR2: 0.721305\n",
      "Epoch: 1057 \tTraining Loss: 0.250486 \tR2: 0.721305\n",
      "Epoch: 1058 \tTraining Loss: 0.247323 \tR2: 0.721305\n",
      "Epoch: 1059 \tTraining Loss: 0.267387 \tR2: 0.721305\n",
      "Epoch: 1060 \tTraining Loss: 0.263023 \tR2: 0.721305\n",
      "Epoch: 1061 \tTraining Loss: 0.253635 \tR2: 0.721305\n",
      "Epoch: 1062 \tTraining Loss: 0.257704 \tR2: 0.721305\n",
      "Epoch: 1063 \tTraining Loss: 0.272290 \tR2: 0.721305\n",
      "Epoch: 1064 \tTraining Loss: 0.258275 \tR2: 0.721305\n",
      "Epoch: 1065 \tTraining Loss: 0.269705 \tR2: 0.721305\n",
      "Epoch: 1066 \tTraining Loss: 0.260170 \tR2: 0.721305\n",
      "Epoch: 1067 \tTraining Loss: 0.243433 \tR2: 0.721305\n",
      "Epoch: 1068 \tTraining Loss: 0.236937 \tR2: 0.721305\n",
      "Epoch: 1069 \tTraining Loss: 0.252689 \tR2: 0.721305\n",
      "Epoch: 1070 \tTraining Loss: 0.259535 \tR2: 0.721305\n",
      "Epoch: 1071 \tTraining Loss: 0.268568 \tR2: 0.721305\n",
      "Epoch: 1072 \tTraining Loss: 0.266624 \tR2: 0.721305\n",
      "Epoch: 1073 \tTraining Loss: 0.253872 \tR2: 0.721305\n",
      "Epoch: 1074 \tTraining Loss: 0.297452 \tR2: 0.721305\n",
      "Epoch: 1075 \tTraining Loss: 0.290406 \tR2: 0.721305\n",
      "Epoch: 1076 \tTraining Loss: 0.273414 \tR2: 0.721305\n",
      "Epoch: 1077 \tTraining Loss: 0.270560 \tR2: 0.721305\n",
      "Epoch: 1078 \tTraining Loss: 0.267475 \tR2: 0.721305\n",
      "Epoch: 1079 \tTraining Loss: 0.246352 \tR2: 0.721305\n",
      "Epoch: 1080 \tTraining Loss: 0.269837 \tR2: 0.721305\n",
      "Epoch: 1081 \tTraining Loss: 0.269825 \tR2: 0.721305\n",
      "Epoch: 1082 \tTraining Loss: 0.269946 \tR2: 0.721305\n",
      "Epoch: 1083 \tTraining Loss: 0.266674 \tR2: 0.721305\n",
      "Epoch: 1084 \tTraining Loss: 0.260566 \tR2: 0.721305\n",
      "Epoch: 1085 \tTraining Loss: 0.260354 \tR2: 0.721305\n",
      "Epoch: 1086 \tTraining Loss: 0.258008 \tR2: 0.721305\n",
      "Epoch: 1087 \tTraining Loss: 0.278910 \tR2: 0.721305\n",
      "Epoch: 1088 \tTraining Loss: 0.270875 \tR2: 0.721305\n",
      "Epoch: 1089 \tTraining Loss: 0.250327 \tR2: 0.721305\n",
      "Epoch: 1090 \tTraining Loss: 0.269467 \tR2: 0.721305\n",
      "Epoch: 1091 \tTraining Loss: 0.247538 \tR2: 0.721305\n",
      "Epoch: 1092 \tTraining Loss: 0.244755 \tR2: 0.721305\n",
      "Epoch: 1093 \tTraining Loss: 0.257242 \tR2: 0.721305\n",
      "Epoch: 1094 \tTraining Loss: 0.259057 \tR2: 0.721305\n",
      "Epoch: 1095 \tTraining Loss: 0.267613 \tR2: 0.721305\n",
      "Epoch: 1096 \tTraining Loss: 0.257401 \tR2: 0.721305\n",
      "Epoch: 1097 \tTraining Loss: 0.257279 \tR2: 0.721305\n",
      "Epoch: 1098 \tTraining Loss: 0.256902 \tR2: 0.721305\n",
      "Epoch: 1099 \tTraining Loss: 0.256160 \tR2: 0.721305\n",
      "Epoch: 1100 \tTraining Loss: 0.271197 \tR2: 0.737459\n",
      "Epoch: 1101 \tTraining Loss: 0.244572 \tR2: 0.737459\n",
      "Epoch: 1102 \tTraining Loss: 0.255769 \tR2: 0.737459\n",
      "Epoch: 1103 \tTraining Loss: 0.245483 \tR2: 0.737459\n",
      "Epoch: 1104 \tTraining Loss: 0.253248 \tR2: 0.737459\n",
      "Epoch: 1105 \tTraining Loss: 0.256753 \tR2: 0.737459\n",
      "Epoch: 1106 \tTraining Loss: 0.250713 \tR2: 0.737459\n",
      "Epoch: 1107 \tTraining Loss: 0.254713 \tR2: 0.737459\n",
      "Epoch: 1108 \tTraining Loss: 0.251458 \tR2: 0.737459\n",
      "Epoch: 1109 \tTraining Loss: 0.243614 \tR2: 0.737459\n",
      "Epoch: 1110 \tTraining Loss: 0.278536 \tR2: 0.737459\n",
      "Epoch: 1111 \tTraining Loss: 0.245831 \tR2: 0.737459\n",
      "Epoch: 1112 \tTraining Loss: 0.239936 \tR2: 0.737459\n",
      "Epoch: 1113 \tTraining Loss: 0.262185 \tR2: 0.737459\n",
      "Epoch: 1114 \tTraining Loss: 0.232504 \tR2: 0.737459\n",
      "Epoch: 1115 \tTraining Loss: 0.244771 \tR2: 0.737459\n",
      "Epoch: 1116 \tTraining Loss: 0.274046 \tR2: 0.737459\n",
      "Epoch: 1117 \tTraining Loss: 0.269854 \tR2: 0.737459\n",
      "Epoch: 1118 \tTraining Loss: 0.269944 \tR2: 0.737459\n",
      "Epoch: 1119 \tTraining Loss: 0.247946 \tR2: 0.737459\n",
      "Epoch: 1120 \tTraining Loss: 0.269256 \tR2: 0.737459\n",
      "Epoch: 1121 \tTraining Loss: 0.251300 \tR2: 0.737459\n",
      "Epoch: 1122 \tTraining Loss: 0.264095 \tR2: 0.737459\n",
      "Epoch: 1123 \tTraining Loss: 0.258736 \tR2: 0.737459\n",
      "Epoch: 1124 \tTraining Loss: 0.267349 \tR2: 0.737459\n",
      "Epoch: 1125 \tTraining Loss: 0.249294 \tR2: 0.737459\n",
      "Epoch: 1126 \tTraining Loss: 0.255197 \tR2: 0.737459\n",
      "Epoch: 1127 \tTraining Loss: 0.255547 \tR2: 0.737459\n",
      "Epoch: 1128 \tTraining Loss: 0.243655 \tR2: 0.737459\n",
      "Epoch: 1129 \tTraining Loss: 0.260414 \tR2: 0.737459\n",
      "Epoch: 1130 \tTraining Loss: 0.262921 \tR2: 0.737459\n",
      "Epoch: 1131 \tTraining Loss: 0.254039 \tR2: 0.737459\n",
      "Epoch: 1132 \tTraining Loss: 0.259387 \tR2: 0.737459\n",
      "Epoch: 1133 \tTraining Loss: 0.239772 \tR2: 0.737459\n",
      "Epoch: 1134 \tTraining Loss: 0.244555 \tR2: 0.737459\n",
      "Epoch: 1135 \tTraining Loss: 0.256875 \tR2: 0.737459\n",
      "Epoch: 1136 \tTraining Loss: 0.225685 \tR2: 0.737459\n",
      "Epoch: 1137 \tTraining Loss: 0.257053 \tR2: 0.737459\n",
      "Epoch: 1138 \tTraining Loss: 0.245683 \tR2: 0.737459\n",
      "Epoch: 1139 \tTraining Loss: 0.223373 \tR2: 0.737459\n",
      "Epoch: 1140 \tTraining Loss: 0.259236 \tR2: 0.737459\n",
      "Epoch: 1141 \tTraining Loss: 0.244639 \tR2: 0.737459\n",
      "Epoch: 1142 \tTraining Loss: 0.242117 \tR2: 0.737459\n",
      "Epoch: 1143 \tTraining Loss: 0.272813 \tR2: 0.737459\n",
      "Epoch: 1144 \tTraining Loss: 0.262071 \tR2: 0.737459\n",
      "Epoch: 1145 \tTraining Loss: 0.250141 \tR2: 0.737459\n",
      "Epoch: 1146 \tTraining Loss: 0.237228 \tR2: 0.737459\n",
      "Epoch: 1147 \tTraining Loss: 0.236302 \tR2: 0.737459\n",
      "Epoch: 1148 \tTraining Loss: 0.229855 \tR2: 0.737459\n",
      "Epoch: 1149 \tTraining Loss: 0.253108 \tR2: 0.737459\n",
      "Epoch: 1150 \tTraining Loss: 0.237936 \tR2: 0.737459\n",
      "Epoch: 1151 \tTraining Loss: 0.245808 \tR2: 0.737459\n",
      "Epoch: 1152 \tTraining Loss: 0.258874 \tR2: 0.737459\n",
      "Epoch: 1153 \tTraining Loss: 0.262496 \tR2: 0.737459\n",
      "Epoch: 1154 \tTraining Loss: 0.272930 \tR2: 0.737459\n",
      "Epoch: 1155 \tTraining Loss: 0.274323 \tR2: 0.737459\n",
      "Epoch: 1156 \tTraining Loss: 0.269726 \tR2: 0.737459\n",
      "Epoch: 1157 \tTraining Loss: 0.265050 \tR2: 0.737459\n",
      "Epoch: 1158 \tTraining Loss: 0.236189 \tR2: 0.737459\n",
      "Epoch: 1159 \tTraining Loss: 0.266822 \tR2: 0.737459\n",
      "Epoch: 1160 \tTraining Loss: 0.253231 \tR2: 0.737459\n",
      "Epoch: 1161 \tTraining Loss: 0.258467 \tR2: 0.737459\n",
      "Epoch: 1162 \tTraining Loss: 0.234954 \tR2: 0.737459\n",
      "Epoch: 1163 \tTraining Loss: 0.246615 \tR2: 0.737459\n",
      "Epoch: 1164 \tTraining Loss: 0.259171 \tR2: 0.737459\n",
      "Epoch: 1165 \tTraining Loss: 0.261689 \tR2: 0.737459\n",
      "Epoch: 1166 \tTraining Loss: 0.248191 \tR2: 0.737459\n",
      "Epoch: 1167 \tTraining Loss: 0.256721 \tR2: 0.737459\n",
      "Epoch: 1168 \tTraining Loss: 0.260340 \tR2: 0.737459\n",
      "Epoch: 1169 \tTraining Loss: 0.255020 \tR2: 0.737459\n",
      "Epoch: 1170 \tTraining Loss: 0.271630 \tR2: 0.737459\n",
      "Epoch: 1171 \tTraining Loss: 0.275166 \tR2: 0.737459\n",
      "Epoch: 1172 \tTraining Loss: 0.249743 \tR2: 0.737459\n",
      "Epoch: 1173 \tTraining Loss: 0.278255 \tR2: 0.737459\n",
      "Epoch: 1174 \tTraining Loss: 0.258119 \tR2: 0.737459\n",
      "Epoch: 1175 \tTraining Loss: 0.246061 \tR2: 0.737459\n",
      "Epoch: 1176 \tTraining Loss: 0.263271 \tR2: 0.737459\n",
      "Epoch: 1177 \tTraining Loss: 0.244022 \tR2: 0.737459\n",
      "Epoch: 1178 \tTraining Loss: 0.236467 \tR2: 0.737459\n",
      "Epoch: 1179 \tTraining Loss: 0.259257 \tR2: 0.737459\n",
      "Epoch: 1180 \tTraining Loss: 0.273761 \tR2: 0.737459\n",
      "Epoch: 1181 \tTraining Loss: 0.281061 \tR2: 0.737459\n",
      "Epoch: 1182 \tTraining Loss: 0.264673 \tR2: 0.737459\n",
      "Epoch: 1183 \tTraining Loss: 0.251468 \tR2: 0.737459\n",
      "Epoch: 1184 \tTraining Loss: 0.242640 \tR2: 0.737459\n",
      "Epoch: 1185 \tTraining Loss: 0.241725 \tR2: 0.737459\n",
      "Epoch: 1186 \tTraining Loss: 0.245780 \tR2: 0.737459\n",
      "Epoch: 1187 \tTraining Loss: 0.230489 \tR2: 0.737459\n",
      "Epoch: 1188 \tTraining Loss: 0.269572 \tR2: 0.737459\n",
      "Epoch: 1189 \tTraining Loss: 0.276229 \tR2: 0.737459\n",
      "Epoch: 1190 \tTraining Loss: 0.251591 \tR2: 0.737459\n",
      "Epoch: 1191 \tTraining Loss: 0.254781 \tR2: 0.737459\n",
      "Epoch: 1192 \tTraining Loss: 0.263011 \tR2: 0.737459\n",
      "Epoch: 1193 \tTraining Loss: 0.254901 \tR2: 0.737459\n",
      "Epoch: 1194 \tTraining Loss: 0.243078 \tR2: 0.737459\n",
      "Epoch: 1195 \tTraining Loss: 0.298762 \tR2: 0.737459\n",
      "Epoch: 1196 \tTraining Loss: 0.270217 \tR2: 0.737459\n",
      "Epoch: 1197 \tTraining Loss: 0.241164 \tR2: 0.737459\n",
      "Epoch: 1198 \tTraining Loss: 0.247266 \tR2: 0.737459\n",
      "Epoch: 1199 \tTraining Loss: 0.235276 \tR2: 0.737459\n",
      "Epoch: 1200 \tTraining Loss: 0.255454 \tR2: 0.345884\n",
      "Epoch: 1201 \tTraining Loss: 0.252083 \tR2: 0.345884\n",
      "Epoch: 1202 \tTraining Loss: 0.245480 \tR2: 0.345884\n",
      "Epoch: 1203 \tTraining Loss: 0.242972 \tR2: 0.345884\n",
      "Epoch: 1204 \tTraining Loss: 0.256991 \tR2: 0.345884\n",
      "Epoch: 1205 \tTraining Loss: 0.230447 \tR2: 0.345884\n",
      "Epoch: 1206 \tTraining Loss: 0.253602 \tR2: 0.345884\n",
      "Epoch: 1207 \tTraining Loss: 0.230499 \tR2: 0.345884\n",
      "Epoch: 1208 \tTraining Loss: 0.252264 \tR2: 0.345884\n",
      "Epoch: 1209 \tTraining Loss: 0.256553 \tR2: 0.345884\n",
      "Epoch: 1210 \tTraining Loss: 0.251570 \tR2: 0.345884\n",
      "Epoch: 1211 \tTraining Loss: 0.270725 \tR2: 0.345884\n",
      "Epoch: 1212 \tTraining Loss: 0.252401 \tR2: 0.345884\n",
      "Epoch: 1213 \tTraining Loss: 0.265118 \tR2: 0.345884\n",
      "Epoch: 1214 \tTraining Loss: 0.280786 \tR2: 0.345884\n",
      "Epoch: 1215 \tTraining Loss: 0.260519 \tR2: 0.345884\n",
      "Epoch: 1216 \tTraining Loss: 0.229941 \tR2: 0.345884\n",
      "Epoch: 1217 \tTraining Loss: 0.237507 \tR2: 0.345884\n",
      "Epoch: 1218 \tTraining Loss: 0.239764 \tR2: 0.345884\n",
      "Epoch: 1219 \tTraining Loss: 0.248447 \tR2: 0.345884\n",
      "Epoch: 1220 \tTraining Loss: 0.248355 \tR2: 0.345884\n",
      "Epoch: 1221 \tTraining Loss: 0.250145 \tR2: 0.345884\n",
      "Epoch: 1222 \tTraining Loss: 0.249736 \tR2: 0.345884\n",
      "Epoch: 1223 \tTraining Loss: 0.251915 \tR2: 0.345884\n",
      "Epoch: 1224 \tTraining Loss: 0.241019 \tR2: 0.345884\n",
      "Epoch: 1225 \tTraining Loss: 0.254420 \tR2: 0.345884\n",
      "Epoch: 1226 \tTraining Loss: 0.265685 \tR2: 0.345884\n",
      "Epoch: 1227 \tTraining Loss: 0.247203 \tR2: 0.345884\n",
      "Epoch: 1228 \tTraining Loss: 0.249937 \tR2: 0.345884\n",
      "Epoch: 1229 \tTraining Loss: 0.260378 \tR2: 0.345884\n",
      "Epoch: 1230 \tTraining Loss: 0.254968 \tR2: 0.345884\n",
      "Epoch: 1231 \tTraining Loss: 0.249435 \tR2: 0.345884\n",
      "Epoch: 1232 \tTraining Loss: 0.234576 \tR2: 0.345884\n",
      "Epoch: 1233 \tTraining Loss: 0.268158 \tR2: 0.345884\n",
      "Epoch: 1234 \tTraining Loss: 0.259986 \tR2: 0.345884\n",
      "Epoch: 1235 \tTraining Loss: 0.251138 \tR2: 0.345884\n",
      "Epoch: 1236 \tTraining Loss: 0.269364 \tR2: 0.345884\n",
      "Epoch: 1237 \tTraining Loss: 0.248701 \tR2: 0.345884\n",
      "Epoch: 1238 \tTraining Loss: 0.241629 \tR2: 0.345884\n",
      "Epoch: 1239 \tTraining Loss: 0.278770 \tR2: 0.345884\n",
      "Epoch: 1240 \tTraining Loss: 0.233074 \tR2: 0.345884\n",
      "Epoch: 1241 \tTraining Loss: 0.267845 \tR2: 0.345884\n",
      "Epoch: 1242 \tTraining Loss: 0.259110 \tR2: 0.345884\n",
      "Epoch: 1243 \tTraining Loss: 0.272655 \tR2: 0.345884\n",
      "Epoch: 1244 \tTraining Loss: 0.251213 \tR2: 0.345884\n",
      "Epoch: 1245 \tTraining Loss: 0.258504 \tR2: 0.345884\n",
      "Epoch: 1246 \tTraining Loss: 0.225663 \tR2: 0.345884\n",
      "Epoch: 1247 \tTraining Loss: 0.250383 \tR2: 0.345884\n",
      "Epoch: 1248 \tTraining Loss: 0.272087 \tR2: 0.345884\n",
      "Epoch: 1249 \tTraining Loss: 0.253190 \tR2: 0.345884\n",
      "Epoch: 1250 \tTraining Loss: 0.295452 \tR2: 0.345884\n",
      "Epoch: 1251 \tTraining Loss: 0.273020 \tR2: 0.345884\n",
      "Epoch: 1252 \tTraining Loss: 0.271398 \tR2: 0.345884\n",
      "Epoch: 1253 \tTraining Loss: 0.254872 \tR2: 0.345884\n",
      "Epoch: 1254 \tTraining Loss: 0.266147 \tR2: 0.345884\n",
      "Epoch: 1255 \tTraining Loss: 0.254533 \tR2: 0.345884\n",
      "Epoch: 1256 \tTraining Loss: 0.265909 \tR2: 0.345884\n",
      "Epoch: 1257 \tTraining Loss: 0.262635 \tR2: 0.345884\n",
      "Epoch: 1258 \tTraining Loss: 0.240888 \tR2: 0.345884\n",
      "Epoch: 1259 \tTraining Loss: 0.267646 \tR2: 0.345884\n",
      "Epoch: 1260 \tTraining Loss: 0.253623 \tR2: 0.345884\n",
      "Epoch: 1261 \tTraining Loss: 0.256883 \tR2: 0.345884\n",
      "Epoch: 1262 \tTraining Loss: 0.259720 \tR2: 0.345884\n",
      "Epoch: 1263 \tTraining Loss: 0.255741 \tR2: 0.345884\n",
      "Epoch: 1264 \tTraining Loss: 0.271431 \tR2: 0.345884\n",
      "Epoch: 1265 \tTraining Loss: 0.233145 \tR2: 0.345884\n",
      "Epoch: 1266 \tTraining Loss: 0.252966 \tR2: 0.345884\n",
      "Epoch: 1267 \tTraining Loss: 0.230687 \tR2: 0.345884\n",
      "Epoch: 1268 \tTraining Loss: 0.246134 \tR2: 0.345884\n",
      "Epoch: 1269 \tTraining Loss: 0.275086 \tR2: 0.345884\n",
      "Epoch: 1270 \tTraining Loss: 0.255390 \tR2: 0.345884\n",
      "Epoch: 1271 \tTraining Loss: 0.248128 \tR2: 0.345884\n",
      "Epoch: 1272 \tTraining Loss: 0.249848 \tR2: 0.345884\n",
      "Epoch: 1273 \tTraining Loss: 0.262375 \tR2: 0.345884\n",
      "Epoch: 1274 \tTraining Loss: 0.225923 \tR2: 0.345884\n",
      "Epoch: 1275 \tTraining Loss: 0.252991 \tR2: 0.345884\n",
      "Epoch: 1276 \tTraining Loss: 0.257981 \tR2: 0.345884\n",
      "Epoch: 1277 \tTraining Loss: 0.261984 \tR2: 0.345884\n",
      "Epoch: 1278 \tTraining Loss: 0.266580 \tR2: 0.345884\n",
      "Epoch: 1279 \tTraining Loss: 0.230648 \tR2: 0.345884\n",
      "Epoch: 1280 \tTraining Loss: 0.269602 \tR2: 0.345884\n",
      "Epoch: 1281 \tTraining Loss: 0.229137 \tR2: 0.345884\n",
      "Epoch: 1282 \tTraining Loss: 0.262926 \tR2: 0.345884\n",
      "Epoch: 1283 \tTraining Loss: 0.241564 \tR2: 0.345884\n",
      "Epoch: 1284 \tTraining Loss: 0.242679 \tR2: 0.345884\n",
      "Epoch: 1285 \tTraining Loss: 0.259553 \tR2: 0.345884\n",
      "Epoch: 1286 \tTraining Loss: 0.246562 \tR2: 0.345884\n",
      "Epoch: 1287 \tTraining Loss: 0.249041 \tR2: 0.345884\n",
      "Epoch: 1288 \tTraining Loss: 0.243518 \tR2: 0.345884\n",
      "Epoch: 1289 \tTraining Loss: 0.236604 \tR2: 0.345884\n",
      "Epoch: 1290 \tTraining Loss: 0.249205 \tR2: 0.345884\n",
      "Epoch: 1291 \tTraining Loss: 0.249111 \tR2: 0.345884\n",
      "Epoch: 1292 \tTraining Loss: 0.246692 \tR2: 0.345884\n",
      "Epoch: 1293 \tTraining Loss: 0.266333 \tR2: 0.345884\n",
      "Epoch: 1294 \tTraining Loss: 0.239482 \tR2: 0.345884\n",
      "Epoch: 1295 \tTraining Loss: 0.248701 \tR2: 0.345884\n",
      "Epoch: 1296 \tTraining Loss: 0.249049 \tR2: 0.345884\n",
      "Epoch: 1297 \tTraining Loss: 0.267651 \tR2: 0.345884\n",
      "Epoch: 1298 \tTraining Loss: 0.234942 \tR2: 0.345884\n",
      "Epoch: 1299 \tTraining Loss: 0.246931 \tR2: 0.345884\n",
      "Epoch: 1300 \tTraining Loss: 0.266772 \tR2: 0.775216\n",
      "Epoch: 1301 \tTraining Loss: 0.263544 \tR2: 0.775216\n",
      "Epoch: 1302 \tTraining Loss: 0.254111 \tR2: 0.775216\n",
      "Epoch: 1303 \tTraining Loss: 0.264864 \tR2: 0.775216\n",
      "Epoch: 1304 \tTraining Loss: 0.251109 \tR2: 0.775216\n",
      "Epoch: 1305 \tTraining Loss: 0.262656 \tR2: 0.775216\n",
      "Epoch: 1306 \tTraining Loss: 0.247817 \tR2: 0.775216\n",
      "Epoch: 1307 \tTraining Loss: 0.267858 \tR2: 0.775216\n",
      "Epoch: 1308 \tTraining Loss: 0.230177 \tR2: 0.775216\n",
      "Epoch: 1309 \tTraining Loss: 0.264217 \tR2: 0.775216\n",
      "Epoch: 1310 \tTraining Loss: 0.250851 \tR2: 0.775216\n",
      "Epoch: 1311 \tTraining Loss: 0.277833 \tR2: 0.775216\n",
      "Epoch: 1312 \tTraining Loss: 0.267512 \tR2: 0.775216\n",
      "Epoch: 1313 \tTraining Loss: 0.231548 \tR2: 0.775216\n",
      "Epoch: 1314 \tTraining Loss: 0.239643 \tR2: 0.775216\n",
      "Epoch: 1315 \tTraining Loss: 0.254169 \tR2: 0.775216\n",
      "Epoch: 1316 \tTraining Loss: 0.236797 \tR2: 0.775216\n",
      "Epoch: 1317 \tTraining Loss: 0.256699 \tR2: 0.775216\n",
      "Epoch: 1318 \tTraining Loss: 0.253719 \tR2: 0.775216\n",
      "Epoch: 1319 \tTraining Loss: 0.259245 \tR2: 0.775216\n",
      "Epoch: 1320 \tTraining Loss: 0.257185 \tR2: 0.775216\n",
      "Epoch: 1321 \tTraining Loss: 0.256811 \tR2: 0.775216\n",
      "Epoch: 1322 \tTraining Loss: 0.228540 \tR2: 0.775216\n",
      "Epoch: 1323 \tTraining Loss: 0.265276 \tR2: 0.775216\n",
      "Epoch: 1324 \tTraining Loss: 0.262215 \tR2: 0.775216\n",
      "Epoch: 1325 \tTraining Loss: 0.234189 \tR2: 0.775216\n",
      "Epoch: 1326 \tTraining Loss: 0.248512 \tR2: 0.775216\n",
      "Epoch: 1327 \tTraining Loss: 0.259811 \tR2: 0.775216\n",
      "Epoch: 1328 \tTraining Loss: 0.257438 \tR2: 0.775216\n",
      "Epoch: 1329 \tTraining Loss: 0.243275 \tR2: 0.775216\n",
      "Epoch: 1330 \tTraining Loss: 0.234411 \tR2: 0.775216\n",
      "Epoch: 1331 \tTraining Loss: 0.253006 \tR2: 0.775216\n",
      "Epoch: 1332 \tTraining Loss: 0.240290 \tR2: 0.775216\n",
      "Epoch: 1333 \tTraining Loss: 0.235141 \tR2: 0.775216\n",
      "Epoch: 1334 \tTraining Loss: 0.251880 \tR2: 0.775216\n",
      "Epoch: 1335 \tTraining Loss: 0.244946 \tR2: 0.775216\n",
      "Epoch: 1336 \tTraining Loss: 0.259359 \tR2: 0.775216\n",
      "Epoch: 1337 \tTraining Loss: 0.227544 \tR2: 0.775216\n",
      "Epoch: 1338 \tTraining Loss: 0.258465 \tR2: 0.775216\n",
      "Epoch: 1339 \tTraining Loss: 0.254285 \tR2: 0.775216\n",
      "Epoch: 1340 \tTraining Loss: 0.237436 \tR2: 0.775216\n",
      "Epoch: 1341 \tTraining Loss: 0.231083 \tR2: 0.775216\n",
      "Epoch: 1342 \tTraining Loss: 0.230815 \tR2: 0.775216\n",
      "Epoch: 1343 \tTraining Loss: 0.238589 \tR2: 0.775216\n",
      "Epoch: 1344 \tTraining Loss: 0.242405 \tR2: 0.775216\n",
      "Epoch: 1345 \tTraining Loss: 0.258272 \tR2: 0.775216\n",
      "Epoch: 1346 \tTraining Loss: 0.271032 \tR2: 0.775216\n",
      "Epoch: 1347 \tTraining Loss: 0.234914 \tR2: 0.775216\n",
      "Epoch: 1348 \tTraining Loss: 0.234337 \tR2: 0.775216\n",
      "Epoch: 1349 \tTraining Loss: 0.239297 \tR2: 0.775216\n",
      "Epoch: 1350 \tTraining Loss: 0.244790 \tR2: 0.775216\n",
      "Epoch: 1351 \tTraining Loss: 0.251982 \tR2: 0.775216\n",
      "Epoch: 1352 \tTraining Loss: 0.249683 \tR2: 0.775216\n",
      "Epoch: 1353 \tTraining Loss: 0.236477 \tR2: 0.775216\n",
      "Epoch: 1354 \tTraining Loss: 0.239583 \tR2: 0.775216\n",
      "Epoch: 1355 \tTraining Loss: 0.250952 \tR2: 0.775216\n",
      "Epoch: 1356 \tTraining Loss: 0.242866 \tR2: 0.775216\n",
      "Epoch: 1357 \tTraining Loss: 0.267255 \tR2: 0.775216\n",
      "Epoch: 1358 \tTraining Loss: 0.245581 \tR2: 0.775216\n",
      "Epoch: 1359 \tTraining Loss: 0.240907 \tR2: 0.775216\n",
      "Epoch: 1360 \tTraining Loss: 0.241834 \tR2: 0.775216\n",
      "Epoch: 1361 \tTraining Loss: 0.249141 \tR2: 0.775216\n",
      "Epoch: 1362 \tTraining Loss: 0.254822 \tR2: 0.775216\n",
      "Epoch: 1363 \tTraining Loss: 0.253099 \tR2: 0.775216\n",
      "Epoch: 1364 \tTraining Loss: 0.247542 \tR2: 0.775216\n",
      "Epoch: 1365 \tTraining Loss: 0.261241 \tR2: 0.775216\n",
      "Epoch: 1366 \tTraining Loss: 0.251177 \tR2: 0.775216\n",
      "Epoch: 1367 \tTraining Loss: 0.254735 \tR2: 0.775216\n",
      "Epoch: 1368 \tTraining Loss: 0.246357 \tR2: 0.775216\n",
      "Epoch: 1369 \tTraining Loss: 0.277017 \tR2: 0.775216\n",
      "Epoch: 1370 \tTraining Loss: 0.254251 \tR2: 0.775216\n",
      "Epoch: 1371 \tTraining Loss: 0.244476 \tR2: 0.775216\n",
      "Epoch: 1372 \tTraining Loss: 0.252826 \tR2: 0.775216\n",
      "Epoch: 1373 \tTraining Loss: 0.248625 \tR2: 0.775216\n",
      "Epoch: 1374 \tTraining Loss: 0.253690 \tR2: 0.775216\n",
      "Epoch: 1375 \tTraining Loss: 0.224766 \tR2: 0.775216\n",
      "Epoch: 1376 \tTraining Loss: 0.233399 \tR2: 0.775216\n",
      "Epoch: 1377 \tTraining Loss: 0.253540 \tR2: 0.775216\n",
      "Epoch: 1378 \tTraining Loss: 0.246991 \tR2: 0.775216\n",
      "Epoch: 1379 \tTraining Loss: 0.229869 \tR2: 0.775216\n",
      "Epoch: 1380 \tTraining Loss: 0.244344 \tR2: 0.775216\n",
      "Epoch: 1381 \tTraining Loss: 0.215891 \tR2: 0.775216\n",
      "Epoch: 1382 \tTraining Loss: 0.260810 \tR2: 0.775216\n",
      "Epoch: 1383 \tTraining Loss: 0.245483 \tR2: 0.775216\n",
      "Epoch: 1384 \tTraining Loss: 0.256288 \tR2: 0.775216\n",
      "Epoch: 1385 \tTraining Loss: 0.254997 \tR2: 0.775216\n",
      "Epoch: 1386 \tTraining Loss: 0.234058 \tR2: 0.775216\n",
      "Epoch: 1387 \tTraining Loss: 0.248352 \tR2: 0.775216\n",
      "Epoch: 1388 \tTraining Loss: 0.241781 \tR2: 0.775216\n",
      "Epoch: 1389 \tTraining Loss: 0.251683 \tR2: 0.775216\n",
      "Epoch: 1390 \tTraining Loss: 0.250376 \tR2: 0.775216\n",
      "Epoch: 1391 \tTraining Loss: 0.244801 \tR2: 0.775216\n",
      "Epoch: 1392 \tTraining Loss: 0.233649 \tR2: 0.775216\n",
      "Epoch: 1393 \tTraining Loss: 0.255721 \tR2: 0.775216\n",
      "Epoch: 1394 \tTraining Loss: 0.269321 \tR2: 0.775216\n",
      "Epoch: 1395 \tTraining Loss: 0.245563 \tR2: 0.775216\n",
      "Epoch: 1396 \tTraining Loss: 0.241601 \tR2: 0.775216\n",
      "Epoch: 1397 \tTraining Loss: 0.254044 \tR2: 0.775216\n",
      "Epoch: 1398 \tTraining Loss: 0.252698 \tR2: 0.775216\n",
      "Epoch: 1399 \tTraining Loss: 0.225189 \tR2: 0.775216\n",
      "Epoch: 1400 \tTraining Loss: 0.237982 \tR2: 0.271027\n",
      "Epoch: 1401 \tTraining Loss: 0.254935 \tR2: 0.271027\n",
      "Epoch: 1402 \tTraining Loss: 0.247118 \tR2: 0.271027\n",
      "Epoch: 1403 \tTraining Loss: 0.239782 \tR2: 0.271027\n",
      "Epoch: 1404 \tTraining Loss: 0.243691 \tR2: 0.271027\n",
      "Epoch: 1405 \tTraining Loss: 0.250131 \tR2: 0.271027\n",
      "Epoch: 1406 \tTraining Loss: 0.260653 \tR2: 0.271027\n",
      "Epoch: 1407 \tTraining Loss: 0.276978 \tR2: 0.271027\n",
      "Epoch: 1408 \tTraining Loss: 0.243680 \tR2: 0.271027\n",
      "Epoch: 1409 \tTraining Loss: 0.248682 \tR2: 0.271027\n",
      "Epoch: 1410 \tTraining Loss: 0.255558 \tR2: 0.271027\n",
      "Epoch: 1411 \tTraining Loss: 0.224828 \tR2: 0.271027\n",
      "Epoch: 1412 \tTraining Loss: 0.232779 \tR2: 0.271027\n",
      "Epoch: 1413 \tTraining Loss: 0.245057 \tR2: 0.271027\n",
      "Epoch: 1414 \tTraining Loss: 0.266925 \tR2: 0.271027\n",
      "Epoch: 1415 \tTraining Loss: 0.282701 \tR2: 0.271027\n",
      "Epoch: 1416 \tTraining Loss: 0.253296 \tR2: 0.271027\n",
      "Epoch: 1417 \tTraining Loss: 0.246504 \tR2: 0.271027\n",
      "Epoch: 1418 \tTraining Loss: 0.246562 \tR2: 0.271027\n",
      "Epoch: 1419 \tTraining Loss: 0.258463 \tR2: 0.271027\n",
      "Epoch: 1420 \tTraining Loss: 0.253545 \tR2: 0.271027\n",
      "Epoch: 1421 \tTraining Loss: 0.251040 \tR2: 0.271027\n",
      "Epoch: 1422 \tTraining Loss: 0.262430 \tR2: 0.271027\n",
      "Epoch: 1423 \tTraining Loss: 0.238464 \tR2: 0.271027\n",
      "Epoch: 1424 \tTraining Loss: 0.238034 \tR2: 0.271027\n",
      "Epoch: 1425 \tTraining Loss: 0.238629 \tR2: 0.271027\n",
      "Epoch: 1426 \tTraining Loss: 0.227682 \tR2: 0.271027\n",
      "Epoch: 1427 \tTraining Loss: 0.234262 \tR2: 0.271027\n",
      "Epoch: 1428 \tTraining Loss: 0.251571 \tR2: 0.271027\n",
      "Epoch: 1429 \tTraining Loss: 0.237443 \tR2: 0.271027\n",
      "Epoch: 1430 \tTraining Loss: 0.251191 \tR2: 0.271027\n",
      "Epoch: 1431 \tTraining Loss: 0.235389 \tR2: 0.271027\n",
      "Epoch: 1432 \tTraining Loss: 0.244240 \tR2: 0.271027\n",
      "Epoch: 1433 \tTraining Loss: 0.261886 \tR2: 0.271027\n",
      "Epoch: 1434 \tTraining Loss: 0.237312 \tR2: 0.271027\n",
      "Epoch: 1435 \tTraining Loss: 0.262346 \tR2: 0.271027\n",
      "Epoch: 1436 \tTraining Loss: 0.236984 \tR2: 0.271027\n",
      "Epoch: 1437 \tTraining Loss: 0.244328 \tR2: 0.271027\n",
      "Epoch: 1438 \tTraining Loss: 0.236159 \tR2: 0.271027\n",
      "Epoch: 1439 \tTraining Loss: 0.240682 \tR2: 0.271027\n",
      "Epoch: 1440 \tTraining Loss: 0.277257 \tR2: 0.271027\n",
      "Epoch: 1441 \tTraining Loss: 0.269644 \tR2: 0.271027\n",
      "Epoch: 1442 \tTraining Loss: 0.245569 \tR2: 0.271027\n",
      "Epoch: 1443 \tTraining Loss: 0.225706 \tR2: 0.271027\n",
      "Epoch: 1444 \tTraining Loss: 0.248374 \tR2: 0.271027\n",
      "Epoch: 1445 \tTraining Loss: 0.228944 \tR2: 0.271027\n",
      "Epoch: 1446 \tTraining Loss: 0.245743 \tR2: 0.271027\n",
      "Epoch: 1447 \tTraining Loss: 0.244882 \tR2: 0.271027\n",
      "Epoch: 1448 \tTraining Loss: 0.253547 \tR2: 0.271027\n",
      "Epoch: 1449 \tTraining Loss: 0.221233 \tR2: 0.271027\n",
      "Epoch: 1450 \tTraining Loss: 0.255636 \tR2: 0.271027\n",
      "Epoch: 1451 \tTraining Loss: 0.222319 \tR2: 0.271027\n",
      "Epoch: 1452 \tTraining Loss: 0.252475 \tR2: 0.271027\n",
      "Epoch: 1453 \tTraining Loss: 0.235219 \tR2: 0.271027\n",
      "Epoch: 1454 \tTraining Loss: 0.240981 \tR2: 0.271027\n",
      "Epoch: 1455 \tTraining Loss: 0.254542 \tR2: 0.271027\n",
      "Epoch: 1456 \tTraining Loss: 0.260403 \tR2: 0.271027\n",
      "Epoch: 1457 \tTraining Loss: 0.259068 \tR2: 0.271027\n",
      "Epoch: 1458 \tTraining Loss: 0.251467 \tR2: 0.271027\n",
      "Epoch: 1459 \tTraining Loss: 0.237358 \tR2: 0.271027\n",
      "Epoch: 1460 \tTraining Loss: 0.238020 \tR2: 0.271027\n",
      "Epoch: 1461 \tTraining Loss: 0.233018 \tR2: 0.271027\n",
      "Epoch: 1462 \tTraining Loss: 0.240698 \tR2: 0.271027\n",
      "Epoch: 1463 \tTraining Loss: 0.261252 \tR2: 0.271027\n",
      "Epoch: 1464 \tTraining Loss: 0.245734 \tR2: 0.271027\n",
      "Epoch: 1465 \tTraining Loss: 0.241570 \tR2: 0.271027\n",
      "Epoch: 1466 \tTraining Loss: 0.242086 \tR2: 0.271027\n",
      "Epoch: 1467 \tTraining Loss: 0.223242 \tR2: 0.271027\n",
      "Epoch: 1468 \tTraining Loss: 0.270959 \tR2: 0.271027\n",
      "Epoch: 1469 \tTraining Loss: 0.234338 \tR2: 0.271027\n",
      "Epoch: 1470 \tTraining Loss: 0.249195 \tR2: 0.271027\n",
      "Epoch: 1471 \tTraining Loss: 0.274826 \tR2: 0.271027\n",
      "Epoch: 1472 \tTraining Loss: 0.222432 \tR2: 0.271027\n",
      "Epoch: 1473 \tTraining Loss: 0.247320 \tR2: 0.271027\n",
      "Epoch: 1474 \tTraining Loss: 0.242848 \tR2: 0.271027\n",
      "Epoch: 1475 \tTraining Loss: 0.228016 \tR2: 0.271027\n",
      "Epoch: 1476 \tTraining Loss: 0.241508 \tR2: 0.271027\n",
      "Epoch: 1477 \tTraining Loss: 0.225438 \tR2: 0.271027\n",
      "Epoch: 1478 \tTraining Loss: 0.255413 \tR2: 0.271027\n",
      "Epoch: 1479 \tTraining Loss: 0.243686 \tR2: 0.271027\n",
      "Epoch: 1480 \tTraining Loss: 0.259850 \tR2: 0.271027\n",
      "Epoch: 1481 \tTraining Loss: 0.247364 \tR2: 0.271027\n",
      "Epoch: 1482 \tTraining Loss: 0.245906 \tR2: 0.271027\n",
      "Epoch: 1483 \tTraining Loss: 0.255057 \tR2: 0.271027\n",
      "Epoch: 1484 \tTraining Loss: 0.267611 \tR2: 0.271027\n",
      "Epoch: 1485 \tTraining Loss: 0.236308 \tR2: 0.271027\n",
      "Epoch: 1486 \tTraining Loss: 0.246342 \tR2: 0.271027\n",
      "Epoch: 1487 \tTraining Loss: 0.246224 \tR2: 0.271027\n",
      "Epoch: 1488 \tTraining Loss: 0.238967 \tR2: 0.271027\n",
      "Epoch: 1489 \tTraining Loss: 0.226012 \tR2: 0.271027\n",
      "Epoch: 1490 \tTraining Loss: 0.232958 \tR2: 0.271027\n",
      "Epoch: 1491 \tTraining Loss: 0.246272 \tR2: 0.271027\n",
      "Epoch: 1492 \tTraining Loss: 0.231276 \tR2: 0.271027\n",
      "Epoch: 1493 \tTraining Loss: 0.227120 \tR2: 0.271027\n",
      "Epoch: 1494 \tTraining Loss: 0.232907 \tR2: 0.271027\n",
      "Epoch: 1495 \tTraining Loss: 0.244679 \tR2: 0.271027\n",
      "Epoch: 1496 \tTraining Loss: 0.264805 \tR2: 0.271027\n",
      "Epoch: 1497 \tTraining Loss: 0.222959 \tR2: 0.271027\n",
      "Epoch: 1498 \tTraining Loss: 0.237247 \tR2: 0.271027\n",
      "Epoch: 1499 \tTraining Loss: 0.246429 \tR2: 0.271027\n",
      "Epoch: 1500 \tTraining Loss: 0.242048 \tR2: 0.679403\n",
      "Epoch: 1501 \tTraining Loss: 0.229588 \tR2: 0.679403\n",
      "Epoch: 1502 \tTraining Loss: 0.245117 \tR2: 0.679403\n",
      "Epoch: 1503 \tTraining Loss: 0.239898 \tR2: 0.679403\n",
      "Epoch: 1504 \tTraining Loss: 0.247257 \tR2: 0.679403\n",
      "Epoch: 1505 \tTraining Loss: 0.247812 \tR2: 0.679403\n",
      "Epoch: 1506 \tTraining Loss: 0.250099 \tR2: 0.679403\n",
      "Epoch: 1507 \tTraining Loss: 0.234478 \tR2: 0.679403\n",
      "Epoch: 1508 \tTraining Loss: 0.236154 \tR2: 0.679403\n",
      "Epoch: 1509 \tTraining Loss: 0.248099 \tR2: 0.679403\n",
      "Epoch: 1510 \tTraining Loss: 0.244359 \tR2: 0.679403\n",
      "Epoch: 1511 \tTraining Loss: 0.248578 \tR2: 0.679403\n",
      "Epoch: 1512 \tTraining Loss: 0.221637 \tR2: 0.679403\n",
      "Epoch: 1513 \tTraining Loss: 0.243568 \tR2: 0.679403\n",
      "Epoch: 1514 \tTraining Loss: 0.230022 \tR2: 0.679403\n",
      "Epoch: 1515 \tTraining Loss: 0.222929 \tR2: 0.679403\n",
      "Epoch: 1516 \tTraining Loss: 0.231023 \tR2: 0.679403\n",
      "Epoch: 1517 \tTraining Loss: 0.265320 \tR2: 0.679403\n",
      "Epoch: 1518 \tTraining Loss: 0.248909 \tR2: 0.679403\n",
      "Epoch: 1519 \tTraining Loss: 0.272415 \tR2: 0.679403\n",
      "Epoch: 1520 \tTraining Loss: 0.234998 \tR2: 0.679403\n",
      "Epoch: 1521 \tTraining Loss: 0.232591 \tR2: 0.679403\n",
      "Epoch: 1522 \tTraining Loss: 0.231654 \tR2: 0.679403\n",
      "Epoch: 1523 \tTraining Loss: 0.253675 \tR2: 0.679403\n",
      "Epoch: 1524 \tTraining Loss: 0.254348 \tR2: 0.679403\n",
      "Epoch: 1525 \tTraining Loss: 0.250659 \tR2: 0.679403\n",
      "Epoch: 1526 \tTraining Loss: 0.232069 \tR2: 0.679403\n",
      "Epoch: 1527 \tTraining Loss: 0.232197 \tR2: 0.679403\n",
      "Epoch: 1528 \tTraining Loss: 0.250490 \tR2: 0.679403\n",
      "Epoch: 1529 \tTraining Loss: 0.233062 \tR2: 0.679403\n",
      "Epoch: 1530 \tTraining Loss: 0.228495 \tR2: 0.679403\n",
      "Epoch: 1531 \tTraining Loss: 0.259515 \tR2: 0.679403\n",
      "Epoch: 1532 \tTraining Loss: 0.254852 \tR2: 0.679403\n",
      "Epoch: 1533 \tTraining Loss: 0.246457 \tR2: 0.679403\n",
      "Epoch: 1534 \tTraining Loss: 0.246532 \tR2: 0.679403\n",
      "Epoch: 1535 \tTraining Loss: 0.238508 \tR2: 0.679403\n",
      "Epoch: 1536 \tTraining Loss: 0.254382 \tR2: 0.679403\n",
      "Epoch: 1537 \tTraining Loss: 0.239613 \tR2: 0.679403\n",
      "Epoch: 1538 \tTraining Loss: 0.271983 \tR2: 0.679403\n",
      "Epoch: 1539 \tTraining Loss: 0.244311 \tR2: 0.679403\n",
      "Epoch: 1540 \tTraining Loss: 0.235606 \tR2: 0.679403\n",
      "Epoch: 1541 \tTraining Loss: 0.242894 \tR2: 0.679403\n",
      "Epoch: 1542 \tTraining Loss: 0.238639 \tR2: 0.679403\n",
      "Epoch: 1543 \tTraining Loss: 0.229935 \tR2: 0.679403\n",
      "Epoch: 1544 \tTraining Loss: 0.260915 \tR2: 0.679403\n",
      "Epoch: 1545 \tTraining Loss: 0.258900 \tR2: 0.679403\n",
      "Epoch: 1546 \tTraining Loss: 0.226626 \tR2: 0.679403\n",
      "Epoch: 1547 \tTraining Loss: 0.258539 \tR2: 0.679403\n",
      "Epoch: 1548 \tTraining Loss: 0.227979 \tR2: 0.679403\n",
      "Epoch: 1549 \tTraining Loss: 0.233250 \tR2: 0.679403\n",
      "Epoch: 1550 \tTraining Loss: 0.255199 \tR2: 0.679403\n",
      "Epoch: 1551 \tTraining Loss: 0.235564 \tR2: 0.679403\n",
      "Epoch: 1552 \tTraining Loss: 0.221966 \tR2: 0.679403\n",
      "Epoch: 1553 \tTraining Loss: 0.255906 \tR2: 0.679403\n",
      "Epoch: 1554 \tTraining Loss: 0.225653 \tR2: 0.679403\n",
      "Epoch: 1555 \tTraining Loss: 0.276302 \tR2: 0.679403\n",
      "Epoch: 1556 \tTraining Loss: 0.224759 \tR2: 0.679403\n",
      "Epoch: 1557 \tTraining Loss: 0.242606 \tR2: 0.679403\n",
      "Epoch: 1558 \tTraining Loss: 0.236463 \tR2: 0.679403\n",
      "Epoch: 1559 \tTraining Loss: 0.222015 \tR2: 0.679403\n",
      "Epoch: 1560 \tTraining Loss: 0.251902 \tR2: 0.679403\n",
      "Epoch: 1561 \tTraining Loss: 0.229305 \tR2: 0.679403\n",
      "Epoch: 1562 \tTraining Loss: 0.241598 \tR2: 0.679403\n",
      "Epoch: 1563 \tTraining Loss: 0.249854 \tR2: 0.679403\n",
      "Epoch: 1564 \tTraining Loss: 0.264557 \tR2: 0.679403\n",
      "Epoch: 1565 \tTraining Loss: 0.229350 \tR2: 0.679403\n",
      "Epoch: 1566 \tTraining Loss: 0.244661 \tR2: 0.679403\n",
      "Epoch: 1567 \tTraining Loss: 0.248395 \tR2: 0.679403\n",
      "Epoch: 1568 \tTraining Loss: 0.242594 \tR2: 0.679403\n",
      "Epoch: 1569 \tTraining Loss: 0.240781 \tR2: 0.679403\n",
      "Epoch: 1570 \tTraining Loss: 0.252457 \tR2: 0.679403\n",
      "Epoch: 1571 \tTraining Loss: 0.238028 \tR2: 0.679403\n",
      "Epoch: 1572 \tTraining Loss: 0.226316 \tR2: 0.679403\n",
      "Epoch: 1573 \tTraining Loss: 0.250132 \tR2: 0.679403\n",
      "Epoch: 1574 \tTraining Loss: 0.226069 \tR2: 0.679403\n",
      "Epoch: 1575 \tTraining Loss: 0.231168 \tR2: 0.679403\n",
      "Epoch: 1576 \tTraining Loss: 0.241071 \tR2: 0.679403\n",
      "Epoch: 1577 \tTraining Loss: 0.240978 \tR2: 0.679403\n",
      "Epoch: 1578 \tTraining Loss: 0.232108 \tR2: 0.679403\n",
      "Epoch: 1579 \tTraining Loss: 0.251027 \tR2: 0.679403\n",
      "Epoch: 1580 \tTraining Loss: 0.240637 \tR2: 0.679403\n",
      "Epoch: 1581 \tTraining Loss: 0.218765 \tR2: 0.679403\n",
      "Epoch: 1582 \tTraining Loss: 0.257305 \tR2: 0.679403\n",
      "Epoch: 1583 \tTraining Loss: 0.244902 \tR2: 0.679403\n",
      "Epoch: 1584 \tTraining Loss: 0.243144 \tR2: 0.679403\n",
      "Epoch: 1585 \tTraining Loss: 0.220489 \tR2: 0.679403\n",
      "Epoch: 1586 \tTraining Loss: 0.241957 \tR2: 0.679403\n",
      "Epoch: 1587 \tTraining Loss: 0.254024 \tR2: 0.679403\n",
      "Epoch: 1588 \tTraining Loss: 0.240873 \tR2: 0.679403\n",
      "Epoch: 1589 \tTraining Loss: 0.245835 \tR2: 0.679403\n",
      "Epoch: 1590 \tTraining Loss: 0.241691 \tR2: 0.679403\n",
      "Epoch: 1591 \tTraining Loss: 0.231991 \tR2: 0.679403\n",
      "Epoch: 1592 \tTraining Loss: 0.239849 \tR2: 0.679403\n",
      "Epoch: 1593 \tTraining Loss: 0.255334 \tR2: 0.679403\n",
      "Epoch: 1594 \tTraining Loss: 0.246192 \tR2: 0.679403\n",
      "Epoch: 1595 \tTraining Loss: 0.228122 \tR2: 0.679403\n",
      "Epoch: 1596 \tTraining Loss: 0.249071 \tR2: 0.679403\n",
      "Epoch: 1597 \tTraining Loss: 0.231285 \tR2: 0.679403\n",
      "Epoch: 1598 \tTraining Loss: 0.238068 \tR2: 0.679403\n",
      "Epoch: 1599 \tTraining Loss: 0.226071 \tR2: 0.679403\n",
      "Epoch: 1600 \tTraining Loss: 0.249954 \tR2: 0.543028\n",
      "Epoch: 1601 \tTraining Loss: 0.257003 \tR2: 0.543028\n",
      "Epoch: 1602 \tTraining Loss: 0.247182 \tR2: 0.543028\n",
      "Epoch: 1603 \tTraining Loss: 0.241600 \tR2: 0.543028\n",
      "Epoch: 1604 \tTraining Loss: 0.232569 \tR2: 0.543028\n",
      "Epoch: 1605 \tTraining Loss: 0.233727 \tR2: 0.543028\n",
      "Epoch: 1606 \tTraining Loss: 0.228777 \tR2: 0.543028\n",
      "Epoch: 1607 \tTraining Loss: 0.276020 \tR2: 0.543028\n",
      "Epoch: 1608 \tTraining Loss: 0.243808 \tR2: 0.543028\n",
      "Epoch: 1609 \tTraining Loss: 0.248476 \tR2: 0.543028\n",
      "Epoch: 1610 \tTraining Loss: 0.255542 \tR2: 0.543028\n",
      "Epoch: 1611 \tTraining Loss: 0.247983 \tR2: 0.543028\n",
      "Epoch: 1612 \tTraining Loss: 0.238616 \tR2: 0.543028\n",
      "Epoch: 1613 \tTraining Loss: 0.241510 \tR2: 0.543028\n",
      "Epoch: 1614 \tTraining Loss: 0.253424 \tR2: 0.543028\n",
      "Epoch: 1615 \tTraining Loss: 0.231101 \tR2: 0.543028\n",
      "Epoch: 1616 \tTraining Loss: 0.221144 \tR2: 0.543028\n",
      "Epoch: 1617 \tTraining Loss: 0.221877 \tR2: 0.543028\n",
      "Epoch: 1618 \tTraining Loss: 0.238120 \tR2: 0.543028\n",
      "Epoch: 1619 \tTraining Loss: 0.225390 \tR2: 0.543028\n",
      "Epoch: 1620 \tTraining Loss: 0.224034 \tR2: 0.543028\n",
      "Epoch: 1621 \tTraining Loss: 0.221515 \tR2: 0.543028\n",
      "Epoch: 1622 \tTraining Loss: 0.236462 \tR2: 0.543028\n",
      "Epoch: 1623 \tTraining Loss: 0.241869 \tR2: 0.543028\n",
      "Epoch: 1624 \tTraining Loss: 0.239133 \tR2: 0.543028\n",
      "Epoch: 1625 \tTraining Loss: 0.249539 \tR2: 0.543028\n",
      "Epoch: 1626 \tTraining Loss: 0.216602 \tR2: 0.543028\n",
      "Epoch: 1627 \tTraining Loss: 0.225082 \tR2: 0.543028\n",
      "Epoch: 1628 \tTraining Loss: 0.254091 \tR2: 0.543028\n",
      "Epoch: 1629 \tTraining Loss: 0.241045 \tR2: 0.543028\n",
      "Epoch: 1630 \tTraining Loss: 0.247837 \tR2: 0.543028\n",
      "Epoch: 1631 \tTraining Loss: 0.233344 \tR2: 0.543028\n",
      "Epoch: 1632 \tTraining Loss: 0.224445 \tR2: 0.543028\n",
      "Epoch: 1633 \tTraining Loss: 0.229204 \tR2: 0.543028\n",
      "Epoch: 1634 \tTraining Loss: 0.275423 \tR2: 0.543028\n",
      "Epoch: 1635 \tTraining Loss: 0.244832 \tR2: 0.543028\n",
      "Epoch: 1636 \tTraining Loss: 0.228670 \tR2: 0.543028\n",
      "Epoch: 1637 \tTraining Loss: 0.237805 \tR2: 0.543028\n",
      "Epoch: 1638 \tTraining Loss: 0.241946 \tR2: 0.543028\n",
      "Epoch: 1639 \tTraining Loss: 0.236213 \tR2: 0.543028\n",
      "Epoch: 1640 \tTraining Loss: 0.233787 \tR2: 0.543028\n",
      "Epoch: 1641 \tTraining Loss: 0.246412 \tR2: 0.543028\n",
      "Epoch: 1642 \tTraining Loss: 0.243227 \tR2: 0.543028\n",
      "Epoch: 1643 \tTraining Loss: 0.253276 \tR2: 0.543028\n",
      "Epoch: 1644 \tTraining Loss: 0.239375 \tR2: 0.543028\n",
      "Epoch: 1645 \tTraining Loss: 0.261218 \tR2: 0.543028\n",
      "Epoch: 1646 \tTraining Loss: 0.232490 \tR2: 0.543028\n",
      "Epoch: 1647 \tTraining Loss: 0.269821 \tR2: 0.543028\n",
      "Epoch: 1648 \tTraining Loss: 0.264279 \tR2: 0.543028\n",
      "Epoch: 1649 \tTraining Loss: 0.236504 \tR2: 0.543028\n",
      "Epoch: 1650 \tTraining Loss: 0.251791 \tR2: 0.543028\n",
      "Epoch: 1651 \tTraining Loss: 0.233503 \tR2: 0.543028\n",
      "Epoch: 1652 \tTraining Loss: 0.277285 \tR2: 0.543028\n",
      "Epoch: 1653 \tTraining Loss: 0.232565 \tR2: 0.543028\n",
      "Epoch: 1654 \tTraining Loss: 0.236627 \tR2: 0.543028\n",
      "Epoch: 1655 \tTraining Loss: 0.258157 \tR2: 0.543028\n",
      "Epoch: 1656 \tTraining Loss: 0.237982 \tR2: 0.543028\n",
      "Epoch: 1657 \tTraining Loss: 0.262163 \tR2: 0.543028\n",
      "Epoch: 1658 \tTraining Loss: 0.221639 \tR2: 0.543028\n",
      "Epoch: 1659 \tTraining Loss: 0.225899 \tR2: 0.543028\n",
      "Epoch: 1660 \tTraining Loss: 0.242716 \tR2: 0.543028\n",
      "Epoch: 1661 \tTraining Loss: 0.239908 \tR2: 0.543028\n",
      "Epoch: 1662 \tTraining Loss: 0.232005 \tR2: 0.543028\n",
      "Epoch: 1663 \tTraining Loss: 0.235881 \tR2: 0.543028\n",
      "Epoch: 1664 \tTraining Loss: 0.250707 \tR2: 0.543028\n",
      "Epoch: 1665 \tTraining Loss: 0.233704 \tR2: 0.543028\n",
      "Epoch: 1666 \tTraining Loss: 0.253618 \tR2: 0.543028\n",
      "Epoch: 1667 \tTraining Loss: 0.227270 \tR2: 0.543028\n",
      "Epoch: 1668 \tTraining Loss: 0.238878 \tR2: 0.543028\n",
      "Epoch: 1669 \tTraining Loss: 0.228490 \tR2: 0.543028\n",
      "Epoch: 1670 \tTraining Loss: 0.253754 \tR2: 0.543028\n",
      "Epoch: 1671 \tTraining Loss: 0.255811 \tR2: 0.543028\n",
      "Epoch: 1672 \tTraining Loss: 0.240057 \tR2: 0.543028\n",
      "Epoch: 1673 \tTraining Loss: 0.234360 \tR2: 0.543028\n",
      "Epoch: 1674 \tTraining Loss: 0.234805 \tR2: 0.543028\n",
      "Epoch: 1675 \tTraining Loss: 0.253713 \tR2: 0.543028\n",
      "Epoch: 1676 \tTraining Loss: 0.244037 \tR2: 0.543028\n",
      "Epoch: 1677 \tTraining Loss: 0.237307 \tR2: 0.543028\n",
      "Epoch: 1678 \tTraining Loss: 0.240475 \tR2: 0.543028\n",
      "Epoch: 1679 \tTraining Loss: 0.222382 \tR2: 0.543028\n",
      "Epoch: 1680 \tTraining Loss: 0.255851 \tR2: 0.543028\n",
      "Epoch: 1681 \tTraining Loss: 0.258727 \tR2: 0.543028\n",
      "Epoch: 1682 \tTraining Loss: 0.236080 \tR2: 0.543028\n",
      "Epoch: 1683 \tTraining Loss: 0.245930 \tR2: 0.543028\n",
      "Epoch: 1684 \tTraining Loss: 0.246970 \tR2: 0.543028\n",
      "Epoch: 1685 \tTraining Loss: 0.234049 \tR2: 0.543028\n",
      "Epoch: 1686 \tTraining Loss: 0.225063 \tR2: 0.543028\n",
      "Epoch: 1687 \tTraining Loss: 0.236616 \tR2: 0.543028\n",
      "Epoch: 1688 \tTraining Loss: 0.252693 \tR2: 0.543028\n",
      "Epoch: 1689 \tTraining Loss: 0.215104 \tR2: 0.543028\n",
      "Epoch: 1690 \tTraining Loss: 0.250956 \tR2: 0.543028\n",
      "Epoch: 1691 \tTraining Loss: 0.222861 \tR2: 0.543028\n",
      "Epoch: 1692 \tTraining Loss: 0.208155 \tR2: 0.543028\n",
      "Epoch: 1693 \tTraining Loss: 0.242677 \tR2: 0.543028\n",
      "Epoch: 1694 \tTraining Loss: 0.227146 \tR2: 0.543028\n",
      "Epoch: 1695 \tTraining Loss: 0.228028 \tR2: 0.543028\n",
      "Epoch: 1696 \tTraining Loss: 0.229786 \tR2: 0.543028\n",
      "Epoch: 1697 \tTraining Loss: 0.234985 \tR2: 0.543028\n",
      "Epoch: 1698 \tTraining Loss: 0.238799 \tR2: 0.543028\n",
      "Epoch: 1699 \tTraining Loss: 0.234240 \tR2: 0.543028\n",
      "Epoch: 1700 \tTraining Loss: 0.232539 \tR2: 0.608766\n",
      "Epoch: 1701 \tTraining Loss: 0.235628 \tR2: 0.608766\n",
      "Epoch: 1702 \tTraining Loss: 0.232853 \tR2: 0.608766\n",
      "Epoch: 1703 \tTraining Loss: 0.232025 \tR2: 0.608766\n",
      "Epoch: 1704 \tTraining Loss: 0.235972 \tR2: 0.608766\n",
      "Epoch: 1705 \tTraining Loss: 0.201605 \tR2: 0.608766\n",
      "Epoch: 1706 \tTraining Loss: 0.257756 \tR2: 0.608766\n",
      "Epoch: 1707 \tTraining Loss: 0.239583 \tR2: 0.608766\n",
      "Epoch: 1708 \tTraining Loss: 0.232722 \tR2: 0.608766\n",
      "Epoch: 1709 \tTraining Loss: 0.239101 \tR2: 0.608766\n",
      "Epoch: 1710 \tTraining Loss: 0.224621 \tR2: 0.608766\n",
      "Epoch: 1711 \tTraining Loss: 0.226782 \tR2: 0.608766\n",
      "Epoch: 1712 \tTraining Loss: 0.226613 \tR2: 0.608766\n",
      "Epoch: 1713 \tTraining Loss: 0.243830 \tR2: 0.608766\n",
      "Epoch: 1714 \tTraining Loss: 0.250457 \tR2: 0.608766\n",
      "Epoch: 1715 \tTraining Loss: 0.214341 \tR2: 0.608766\n",
      "Epoch: 1716 \tTraining Loss: 0.241088 \tR2: 0.608766\n",
      "Epoch: 1717 \tTraining Loss: 0.230753 \tR2: 0.608766\n",
      "Epoch: 1718 \tTraining Loss: 0.234126 \tR2: 0.608766\n",
      "Epoch: 1719 \tTraining Loss: 0.262459 \tR2: 0.608766\n",
      "Epoch: 1720 \tTraining Loss: 0.267191 \tR2: 0.608766\n",
      "Epoch: 1721 \tTraining Loss: 0.237657 \tR2: 0.608766\n",
      "Epoch: 1722 \tTraining Loss: 0.262910 \tR2: 0.608766\n",
      "Epoch: 1723 \tTraining Loss: 0.242261 \tR2: 0.608766\n",
      "Epoch: 1724 \tTraining Loss: 0.263169 \tR2: 0.608766\n",
      "Epoch: 1725 \tTraining Loss: 0.228749 \tR2: 0.608766\n",
      "Epoch: 1726 \tTraining Loss: 0.250229 \tR2: 0.608766\n",
      "Epoch: 1727 \tTraining Loss: 0.233647 \tR2: 0.608766\n",
      "Epoch: 1728 \tTraining Loss: 0.228793 \tR2: 0.608766\n",
      "Epoch: 1729 \tTraining Loss: 0.229698 \tR2: 0.608766\n",
      "Epoch: 1730 \tTraining Loss: 0.235329 \tR2: 0.608766\n",
      "Epoch: 1731 \tTraining Loss: 0.230612 \tR2: 0.608766\n",
      "Epoch: 1732 \tTraining Loss: 0.260549 \tR2: 0.608766\n",
      "Epoch: 1733 \tTraining Loss: 0.256772 \tR2: 0.608766\n",
      "Epoch: 1734 \tTraining Loss: 0.236819 \tR2: 0.608766\n",
      "Epoch: 1735 \tTraining Loss: 0.237632 \tR2: 0.608766\n",
      "Epoch: 1736 \tTraining Loss: 0.245884 \tR2: 0.608766\n",
      "Epoch: 1737 \tTraining Loss: 0.229195 \tR2: 0.608766\n",
      "Epoch: 1738 \tTraining Loss: 0.225716 \tR2: 0.608766\n",
      "Epoch: 1739 \tTraining Loss: 0.232070 \tR2: 0.608766\n",
      "Epoch: 1740 \tTraining Loss: 0.241058 \tR2: 0.608766\n",
      "Epoch: 1741 \tTraining Loss: 0.256905 \tR2: 0.608766\n",
      "Epoch: 1742 \tTraining Loss: 0.241657 \tR2: 0.608766\n",
      "Epoch: 1743 \tTraining Loss: 0.230011 \tR2: 0.608766\n",
      "Epoch: 1744 \tTraining Loss: 0.244199 \tR2: 0.608766\n",
      "Epoch: 1745 \tTraining Loss: 0.244178 \tR2: 0.608766\n",
      "Epoch: 1746 \tTraining Loss: 0.223520 \tR2: 0.608766\n",
      "Epoch: 1747 \tTraining Loss: 0.218827 \tR2: 0.608766\n",
      "Epoch: 1748 \tTraining Loss: 0.248244 \tR2: 0.608766\n",
      "Epoch: 1749 \tTraining Loss: 0.229528 \tR2: 0.608766\n",
      "Epoch: 1750 \tTraining Loss: 0.262063 \tR2: 0.608766\n",
      "Epoch: 1751 \tTraining Loss: 0.238983 \tR2: 0.608766\n",
      "Epoch: 1752 \tTraining Loss: 0.238802 \tR2: 0.608766\n",
      "Epoch: 1753 \tTraining Loss: 0.216302 \tR2: 0.608766\n",
      "Epoch: 1754 \tTraining Loss: 0.255042 \tR2: 0.608766\n",
      "Epoch: 1755 \tTraining Loss: 0.224849 \tR2: 0.608766\n",
      "Epoch: 1756 \tTraining Loss: 0.256061 \tR2: 0.608766\n",
      "Epoch: 1757 \tTraining Loss: 0.222843 \tR2: 0.608766\n",
      "Epoch: 1758 \tTraining Loss: 0.236165 \tR2: 0.608766\n",
      "Epoch: 1759 \tTraining Loss: 0.238404 \tR2: 0.608766\n",
      "Epoch: 1760 \tTraining Loss: 0.235846 \tR2: 0.608766\n",
      "Epoch: 1761 \tTraining Loss: 0.230189 \tR2: 0.608766\n",
      "Epoch: 1762 \tTraining Loss: 0.228284 \tR2: 0.608766\n",
      "Epoch: 1763 \tTraining Loss: 0.220062 \tR2: 0.608766\n",
      "Epoch: 1764 \tTraining Loss: 0.263159 \tR2: 0.608766\n",
      "Epoch: 1765 \tTraining Loss: 0.232076 \tR2: 0.608766\n",
      "Epoch: 1766 \tTraining Loss: 0.241041 \tR2: 0.608766\n",
      "Epoch: 1767 \tTraining Loss: 0.243940 \tR2: 0.608766\n",
      "Epoch: 1768 \tTraining Loss: 0.238617 \tR2: 0.608766\n",
      "Epoch: 1769 \tTraining Loss: 0.237275 \tR2: 0.608766\n",
      "Epoch: 1770 \tTraining Loss: 0.258579 \tR2: 0.608766\n",
      "Epoch: 1771 \tTraining Loss: 0.217512 \tR2: 0.608766\n",
      "Epoch: 1772 \tTraining Loss: 0.217466 \tR2: 0.608766\n",
      "Epoch: 1773 \tTraining Loss: 0.255362 \tR2: 0.608766\n",
      "Epoch: 1774 \tTraining Loss: 0.216694 \tR2: 0.608766\n",
      "Epoch: 1775 \tTraining Loss: 0.230232 \tR2: 0.608766\n",
      "Epoch: 1776 \tTraining Loss: 0.229740 \tR2: 0.608766\n",
      "Epoch: 1777 \tTraining Loss: 0.240818 \tR2: 0.608766\n",
      "Epoch: 1778 \tTraining Loss: 0.232880 \tR2: 0.608766\n",
      "Epoch: 1779 \tTraining Loss: 0.220015 \tR2: 0.608766\n",
      "Epoch: 1780 \tTraining Loss: 0.245125 \tR2: 0.608766\n",
      "Epoch: 1781 \tTraining Loss: 0.248868 \tR2: 0.608766\n",
      "Epoch: 1782 \tTraining Loss: 0.239729 \tR2: 0.608766\n",
      "Epoch: 1783 \tTraining Loss: 0.244591 \tR2: 0.608766\n",
      "Epoch: 1784 \tTraining Loss: 0.234042 \tR2: 0.608766\n",
      "Epoch: 1785 \tTraining Loss: 0.246282 \tR2: 0.608766\n",
      "Epoch: 1786 \tTraining Loss: 0.231803 \tR2: 0.608766\n",
      "Epoch: 1787 \tTraining Loss: 0.243940 \tR2: 0.608766\n",
      "Epoch: 1788 \tTraining Loss: 0.216588 \tR2: 0.608766\n",
      "Epoch: 1789 \tTraining Loss: 0.222249 \tR2: 0.608766\n",
      "Epoch: 1790 \tTraining Loss: 0.220326 \tR2: 0.608766\n",
      "Epoch: 1791 \tTraining Loss: 0.220441 \tR2: 0.608766\n",
      "Epoch: 1792 \tTraining Loss: 0.217910 \tR2: 0.608766\n",
      "Epoch: 1793 \tTraining Loss: 0.248319 \tR2: 0.608766\n",
      "Epoch: 1794 \tTraining Loss: 0.246265 \tR2: 0.608766\n",
      "Epoch: 1795 \tTraining Loss: 0.226699 \tR2: 0.608766\n",
      "Epoch: 1796 \tTraining Loss: 0.228359 \tR2: 0.608766\n",
      "Epoch: 1797 \tTraining Loss: 0.234509 \tR2: 0.608766\n",
      "Epoch: 1798 \tTraining Loss: 0.238686 \tR2: 0.608766\n",
      "Epoch: 1799 \tTraining Loss: 0.247170 \tR2: 0.608766\n",
      "Epoch: 1800 \tTraining Loss: 0.270359 \tR2: 0.743597\n",
      "Epoch: 1801 \tTraining Loss: 0.221910 \tR2: 0.743597\n",
      "Epoch: 1802 \tTraining Loss: 0.238591 \tR2: 0.743597\n",
      "Epoch: 1803 \tTraining Loss: 0.226615 \tR2: 0.743597\n",
      "Epoch: 1804 \tTraining Loss: 0.227929 \tR2: 0.743597\n",
      "Epoch: 1805 \tTraining Loss: 0.232421 \tR2: 0.743597\n",
      "Epoch: 1806 \tTraining Loss: 0.245306 \tR2: 0.743597\n",
      "Epoch: 1807 \tTraining Loss: 0.254615 \tR2: 0.743597\n",
      "Epoch: 1808 \tTraining Loss: 0.219464 \tR2: 0.743597\n",
      "Epoch: 1809 \tTraining Loss: 0.212693 \tR2: 0.743597\n",
      "Epoch: 1810 \tTraining Loss: 0.244474 \tR2: 0.743597\n",
      "Epoch: 1811 \tTraining Loss: 0.254005 \tR2: 0.743597\n",
      "Epoch: 1812 \tTraining Loss: 0.235194 \tR2: 0.743597\n",
      "Epoch: 1813 \tTraining Loss: 0.220072 \tR2: 0.743597\n",
      "Epoch: 1814 \tTraining Loss: 0.221252 \tR2: 0.743597\n",
      "Epoch: 1815 \tTraining Loss: 0.223400 \tR2: 0.743597\n",
      "Epoch: 1816 \tTraining Loss: 0.238051 \tR2: 0.743597\n",
      "Epoch: 1817 \tTraining Loss: 0.236519 \tR2: 0.743597\n",
      "Epoch: 1818 \tTraining Loss: 0.212998 \tR2: 0.743597\n",
      "Epoch: 1819 \tTraining Loss: 0.238007 \tR2: 0.743597\n",
      "Epoch: 1820 \tTraining Loss: 0.230825 \tR2: 0.743597\n",
      "Epoch: 1821 \tTraining Loss: 0.287884 \tR2: 0.743597\n",
      "Epoch: 1822 \tTraining Loss: 0.229711 \tR2: 0.743597\n",
      "Epoch: 1823 \tTraining Loss: 0.228993 \tR2: 0.743597\n",
      "Epoch: 1824 \tTraining Loss: 0.246181 \tR2: 0.743597\n",
      "Epoch: 1825 \tTraining Loss: 0.239503 \tR2: 0.743597\n",
      "Epoch: 1826 \tTraining Loss: 0.231099 \tR2: 0.743597\n",
      "Epoch: 1827 \tTraining Loss: 0.221834 \tR2: 0.743597\n",
      "Epoch: 1828 \tTraining Loss: 0.232091 \tR2: 0.743597\n",
      "Epoch: 1829 \tTraining Loss: 0.221802 \tR2: 0.743597\n",
      "Epoch: 1830 \tTraining Loss: 0.241726 \tR2: 0.743597\n",
      "Epoch: 1831 \tTraining Loss: 0.223648 \tR2: 0.743597\n",
      "Epoch: 1832 \tTraining Loss: 0.234269 \tR2: 0.743597\n",
      "Epoch: 1833 \tTraining Loss: 0.243562 \tR2: 0.743597\n",
      "Epoch: 1834 \tTraining Loss: 0.223692 \tR2: 0.743597\n",
      "Epoch: 1835 \tTraining Loss: 0.242367 \tR2: 0.743597\n",
      "Epoch: 1836 \tTraining Loss: 0.226166 \tR2: 0.743597\n",
      "Epoch: 1837 \tTraining Loss: 0.246291 \tR2: 0.743597\n",
      "Epoch: 1838 \tTraining Loss: 0.241719 \tR2: 0.743597\n",
      "Epoch: 1839 \tTraining Loss: 0.222861 \tR2: 0.743597\n",
      "Epoch: 1840 \tTraining Loss: 0.228379 \tR2: 0.743597\n",
      "Epoch: 1841 \tTraining Loss: 0.223610 \tR2: 0.743597\n",
      "Epoch: 1842 \tTraining Loss: 0.238864 \tR2: 0.743597\n",
      "Epoch: 1843 \tTraining Loss: 0.217862 \tR2: 0.743597\n",
      "Epoch: 1844 \tTraining Loss: 0.229651 \tR2: 0.743597\n",
      "Epoch: 1845 \tTraining Loss: 0.210108 \tR2: 0.743597\n",
      "Epoch: 1846 \tTraining Loss: 0.221795 \tR2: 0.743597\n",
      "Epoch: 1847 \tTraining Loss: 0.230211 \tR2: 0.743597\n",
      "Epoch: 1848 \tTraining Loss: 0.215176 \tR2: 0.743597\n",
      "Epoch: 1849 \tTraining Loss: 0.231955 \tR2: 0.743597\n",
      "Epoch: 1850 \tTraining Loss: 0.221323 \tR2: 0.743597\n",
      "Epoch: 1851 \tTraining Loss: 0.232933 \tR2: 0.743597\n",
      "Epoch: 1852 \tTraining Loss: 0.274112 \tR2: 0.743597\n",
      "Epoch: 1853 \tTraining Loss: 0.262753 \tR2: 0.743597\n",
      "Epoch: 1854 \tTraining Loss: 0.236478 \tR2: 0.743597\n",
      "Epoch: 1855 \tTraining Loss: 0.240007 \tR2: 0.743597\n",
      "Epoch: 1856 \tTraining Loss: 0.229140 \tR2: 0.743597\n",
      "Epoch: 1857 \tTraining Loss: 0.247388 \tR2: 0.743597\n",
      "Epoch: 1858 \tTraining Loss: 0.237335 \tR2: 0.743597\n",
      "Epoch: 1859 \tTraining Loss: 0.225823 \tR2: 0.743597\n",
      "Epoch: 1860 \tTraining Loss: 0.226277 \tR2: 0.743597\n",
      "Epoch: 1861 \tTraining Loss: 0.237929 \tR2: 0.743597\n",
      "Epoch: 1862 \tTraining Loss: 0.241196 \tR2: 0.743597\n",
      "Epoch: 1863 \tTraining Loss: 0.224763 \tR2: 0.743597\n",
      "Epoch: 1864 \tTraining Loss: 0.232805 \tR2: 0.743597\n",
      "Epoch: 1865 \tTraining Loss: 0.253089 \tR2: 0.743597\n",
      "Epoch: 1866 \tTraining Loss: 0.241189 \tR2: 0.743597\n",
      "Epoch: 1867 \tTraining Loss: 0.232684 \tR2: 0.743597\n",
      "Epoch: 1868 \tTraining Loss: 0.229466 \tR2: 0.743597\n",
      "Epoch: 1869 \tTraining Loss: 0.238256 \tR2: 0.743597\n",
      "Epoch: 1870 \tTraining Loss: 0.212437 \tR2: 0.743597\n",
      "Epoch: 1871 \tTraining Loss: 0.234937 \tR2: 0.743597\n",
      "Epoch: 1872 \tTraining Loss: 0.240552 \tR2: 0.743597\n",
      "Epoch: 1873 \tTraining Loss: 0.233630 \tR2: 0.743597\n",
      "Epoch: 1874 \tTraining Loss: 0.253476 \tR2: 0.743597\n",
      "Epoch: 1875 \tTraining Loss: 0.233004 \tR2: 0.743597\n",
      "Epoch: 1876 \tTraining Loss: 0.235877 \tR2: 0.743597\n",
      "Epoch: 1877 \tTraining Loss: 0.246555 \tR2: 0.743597\n",
      "Epoch: 1878 \tTraining Loss: 0.207934 \tR2: 0.743597\n",
      "Epoch: 1879 \tTraining Loss: 0.226236 \tR2: 0.743597\n",
      "Epoch: 1880 \tTraining Loss: 0.221922 \tR2: 0.743597\n",
      "Epoch: 1881 \tTraining Loss: 0.224129 \tR2: 0.743597\n",
      "Epoch: 1882 \tTraining Loss: 0.232821 \tR2: 0.743597\n",
      "Epoch: 1883 \tTraining Loss: 0.223044 \tR2: 0.743597\n",
      "Epoch: 1884 \tTraining Loss: 0.237973 \tR2: 0.743597\n",
      "Epoch: 1885 \tTraining Loss: 0.242444 \tR2: 0.743597\n",
      "Epoch: 1886 \tTraining Loss: 0.252211 \tR2: 0.743597\n",
      "Epoch: 1887 \tTraining Loss: 0.219618 \tR2: 0.743597\n",
      "Epoch: 1888 \tTraining Loss: 0.233324 \tR2: 0.743597\n",
      "Epoch: 1889 \tTraining Loss: 0.226505 \tR2: 0.743597\n",
      "Epoch: 1890 \tTraining Loss: 0.240231 \tR2: 0.743597\n",
      "Epoch: 1891 \tTraining Loss: 0.229696 \tR2: 0.743597\n",
      "Epoch: 1892 \tTraining Loss: 0.217670 \tR2: 0.743597\n",
      "Epoch: 1893 \tTraining Loss: 0.230671 \tR2: 0.743597\n",
      "Epoch: 1894 \tTraining Loss: 0.244692 \tR2: 0.743597\n",
      "Epoch: 1895 \tTraining Loss: 0.238073 \tR2: 0.743597\n",
      "Epoch: 1896 \tTraining Loss: 0.226091 \tR2: 0.743597\n",
      "Epoch: 1897 \tTraining Loss: 0.203495 \tR2: 0.743597\n",
      "Epoch: 1898 \tTraining Loss: 0.234720 \tR2: 0.743597\n",
      "Epoch: 1899 \tTraining Loss: 0.237716 \tR2: 0.743597\n",
      "Epoch: 1900 \tTraining Loss: 0.220792 \tR2: 0.337750\n",
      "Epoch: 1901 \tTraining Loss: 0.262304 \tR2: 0.337750\n",
      "Epoch: 1902 \tTraining Loss: 0.255728 \tR2: 0.337750\n",
      "Epoch: 1903 \tTraining Loss: 0.242419 \tR2: 0.337750\n",
      "Epoch: 1904 \tTraining Loss: 0.235478 \tR2: 0.337750\n",
      "Epoch: 1905 \tTraining Loss: 0.228201 \tR2: 0.337750\n",
      "Epoch: 1906 \tTraining Loss: 0.235386 \tR2: 0.337750\n",
      "Epoch: 1907 \tTraining Loss: 0.225301 \tR2: 0.337750\n",
      "Epoch: 1908 \tTraining Loss: 0.219248 \tR2: 0.337750\n",
      "Epoch: 1909 \tTraining Loss: 0.233894 \tR2: 0.337750\n",
      "Epoch: 1910 \tTraining Loss: 0.238284 \tR2: 0.337750\n",
      "Epoch: 1911 \tTraining Loss: 0.228864 \tR2: 0.337750\n",
      "Epoch: 1912 \tTraining Loss: 0.252654 \tR2: 0.337750\n",
      "Epoch: 1913 \tTraining Loss: 0.225398 \tR2: 0.337750\n",
      "Epoch: 1914 \tTraining Loss: 0.213135 \tR2: 0.337750\n",
      "Epoch: 1915 \tTraining Loss: 0.249687 \tR2: 0.337750\n",
      "Epoch: 1916 \tTraining Loss: 0.216917 \tR2: 0.337750\n",
      "Epoch: 1917 \tTraining Loss: 0.216902 \tR2: 0.337750\n",
      "Epoch: 1918 \tTraining Loss: 0.253865 \tR2: 0.337750\n",
      "Epoch: 1919 \tTraining Loss: 0.223563 \tR2: 0.337750\n",
      "Epoch: 1920 \tTraining Loss: 0.250543 \tR2: 0.337750\n",
      "Epoch: 1921 \tTraining Loss: 0.229415 \tR2: 0.337750\n",
      "Epoch: 1922 \tTraining Loss: 0.227003 \tR2: 0.337750\n",
      "Epoch: 1923 \tTraining Loss: 0.234054 \tR2: 0.337750\n",
      "Epoch: 1924 \tTraining Loss: 0.244879 \tR2: 0.337750\n",
      "Epoch: 1925 \tTraining Loss: 0.233339 \tR2: 0.337750\n",
      "Epoch: 1926 \tTraining Loss: 0.212727 \tR2: 0.337750\n",
      "Epoch: 1927 \tTraining Loss: 0.237631 \tR2: 0.337750\n",
      "Epoch: 1928 \tTraining Loss: 0.226800 \tR2: 0.337750\n",
      "Epoch: 1929 \tTraining Loss: 0.236960 \tR2: 0.337750\n",
      "Epoch: 1930 \tTraining Loss: 0.233486 \tR2: 0.337750\n",
      "Epoch: 1931 \tTraining Loss: 0.215951 \tR2: 0.337750\n",
      "Epoch: 1932 \tTraining Loss: 0.215819 \tR2: 0.337750\n",
      "Epoch: 1933 \tTraining Loss: 0.227519 \tR2: 0.337750\n",
      "Epoch: 1934 \tTraining Loss: 0.243597 \tR2: 0.337750\n",
      "Epoch: 1935 \tTraining Loss: 0.235457 \tR2: 0.337750\n",
      "Epoch: 1936 \tTraining Loss: 0.234604 \tR2: 0.337750\n",
      "Epoch: 1937 \tTraining Loss: 0.228500 \tR2: 0.337750\n",
      "Epoch: 1938 \tTraining Loss: 0.229831 \tR2: 0.337750\n",
      "Epoch: 1939 \tTraining Loss: 0.237287 \tR2: 0.337750\n",
      "Epoch: 1940 \tTraining Loss: 0.255444 \tR2: 0.337750\n",
      "Epoch: 1941 \tTraining Loss: 0.220797 \tR2: 0.337750\n",
      "Epoch: 1942 \tTraining Loss: 0.224707 \tR2: 0.337750\n",
      "Epoch: 1943 \tTraining Loss: 0.235972 \tR2: 0.337750\n",
      "Epoch: 1944 \tTraining Loss: 0.237865 \tR2: 0.337750\n",
      "Epoch: 1945 \tTraining Loss: 0.217421 \tR2: 0.337750\n",
      "Epoch: 1946 \tTraining Loss: 0.236004 \tR2: 0.337750\n",
      "Epoch: 1947 \tTraining Loss: 0.221052 \tR2: 0.337750\n",
      "Epoch: 1948 \tTraining Loss: 0.223424 \tR2: 0.337750\n",
      "Epoch: 1949 \tTraining Loss: 0.232150 \tR2: 0.337750\n",
      "Epoch: 1950 \tTraining Loss: 0.231491 \tR2: 0.337750\n",
      "Epoch: 1951 \tTraining Loss: 0.218287 \tR2: 0.337750\n",
      "Epoch: 1952 \tTraining Loss: 0.218204 \tR2: 0.337750\n",
      "Epoch: 1953 \tTraining Loss: 0.236784 \tR2: 0.337750\n",
      "Epoch: 1954 \tTraining Loss: 0.236539 \tR2: 0.337750\n",
      "Epoch: 1955 \tTraining Loss: 0.229804 \tR2: 0.337750\n",
      "Epoch: 1956 \tTraining Loss: 0.237606 \tR2: 0.337750\n",
      "Epoch: 1957 \tTraining Loss: 0.238276 \tR2: 0.337750\n",
      "Epoch: 1958 \tTraining Loss: 0.214910 \tR2: 0.337750\n",
      "Epoch: 1959 \tTraining Loss: 0.231485 \tR2: 0.337750\n",
      "Epoch: 1960 \tTraining Loss: 0.217097 \tR2: 0.337750\n",
      "Epoch: 1961 \tTraining Loss: 0.244569 \tR2: 0.337750\n",
      "Epoch: 1962 \tTraining Loss: 0.228314 \tR2: 0.337750\n",
      "Epoch: 1963 \tTraining Loss: 0.216500 \tR2: 0.337750\n",
      "Epoch: 1964 \tTraining Loss: 0.221017 \tR2: 0.337750\n",
      "Epoch: 1965 \tTraining Loss: 0.246948 \tR2: 0.337750\n",
      "Epoch: 1966 \tTraining Loss: 0.263219 \tR2: 0.337750\n",
      "Epoch: 1967 \tTraining Loss: 0.236549 \tR2: 0.337750\n",
      "Epoch: 1968 \tTraining Loss: 0.215690 \tR2: 0.337750\n",
      "Epoch: 1969 \tTraining Loss: 0.230786 \tR2: 0.337750\n",
      "Epoch: 1970 \tTraining Loss: 0.215531 \tR2: 0.337750\n",
      "Epoch: 1971 \tTraining Loss: 0.242263 \tR2: 0.337750\n",
      "Epoch: 1972 \tTraining Loss: 0.222809 \tR2: 0.337750\n",
      "Epoch: 1973 \tTraining Loss: 0.227384 \tR2: 0.337750\n",
      "Epoch: 1974 \tTraining Loss: 0.241929 \tR2: 0.337750\n",
      "Epoch: 1975 \tTraining Loss: 0.235063 \tR2: 0.337750\n",
      "Epoch: 1976 \tTraining Loss: 0.225617 \tR2: 0.337750\n",
      "Epoch: 1977 \tTraining Loss: 0.237132 \tR2: 0.337750\n",
      "Epoch: 1978 \tTraining Loss: 0.224741 \tR2: 0.337750\n",
      "Epoch: 1979 \tTraining Loss: 0.234597 \tR2: 0.337750\n",
      "Epoch: 1980 \tTraining Loss: 0.208679 \tR2: 0.337750\n",
      "Epoch: 1981 \tTraining Loss: 0.222466 \tR2: 0.337750\n",
      "Epoch: 1982 \tTraining Loss: 0.217884 \tR2: 0.337750\n",
      "Epoch: 1983 \tTraining Loss: 0.250441 \tR2: 0.337750\n",
      "Epoch: 1984 \tTraining Loss: 0.224906 \tR2: 0.337750\n",
      "Epoch: 1985 \tTraining Loss: 0.225836 \tR2: 0.337750\n",
      "Epoch: 1986 \tTraining Loss: 0.221367 \tR2: 0.337750\n",
      "Epoch: 1987 \tTraining Loss: 0.233687 \tR2: 0.337750\n",
      "Epoch: 1988 \tTraining Loss: 0.253141 \tR2: 0.337750\n",
      "Epoch: 1989 \tTraining Loss: 0.232968 \tR2: 0.337750\n",
      "Epoch: 1990 \tTraining Loss: 0.226132 \tR2: 0.337750\n",
      "Epoch: 1991 \tTraining Loss: 0.224747 \tR2: 0.337750\n",
      "Epoch: 1992 \tTraining Loss: 0.208536 \tR2: 0.337750\n",
      "Epoch: 1993 \tTraining Loss: 0.223920 \tR2: 0.337750\n",
      "Epoch: 1994 \tTraining Loss: 0.218949 \tR2: 0.337750\n",
      "Epoch: 1995 \tTraining Loss: 0.226034 \tR2: 0.337750\n",
      "Epoch: 1996 \tTraining Loss: 0.244933 \tR2: 0.337750\n",
      "Epoch: 1997 \tTraining Loss: 0.229191 \tR2: 0.337750\n",
      "Epoch: 1998 \tTraining Loss: 0.223243 \tR2: 0.337750\n",
      "Epoch: 1999 \tTraining Loss: 0.238665 \tR2: 0.337750\n",
      "Epoch: 2000 \tTraining Loss: 0.268981 \tR2: 0.713659\n",
      "Epoch: 2001 \tTraining Loss: 0.224803 \tR2: 0.713659\n",
      "Epoch: 2002 \tTraining Loss: 0.236089 \tR2: 0.713659\n",
      "Epoch: 2003 \tTraining Loss: 0.228597 \tR2: 0.713659\n",
      "Epoch: 2004 \tTraining Loss: 0.231691 \tR2: 0.713659\n",
      "Epoch: 2005 \tTraining Loss: 0.235219 \tR2: 0.713659\n",
      "Epoch: 2006 \tTraining Loss: 0.230774 \tR2: 0.713659\n",
      "Epoch: 2007 \tTraining Loss: 0.233454 \tR2: 0.713659\n",
      "Epoch: 2008 \tTraining Loss: 0.223821 \tR2: 0.713659\n",
      "Epoch: 2009 \tTraining Loss: 0.229058 \tR2: 0.713659\n",
      "Epoch: 2010 \tTraining Loss: 0.250252 \tR2: 0.713659\n",
      "Epoch: 2011 \tTraining Loss: 0.223849 \tR2: 0.713659\n",
      "Epoch: 2012 \tTraining Loss: 0.239078 \tR2: 0.713659\n",
      "Epoch: 2013 \tTraining Loss: 0.232743 \tR2: 0.713659\n",
      "Epoch: 2014 \tTraining Loss: 0.226460 \tR2: 0.713659\n",
      "Epoch: 2015 \tTraining Loss: 0.214368 \tR2: 0.713659\n",
      "Epoch: 2016 \tTraining Loss: 0.233078 \tR2: 0.713659\n",
      "Epoch: 2017 \tTraining Loss: 0.249565 \tR2: 0.713659\n",
      "Epoch: 2018 \tTraining Loss: 0.218335 \tR2: 0.713659\n",
      "Epoch: 2019 \tTraining Loss: 0.229203 \tR2: 0.713659\n",
      "Epoch: 2020 \tTraining Loss: 0.255155 \tR2: 0.713659\n",
      "Epoch: 2021 \tTraining Loss: 0.217555 \tR2: 0.713659\n",
      "Epoch: 2022 \tTraining Loss: 0.206055 \tR2: 0.713659\n",
      "Epoch: 2023 \tTraining Loss: 0.233833 \tR2: 0.713659\n",
      "Epoch: 2024 \tTraining Loss: 0.234108 \tR2: 0.713659\n",
      "Epoch: 2025 \tTraining Loss: 0.231366 \tR2: 0.713659\n",
      "Epoch: 2026 \tTraining Loss: 0.226819 \tR2: 0.713659\n",
      "Epoch: 2027 \tTraining Loss: 0.237076 \tR2: 0.713659\n",
      "Epoch: 2028 \tTraining Loss: 0.223100 \tR2: 0.713659\n",
      "Epoch: 2029 \tTraining Loss: 0.225757 \tR2: 0.713659\n",
      "Epoch: 2030 \tTraining Loss: 0.223144 \tR2: 0.713659\n",
      "Epoch: 2031 \tTraining Loss: 0.221279 \tR2: 0.713659\n",
      "Epoch: 2032 \tTraining Loss: 0.229805 \tR2: 0.713659\n",
      "Epoch: 2033 \tTraining Loss: 0.225669 \tR2: 0.713659\n",
      "Epoch: 2034 \tTraining Loss: 0.229863 \tR2: 0.713659\n",
      "Epoch: 2035 \tTraining Loss: 0.227071 \tR2: 0.713659\n",
      "Epoch: 2036 \tTraining Loss: 0.203940 \tR2: 0.713659\n",
      "Epoch: 2037 \tTraining Loss: 0.219826 \tR2: 0.713659\n",
      "Epoch: 2038 \tTraining Loss: 0.221795 \tR2: 0.713659\n",
      "Epoch: 2039 \tTraining Loss: 0.220425 \tR2: 0.713659\n",
      "Epoch: 2040 \tTraining Loss: 0.232998 \tR2: 0.713659\n",
      "Epoch: 2041 \tTraining Loss: 0.238214 \tR2: 0.713659\n",
      "Epoch: 2042 \tTraining Loss: 0.242345 \tR2: 0.713659\n",
      "Epoch: 2043 \tTraining Loss: 0.245716 \tR2: 0.713659\n",
      "Epoch: 2044 \tTraining Loss: 0.238125 \tR2: 0.713659\n",
      "Epoch: 2045 \tTraining Loss: 0.228152 \tR2: 0.713659\n",
      "Epoch: 2046 \tTraining Loss: 0.226185 \tR2: 0.713659\n",
      "Epoch: 2047 \tTraining Loss: 0.216353 \tR2: 0.713659\n",
      "Epoch: 2048 \tTraining Loss: 0.243856 \tR2: 0.713659\n",
      "Epoch: 2049 \tTraining Loss: 0.240145 \tR2: 0.713659\n",
      "Epoch: 2050 \tTraining Loss: 0.218958 \tR2: 0.713659\n",
      "Epoch: 2051 \tTraining Loss: 0.241403 \tR2: 0.713659\n",
      "Epoch: 2052 \tTraining Loss: 0.239275 \tR2: 0.713659\n",
      "Epoch: 2053 \tTraining Loss: 0.219501 \tR2: 0.713659\n",
      "Epoch: 2054 \tTraining Loss: 0.241330 \tR2: 0.713659\n",
      "Epoch: 2055 \tTraining Loss: 0.218914 \tR2: 0.713659\n",
      "Epoch: 2056 \tTraining Loss: 0.244966 \tR2: 0.713659\n",
      "Epoch: 2057 \tTraining Loss: 0.238594 \tR2: 0.713659\n",
      "Epoch: 2058 \tTraining Loss: 0.229827 \tR2: 0.713659\n",
      "Epoch: 2059 \tTraining Loss: 0.218817 \tR2: 0.713659\n",
      "Epoch: 2060 \tTraining Loss: 0.221224 \tR2: 0.713659\n",
      "Epoch: 2061 \tTraining Loss: 0.212302 \tR2: 0.713659\n",
      "Epoch: 2062 \tTraining Loss: 0.228512 \tR2: 0.713659\n",
      "Epoch: 2063 \tTraining Loss: 0.215855 \tR2: 0.713659\n",
      "Epoch: 2064 \tTraining Loss: 0.205379 \tR2: 0.713659\n",
      "Epoch: 2065 \tTraining Loss: 0.234477 \tR2: 0.713659\n",
      "Epoch: 2066 \tTraining Loss: 0.257133 \tR2: 0.713659\n",
      "Epoch: 2067 \tTraining Loss: 0.222569 \tR2: 0.713659\n",
      "Epoch: 2068 \tTraining Loss: 0.230751 \tR2: 0.713659\n",
      "Epoch: 2069 \tTraining Loss: 0.236233 \tR2: 0.713659\n",
      "Epoch: 2070 \tTraining Loss: 0.245161 \tR2: 0.713659\n",
      "Epoch: 2071 \tTraining Loss: 0.218124 \tR2: 0.713659\n",
      "Epoch: 2072 \tTraining Loss: 0.220071 \tR2: 0.713659\n",
      "Epoch: 2073 \tTraining Loss: 0.222959 \tR2: 0.713659\n",
      "Epoch: 2074 \tTraining Loss: 0.208455 \tR2: 0.713659\n",
      "Epoch: 2075 \tTraining Loss: 0.227912 \tR2: 0.713659\n",
      "Epoch: 2076 \tTraining Loss: 0.234093 \tR2: 0.713659\n",
      "Epoch: 2077 \tTraining Loss: 0.221612 \tR2: 0.713659\n",
      "Epoch: 2078 \tTraining Loss: 0.218947 \tR2: 0.713659\n",
      "Epoch: 2079 \tTraining Loss: 0.230319 \tR2: 0.713659\n",
      "Epoch: 2080 \tTraining Loss: 0.220981 \tR2: 0.713659\n",
      "Epoch: 2081 \tTraining Loss: 0.220320 \tR2: 0.713659\n",
      "Epoch: 2082 \tTraining Loss: 0.245559 \tR2: 0.713659\n",
      "Epoch: 2083 \tTraining Loss: 0.236255 \tR2: 0.713659\n",
      "Epoch: 2084 \tTraining Loss: 0.242138 \tR2: 0.713659\n",
      "Epoch: 2085 \tTraining Loss: 0.227586 \tR2: 0.713659\n",
      "Epoch: 2086 \tTraining Loss: 0.207209 \tR2: 0.713659\n",
      "Epoch: 2087 \tTraining Loss: 0.217214 \tR2: 0.713659\n",
      "Epoch: 2088 \tTraining Loss: 0.246444 \tR2: 0.713659\n",
      "Epoch: 2089 \tTraining Loss: 0.212815 \tR2: 0.713659\n",
      "Epoch: 2090 \tTraining Loss: 0.230106 \tR2: 0.713659\n",
      "Epoch: 2091 \tTraining Loss: 0.235957 \tR2: 0.713659\n",
      "Epoch: 2092 \tTraining Loss: 0.238240 \tR2: 0.713659\n",
      "Epoch: 2093 \tTraining Loss: 0.226402 \tR2: 0.713659\n",
      "Epoch: 2094 \tTraining Loss: 0.244514 \tR2: 0.713659\n",
      "Epoch: 2095 \tTraining Loss: 0.248695 \tR2: 0.713659\n",
      "Epoch: 2096 \tTraining Loss: 0.233331 \tR2: 0.713659\n",
      "Epoch: 2097 \tTraining Loss: 0.226813 \tR2: 0.713659\n",
      "Epoch: 2098 \tTraining Loss: 0.234665 \tR2: 0.713659\n",
      "Epoch: 2099 \tTraining Loss: 0.223293 \tR2: 0.713659\n",
      "Epoch: 2100 \tTraining Loss: 0.235924 \tR2: 0.584771\n",
      "Epoch: 2101 \tTraining Loss: 0.227502 \tR2: 0.584771\n",
      "Epoch: 2102 \tTraining Loss: 0.248130 \tR2: 0.584771\n",
      "Epoch: 2103 \tTraining Loss: 0.228233 \tR2: 0.584771\n",
      "Epoch: 2104 \tTraining Loss: 0.246864 \tR2: 0.584771\n",
      "Epoch: 2105 \tTraining Loss: 0.213702 \tR2: 0.584771\n",
      "Epoch: 2106 \tTraining Loss: 0.230285 \tR2: 0.584771\n",
      "Epoch: 2107 \tTraining Loss: 0.223429 \tR2: 0.584771\n",
      "Epoch: 2108 \tTraining Loss: 0.211991 \tR2: 0.584771\n",
      "Epoch: 2109 \tTraining Loss: 0.223265 \tR2: 0.584771\n",
      "Epoch: 2110 \tTraining Loss: 0.219441 \tR2: 0.584771\n",
      "Epoch: 2111 \tTraining Loss: 0.219429 \tR2: 0.584771\n",
      "Epoch: 2112 \tTraining Loss: 0.230676 \tR2: 0.584771\n",
      "Epoch: 2113 \tTraining Loss: 0.229911 \tR2: 0.584771\n",
      "Epoch: 2114 \tTraining Loss: 0.229520 \tR2: 0.584771\n",
      "Epoch: 2115 \tTraining Loss: 0.201329 \tR2: 0.584771\n",
      "Epoch: 2116 \tTraining Loss: 0.235451 \tR2: 0.584771\n",
      "Epoch: 2117 \tTraining Loss: 0.233842 \tR2: 0.584771\n",
      "Epoch: 2118 \tTraining Loss: 0.229255 \tR2: 0.584771\n",
      "Epoch: 2119 \tTraining Loss: 0.240271 \tR2: 0.584771\n",
      "Epoch: 2120 \tTraining Loss: 0.227381 \tR2: 0.584771\n",
      "Epoch: 2121 \tTraining Loss: 0.233889 \tR2: 0.584771\n",
      "Epoch: 2122 \tTraining Loss: 0.224721 \tR2: 0.584771\n",
      "Epoch: 2123 \tTraining Loss: 0.205689 \tR2: 0.584771\n",
      "Epoch: 2124 \tTraining Loss: 0.251467 \tR2: 0.584771\n",
      "Epoch: 2125 \tTraining Loss: 0.234596 \tR2: 0.584771\n",
      "Epoch: 2126 \tTraining Loss: 0.215087 \tR2: 0.584771\n",
      "Epoch: 2127 \tTraining Loss: 0.243544 \tR2: 0.584771\n",
      "Epoch: 2128 \tTraining Loss: 0.224502 \tR2: 0.584771\n",
      "Epoch: 2129 \tTraining Loss: 0.215462 \tR2: 0.584771\n",
      "Epoch: 2130 \tTraining Loss: 0.307246 \tR2: 0.584771\n",
      "Epoch: 2131 \tTraining Loss: 0.249059 \tR2: 0.584771\n",
      "Epoch: 2132 \tTraining Loss: 0.249601 \tR2: 0.584771\n",
      "Epoch: 2133 \tTraining Loss: 0.254259 \tR2: 0.584771\n",
      "Epoch: 2134 \tTraining Loss: 0.221827 \tR2: 0.584771\n",
      "Epoch: 2135 \tTraining Loss: 0.222991 \tR2: 0.584771\n",
      "Epoch: 2136 \tTraining Loss: 0.219687 \tR2: 0.584771\n",
      "Epoch: 2137 \tTraining Loss: 0.235319 \tR2: 0.584771\n",
      "Epoch: 2138 \tTraining Loss: 0.227563 \tR2: 0.584771\n",
      "Epoch: 2139 \tTraining Loss: 0.235669 \tR2: 0.584771\n",
      "Epoch: 2140 \tTraining Loss: 0.237911 \tR2: 0.584771\n",
      "Epoch: 2141 \tTraining Loss: 0.214673 \tR2: 0.584771\n",
      "Epoch: 2142 \tTraining Loss: 0.237515 \tR2: 0.584771\n",
      "Epoch: 2143 \tTraining Loss: 0.237732 \tR2: 0.584771\n",
      "Epoch: 2144 \tTraining Loss: 0.242691 \tR2: 0.584771\n",
      "Epoch: 2145 \tTraining Loss: 0.225064 \tR2: 0.584771\n",
      "Epoch: 2146 \tTraining Loss: 0.217842 \tR2: 0.584771\n",
      "Epoch: 2147 \tTraining Loss: 0.253350 \tR2: 0.584771\n",
      "Epoch: 2148 \tTraining Loss: 0.226820 \tR2: 0.584771\n",
      "Epoch: 2149 \tTraining Loss: 0.211510 \tR2: 0.584771\n",
      "Epoch: 2150 \tTraining Loss: 0.213352 \tR2: 0.584771\n",
      "Epoch: 2151 \tTraining Loss: 0.230015 \tR2: 0.584771\n",
      "Epoch: 2152 \tTraining Loss: 0.213053 \tR2: 0.584771\n",
      "Epoch: 2153 \tTraining Loss: 0.209705 \tR2: 0.584771\n",
      "Epoch: 2154 \tTraining Loss: 0.210313 \tR2: 0.584771\n",
      "Epoch: 2155 \tTraining Loss: 0.216381 \tR2: 0.584771\n",
      "Epoch: 2156 \tTraining Loss: 0.232686 \tR2: 0.584771\n",
      "Epoch: 2157 \tTraining Loss: 0.251635 \tR2: 0.584771\n",
      "Epoch: 2158 \tTraining Loss: 0.262710 \tR2: 0.584771\n",
      "Epoch: 2159 \tTraining Loss: 0.225678 \tR2: 0.584771\n",
      "Epoch: 2160 \tTraining Loss: 0.219513 \tR2: 0.584771\n",
      "Epoch: 2161 \tTraining Loss: 0.228708 \tR2: 0.584771\n",
      "Epoch: 2162 \tTraining Loss: 0.227691 \tR2: 0.584771\n",
      "Epoch: 2163 \tTraining Loss: 0.220868 \tR2: 0.584771\n",
      "Epoch: 2164 \tTraining Loss: 0.224199 \tR2: 0.584771\n",
      "Epoch: 2165 \tTraining Loss: 0.231136 \tR2: 0.584771\n",
      "Epoch: 2166 \tTraining Loss: 0.228869 \tR2: 0.584771\n",
      "Epoch: 2167 \tTraining Loss: 0.229349 \tR2: 0.584771\n",
      "Epoch: 2168 \tTraining Loss: 0.217260 \tR2: 0.584771\n",
      "Epoch: 2169 \tTraining Loss: 0.232279 \tR2: 0.584771\n",
      "Epoch: 2170 \tTraining Loss: 0.233761 \tR2: 0.584771\n",
      "Epoch: 2171 \tTraining Loss: 0.218284 \tR2: 0.584771\n",
      "Epoch: 2172 \tTraining Loss: 0.213510 \tR2: 0.584771\n",
      "Epoch: 2173 \tTraining Loss: 0.209796 \tR2: 0.584771\n",
      "Epoch: 2174 \tTraining Loss: 0.230433 \tR2: 0.584771\n",
      "Epoch: 2175 \tTraining Loss: 0.234015 \tR2: 0.584771\n",
      "Epoch: 2176 \tTraining Loss: 0.223142 \tR2: 0.584771\n",
      "Epoch: 2177 \tTraining Loss: 0.217270 \tR2: 0.584771\n",
      "Epoch: 2178 \tTraining Loss: 0.239817 \tR2: 0.584771\n",
      "Epoch: 2179 \tTraining Loss: 0.227677 \tR2: 0.584771\n",
      "Epoch: 2180 \tTraining Loss: 0.228922 \tR2: 0.584771\n",
      "Epoch: 2181 \tTraining Loss: 0.214489 \tR2: 0.584771\n",
      "Epoch: 2182 \tTraining Loss: 0.222879 \tR2: 0.584771\n",
      "Epoch: 2183 \tTraining Loss: 0.237848 \tR2: 0.584771\n",
      "Epoch: 2184 \tTraining Loss: 0.230677 \tR2: 0.584771\n",
      "Epoch: 2185 \tTraining Loss: 0.217768 \tR2: 0.584771\n",
      "Epoch: 2186 \tTraining Loss: 0.216897 \tR2: 0.584771\n",
      "Epoch: 2187 \tTraining Loss: 0.226730 \tR2: 0.584771\n",
      "Epoch: 2188 \tTraining Loss: 0.245101 \tR2: 0.584771\n",
      "Epoch: 2189 \tTraining Loss: 0.213239 \tR2: 0.584771\n",
      "Epoch: 2190 \tTraining Loss: 0.215558 \tR2: 0.584771\n",
      "Epoch: 2191 \tTraining Loss: 0.231546 \tR2: 0.584771\n",
      "Epoch: 2192 \tTraining Loss: 0.231891 \tR2: 0.584771\n",
      "Epoch: 2193 \tTraining Loss: 0.221459 \tR2: 0.584771\n",
      "Epoch: 2194 \tTraining Loss: 0.258089 \tR2: 0.584771\n",
      "Epoch: 2195 \tTraining Loss: 0.231626 \tR2: 0.584771\n",
      "Epoch: 2196 \tTraining Loss: 0.220935 \tR2: 0.584771\n",
      "Epoch: 2197 \tTraining Loss: 0.226383 \tR2: 0.584771\n",
      "Epoch: 2198 \tTraining Loss: 0.218801 \tR2: 0.584771\n",
      "Epoch: 2199 \tTraining Loss: 0.221623 \tR2: 0.584771\n",
      "Epoch: 2200 \tTraining Loss: 0.230754 \tR2: 0.545885\n",
      "Epoch: 2201 \tTraining Loss: 0.261627 \tR2: 0.545885\n",
      "Epoch: 2202 \tTraining Loss: 0.236741 \tR2: 0.545885\n",
      "Epoch: 2203 \tTraining Loss: 0.210264 \tR2: 0.545885\n",
      "Epoch: 2204 \tTraining Loss: 0.240711 \tR2: 0.545885\n",
      "Epoch: 2205 \tTraining Loss: 0.232565 \tR2: 0.545885\n",
      "Epoch: 2206 \tTraining Loss: 0.235506 \tR2: 0.545885\n",
      "Epoch: 2207 \tTraining Loss: 0.233301 \tR2: 0.545885\n",
      "Epoch: 2208 \tTraining Loss: 0.215384 \tR2: 0.545885\n",
      "Epoch: 2209 \tTraining Loss: 0.242810 \tR2: 0.545885\n",
      "Epoch: 2210 \tTraining Loss: 0.213943 \tR2: 0.545885\n",
      "Epoch: 2211 \tTraining Loss: 0.216388 \tR2: 0.545885\n",
      "Epoch: 2212 \tTraining Loss: 0.234204 \tR2: 0.545885\n",
      "Epoch: 2213 \tTraining Loss: 0.219111 \tR2: 0.545885\n",
      "Epoch: 2214 \tTraining Loss: 0.242146 \tR2: 0.545885\n",
      "Epoch: 2215 \tTraining Loss: 0.206946 \tR2: 0.545885\n",
      "Epoch: 2216 \tTraining Loss: 0.244204 \tR2: 0.545885\n",
      "Epoch: 2217 \tTraining Loss: 0.241719 \tR2: 0.545885\n",
      "Epoch: 2218 \tTraining Loss: 0.218288 \tR2: 0.545885\n",
      "Epoch: 2219 \tTraining Loss: 0.223212 \tR2: 0.545885\n",
      "Epoch: 2220 \tTraining Loss: 0.219846 \tR2: 0.545885\n",
      "Epoch: 2221 \tTraining Loss: 0.224906 \tR2: 0.545885\n",
      "Epoch: 2222 \tTraining Loss: 0.227598 \tR2: 0.545885\n",
      "Epoch: 2223 \tTraining Loss: 0.205424 \tR2: 0.545885\n",
      "Epoch: 2224 \tTraining Loss: 0.209290 \tR2: 0.545885\n",
      "Epoch: 2225 \tTraining Loss: 0.209390 \tR2: 0.545885\n",
      "Epoch: 2226 \tTraining Loss: 0.228050 \tR2: 0.545885\n",
      "Epoch: 2227 \tTraining Loss: 0.210662 \tR2: 0.545885\n",
      "Epoch: 2228 \tTraining Loss: 0.236602 \tR2: 0.545885\n",
      "Epoch: 2229 \tTraining Loss: 0.230403 \tR2: 0.545885\n",
      "Epoch: 2230 \tTraining Loss: 0.225644 \tR2: 0.545885\n",
      "Epoch: 2231 \tTraining Loss: 0.210967 \tR2: 0.545885\n",
      "Epoch: 2232 \tTraining Loss: 0.231530 \tR2: 0.545885\n",
      "Epoch: 2233 \tTraining Loss: 0.222544 \tR2: 0.545885\n",
      "Epoch: 2234 \tTraining Loss: 0.235139 \tR2: 0.545885\n",
      "Epoch: 2235 \tTraining Loss: 0.229336 \tR2: 0.545885\n",
      "Epoch: 2236 \tTraining Loss: 0.225009 \tR2: 0.545885\n",
      "Epoch: 2237 \tTraining Loss: 0.235459 \tR2: 0.545885\n",
      "Epoch: 2238 \tTraining Loss: 0.216434 \tR2: 0.545885\n",
      "Epoch: 2239 \tTraining Loss: 0.228986 \tR2: 0.545885\n",
      "Epoch: 2240 \tTraining Loss: 0.235048 \tR2: 0.545885\n",
      "Epoch: 2241 \tTraining Loss: 0.250758 \tR2: 0.545885\n",
      "Epoch: 2242 \tTraining Loss: 0.223748 \tR2: 0.545885\n",
      "Epoch: 2243 \tTraining Loss: 0.293913 \tR2: 0.545885\n",
      "Epoch: 2244 \tTraining Loss: 0.222818 \tR2: 0.545885\n",
      "Epoch: 2245 \tTraining Loss: 0.225246 \tR2: 0.545885\n",
      "Epoch: 2246 \tTraining Loss: 0.218436 \tR2: 0.545885\n",
      "Epoch: 2247 \tTraining Loss: 0.206463 \tR2: 0.545885\n",
      "Epoch: 2248 \tTraining Loss: 0.227479 \tR2: 0.545885\n",
      "Epoch: 2249 \tTraining Loss: 0.259967 \tR2: 0.545885\n",
      "Epoch: 2250 \tTraining Loss: 0.228638 \tR2: 0.545885\n",
      "Epoch: 2251 \tTraining Loss: 0.245272 \tR2: 0.545885\n",
      "Epoch: 2252 \tTraining Loss: 0.220709 \tR2: 0.545885\n",
      "Epoch: 2253 \tTraining Loss: 0.214271 \tR2: 0.545885\n",
      "Epoch: 2254 \tTraining Loss: 0.221503 \tR2: 0.545885\n",
      "Epoch: 2255 \tTraining Loss: 0.206936 \tR2: 0.545885\n",
      "Epoch: 2256 \tTraining Loss: 0.234013 \tR2: 0.545885\n",
      "Epoch: 2257 \tTraining Loss: 0.222991 \tR2: 0.545885\n",
      "Epoch: 2258 \tTraining Loss: 0.223450 \tR2: 0.545885\n",
      "Epoch: 2259 \tTraining Loss: 0.221140 \tR2: 0.545885\n",
      "Epoch: 2260 \tTraining Loss: 0.218338 \tR2: 0.545885\n",
      "Epoch: 2261 \tTraining Loss: 0.206639 \tR2: 0.545885\n",
      "Epoch: 2262 \tTraining Loss: 0.241048 \tR2: 0.545885\n",
      "Epoch: 2263 \tTraining Loss: 0.225401 \tR2: 0.545885\n",
      "Epoch: 2264 \tTraining Loss: 0.227488 \tR2: 0.545885\n",
      "Epoch: 2265 \tTraining Loss: 0.229356 \tR2: 0.545885\n",
      "Epoch: 2266 \tTraining Loss: 0.247348 \tR2: 0.545885\n",
      "Epoch: 2267 \tTraining Loss: 0.226998 \tR2: 0.545885\n",
      "Epoch: 2268 \tTraining Loss: 0.223000 \tR2: 0.545885\n",
      "Epoch: 2269 \tTraining Loss: 0.224237 \tR2: 0.545885\n",
      "Epoch: 2270 \tTraining Loss: 0.227034 \tR2: 0.545885\n",
      "Epoch: 2271 \tTraining Loss: 0.225033 \tR2: 0.545885\n",
      "Epoch: 2272 \tTraining Loss: 0.239263 \tR2: 0.545885\n",
      "Epoch: 2273 \tTraining Loss: 0.237954 \tR2: 0.545885\n",
      "Epoch: 2274 \tTraining Loss: 0.214886 \tR2: 0.545885\n",
      "Epoch: 2275 \tTraining Loss: 0.231003 \tR2: 0.545885\n",
      "Epoch: 2276 \tTraining Loss: 0.219984 \tR2: 0.545885\n",
      "Epoch: 2277 \tTraining Loss: 0.222164 \tR2: 0.545885\n",
      "Epoch: 2278 \tTraining Loss: 0.250673 \tR2: 0.545885\n",
      "Epoch: 2279 \tTraining Loss: 0.242921 \tR2: 0.545885\n",
      "Epoch: 2280 \tTraining Loss: 0.211951 \tR2: 0.545885\n",
      "Epoch: 2281 \tTraining Loss: 0.218590 \tR2: 0.545885\n",
      "Epoch: 2282 \tTraining Loss: 0.231414 \tR2: 0.545885\n",
      "Epoch: 2283 \tTraining Loss: 0.232368 \tR2: 0.545885\n",
      "Epoch: 2284 \tTraining Loss: 0.222821 \tR2: 0.545885\n",
      "Epoch: 2285 \tTraining Loss: 0.218330 \tR2: 0.545885\n",
      "Epoch: 2286 \tTraining Loss: 0.227896 \tR2: 0.545885\n",
      "Epoch: 2287 \tTraining Loss: 0.237197 \tR2: 0.545885\n",
      "Epoch: 2288 \tTraining Loss: 0.227671 \tR2: 0.545885\n",
      "Epoch: 2289 \tTraining Loss: 0.220930 \tR2: 0.545885\n",
      "Epoch: 2290 \tTraining Loss: 0.226351 \tR2: 0.545885\n",
      "Epoch: 2291 \tTraining Loss: 0.228983 \tR2: 0.545885\n",
      "Epoch: 2292 \tTraining Loss: 0.214111 \tR2: 0.545885\n",
      "Epoch: 2293 \tTraining Loss: 0.206725 \tR2: 0.545885\n",
      "Epoch: 2294 \tTraining Loss: 0.225598 \tR2: 0.545885\n",
      "Epoch: 2295 \tTraining Loss: 0.219193 \tR2: 0.545885\n",
      "Epoch: 2296 \tTraining Loss: 0.225650 \tR2: 0.545885\n",
      "Epoch: 2297 \tTraining Loss: 0.239662 \tR2: 0.545885\n",
      "Epoch: 2298 \tTraining Loss: 0.249607 \tR2: 0.545885\n",
      "Epoch: 2299 \tTraining Loss: 0.235457 \tR2: 0.545885\n",
      "Epoch: 2300 \tTraining Loss: 0.230713 \tR2: 0.654455\n",
      "Epoch: 2301 \tTraining Loss: 0.226569 \tR2: 0.654455\n",
      "Epoch: 2302 \tTraining Loss: 0.213473 \tR2: 0.654455\n",
      "Epoch: 2303 \tTraining Loss: 0.226990 \tR2: 0.654455\n",
      "Epoch: 2304 \tTraining Loss: 0.225923 \tR2: 0.654455\n",
      "Epoch: 2305 \tTraining Loss: 0.226543 \tR2: 0.654455\n",
      "Epoch: 2306 \tTraining Loss: 0.230796 \tR2: 0.654455\n",
      "Epoch: 2307 \tTraining Loss: 0.207276 \tR2: 0.654455\n",
      "Epoch: 2308 \tTraining Loss: 0.215373 \tR2: 0.654455\n",
      "Epoch: 2309 \tTraining Loss: 0.239323 \tR2: 0.654455\n",
      "Epoch: 2310 \tTraining Loss: 0.227543 \tR2: 0.654455\n",
      "Epoch: 2311 \tTraining Loss: 0.228811 \tR2: 0.654455\n",
      "Epoch: 2312 \tTraining Loss: 0.237017 \tR2: 0.654455\n",
      "Epoch: 2313 \tTraining Loss: 0.212828 \tR2: 0.654455\n",
      "Epoch: 2314 \tTraining Loss: 0.239834 \tR2: 0.654455\n",
      "Epoch: 2315 \tTraining Loss: 0.217140 \tR2: 0.654455\n",
      "Epoch: 2316 \tTraining Loss: 0.209503 \tR2: 0.654455\n",
      "Epoch: 2317 \tTraining Loss: 0.221279 \tR2: 0.654455\n",
      "Epoch: 2318 \tTraining Loss: 0.206503 \tR2: 0.654455\n",
      "Epoch: 2319 \tTraining Loss: 0.227792 \tR2: 0.654455\n",
      "Epoch: 2320 \tTraining Loss: 0.236992 \tR2: 0.654455\n",
      "Epoch: 2321 \tTraining Loss: 0.213790 \tR2: 0.654455\n",
      "Epoch: 2322 \tTraining Loss: 0.222729 \tR2: 0.654455\n",
      "Epoch: 2323 \tTraining Loss: 0.207521 \tR2: 0.654455\n",
      "Epoch: 2324 \tTraining Loss: 0.218400 \tR2: 0.654455\n",
      "Epoch: 2325 \tTraining Loss: 0.201330 \tR2: 0.654455\n",
      "Epoch: 2326 \tTraining Loss: 0.224534 \tR2: 0.654455\n",
      "Epoch: 2327 \tTraining Loss: 0.233060 \tR2: 0.654455\n",
      "Epoch: 2328 \tTraining Loss: 0.211667 \tR2: 0.654455\n",
      "Epoch: 2329 \tTraining Loss: 0.224027 \tR2: 0.654455\n",
      "Epoch: 2330 \tTraining Loss: 0.204228 \tR2: 0.654455\n",
      "Epoch: 2331 \tTraining Loss: 0.232683 \tR2: 0.654455\n",
      "Epoch: 2332 \tTraining Loss: 0.215156 \tR2: 0.654455\n",
      "Epoch: 2333 \tTraining Loss: 0.216028 \tR2: 0.654455\n",
      "Epoch: 2334 \tTraining Loss: 0.206181 \tR2: 0.654455\n",
      "Epoch: 2335 \tTraining Loss: 0.216369 \tR2: 0.654455\n",
      "Epoch: 2336 \tTraining Loss: 0.204746 \tR2: 0.654455\n",
      "Epoch: 2337 \tTraining Loss: 0.227953 \tR2: 0.654455\n",
      "Epoch: 2338 \tTraining Loss: 0.225452 \tR2: 0.654455\n",
      "Epoch: 2339 \tTraining Loss: 0.217031 \tR2: 0.654455\n",
      "Epoch: 2340 \tTraining Loss: 0.252842 \tR2: 0.654455\n",
      "Epoch: 2341 \tTraining Loss: 0.206885 \tR2: 0.654455\n",
      "Epoch: 2342 \tTraining Loss: 0.237099 \tR2: 0.654455\n",
      "Epoch: 2343 \tTraining Loss: 0.220654 \tR2: 0.654455\n",
      "Epoch: 2344 \tTraining Loss: 0.202981 \tR2: 0.654455\n",
      "Epoch: 2345 \tTraining Loss: 0.237776 \tR2: 0.654455\n",
      "Epoch: 2346 \tTraining Loss: 0.215975 \tR2: 0.654455\n",
      "Epoch: 2347 \tTraining Loss: 0.208828 \tR2: 0.654455\n",
      "Epoch: 2348 \tTraining Loss: 0.220052 \tR2: 0.654455\n",
      "Epoch: 2349 \tTraining Loss: 0.220061 \tR2: 0.654455\n",
      "Epoch: 2350 \tTraining Loss: 0.230745 \tR2: 0.654455\n",
      "Epoch: 2351 \tTraining Loss: 0.247561 \tR2: 0.654455\n",
      "Epoch: 2352 \tTraining Loss: 0.234326 \tR2: 0.654455\n",
      "Epoch: 2353 \tTraining Loss: 0.216389 \tR2: 0.654455\n",
      "Epoch: 2354 \tTraining Loss: 0.216699 \tR2: 0.654455\n",
      "Epoch: 2355 \tTraining Loss: 0.219210 \tR2: 0.654455\n",
      "Epoch: 2356 \tTraining Loss: 0.229123 \tR2: 0.654455\n",
      "Epoch: 2357 \tTraining Loss: 0.239053 \tR2: 0.654455\n",
      "Epoch: 2358 \tTraining Loss: 0.226812 \tR2: 0.654455\n",
      "Epoch: 2359 \tTraining Loss: 0.215941 \tR2: 0.654455\n",
      "Epoch: 2360 \tTraining Loss: 0.224507 \tR2: 0.654455\n",
      "Epoch: 2361 \tTraining Loss: 0.225170 \tR2: 0.654455\n",
      "Epoch: 2362 \tTraining Loss: 0.214509 \tR2: 0.654455\n",
      "Epoch: 2363 \tTraining Loss: 0.232093 \tR2: 0.654455\n",
      "Epoch: 2364 \tTraining Loss: 0.214608 \tR2: 0.654455\n",
      "Epoch: 2365 \tTraining Loss: 0.215822 \tR2: 0.654455\n",
      "Epoch: 2366 \tTraining Loss: 0.208436 \tR2: 0.654455\n",
      "Epoch: 2367 \tTraining Loss: 0.218606 \tR2: 0.654455\n",
      "Epoch: 2368 \tTraining Loss: 0.233836 \tR2: 0.654455\n",
      "Epoch: 2369 \tTraining Loss: 0.219485 \tR2: 0.654455\n",
      "Epoch: 2370 \tTraining Loss: 0.234429 \tR2: 0.654455\n",
      "Epoch: 2371 \tTraining Loss: 0.219722 \tR2: 0.654455\n",
      "Epoch: 2372 \tTraining Loss: 0.225868 \tR2: 0.654455\n",
      "Epoch: 2373 \tTraining Loss: 0.216700 \tR2: 0.654455\n",
      "Epoch: 2374 \tTraining Loss: 0.220619 \tR2: 0.654455\n",
      "Epoch: 2375 \tTraining Loss: 0.219262 \tR2: 0.654455\n",
      "Epoch: 2376 \tTraining Loss: 0.231927 \tR2: 0.654455\n",
      "Epoch: 2377 \tTraining Loss: 0.242931 \tR2: 0.654455\n",
      "Epoch: 2378 \tTraining Loss: 0.236453 \tR2: 0.654455\n",
      "Epoch: 2379 \tTraining Loss: 0.242656 \tR2: 0.654455\n",
      "Epoch: 2380 \tTraining Loss: 0.240373 \tR2: 0.654455\n",
      "Epoch: 2381 \tTraining Loss: 0.213956 \tR2: 0.654455\n",
      "Epoch: 2382 \tTraining Loss: 0.231375 \tR2: 0.654455\n",
      "Epoch: 2383 \tTraining Loss: 0.211099 \tR2: 0.654455\n",
      "Epoch: 2384 \tTraining Loss: 0.223033 \tR2: 0.654455\n",
      "Epoch: 2385 \tTraining Loss: 0.209804 \tR2: 0.654455\n",
      "Epoch: 2386 \tTraining Loss: 0.237364 \tR2: 0.654455\n",
      "Epoch: 2387 \tTraining Loss: 0.204038 \tR2: 0.654455\n",
      "Epoch: 2388 \tTraining Loss: 0.227892 \tR2: 0.654455\n",
      "Epoch: 2389 \tTraining Loss: 0.245959 \tR2: 0.654455\n",
      "Epoch: 2390 \tTraining Loss: 0.229302 \tR2: 0.654455\n",
      "Epoch: 2391 \tTraining Loss: 0.233272 \tR2: 0.654455\n",
      "Epoch: 2392 \tTraining Loss: 0.230893 \tR2: 0.654455\n",
      "Epoch: 2393 \tTraining Loss: 0.211496 \tR2: 0.654455\n",
      "Epoch: 2394 \tTraining Loss: 0.213066 \tR2: 0.654455\n",
      "Epoch: 2395 \tTraining Loss: 0.225459 \tR2: 0.654455\n",
      "Epoch: 2396 \tTraining Loss: 0.221613 \tR2: 0.654455\n",
      "Epoch: 2397 \tTraining Loss: 0.221230 \tR2: 0.654455\n",
      "Epoch: 2398 \tTraining Loss: 0.237567 \tR2: 0.654455\n",
      "Epoch: 2399 \tTraining Loss: 0.211502 \tR2: 0.654455\n",
      "Epoch: 2400 \tTraining Loss: 0.212430 \tR2: 0.494276\n",
      "Epoch: 2401 \tTraining Loss: 0.205146 \tR2: 0.494276\n",
      "Epoch: 2402 \tTraining Loss: 0.228460 \tR2: 0.494276\n",
      "Epoch: 2403 \tTraining Loss: 0.226109 \tR2: 0.494276\n",
      "Epoch: 2404 \tTraining Loss: 0.225969 \tR2: 0.494276\n",
      "Epoch: 2405 \tTraining Loss: 0.210455 \tR2: 0.494276\n",
      "Epoch: 2406 \tTraining Loss: 0.223479 \tR2: 0.494276\n",
      "Epoch: 2407 \tTraining Loss: 0.223371 \tR2: 0.494276\n",
      "Epoch: 2408 \tTraining Loss: 0.226382 \tR2: 0.494276\n",
      "Epoch: 2409 \tTraining Loss: 0.238170 \tR2: 0.494276\n",
      "Epoch: 2410 \tTraining Loss: 0.219896 \tR2: 0.494276\n",
      "Epoch: 2411 \tTraining Loss: 0.240536 \tR2: 0.494276\n",
      "Epoch: 2412 \tTraining Loss: 0.241304 \tR2: 0.494276\n",
      "Epoch: 2413 \tTraining Loss: 0.214676 \tR2: 0.494276\n",
      "Epoch: 2414 \tTraining Loss: 0.240725 \tR2: 0.494276\n",
      "Epoch: 2415 \tTraining Loss: 0.239527 \tR2: 0.494276\n",
      "Epoch: 2416 \tTraining Loss: 0.229918 \tR2: 0.494276\n",
      "Epoch: 2417 \tTraining Loss: 0.210668 \tR2: 0.494276\n",
      "Epoch: 2418 \tTraining Loss: 0.215579 \tR2: 0.494276\n",
      "Epoch: 2419 \tTraining Loss: 0.232583 \tR2: 0.494276\n",
      "Epoch: 2420 \tTraining Loss: 0.232972 \tR2: 0.494276\n",
      "Epoch: 2421 \tTraining Loss: 0.242238 \tR2: 0.494276\n",
      "Epoch: 2422 \tTraining Loss: 0.218301 \tR2: 0.494276\n",
      "Epoch: 2423 \tTraining Loss: 0.211772 \tR2: 0.494276\n",
      "Epoch: 2424 \tTraining Loss: 0.218020 \tR2: 0.494276\n",
      "Epoch: 2425 \tTraining Loss: 0.202097 \tR2: 0.494276\n",
      "Epoch: 2426 \tTraining Loss: 0.228900 \tR2: 0.494276\n",
      "Epoch: 2427 \tTraining Loss: 0.219649 \tR2: 0.494276\n",
      "Epoch: 2428 \tTraining Loss: 0.220207 \tR2: 0.494276\n",
      "Epoch: 2429 \tTraining Loss: 0.212503 \tR2: 0.494276\n",
      "Epoch: 2430 \tTraining Loss: 0.230535 \tR2: 0.494276\n",
      "Epoch: 2431 \tTraining Loss: 0.230133 \tR2: 0.494276\n",
      "Epoch: 2432 \tTraining Loss: 0.206421 \tR2: 0.494276\n",
      "Epoch: 2433 \tTraining Loss: 0.219609 \tR2: 0.494276\n",
      "Epoch: 2434 \tTraining Loss: 0.221993 \tR2: 0.494276\n",
      "Epoch: 2435 \tTraining Loss: 0.212351 \tR2: 0.494276\n",
      "Epoch: 2436 \tTraining Loss: 0.219691 \tR2: 0.494276\n",
      "Epoch: 2437 \tTraining Loss: 0.223549 \tR2: 0.494276\n",
      "Epoch: 2438 \tTraining Loss: 0.245004 \tR2: 0.494276\n",
      "Epoch: 2439 \tTraining Loss: 0.220484 \tR2: 0.494276\n",
      "Epoch: 2440 \tTraining Loss: 0.204561 \tR2: 0.494276\n",
      "Epoch: 2441 \tTraining Loss: 0.225048 \tR2: 0.494276\n",
      "Epoch: 2442 \tTraining Loss: 0.207556 \tR2: 0.494276\n",
      "Epoch: 2443 \tTraining Loss: 0.221588 \tR2: 0.494276\n",
      "Epoch: 2444 \tTraining Loss: 0.209453 \tR2: 0.494276\n",
      "Epoch: 2445 \tTraining Loss: 0.228758 \tR2: 0.494276\n",
      "Epoch: 2446 \tTraining Loss: 0.229231 \tR2: 0.494276\n",
      "Epoch: 2447 \tTraining Loss: 0.213363 \tR2: 0.494276\n",
      "Epoch: 2448 \tTraining Loss: 0.222734 \tR2: 0.494276\n",
      "Epoch: 2449 \tTraining Loss: 0.230525 \tR2: 0.494276\n",
      "Epoch: 2450 \tTraining Loss: 0.218613 \tR2: 0.494276\n",
      "Epoch: 2451 \tTraining Loss: 0.208498 \tR2: 0.494276\n",
      "Epoch: 2452 \tTraining Loss: 0.249321 \tR2: 0.494276\n",
      "Epoch: 2453 \tTraining Loss: 0.220795 \tR2: 0.494276\n",
      "Epoch: 2454 \tTraining Loss: 0.248401 \tR2: 0.494276\n",
      "Epoch: 2455 \tTraining Loss: 0.207256 \tR2: 0.494276\n",
      "Epoch: 2456 \tTraining Loss: 0.224466 \tR2: 0.494276\n",
      "Epoch: 2457 \tTraining Loss: 0.224327 \tR2: 0.494276\n",
      "Epoch: 2458 \tTraining Loss: 0.226345 \tR2: 0.494276\n",
      "Epoch: 2459 \tTraining Loss: 0.218868 \tR2: 0.494276\n",
      "Epoch: 2460 \tTraining Loss: 0.217728 \tR2: 0.494276\n",
      "Epoch: 2461 \tTraining Loss: 0.224297 \tR2: 0.494276\n",
      "Epoch: 2462 \tTraining Loss: 0.219607 \tR2: 0.494276\n",
      "Epoch: 2463 \tTraining Loss: 0.222065 \tR2: 0.494276\n",
      "Epoch: 2464 \tTraining Loss: 0.212926 \tR2: 0.494276\n",
      "Epoch: 2465 \tTraining Loss: 0.224422 \tR2: 0.494276\n",
      "Epoch: 2466 \tTraining Loss: 0.213388 \tR2: 0.494276\n",
      "Epoch: 2467 \tTraining Loss: 0.211232 \tR2: 0.494276\n",
      "Epoch: 2468 \tTraining Loss: 0.237632 \tR2: 0.494276\n",
      "Epoch: 2469 \tTraining Loss: 0.249943 \tR2: 0.494276\n",
      "Epoch: 2470 \tTraining Loss: 0.222814 \tR2: 0.494276\n",
      "Epoch: 2471 \tTraining Loss: 0.204605 \tR2: 0.494276\n",
      "Epoch: 2472 \tTraining Loss: 0.218926 \tR2: 0.494276\n",
      "Epoch: 2473 \tTraining Loss: 0.215359 \tR2: 0.494276\n",
      "Epoch: 2474 \tTraining Loss: 0.217975 \tR2: 0.494276\n",
      "Epoch: 2475 \tTraining Loss: 0.223284 \tR2: 0.494276\n",
      "Epoch: 2476 \tTraining Loss: 0.213987 \tR2: 0.494276\n",
      "Epoch: 2477 \tTraining Loss: 0.228392 \tR2: 0.494276\n",
      "Epoch: 2478 \tTraining Loss: 0.241224 \tR2: 0.494276\n",
      "Epoch: 2479 \tTraining Loss: 0.230592 \tR2: 0.494276\n",
      "Epoch: 2480 \tTraining Loss: 0.216340 \tR2: 0.494276\n",
      "Epoch: 2481 \tTraining Loss: 0.201903 \tR2: 0.494276\n",
      "Epoch: 2482 \tTraining Loss: 0.216143 \tR2: 0.494276\n",
      "Epoch: 2483 \tTraining Loss: 0.221670 \tR2: 0.494276\n",
      "Epoch: 2484 \tTraining Loss: 0.211268 \tR2: 0.494276\n",
      "Epoch: 2485 \tTraining Loss: 0.214170 \tR2: 0.494276\n",
      "Epoch: 2486 \tTraining Loss: 0.226236 \tR2: 0.494276\n",
      "Epoch: 2487 \tTraining Loss: 0.210628 \tR2: 0.494276\n",
      "Epoch: 2488 \tTraining Loss: 0.225160 \tR2: 0.494276\n",
      "Epoch: 2489 \tTraining Loss: 0.227169 \tR2: 0.494276\n",
      "Epoch: 2490 \tTraining Loss: 0.218588 \tR2: 0.494276\n",
      "Epoch: 2491 \tTraining Loss: 0.212804 \tR2: 0.494276\n",
      "Epoch: 2492 \tTraining Loss: 0.222650 \tR2: 0.494276\n",
      "Epoch: 2493 \tTraining Loss: 0.229826 \tR2: 0.494276\n",
      "Epoch: 2494 \tTraining Loss: 0.217746 \tR2: 0.494276\n",
      "Epoch: 2495 \tTraining Loss: 0.240731 \tR2: 0.494276\n",
      "Epoch: 2496 \tTraining Loss: 0.234496 \tR2: 0.494276\n",
      "Epoch: 2497 \tTraining Loss: 0.205419 \tR2: 0.494276\n",
      "Epoch: 2498 \tTraining Loss: 0.210600 \tR2: 0.494276\n",
      "Epoch: 2499 \tTraining Loss: 0.197827 \tR2: 0.494276\n",
      "Epoch: 2500 \tTraining Loss: 0.231278 \tR2: 0.273215\n",
      "Epoch: 2501 \tTraining Loss: 0.227347 \tR2: 0.273215\n",
      "Epoch: 2502 \tTraining Loss: 0.217424 \tR2: 0.273215\n",
      "Epoch: 2503 \tTraining Loss: 0.233461 \tR2: 0.273215\n",
      "Epoch: 2504 \tTraining Loss: 0.212437 \tR2: 0.273215\n",
      "Epoch: 2505 \tTraining Loss: 0.220766 \tR2: 0.273215\n",
      "Epoch: 2506 \tTraining Loss: 0.218534 \tR2: 0.273215\n",
      "Epoch: 2507 \tTraining Loss: 0.226587 \tR2: 0.273215\n",
      "Epoch: 2508 \tTraining Loss: 0.219013 \tR2: 0.273215\n",
      "Epoch: 2509 \tTraining Loss: 0.227610 \tR2: 0.273215\n",
      "Epoch: 2510 \tTraining Loss: 0.236953 \tR2: 0.273215\n",
      "Epoch: 2511 \tTraining Loss: 0.209996 \tR2: 0.273215\n",
      "Epoch: 2512 \tTraining Loss: 0.229715 \tR2: 0.273215\n",
      "Epoch: 2513 \tTraining Loss: 0.234090 \tR2: 0.273215\n",
      "Epoch: 2514 \tTraining Loss: 0.207644 \tR2: 0.273215\n",
      "Epoch: 2515 \tTraining Loss: 0.214218 \tR2: 0.273215\n",
      "Epoch: 2516 \tTraining Loss: 0.230086 \tR2: 0.273215\n",
      "Epoch: 2517 \tTraining Loss: 0.204415 \tR2: 0.273215\n",
      "Epoch: 2518 \tTraining Loss: 0.225051 \tR2: 0.273215\n",
      "Epoch: 2519 \tTraining Loss: 0.217273 \tR2: 0.273215\n",
      "Epoch: 2520 \tTraining Loss: 0.215044 \tR2: 0.273215\n",
      "Epoch: 2521 \tTraining Loss: 0.217061 \tR2: 0.273215\n",
      "Epoch: 2522 \tTraining Loss: 0.246750 \tR2: 0.273215\n",
      "Epoch: 2523 \tTraining Loss: 0.229643 \tR2: 0.273215\n",
      "Epoch: 2524 \tTraining Loss: 0.216358 \tR2: 0.273215\n",
      "Epoch: 2525 \tTraining Loss: 0.238274 \tR2: 0.273215\n",
      "Epoch: 2526 \tTraining Loss: 0.224594 \tR2: 0.273215\n",
      "Epoch: 2527 \tTraining Loss: 0.194932 \tR2: 0.273215\n",
      "Epoch: 2528 \tTraining Loss: 0.221368 \tR2: 0.273215\n",
      "Epoch: 2529 \tTraining Loss: 0.228469 \tR2: 0.273215\n",
      "Epoch: 2530 \tTraining Loss: 0.231515 \tR2: 0.273215\n",
      "Epoch: 2531 \tTraining Loss: 0.240189 \tR2: 0.273215\n",
      "Epoch: 2532 \tTraining Loss: 0.216447 \tR2: 0.273215\n",
      "Epoch: 2533 \tTraining Loss: 0.208431 \tR2: 0.273215\n",
      "Epoch: 2534 \tTraining Loss: 0.225608 \tR2: 0.273215\n",
      "Epoch: 2535 \tTraining Loss: 0.208896 \tR2: 0.273215\n",
      "Epoch: 2536 \tTraining Loss: 0.196007 \tR2: 0.273215\n",
      "Epoch: 2537 \tTraining Loss: 0.205560 \tR2: 0.273215\n",
      "Epoch: 2538 \tTraining Loss: 0.212050 \tR2: 0.273215\n",
      "Epoch: 2539 \tTraining Loss: 0.209646 \tR2: 0.273215\n",
      "Epoch: 2540 \tTraining Loss: 0.220898 \tR2: 0.273215\n",
      "Epoch: 2541 \tTraining Loss: 0.229935 \tR2: 0.273215\n",
      "Epoch: 2542 \tTraining Loss: 0.226891 \tR2: 0.273215\n",
      "Epoch: 2543 \tTraining Loss: 0.198814 \tR2: 0.273215\n",
      "Epoch: 2544 \tTraining Loss: 0.229096 \tR2: 0.273215\n",
      "Epoch: 2545 \tTraining Loss: 0.231327 \tR2: 0.273215\n",
      "Epoch: 2546 \tTraining Loss: 0.226548 \tR2: 0.273215\n",
      "Epoch: 2547 \tTraining Loss: 0.215955 \tR2: 0.273215\n",
      "Epoch: 2548 \tTraining Loss: 0.231527 \tR2: 0.273215\n",
      "Epoch: 2549 \tTraining Loss: 0.200729 \tR2: 0.273215\n",
      "Epoch: 2550 \tTraining Loss: 0.225314 \tR2: 0.273215\n",
      "Epoch: 2551 \tTraining Loss: 0.193860 \tR2: 0.273215\n",
      "Epoch: 2552 \tTraining Loss: 0.238467 \tR2: 0.273215\n",
      "Epoch: 2553 \tTraining Loss: 0.222703 \tR2: 0.273215\n",
      "Epoch: 2554 \tTraining Loss: 0.227578 \tR2: 0.273215\n",
      "Epoch: 2555 \tTraining Loss: 0.223589 \tR2: 0.273215\n",
      "Epoch: 2556 \tTraining Loss: 0.232400 \tR2: 0.273215\n",
      "Epoch: 2557 \tTraining Loss: 0.233545 \tR2: 0.273215\n",
      "Epoch: 2558 \tTraining Loss: 0.209814 \tR2: 0.273215\n",
      "Epoch: 2559 \tTraining Loss: 0.217778 \tR2: 0.273215\n",
      "Epoch: 2560 \tTraining Loss: 0.216161 \tR2: 0.273215\n",
      "Epoch: 2561 \tTraining Loss: 0.214631 \tR2: 0.273215\n",
      "Epoch: 2562 \tTraining Loss: 0.229885 \tR2: 0.273215\n",
      "Epoch: 2563 \tTraining Loss: 0.212460 \tR2: 0.273215\n",
      "Epoch: 2564 \tTraining Loss: 0.225011 \tR2: 0.273215\n",
      "Epoch: 2565 \tTraining Loss: 0.221667 \tR2: 0.273215\n",
      "Epoch: 2566 \tTraining Loss: 0.213374 \tR2: 0.273215\n",
      "Epoch: 2567 \tTraining Loss: 0.237488 \tR2: 0.273215\n",
      "Epoch: 2568 \tTraining Loss: 0.224882 \tR2: 0.273215\n",
      "Epoch: 2569 \tTraining Loss: 0.246863 \tR2: 0.273215\n",
      "Epoch: 2570 \tTraining Loss: 0.212778 \tR2: 0.273215\n",
      "Epoch: 2571 \tTraining Loss: 0.209683 \tR2: 0.273215\n",
      "Epoch: 2572 \tTraining Loss: 0.219822 \tR2: 0.273215\n",
      "Epoch: 2573 \tTraining Loss: 0.230114 \tR2: 0.273215\n",
      "Epoch: 2574 \tTraining Loss: 0.220242 \tR2: 0.273215\n",
      "Epoch: 2575 \tTraining Loss: 0.212192 \tR2: 0.273215\n",
      "Epoch: 2576 \tTraining Loss: 0.219194 \tR2: 0.273215\n",
      "Epoch: 2577 \tTraining Loss: 0.216305 \tR2: 0.273215\n",
      "Epoch: 2578 \tTraining Loss: 0.212494 \tR2: 0.273215\n",
      "Epoch: 2579 \tTraining Loss: 0.216022 \tR2: 0.273215\n",
      "Epoch: 2580 \tTraining Loss: 0.217824 \tR2: 0.273215\n",
      "Epoch: 2581 \tTraining Loss: 0.218628 \tR2: 0.273215\n",
      "Epoch: 2582 \tTraining Loss: 0.216762 \tR2: 0.273215\n",
      "Epoch: 2583 \tTraining Loss: 0.218647 \tR2: 0.273215\n",
      "Epoch: 2584 \tTraining Loss: 0.214895 \tR2: 0.273215\n",
      "Epoch: 2585 \tTraining Loss: 0.211753 \tR2: 0.273215\n",
      "Epoch: 2586 \tTraining Loss: 0.238776 \tR2: 0.273215\n",
      "Epoch: 2587 \tTraining Loss: 0.223488 \tR2: 0.273215\n",
      "Epoch: 2588 \tTraining Loss: 0.228323 \tR2: 0.273215\n",
      "Epoch: 2589 \tTraining Loss: 0.222681 \tR2: 0.273215\n",
      "Epoch: 2590 \tTraining Loss: 0.215875 \tR2: 0.273215\n",
      "Epoch: 2591 \tTraining Loss: 0.217309 \tR2: 0.273215\n",
      "Epoch: 2592 \tTraining Loss: 0.213027 \tR2: 0.273215\n",
      "Epoch: 2593 \tTraining Loss: 0.225417 \tR2: 0.273215\n",
      "Epoch: 2594 \tTraining Loss: 0.233933 \tR2: 0.273215\n",
      "Epoch: 2595 \tTraining Loss: 0.222244 \tR2: 0.273215\n",
      "Epoch: 2596 \tTraining Loss: 0.232799 \tR2: 0.273215\n",
      "Epoch: 2597 \tTraining Loss: 0.217629 \tR2: 0.273215\n",
      "Epoch: 2598 \tTraining Loss: 0.226990 \tR2: 0.273215\n",
      "Epoch: 2599 \tTraining Loss: 0.226686 \tR2: 0.273215\n",
      "Epoch: 2600 \tTraining Loss: 0.217170 \tR2: 0.619895\n",
      "Epoch: 2601 \tTraining Loss: 0.214280 \tR2: 0.619895\n",
      "Epoch: 2602 \tTraining Loss: 0.213441 \tR2: 0.619895\n",
      "Epoch: 2603 \tTraining Loss: 0.215806 \tR2: 0.619895\n",
      "Epoch: 2604 \tTraining Loss: 0.221296 \tR2: 0.619895\n",
      "Epoch: 2605 \tTraining Loss: 0.212015 \tR2: 0.619895\n",
      "Epoch: 2606 \tTraining Loss: 0.241826 \tR2: 0.619895\n",
      "Epoch: 2607 \tTraining Loss: 0.226696 \tR2: 0.619895\n",
      "Epoch: 2608 \tTraining Loss: 0.212462 \tR2: 0.619895\n",
      "Epoch: 2609 \tTraining Loss: 0.220711 \tR2: 0.619895\n",
      "Epoch: 2610 \tTraining Loss: 0.224703 \tR2: 0.619895\n",
      "Epoch: 2611 \tTraining Loss: 0.232196 \tR2: 0.619895\n",
      "Epoch: 2612 \tTraining Loss: 0.225362 \tR2: 0.619895\n",
      "Epoch: 2613 \tTraining Loss: 0.218647 \tR2: 0.619895\n",
      "Epoch: 2614 \tTraining Loss: 0.208509 \tR2: 0.619895\n",
      "Epoch: 2615 \tTraining Loss: 0.210237 \tR2: 0.619895\n",
      "Epoch: 2616 \tTraining Loss: 0.211361 \tR2: 0.619895\n",
      "Epoch: 2617 \tTraining Loss: 0.223310 \tR2: 0.619895\n",
      "Epoch: 2618 \tTraining Loss: 0.238365 \tR2: 0.619895\n",
      "Epoch: 2619 \tTraining Loss: 0.219901 \tR2: 0.619895\n",
      "Epoch: 2620 \tTraining Loss: 0.225232 \tR2: 0.619895\n",
      "Epoch: 2621 \tTraining Loss: 0.238057 \tR2: 0.619895\n",
      "Epoch: 2622 \tTraining Loss: 0.218374 \tR2: 0.619895\n",
      "Epoch: 2623 \tTraining Loss: 0.214272 \tR2: 0.619895\n",
      "Epoch: 2624 \tTraining Loss: 0.231911 \tR2: 0.619895\n",
      "Epoch: 2625 \tTraining Loss: 0.222130 \tR2: 0.619895\n",
      "Epoch: 2626 \tTraining Loss: 0.207065 \tR2: 0.619895\n",
      "Epoch: 2627 \tTraining Loss: 0.238027 \tR2: 0.619895\n",
      "Epoch: 2628 \tTraining Loss: 0.231567 \tR2: 0.619895\n",
      "Epoch: 2629 \tTraining Loss: 0.226323 \tR2: 0.619895\n",
      "Epoch: 2630 \tTraining Loss: 0.218653 \tR2: 0.619895\n",
      "Epoch: 2631 \tTraining Loss: 0.240068 \tR2: 0.619895\n",
      "Epoch: 2632 \tTraining Loss: 0.228437 \tR2: 0.619895\n",
      "Epoch: 2633 \tTraining Loss: 0.222475 \tR2: 0.619895\n",
      "Epoch: 2634 \tTraining Loss: 0.195228 \tR2: 0.619895\n",
      "Epoch: 2635 \tTraining Loss: 0.212038 \tR2: 0.619895\n",
      "Epoch: 2636 \tTraining Loss: 0.216363 \tR2: 0.619895\n",
      "Epoch: 2637 \tTraining Loss: 0.204575 \tR2: 0.619895\n",
      "Epoch: 2638 \tTraining Loss: 0.208879 \tR2: 0.619895\n",
      "Epoch: 2639 \tTraining Loss: 0.232555 \tR2: 0.619895\n",
      "Epoch: 2640 \tTraining Loss: 0.223771 \tR2: 0.619895\n",
      "Epoch: 2641 \tTraining Loss: 0.216352 \tR2: 0.619895\n",
      "Epoch: 2642 \tTraining Loss: 0.208436 \tR2: 0.619895\n",
      "Epoch: 2643 \tTraining Loss: 0.198852 \tR2: 0.619895\n",
      "Epoch: 2644 \tTraining Loss: 0.206288 \tR2: 0.619895\n",
      "Epoch: 2645 \tTraining Loss: 0.204117 \tR2: 0.619895\n",
      "Epoch: 2646 \tTraining Loss: 0.201259 \tR2: 0.619895\n",
      "Epoch: 2647 \tTraining Loss: 0.195086 \tR2: 0.619895\n",
      "Epoch: 2648 \tTraining Loss: 0.216436 \tR2: 0.619895\n",
      "Epoch: 2649 \tTraining Loss: 0.208677 \tR2: 0.619895\n",
      "Epoch: 2650 \tTraining Loss: 0.206560 \tR2: 0.619895\n",
      "Epoch: 2651 \tTraining Loss: 0.206507 \tR2: 0.619895\n",
      "Epoch: 2652 \tTraining Loss: 0.217092 \tR2: 0.619895\n",
      "Epoch: 2653 \tTraining Loss: 0.206134 \tR2: 0.619895\n",
      "Epoch: 2654 \tTraining Loss: 0.200710 \tR2: 0.619895\n",
      "Epoch: 2655 \tTraining Loss: 0.226276 \tR2: 0.619895\n",
      "Epoch: 2656 \tTraining Loss: 0.230452 \tR2: 0.619895\n",
      "Epoch: 2657 \tTraining Loss: 0.226541 \tR2: 0.619895\n",
      "Epoch: 2658 \tTraining Loss: 0.207693 \tR2: 0.619895\n",
      "Epoch: 2659 \tTraining Loss: 0.227261 \tR2: 0.619895\n",
      "Epoch: 2660 \tTraining Loss: 0.212392 \tR2: 0.619895\n",
      "Epoch: 2661 \tTraining Loss: 0.229586 \tR2: 0.619895\n",
      "Epoch: 2662 \tTraining Loss: 0.216370 \tR2: 0.619895\n",
      "Epoch: 2663 \tTraining Loss: 0.219822 \tR2: 0.619895\n",
      "Epoch: 2664 \tTraining Loss: 0.250662 \tR2: 0.619895\n",
      "Epoch: 2665 \tTraining Loss: 0.219837 \tR2: 0.619895\n",
      "Epoch: 2666 \tTraining Loss: 0.218217 \tR2: 0.619895\n",
      "Epoch: 2667 \tTraining Loss: 0.239527 \tR2: 0.619895\n",
      "Epoch: 2668 \tTraining Loss: 0.225339 \tR2: 0.619895\n",
      "Epoch: 2669 \tTraining Loss: 0.212759 \tR2: 0.619895\n",
      "Epoch: 2670 \tTraining Loss: 0.204309 \tR2: 0.619895\n",
      "Epoch: 2671 \tTraining Loss: 0.221090 \tR2: 0.619895\n",
      "Epoch: 2672 \tTraining Loss: 0.260322 \tR2: 0.619895\n",
      "Epoch: 2673 \tTraining Loss: 0.199753 \tR2: 0.619895\n",
      "Epoch: 2674 \tTraining Loss: 0.217065 \tR2: 0.619895\n",
      "Epoch: 2675 \tTraining Loss: 0.225897 \tR2: 0.619895\n",
      "Epoch: 2676 \tTraining Loss: 0.225102 \tR2: 0.619895\n",
      "Epoch: 2677 \tTraining Loss: 0.207021 \tR2: 0.619895\n",
      "Epoch: 2678 \tTraining Loss: 0.211531 \tR2: 0.619895\n",
      "Epoch: 2679 \tTraining Loss: 0.213212 \tR2: 0.619895\n",
      "Epoch: 2680 \tTraining Loss: 0.230444 \tR2: 0.619895\n",
      "Epoch: 2681 \tTraining Loss: 0.220130 \tR2: 0.619895\n",
      "Epoch: 2682 \tTraining Loss: 0.194304 \tR2: 0.619895\n",
      "Epoch: 2683 \tTraining Loss: 0.219293 \tR2: 0.619895\n",
      "Epoch: 2684 \tTraining Loss: 0.196084 \tR2: 0.619895\n",
      "Epoch: 2685 \tTraining Loss: 0.198850 \tR2: 0.619895\n",
      "Epoch: 2686 \tTraining Loss: 0.218858 \tR2: 0.619895\n",
      "Epoch: 2687 \tTraining Loss: 0.229318 \tR2: 0.619895\n",
      "Epoch: 2688 \tTraining Loss: 0.201994 \tR2: 0.619895\n",
      "Epoch: 2689 \tTraining Loss: 0.234043 \tR2: 0.619895\n",
      "Epoch: 2690 \tTraining Loss: 0.222841 \tR2: 0.619895\n",
      "Epoch: 2691 \tTraining Loss: 0.229420 \tR2: 0.619895\n",
      "Epoch: 2692 \tTraining Loss: 0.222637 \tR2: 0.619895\n",
      "Epoch: 2693 \tTraining Loss: 0.213807 \tR2: 0.619895\n",
      "Epoch: 2694 \tTraining Loss: 0.232357 \tR2: 0.619895\n",
      "Epoch: 2695 \tTraining Loss: 0.220647 \tR2: 0.619895\n",
      "Epoch: 2696 \tTraining Loss: 0.219491 \tR2: 0.619895\n",
      "Epoch: 2697 \tTraining Loss: 0.211419 \tR2: 0.619895\n",
      "Epoch: 2698 \tTraining Loss: 0.218750 \tR2: 0.619895\n",
      "Epoch: 2699 \tTraining Loss: 0.219681 \tR2: 0.619895\n",
      "Epoch: 2700 \tTraining Loss: 0.217003 \tR2: 0.738406\n",
      "Epoch: 2701 \tTraining Loss: 0.230004 \tR2: 0.738406\n",
      "Epoch: 2702 \tTraining Loss: 0.211505 \tR2: 0.738406\n",
      "Epoch: 2703 \tTraining Loss: 0.216328 \tR2: 0.738406\n",
      "Epoch: 2704 \tTraining Loss: 0.208909 \tR2: 0.738406\n",
      "Epoch: 2705 \tTraining Loss: 0.209425 \tR2: 0.738406\n",
      "Epoch: 2706 \tTraining Loss: 0.217771 \tR2: 0.738406\n",
      "Epoch: 2707 \tTraining Loss: 0.227714 \tR2: 0.738406\n",
      "Epoch: 2708 \tTraining Loss: 0.215416 \tR2: 0.738406\n",
      "Epoch: 2709 \tTraining Loss: 0.206852 \tR2: 0.738406\n",
      "Epoch: 2710 \tTraining Loss: 0.223529 \tR2: 0.738406\n",
      "Epoch: 2711 \tTraining Loss: 0.215570 \tR2: 0.738406\n",
      "Epoch: 2712 \tTraining Loss: 0.215140 \tR2: 0.738406\n",
      "Epoch: 2713 \tTraining Loss: 0.232561 \tR2: 0.738406\n",
      "Epoch: 2714 \tTraining Loss: 0.216520 \tR2: 0.738406\n",
      "Epoch: 2715 \tTraining Loss: 0.235136 \tR2: 0.738406\n",
      "Epoch: 2716 \tTraining Loss: 0.198494 \tR2: 0.738406\n",
      "Epoch: 2717 \tTraining Loss: 0.219775 \tR2: 0.738406\n",
      "Epoch: 2718 \tTraining Loss: 0.210608 \tR2: 0.738406\n",
      "Epoch: 2719 \tTraining Loss: 0.226665 \tR2: 0.738406\n",
      "Epoch: 2720 \tTraining Loss: 0.214861 \tR2: 0.738406\n",
      "Epoch: 2721 \tTraining Loss: 0.212938 \tR2: 0.738406\n",
      "Epoch: 2722 \tTraining Loss: 0.230492 \tR2: 0.738406\n",
      "Epoch: 2723 \tTraining Loss: 0.222008 \tR2: 0.738406\n",
      "Epoch: 2724 \tTraining Loss: 0.224002 \tR2: 0.738406\n",
      "Epoch: 2725 \tTraining Loss: 0.202659 \tR2: 0.738406\n",
      "Epoch: 2726 \tTraining Loss: 0.215580 \tR2: 0.738406\n",
      "Epoch: 2727 \tTraining Loss: 0.211695 \tR2: 0.738406\n",
      "Epoch: 2728 \tTraining Loss: 0.230401 \tR2: 0.738406\n",
      "Epoch: 2729 \tTraining Loss: 0.219719 \tR2: 0.738406\n",
      "Epoch: 2730 \tTraining Loss: 0.218638 \tR2: 0.738406\n",
      "Epoch: 2731 \tTraining Loss: 0.210503 \tR2: 0.738406\n",
      "Epoch: 2732 \tTraining Loss: 0.258059 \tR2: 0.738406\n",
      "Epoch: 2733 \tTraining Loss: 0.232037 \tR2: 0.738406\n",
      "Epoch: 2734 \tTraining Loss: 0.247988 \tR2: 0.738406\n",
      "Epoch: 2735 \tTraining Loss: 0.234076 \tR2: 0.738406\n",
      "Epoch: 2736 \tTraining Loss: 0.217005 \tR2: 0.738406\n",
      "Epoch: 2737 \tTraining Loss: 0.211120 \tR2: 0.738406\n",
      "Epoch: 2738 \tTraining Loss: 0.233310 \tR2: 0.738406\n",
      "Epoch: 2739 \tTraining Loss: 0.221180 \tR2: 0.738406\n",
      "Epoch: 2740 \tTraining Loss: 0.214673 \tR2: 0.738406\n",
      "Epoch: 2741 \tTraining Loss: 0.209904 \tR2: 0.738406\n",
      "Epoch: 2742 \tTraining Loss: 0.220759 \tR2: 0.738406\n",
      "Epoch: 2743 \tTraining Loss: 0.202769 \tR2: 0.738406\n",
      "Epoch: 2744 \tTraining Loss: 0.225810 \tR2: 0.738406\n",
      "Epoch: 2745 \tTraining Loss: 0.233705 \tR2: 0.738406\n",
      "Epoch: 2746 \tTraining Loss: 0.208326 \tR2: 0.738406\n",
      "Epoch: 2747 \tTraining Loss: 0.209615 \tR2: 0.738406\n",
      "Epoch: 2748 \tTraining Loss: 0.198513 \tR2: 0.738406\n",
      "Epoch: 2749 \tTraining Loss: 0.218730 \tR2: 0.738406\n",
      "Epoch: 2750 \tTraining Loss: 0.208787 \tR2: 0.738406\n",
      "Epoch: 2751 \tTraining Loss: 0.228698 \tR2: 0.738406\n",
      "Epoch: 2752 \tTraining Loss: 0.210947 \tR2: 0.738406\n",
      "Epoch: 2753 \tTraining Loss: 0.210343 \tR2: 0.738406\n",
      "Epoch: 2754 \tTraining Loss: 0.213579 \tR2: 0.738406\n",
      "Epoch: 2755 \tTraining Loss: 0.212999 \tR2: 0.738406\n",
      "Epoch: 2756 \tTraining Loss: 0.205123 \tR2: 0.738406\n",
      "Epoch: 2757 \tTraining Loss: 0.212346 \tR2: 0.738406\n",
      "Epoch: 2758 \tTraining Loss: 0.212516 \tR2: 0.738406\n",
      "Epoch: 2759 \tTraining Loss: 0.217215 \tR2: 0.738406\n",
      "Epoch: 2760 \tTraining Loss: 0.226328 \tR2: 0.738406\n",
      "Epoch: 2761 \tTraining Loss: 0.232997 \tR2: 0.738406\n",
      "Epoch: 2762 \tTraining Loss: 0.229535 \tR2: 0.738406\n",
      "Epoch: 2763 \tTraining Loss: 0.205018 \tR2: 0.738406\n",
      "Epoch: 2764 \tTraining Loss: 0.214614 \tR2: 0.738406\n",
      "Epoch: 2765 \tTraining Loss: 0.212690 \tR2: 0.738406\n",
      "Epoch: 2766 \tTraining Loss: 0.205857 \tR2: 0.738406\n",
      "Epoch: 2767 \tTraining Loss: 0.214359 \tR2: 0.738406\n",
      "Epoch: 2768 \tTraining Loss: 0.210221 \tR2: 0.738406\n",
      "Epoch: 2769 \tTraining Loss: 0.250570 \tR2: 0.738406\n",
      "Epoch: 2770 \tTraining Loss: 0.212348 \tR2: 0.738406\n",
      "Epoch: 2771 \tTraining Loss: 0.225041 \tR2: 0.738406\n",
      "Epoch: 2772 \tTraining Loss: 0.219268 \tR2: 0.738406\n",
      "Epoch: 2773 \tTraining Loss: 0.226879 \tR2: 0.738406\n",
      "Epoch: 2774 \tTraining Loss: 0.199151 \tR2: 0.738406\n",
      "Epoch: 2775 \tTraining Loss: 0.206395 \tR2: 0.738406\n",
      "Epoch: 2776 \tTraining Loss: 0.213525 \tR2: 0.738406\n",
      "Epoch: 2777 \tTraining Loss: 0.224208 \tR2: 0.738406\n",
      "Epoch: 2778 \tTraining Loss: 0.238933 \tR2: 0.738406\n",
      "Epoch: 2779 \tTraining Loss: 0.195566 \tR2: 0.738406\n",
      "Epoch: 2780 \tTraining Loss: 0.216688 \tR2: 0.738406\n",
      "Epoch: 2781 \tTraining Loss: 0.220509 \tR2: 0.738406\n",
      "Epoch: 2782 \tTraining Loss: 0.211704 \tR2: 0.738406\n",
      "Epoch: 2783 \tTraining Loss: 0.216825 \tR2: 0.738406\n",
      "Epoch: 2784 \tTraining Loss: 0.217954 \tR2: 0.738406\n",
      "Epoch: 2785 \tTraining Loss: 0.223982 \tR2: 0.738406\n",
      "Epoch: 2786 \tTraining Loss: 0.217865 \tR2: 0.738406\n",
      "Epoch: 2787 \tTraining Loss: 0.226697 \tR2: 0.738406\n",
      "Epoch: 2788 \tTraining Loss: 0.216963 \tR2: 0.738406\n",
      "Epoch: 2789 \tTraining Loss: 0.223968 \tR2: 0.738406\n",
      "Epoch: 2790 \tTraining Loss: 0.209831 \tR2: 0.738406\n",
      "Epoch: 2791 \tTraining Loss: 0.213215 \tR2: 0.738406\n",
      "Epoch: 2792 \tTraining Loss: 0.216450 \tR2: 0.738406\n",
      "Epoch: 2793 \tTraining Loss: 0.209412 \tR2: 0.738406\n",
      "Epoch: 2794 \tTraining Loss: 0.207806 \tR2: 0.738406\n",
      "Epoch: 2795 \tTraining Loss: 0.213514 \tR2: 0.738406\n",
      "Epoch: 2796 \tTraining Loss: 0.201870 \tR2: 0.738406\n",
      "Epoch: 2797 \tTraining Loss: 0.225451 \tR2: 0.738406\n",
      "Epoch: 2798 \tTraining Loss: 0.219319 \tR2: 0.738406\n",
      "Epoch: 2799 \tTraining Loss: 0.215420 \tR2: 0.738406\n",
      "Epoch: 2800 \tTraining Loss: 0.206053 \tR2: 0.815800\n",
      "Epoch: 2801 \tTraining Loss: 0.209282 \tR2: 0.815800\n",
      "Epoch: 2802 \tTraining Loss: 0.208371 \tR2: 0.815800\n",
      "Epoch: 2803 \tTraining Loss: 0.220153 \tR2: 0.815800\n",
      "Epoch: 2804 \tTraining Loss: 0.227547 \tR2: 0.815800\n",
      "Epoch: 2805 \tTraining Loss: 0.212519 \tR2: 0.815800\n",
      "Epoch: 2806 \tTraining Loss: 0.212011 \tR2: 0.815800\n",
      "Epoch: 2807 \tTraining Loss: 0.213158 \tR2: 0.815800\n",
      "Epoch: 2808 \tTraining Loss: 0.213053 \tR2: 0.815800\n",
      "Epoch: 2809 \tTraining Loss: 0.206960 \tR2: 0.815800\n",
      "Epoch: 2810 \tTraining Loss: 0.220886 \tR2: 0.815800\n",
      "Epoch: 2811 \tTraining Loss: 0.201740 \tR2: 0.815800\n",
      "Epoch: 2812 \tTraining Loss: 0.221177 \tR2: 0.815800\n",
      "Epoch: 2813 \tTraining Loss: 0.213906 \tR2: 0.815800\n",
      "Epoch: 2814 \tTraining Loss: 0.204408 \tR2: 0.815800\n",
      "Epoch: 2815 \tTraining Loss: 0.202684 \tR2: 0.815800\n",
      "Epoch: 2816 \tTraining Loss: 0.211902 \tR2: 0.815800\n",
      "Epoch: 2817 \tTraining Loss: 0.225769 \tR2: 0.815800\n",
      "Epoch: 2818 \tTraining Loss: 0.206959 \tR2: 0.815800\n",
      "Epoch: 2819 \tTraining Loss: 0.205146 \tR2: 0.815800\n",
      "Epoch: 2820 \tTraining Loss: 0.223356 \tR2: 0.815800\n",
      "Epoch: 2821 \tTraining Loss: 0.217934 \tR2: 0.815800\n",
      "Epoch: 2822 \tTraining Loss: 0.207499 \tR2: 0.815800\n",
      "Epoch: 2823 \tTraining Loss: 0.219956 \tR2: 0.815800\n",
      "Epoch: 2824 \tTraining Loss: 0.211566 \tR2: 0.815800\n",
      "Epoch: 2825 \tTraining Loss: 0.208530 \tR2: 0.815800\n",
      "Epoch: 2826 \tTraining Loss: 0.221724 \tR2: 0.815800\n",
      "Epoch: 2827 \tTraining Loss: 0.236836 \tR2: 0.815800\n",
      "Epoch: 2828 \tTraining Loss: 0.204611 \tR2: 0.815800\n",
      "Epoch: 2829 \tTraining Loss: 0.221320 \tR2: 0.815800\n",
      "Epoch: 2830 \tTraining Loss: 0.225045 \tR2: 0.815800\n",
      "Epoch: 2831 \tTraining Loss: 0.219031 \tR2: 0.815800\n",
      "Epoch: 2832 \tTraining Loss: 0.203119 \tR2: 0.815800\n",
      "Epoch: 2833 \tTraining Loss: 0.212660 \tR2: 0.815800\n",
      "Epoch: 2834 \tTraining Loss: 0.228438 \tR2: 0.815800\n",
      "Epoch: 2835 \tTraining Loss: 0.212835 \tR2: 0.815800\n",
      "Epoch: 2836 \tTraining Loss: 0.287417 \tR2: 0.815800\n",
      "Epoch: 2837 \tTraining Loss: 0.225305 \tR2: 0.815800\n",
      "Epoch: 2838 \tTraining Loss: 0.222818 \tR2: 0.815800\n",
      "Epoch: 2839 \tTraining Loss: 0.222468 \tR2: 0.815800\n",
      "Epoch: 2840 \tTraining Loss: 0.237532 \tR2: 0.815800\n",
      "Epoch: 2841 \tTraining Loss: 0.224096 \tR2: 0.815800\n",
      "Epoch: 2842 \tTraining Loss: 0.222881 \tR2: 0.815800\n",
      "Epoch: 2843 \tTraining Loss: 0.223882 \tR2: 0.815800\n",
      "Epoch: 2844 \tTraining Loss: 0.222580 \tR2: 0.815800\n",
      "Epoch: 2845 \tTraining Loss: 0.204748 \tR2: 0.815800\n",
      "Epoch: 2846 \tTraining Loss: 0.230930 \tR2: 0.815800\n",
      "Epoch: 2847 \tTraining Loss: 0.206716 \tR2: 0.815800\n",
      "Epoch: 2848 \tTraining Loss: 0.196970 \tR2: 0.815800\n",
      "Epoch: 2849 \tTraining Loss: 0.218393 \tR2: 0.815800\n",
      "Epoch: 2850 \tTraining Loss: 0.232530 \tR2: 0.815800\n",
      "Epoch: 2851 \tTraining Loss: 0.217784 \tR2: 0.815800\n",
      "Epoch: 2852 \tTraining Loss: 0.231065 \tR2: 0.815800\n",
      "Epoch: 2853 \tTraining Loss: 0.205852 \tR2: 0.815800\n",
      "Epoch: 2854 \tTraining Loss: 0.206580 \tR2: 0.815800\n",
      "Epoch: 2855 \tTraining Loss: 0.224401 \tR2: 0.815800\n",
      "Epoch: 2856 \tTraining Loss: 0.216582 \tR2: 0.815800\n",
      "Epoch: 2857 \tTraining Loss: 0.204365 \tR2: 0.815800\n",
      "Epoch: 2858 \tTraining Loss: 0.232322 \tR2: 0.815800\n",
      "Epoch: 2859 \tTraining Loss: 0.210774 \tR2: 0.815800\n",
      "Epoch: 2860 \tTraining Loss: 0.224307 \tR2: 0.815800\n",
      "Epoch: 2861 \tTraining Loss: 0.222986 \tR2: 0.815800\n",
      "Epoch: 2862 \tTraining Loss: 0.223049 \tR2: 0.815800\n",
      "Epoch: 2863 \tTraining Loss: 0.237252 \tR2: 0.815800\n",
      "Epoch: 2864 \tTraining Loss: 0.223663 \tR2: 0.815800\n",
      "Epoch: 2865 \tTraining Loss: 0.222733 \tR2: 0.815800\n",
      "Epoch: 2866 \tTraining Loss: 0.233255 \tR2: 0.815800\n",
      "Epoch: 2867 \tTraining Loss: 0.202839 \tR2: 0.815800\n",
      "Epoch: 2868 \tTraining Loss: 0.212281 \tR2: 0.815800\n",
      "Epoch: 2869 \tTraining Loss: 0.210770 \tR2: 0.815800\n",
      "Epoch: 2870 \tTraining Loss: 0.228443 \tR2: 0.815800\n",
      "Epoch: 2871 \tTraining Loss: 0.206973 \tR2: 0.815800\n",
      "Epoch: 2872 \tTraining Loss: 0.211627 \tR2: 0.815800\n",
      "Epoch: 2873 \tTraining Loss: 0.222123 \tR2: 0.815800\n",
      "Epoch: 2874 \tTraining Loss: 0.214942 \tR2: 0.815800\n",
      "Epoch: 2875 \tTraining Loss: 0.216123 \tR2: 0.815800\n",
      "Epoch: 2876 \tTraining Loss: 0.217055 \tR2: 0.815800\n",
      "Epoch: 2877 \tTraining Loss: 0.213714 \tR2: 0.815800\n",
      "Epoch: 2878 \tTraining Loss: 0.201970 \tR2: 0.815800\n",
      "Epoch: 2879 \tTraining Loss: 0.213054 \tR2: 0.815800\n",
      "Epoch: 2880 \tTraining Loss: 0.213399 \tR2: 0.815800\n",
      "Epoch: 2881 \tTraining Loss: 0.217582 \tR2: 0.815800\n",
      "Epoch: 2882 \tTraining Loss: 0.219685 \tR2: 0.815800\n",
      "Epoch: 2883 \tTraining Loss: 0.211542 \tR2: 0.815800\n",
      "Epoch: 2884 \tTraining Loss: 0.228820 \tR2: 0.815800\n",
      "Epoch: 2885 \tTraining Loss: 0.232980 \tR2: 0.815800\n",
      "Epoch: 2886 \tTraining Loss: 0.206743 \tR2: 0.815800\n",
      "Epoch: 2887 \tTraining Loss: 0.215365 \tR2: 0.815800\n",
      "Epoch: 2888 \tTraining Loss: 0.200015 \tR2: 0.815800\n",
      "Epoch: 2889 \tTraining Loss: 0.224692 \tR2: 0.815800\n",
      "Epoch: 2890 \tTraining Loss: 0.222987 \tR2: 0.815800\n",
      "Epoch: 2891 \tTraining Loss: 0.211131 \tR2: 0.815800\n",
      "Epoch: 2892 \tTraining Loss: 0.206282 \tR2: 0.815800\n",
      "Epoch: 2893 \tTraining Loss: 0.219102 \tR2: 0.815800\n",
      "Epoch: 2894 \tTraining Loss: 0.202805 \tR2: 0.815800\n",
      "Epoch: 2895 \tTraining Loss: 0.208139 \tR2: 0.815800\n",
      "Epoch: 2896 \tTraining Loss: 0.212524 \tR2: 0.815800\n",
      "Epoch: 2897 \tTraining Loss: 0.205003 \tR2: 0.815800\n",
      "Epoch: 2898 \tTraining Loss: 0.218816 \tR2: 0.815800\n",
      "Epoch: 2899 \tTraining Loss: 0.222137 \tR2: 0.815800\n",
      "Epoch: 2900 \tTraining Loss: 0.213747 \tR2: 0.587765\n",
      "Epoch: 2901 \tTraining Loss: 0.207374 \tR2: 0.587765\n",
      "Epoch: 2902 \tTraining Loss: 0.230130 \tR2: 0.587765\n",
      "Epoch: 2903 \tTraining Loss: 0.238108 \tR2: 0.587765\n",
      "Epoch: 2904 \tTraining Loss: 0.215387 \tR2: 0.587765\n",
      "Epoch: 2905 \tTraining Loss: 0.215745 \tR2: 0.587765\n",
      "Epoch: 2906 \tTraining Loss: 0.217261 \tR2: 0.587765\n",
      "Epoch: 2907 \tTraining Loss: 0.220057 \tR2: 0.587765\n",
      "Epoch: 2908 \tTraining Loss: 0.227539 \tR2: 0.587765\n",
      "Epoch: 2909 \tTraining Loss: 0.218648 \tR2: 0.587765\n",
      "Epoch: 2910 \tTraining Loss: 0.213818 \tR2: 0.587765\n",
      "Epoch: 2911 \tTraining Loss: 0.222743 \tR2: 0.587765\n",
      "Epoch: 2912 \tTraining Loss: 0.214941 \tR2: 0.587765\n",
      "Epoch: 2913 \tTraining Loss: 0.211462 \tR2: 0.587765\n",
      "Epoch: 2914 \tTraining Loss: 0.217057 \tR2: 0.587765\n",
      "Epoch: 2915 \tTraining Loss: 0.204590 \tR2: 0.587765\n",
      "Epoch: 2916 \tTraining Loss: 0.217145 \tR2: 0.587765\n",
      "Epoch: 2917 \tTraining Loss: 0.214863 \tR2: 0.587765\n",
      "Epoch: 2918 \tTraining Loss: 0.200106 \tR2: 0.587765\n",
      "Epoch: 2919 \tTraining Loss: 0.199698 \tR2: 0.587765\n",
      "Epoch: 2920 \tTraining Loss: 0.222141 \tR2: 0.587765\n",
      "Epoch: 2921 \tTraining Loss: 0.215759 \tR2: 0.587765\n",
      "Epoch: 2922 \tTraining Loss: 0.198151 \tR2: 0.587765\n",
      "Epoch: 2923 \tTraining Loss: 0.202917 \tR2: 0.587765\n",
      "Epoch: 2924 \tTraining Loss: 0.236158 \tR2: 0.587765\n",
      "Epoch: 2925 \tTraining Loss: 0.215523 \tR2: 0.587765\n",
      "Epoch: 2926 \tTraining Loss: 0.234163 \tR2: 0.587765\n",
      "Epoch: 2927 \tTraining Loss: 0.207743 \tR2: 0.587765\n",
      "Epoch: 2928 \tTraining Loss: 0.232043 \tR2: 0.587765\n",
      "Epoch: 2929 \tTraining Loss: 0.214082 \tR2: 0.587765\n",
      "Epoch: 2930 \tTraining Loss: 0.222141 \tR2: 0.587765\n",
      "Epoch: 2931 \tTraining Loss: 0.221157 \tR2: 0.587765\n",
      "Epoch: 2932 \tTraining Loss: 0.230251 \tR2: 0.587765\n",
      "Epoch: 2933 \tTraining Loss: 0.221278 \tR2: 0.587765\n",
      "Epoch: 2934 \tTraining Loss: 0.200646 \tR2: 0.587765\n",
      "Epoch: 2935 \tTraining Loss: 0.218419 \tR2: 0.587765\n",
      "Epoch: 2936 \tTraining Loss: 0.209884 \tR2: 0.587765\n",
      "Epoch: 2937 \tTraining Loss: 0.222351 \tR2: 0.587765\n",
      "Epoch: 2938 \tTraining Loss: 0.208829 \tR2: 0.587765\n",
      "Epoch: 2939 \tTraining Loss: 0.232076 \tR2: 0.587765\n",
      "Epoch: 2940 \tTraining Loss: 0.239502 \tR2: 0.587765\n",
      "Epoch: 2941 \tTraining Loss: 0.233893 \tR2: 0.587765\n",
      "Epoch: 2942 \tTraining Loss: 0.227771 \tR2: 0.587765\n",
      "Epoch: 2943 \tTraining Loss: 0.217541 \tR2: 0.587765\n",
      "Epoch: 2944 \tTraining Loss: 0.199914 \tR2: 0.587765\n",
      "Epoch: 2945 \tTraining Loss: 0.209507 \tR2: 0.587765\n",
      "Epoch: 2946 \tTraining Loss: 0.208240 \tR2: 0.587765\n",
      "Epoch: 2947 \tTraining Loss: 0.230927 \tR2: 0.587765\n",
      "Epoch: 2948 \tTraining Loss: 0.206697 \tR2: 0.587765\n",
      "Epoch: 2949 \tTraining Loss: 0.223871 \tR2: 0.587765\n",
      "Epoch: 2950 \tTraining Loss: 0.231489 \tR2: 0.587765\n",
      "Epoch: 2951 \tTraining Loss: 0.223373 \tR2: 0.587765\n",
      "Epoch: 2952 \tTraining Loss: 0.210461 \tR2: 0.587765\n",
      "Epoch: 2953 \tTraining Loss: 0.253290 \tR2: 0.587765\n",
      "Epoch: 2954 \tTraining Loss: 0.215081 \tR2: 0.587765\n",
      "Epoch: 2955 \tTraining Loss: 0.218696 \tR2: 0.587765\n",
      "Epoch: 2956 \tTraining Loss: 0.218765 \tR2: 0.587765\n",
      "Epoch: 2957 \tTraining Loss: 0.216282 \tR2: 0.587765\n",
      "Epoch: 2958 \tTraining Loss: 0.239902 \tR2: 0.587765\n",
      "Epoch: 2959 \tTraining Loss: 0.213663 \tR2: 0.587765\n",
      "Epoch: 2960 \tTraining Loss: 0.195971 \tR2: 0.587765\n",
      "Epoch: 2961 \tTraining Loss: 0.229342 \tR2: 0.587765\n",
      "Epoch: 2962 \tTraining Loss: 0.227117 \tR2: 0.587765\n",
      "Epoch: 2963 \tTraining Loss: 0.239003 \tR2: 0.587765\n",
      "Epoch: 2964 \tTraining Loss: 0.214446 \tR2: 0.587765\n",
      "Epoch: 2965 \tTraining Loss: 0.203156 \tR2: 0.587765\n",
      "Epoch: 2966 \tTraining Loss: 0.211336 \tR2: 0.587765\n",
      "Epoch: 2967 \tTraining Loss: 0.211339 \tR2: 0.587765\n",
      "Epoch: 2968 \tTraining Loss: 0.222436 \tR2: 0.587765\n",
      "Epoch: 2969 \tTraining Loss: 0.201624 \tR2: 0.587765\n",
      "Epoch: 2970 \tTraining Loss: 0.199299 \tR2: 0.587765\n",
      "Epoch: 2971 \tTraining Loss: 0.198634 \tR2: 0.587765\n",
      "Epoch: 2972 \tTraining Loss: 0.205737 \tR2: 0.587765\n",
      "Epoch: 2973 \tTraining Loss: 0.214114 \tR2: 0.587765\n",
      "Epoch: 2974 \tTraining Loss: 0.200140 \tR2: 0.587765\n",
      "Epoch: 2975 \tTraining Loss: 0.209626 \tR2: 0.587765\n",
      "Epoch: 2976 \tTraining Loss: 0.217145 \tR2: 0.587765\n",
      "Epoch: 2977 \tTraining Loss: 0.220600 \tR2: 0.587765\n",
      "Epoch: 2978 \tTraining Loss: 0.218619 \tR2: 0.587765\n",
      "Epoch: 2979 \tTraining Loss: 0.205844 \tR2: 0.587765\n",
      "Epoch: 2980 \tTraining Loss: 0.202810 \tR2: 0.587765\n",
      "Epoch: 2981 \tTraining Loss: 0.201162 \tR2: 0.587765\n",
      "Epoch: 2982 \tTraining Loss: 0.213823 \tR2: 0.587765\n",
      "Epoch: 2983 \tTraining Loss: 0.216751 \tR2: 0.587765\n",
      "Epoch: 2984 \tTraining Loss: 0.215820 \tR2: 0.587765\n",
      "Epoch: 2985 \tTraining Loss: 0.219315 \tR2: 0.587765\n",
      "Epoch: 2986 \tTraining Loss: 0.233697 \tR2: 0.587765\n",
      "Epoch: 2987 \tTraining Loss: 0.214505 \tR2: 0.587765\n",
      "Epoch: 2988 \tTraining Loss: 0.217118 \tR2: 0.587765\n",
      "Epoch: 2989 \tTraining Loss: 0.199250 \tR2: 0.587765\n",
      "Epoch: 2990 \tTraining Loss: 0.232842 \tR2: 0.587765\n",
      "Epoch: 2991 \tTraining Loss: 0.230531 \tR2: 0.587765\n",
      "Epoch: 2992 \tTraining Loss: 0.218292 \tR2: 0.587765\n",
      "Epoch: 2993 \tTraining Loss: 0.271907 \tR2: 0.587765\n",
      "Epoch: 2994 \tTraining Loss: 0.209447 \tR2: 0.587765\n",
      "Epoch: 2995 \tTraining Loss: 0.200830 \tR2: 0.587765\n",
      "Epoch: 2996 \tTraining Loss: 0.221835 \tR2: 0.587765\n",
      "Epoch: 2997 \tTraining Loss: 0.205858 \tR2: 0.587765\n",
      "Epoch: 2998 \tTraining Loss: 0.210082 \tR2: 0.587765\n",
      "Epoch: 2999 \tTraining Loss: 0.219440 \tR2: 0.587765\n",
      "Epoch: 3000 \tTraining Loss: 0.215891 \tR2: 0.655680\n",
      "Epoch: 3001 \tTraining Loss: 0.213092 \tR2: 0.655680\n",
      "Epoch: 3002 \tTraining Loss: 0.231835 \tR2: 0.655680\n",
      "Epoch: 3003 \tTraining Loss: 0.222229 \tR2: 0.655680\n",
      "Epoch: 3004 \tTraining Loss: 0.231667 \tR2: 0.655680\n",
      "Epoch: 3005 \tTraining Loss: 0.213498 \tR2: 0.655680\n",
      "Epoch: 3006 \tTraining Loss: 0.211156 \tR2: 0.655680\n",
      "Epoch: 3007 \tTraining Loss: 0.214907 \tR2: 0.655680\n",
      "Epoch: 3008 \tTraining Loss: 0.211104 \tR2: 0.655680\n",
      "Epoch: 3009 \tTraining Loss: 0.211136 \tR2: 0.655680\n",
      "Epoch: 3010 \tTraining Loss: 0.204848 \tR2: 0.655680\n",
      "Epoch: 3011 \tTraining Loss: 0.222784 \tR2: 0.655680\n",
      "Epoch: 3012 \tTraining Loss: 0.195055 \tR2: 0.655680\n",
      "Epoch: 3013 \tTraining Loss: 0.223495 \tR2: 0.655680\n",
      "Epoch: 3014 \tTraining Loss: 0.201311 \tR2: 0.655680\n",
      "Epoch: 3015 \tTraining Loss: 0.222511 \tR2: 0.655680\n",
      "Epoch: 3016 \tTraining Loss: 0.225345 \tR2: 0.655680\n",
      "Epoch: 3017 \tTraining Loss: 0.208600 \tR2: 0.655680\n",
      "Epoch: 3018 \tTraining Loss: 0.221595 \tR2: 0.655680\n",
      "Epoch: 3019 \tTraining Loss: 0.205543 \tR2: 0.655680\n",
      "Epoch: 3020 \tTraining Loss: 0.217825 \tR2: 0.655680\n",
      "Epoch: 3021 \tTraining Loss: 0.265513 \tR2: 0.655680\n",
      "Epoch: 3022 \tTraining Loss: 0.204360 \tR2: 0.655680\n",
      "Epoch: 3023 \tTraining Loss: 0.232094 \tR2: 0.655680\n",
      "Epoch: 3024 \tTraining Loss: 0.223201 \tR2: 0.655680\n",
      "Epoch: 3025 \tTraining Loss: 0.195740 \tR2: 0.655680\n",
      "Epoch: 3026 \tTraining Loss: 0.217720 \tR2: 0.655680\n",
      "Epoch: 3027 \tTraining Loss: 0.220900 \tR2: 0.655680\n",
      "Epoch: 3028 \tTraining Loss: 0.219169 \tR2: 0.655680\n",
      "Epoch: 3029 \tTraining Loss: 0.213152 \tR2: 0.655680\n",
      "Epoch: 3030 \tTraining Loss: 0.229406 \tR2: 0.655680\n",
      "Epoch: 3031 \tTraining Loss: 0.201551 \tR2: 0.655680\n",
      "Epoch: 3032 \tTraining Loss: 0.221711 \tR2: 0.655680\n",
      "Epoch: 3033 \tTraining Loss: 0.210730 \tR2: 0.655680\n",
      "Epoch: 3034 \tTraining Loss: 0.221918 \tR2: 0.655680\n",
      "Epoch: 3035 \tTraining Loss: 0.221494 \tR2: 0.655680\n",
      "Epoch: 3036 \tTraining Loss: 0.196455 \tR2: 0.655680\n",
      "Epoch: 3037 \tTraining Loss: 0.218489 \tR2: 0.655680\n",
      "Epoch: 3038 \tTraining Loss: 0.204559 \tR2: 0.655680\n",
      "Epoch: 3039 \tTraining Loss: 0.208125 \tR2: 0.655680\n",
      "Epoch: 3040 \tTraining Loss: 0.210780 \tR2: 0.655680\n",
      "Epoch: 3041 \tTraining Loss: 0.204534 \tR2: 0.655680\n",
      "Epoch: 3042 \tTraining Loss: 0.210192 \tR2: 0.655680\n",
      "Epoch: 3043 \tTraining Loss: 0.202081 \tR2: 0.655680\n",
      "Epoch: 3044 \tTraining Loss: 0.204526 \tR2: 0.655680\n",
      "Epoch: 3045 \tTraining Loss: 0.201856 \tR2: 0.655680\n",
      "Epoch: 3046 \tTraining Loss: 0.215862 \tR2: 0.655680\n",
      "Epoch: 3047 \tTraining Loss: 0.213546 \tR2: 0.655680\n",
      "Epoch: 3048 \tTraining Loss: 0.213766 \tR2: 0.655680\n",
      "Epoch: 3049 \tTraining Loss: 0.208853 \tR2: 0.655680\n",
      "Epoch: 3050 \tTraining Loss: 0.212111 \tR2: 0.655680\n",
      "Epoch: 3051 \tTraining Loss: 0.217587 \tR2: 0.655680\n",
      "Epoch: 3052 \tTraining Loss: 0.217346 \tR2: 0.655680\n",
      "Epoch: 3053 \tTraining Loss: 0.198373 \tR2: 0.655680\n",
      "Epoch: 3054 \tTraining Loss: 0.215827 \tR2: 0.655680\n",
      "Epoch: 3055 \tTraining Loss: 0.209292 \tR2: 0.655680\n",
      "Epoch: 3056 \tTraining Loss: 0.200544 \tR2: 0.655680\n",
      "Epoch: 3057 \tTraining Loss: 0.206673 \tR2: 0.655680\n",
      "Epoch: 3058 \tTraining Loss: 0.207745 \tR2: 0.655680\n",
      "Epoch: 3059 \tTraining Loss: 0.229623 \tR2: 0.655680\n",
      "Epoch: 3060 \tTraining Loss: 0.213516 \tR2: 0.655680\n",
      "Epoch: 3061 \tTraining Loss: 0.218364 \tR2: 0.655680\n",
      "Epoch: 3062 \tTraining Loss: 0.212481 \tR2: 0.655680\n",
      "Epoch: 3063 \tTraining Loss: 0.211488 \tR2: 0.655680\n",
      "Epoch: 3064 \tTraining Loss: 0.232479 \tR2: 0.655680\n",
      "Epoch: 3065 \tTraining Loss: 0.210371 \tR2: 0.655680\n",
      "Epoch: 3066 \tTraining Loss: 0.225992 \tR2: 0.655680\n",
      "Epoch: 3067 \tTraining Loss: 0.230404 \tR2: 0.655680\n",
      "Epoch: 3068 \tTraining Loss: 0.217966 \tR2: 0.655680\n",
      "Epoch: 3069 \tTraining Loss: 0.208846 \tR2: 0.655680\n",
      "Epoch: 3070 \tTraining Loss: 0.239709 \tR2: 0.655680\n",
      "Epoch: 3071 \tTraining Loss: 0.204585 \tR2: 0.655680\n",
      "Epoch: 3072 \tTraining Loss: 0.219221 \tR2: 0.655680\n",
      "Epoch: 3073 \tTraining Loss: 0.199589 \tR2: 0.655680\n",
      "Epoch: 3074 \tTraining Loss: 0.203212 \tR2: 0.655680\n",
      "Epoch: 3075 \tTraining Loss: 0.225801 \tR2: 0.655680\n",
      "Epoch: 3076 \tTraining Loss: 0.207590 \tR2: 0.655680\n",
      "Epoch: 3077 \tTraining Loss: 0.198081 \tR2: 0.655680\n",
      "Epoch: 3078 \tTraining Loss: 0.214948 \tR2: 0.655680\n",
      "Epoch: 3079 \tTraining Loss: 0.202769 \tR2: 0.655680\n",
      "Epoch: 3080 \tTraining Loss: 0.200533 \tR2: 0.655680\n",
      "Epoch: 3081 \tTraining Loss: 0.218146 \tR2: 0.655680\n",
      "Epoch: 3082 \tTraining Loss: 0.208676 \tR2: 0.655680\n",
      "Epoch: 3083 \tTraining Loss: 0.209361 \tR2: 0.655680\n",
      "Epoch: 3084 \tTraining Loss: 0.205006 \tR2: 0.655680\n",
      "Epoch: 3085 \tTraining Loss: 0.221204 \tR2: 0.655680\n",
      "Epoch: 3086 \tTraining Loss: 0.214735 \tR2: 0.655680\n",
      "Epoch: 3087 \tTraining Loss: 0.224479 \tR2: 0.655680\n",
      "Epoch: 3088 \tTraining Loss: 0.226686 \tR2: 0.655680\n",
      "Epoch: 3089 \tTraining Loss: 0.209128 \tR2: 0.655680\n",
      "Epoch: 3090 \tTraining Loss: 0.200426 \tR2: 0.655680\n",
      "Epoch: 3091 \tTraining Loss: 0.226712 \tR2: 0.655680\n",
      "Epoch: 3092 \tTraining Loss: 0.253343 \tR2: 0.655680\n",
      "Epoch: 3093 \tTraining Loss: 0.220257 \tR2: 0.655680\n",
      "Epoch: 3094 \tTraining Loss: 0.220188 \tR2: 0.655680\n",
      "Epoch: 3095 \tTraining Loss: 0.206737 \tR2: 0.655680\n",
      "Epoch: 3096 \tTraining Loss: 0.219877 \tR2: 0.655680\n",
      "Epoch: 3097 \tTraining Loss: 0.209931 \tR2: 0.655680\n",
      "Epoch: 3098 \tTraining Loss: 0.215403 \tR2: 0.655680\n",
      "Epoch: 3099 \tTraining Loss: 0.214952 \tR2: 0.655680\n",
      "Epoch: 3100 \tTraining Loss: 0.221675 \tR2: 0.636531\n",
      "Epoch: 3101 \tTraining Loss: 0.216801 \tR2: 0.636531\n",
      "Epoch: 3102 \tTraining Loss: 0.243019 \tR2: 0.636531\n",
      "Epoch: 3103 \tTraining Loss: 0.216335 \tR2: 0.636531\n",
      "Epoch: 3104 \tTraining Loss: 0.222818 \tR2: 0.636531\n",
      "Epoch: 3105 \tTraining Loss: 0.191424 \tR2: 0.636531\n",
      "Epoch: 3106 \tTraining Loss: 0.232975 \tR2: 0.636531\n",
      "Epoch: 3107 \tTraining Loss: 0.220147 \tR2: 0.636531\n",
      "Epoch: 3108 \tTraining Loss: 0.207019 \tR2: 0.636531\n",
      "Epoch: 3109 \tTraining Loss: 0.223368 \tR2: 0.636531\n",
      "Epoch: 3110 \tTraining Loss: 0.201891 \tR2: 0.636531\n",
      "Epoch: 3111 \tTraining Loss: 0.208927 \tR2: 0.636531\n",
      "Epoch: 3112 \tTraining Loss: 0.236644 \tR2: 0.636531\n",
      "Epoch: 3113 \tTraining Loss: 0.205574 \tR2: 0.636531\n",
      "Epoch: 3114 \tTraining Loss: 0.209362 \tR2: 0.636531\n",
      "Epoch: 3115 \tTraining Loss: 0.216247 \tR2: 0.636531\n",
      "Epoch: 3116 \tTraining Loss: 0.208439 \tR2: 0.636531\n",
      "Epoch: 3117 \tTraining Loss: 0.209189 \tR2: 0.636531\n",
      "Epoch: 3118 \tTraining Loss: 0.215223 \tR2: 0.636531\n",
      "Epoch: 3119 \tTraining Loss: 0.210670 \tR2: 0.636531\n",
      "Epoch: 3120 \tTraining Loss: 0.193435 \tR2: 0.636531\n",
      "Epoch: 3121 \tTraining Loss: 0.211967 \tR2: 0.636531\n",
      "Epoch: 3122 \tTraining Loss: 0.224535 \tR2: 0.636531\n",
      "Epoch: 3123 \tTraining Loss: 0.210460 \tR2: 0.636531\n",
      "Epoch: 3124 \tTraining Loss: 0.210922 \tR2: 0.636531\n",
      "Epoch: 3125 \tTraining Loss: 0.201763 \tR2: 0.636531\n",
      "Epoch: 3126 \tTraining Loss: 0.229178 \tR2: 0.636531\n",
      "Epoch: 3127 \tTraining Loss: 0.240422 \tR2: 0.636531\n",
      "Epoch: 3128 \tTraining Loss: 0.238826 \tR2: 0.636531\n",
      "Epoch: 3129 \tTraining Loss: 0.222037 \tR2: 0.636531\n",
      "Epoch: 3130 \tTraining Loss: 0.203337 \tR2: 0.636531\n",
      "Epoch: 3131 \tTraining Loss: 0.227125 \tR2: 0.636531\n",
      "Epoch: 3132 \tTraining Loss: 0.219612 \tR2: 0.636531\n",
      "Epoch: 3133 \tTraining Loss: 0.193758 \tR2: 0.636531\n",
      "Epoch: 3134 \tTraining Loss: 0.210167 \tR2: 0.636531\n",
      "Epoch: 3135 \tTraining Loss: 0.206665 \tR2: 0.636531\n",
      "Epoch: 3136 \tTraining Loss: 0.230190 \tR2: 0.636531\n",
      "Epoch: 3137 \tTraining Loss: 0.215362 \tR2: 0.636531\n",
      "Epoch: 3138 \tTraining Loss: 0.205968 \tR2: 0.636531\n",
      "Epoch: 3139 \tTraining Loss: 0.210499 \tR2: 0.636531\n",
      "Epoch: 3140 \tTraining Loss: 0.226142 \tR2: 0.636531\n",
      "Epoch: 3141 \tTraining Loss: 0.199118 \tR2: 0.636531\n",
      "Epoch: 3142 \tTraining Loss: 0.213673 \tR2: 0.636531\n",
      "Epoch: 3143 \tTraining Loss: 0.214492 \tR2: 0.636531\n",
      "Epoch: 3144 \tTraining Loss: 0.188136 \tR2: 0.636531\n",
      "Epoch: 3145 \tTraining Loss: 0.205133 \tR2: 0.636531\n",
      "Epoch: 3146 \tTraining Loss: 0.207282 \tR2: 0.636531\n",
      "Epoch: 3147 \tTraining Loss: 0.224335 \tR2: 0.636531\n",
      "Epoch: 3148 \tTraining Loss: 0.197985 \tR2: 0.636531\n",
      "Epoch: 3149 \tTraining Loss: 0.199405 \tR2: 0.636531\n",
      "Epoch: 3150 \tTraining Loss: 0.216647 \tR2: 0.636531\n",
      "Epoch: 3151 \tTraining Loss: 0.206734 \tR2: 0.636531\n",
      "Epoch: 3152 \tTraining Loss: 0.234691 \tR2: 0.636531\n",
      "Epoch: 3153 \tTraining Loss: 0.216293 \tR2: 0.636531\n",
      "Epoch: 3154 \tTraining Loss: 0.201432 \tR2: 0.636531\n",
      "Epoch: 3155 \tTraining Loss: 0.199300 \tR2: 0.636531\n",
      "Epoch: 3156 \tTraining Loss: 0.208713 \tR2: 0.636531\n",
      "Epoch: 3157 \tTraining Loss: 0.212195 \tR2: 0.636531\n",
      "Epoch: 3158 \tTraining Loss: 0.212904 \tR2: 0.636531\n",
      "Epoch: 3159 \tTraining Loss: 0.215471 \tR2: 0.636531\n",
      "Epoch: 3160 \tTraining Loss: 0.187374 \tR2: 0.636531\n",
      "Epoch: 3161 \tTraining Loss: 0.230670 \tR2: 0.636531\n",
      "Epoch: 3162 \tTraining Loss: 0.211895 \tR2: 0.636531\n",
      "Epoch: 3163 \tTraining Loss: 0.195639 \tR2: 0.636531\n",
      "Epoch: 3164 \tTraining Loss: 0.215652 \tR2: 0.636531\n",
      "Epoch: 3165 \tTraining Loss: 0.218437 \tR2: 0.636531\n",
      "Epoch: 3166 \tTraining Loss: 0.231870 \tR2: 0.636531\n",
      "Epoch: 3167 \tTraining Loss: 0.200335 \tR2: 0.636531\n",
      "Epoch: 3168 \tTraining Loss: 0.224012 \tR2: 0.636531\n",
      "Epoch: 3169 \tTraining Loss: 0.196827 \tR2: 0.636531\n",
      "Epoch: 3170 \tTraining Loss: 0.204608 \tR2: 0.636531\n",
      "Epoch: 3171 \tTraining Loss: 0.210191 \tR2: 0.636531\n",
      "Epoch: 3172 \tTraining Loss: 0.209180 \tR2: 0.636531\n",
      "Epoch: 3173 \tTraining Loss: 0.201628 \tR2: 0.636531\n",
      "Epoch: 3174 \tTraining Loss: 0.235983 \tR2: 0.636531\n",
      "Epoch: 3175 \tTraining Loss: 0.211235 \tR2: 0.636531\n",
      "Epoch: 3176 \tTraining Loss: 0.199993 \tR2: 0.636531\n",
      "Epoch: 3177 \tTraining Loss: 0.214608 \tR2: 0.636531\n",
      "Epoch: 3178 \tTraining Loss: 0.219066 \tR2: 0.636531\n",
      "Epoch: 3179 \tTraining Loss: 0.222437 \tR2: 0.636531\n",
      "Epoch: 3180 \tTraining Loss: 0.206739 \tR2: 0.636531\n",
      "Epoch: 3181 \tTraining Loss: 0.191264 \tR2: 0.636531\n",
      "Epoch: 3182 \tTraining Loss: 0.204949 \tR2: 0.636531\n",
      "Epoch: 3183 \tTraining Loss: 0.214065 \tR2: 0.636531\n",
      "Epoch: 3184 \tTraining Loss: 0.205196 \tR2: 0.636531\n",
      "Epoch: 3185 \tTraining Loss: 0.216361 \tR2: 0.636531\n",
      "Epoch: 3186 \tTraining Loss: 0.204773 \tR2: 0.636531\n",
      "Epoch: 3187 \tTraining Loss: 0.198523 \tR2: 0.636531\n",
      "Epoch: 3188 \tTraining Loss: 0.196435 \tR2: 0.636531\n",
      "Epoch: 3189 \tTraining Loss: 0.219954 \tR2: 0.636531\n",
      "Epoch: 3190 \tTraining Loss: 0.202562 \tR2: 0.636531\n",
      "Epoch: 3191 \tTraining Loss: 0.203820 \tR2: 0.636531\n",
      "Epoch: 3192 \tTraining Loss: 0.204919 \tR2: 0.636531\n",
      "Epoch: 3193 \tTraining Loss: 0.210150 \tR2: 0.636531\n",
      "Epoch: 3194 \tTraining Loss: 0.201465 \tR2: 0.636531\n",
      "Epoch: 3195 \tTraining Loss: 0.221678 \tR2: 0.636531\n",
      "Epoch: 3196 \tTraining Loss: 0.241190 \tR2: 0.636531\n",
      "Epoch: 3197 \tTraining Loss: 0.223045 \tR2: 0.636531\n",
      "Epoch: 3198 \tTraining Loss: 0.211752 \tR2: 0.636531\n",
      "Epoch: 3199 \tTraining Loss: 0.201762 \tR2: 0.636531\n",
      "Epoch: 3200 \tTraining Loss: 0.206542 \tR2: 0.608901\n",
      "Epoch: 3201 \tTraining Loss: 0.214738 \tR2: 0.608901\n",
      "Epoch: 3202 \tTraining Loss: 0.210018 \tR2: 0.608901\n",
      "Epoch: 3203 \tTraining Loss: 0.219566 \tR2: 0.608901\n",
      "Epoch: 3204 \tTraining Loss: 0.232661 \tR2: 0.608901\n",
      "Epoch: 3205 \tTraining Loss: 0.217910 \tR2: 0.608901\n",
      "Epoch: 3206 \tTraining Loss: 0.221328 \tR2: 0.608901\n",
      "Epoch: 3207 \tTraining Loss: 0.232224 \tR2: 0.608901\n",
      "Epoch: 3208 \tTraining Loss: 0.204900 \tR2: 0.608901\n",
      "Epoch: 3209 \tTraining Loss: 0.220465 \tR2: 0.608901\n",
      "Epoch: 3210 \tTraining Loss: 0.196604 \tR2: 0.608901\n",
      "Epoch: 3211 \tTraining Loss: 0.221630 \tR2: 0.608901\n",
      "Epoch: 3212 \tTraining Loss: 0.212633 \tR2: 0.608901\n",
      "Epoch: 3213 \tTraining Loss: 0.213719 \tR2: 0.608901\n",
      "Epoch: 3214 \tTraining Loss: 0.236304 \tR2: 0.608901\n",
      "Epoch: 3215 \tTraining Loss: 0.223086 \tR2: 0.608901\n",
      "Epoch: 3216 \tTraining Loss: 0.212231 \tR2: 0.608901\n",
      "Epoch: 3217 \tTraining Loss: 0.196891 \tR2: 0.608901\n",
      "Epoch: 3218 \tTraining Loss: 0.270363 \tR2: 0.608901\n",
      "Epoch: 3219 \tTraining Loss: 0.233501 \tR2: 0.608901\n",
      "Epoch: 3220 \tTraining Loss: 0.240669 \tR2: 0.608901\n",
      "Epoch: 3221 \tTraining Loss: 0.203634 \tR2: 0.608901\n",
      "Epoch: 3222 \tTraining Loss: 0.205149 \tR2: 0.608901\n",
      "Epoch: 3223 \tTraining Loss: 0.211252 \tR2: 0.608901\n",
      "Epoch: 3224 \tTraining Loss: 0.195082 \tR2: 0.608901\n",
      "Epoch: 3225 \tTraining Loss: 0.230397 \tR2: 0.608901\n",
      "Epoch: 3226 \tTraining Loss: 0.207216 \tR2: 0.608901\n",
      "Epoch: 3227 \tTraining Loss: 0.228399 \tR2: 0.608901\n",
      "Epoch: 3228 \tTraining Loss: 0.203666 \tR2: 0.608901\n",
      "Epoch: 3229 \tTraining Loss: 0.229248 \tR2: 0.608901\n",
      "Epoch: 3230 \tTraining Loss: 0.213361 \tR2: 0.608901\n",
      "Epoch: 3231 \tTraining Loss: 0.203896 \tR2: 0.608901\n",
      "Epoch: 3232 \tTraining Loss: 0.224414 \tR2: 0.608901\n",
      "Epoch: 3233 \tTraining Loss: 0.228863 \tR2: 0.608901\n",
      "Epoch: 3234 \tTraining Loss: 0.236458 \tR2: 0.608901\n",
      "Epoch: 3235 \tTraining Loss: 0.205701 \tR2: 0.608901\n",
      "Epoch: 3236 \tTraining Loss: 0.199713 \tR2: 0.608901\n",
      "Epoch: 3237 \tTraining Loss: 0.220492 \tR2: 0.608901\n",
      "Epoch: 3238 \tTraining Loss: 0.215417 \tR2: 0.608901\n",
      "Epoch: 3239 \tTraining Loss: 0.200825 \tR2: 0.608901\n",
      "Epoch: 3240 \tTraining Loss: 0.194026 \tR2: 0.608901\n",
      "Epoch: 3241 \tTraining Loss: 0.209731 \tR2: 0.608901\n",
      "Epoch: 3242 \tTraining Loss: 0.208652 \tR2: 0.608901\n",
      "Epoch: 3243 \tTraining Loss: 0.214047 \tR2: 0.608901\n",
      "Epoch: 3244 \tTraining Loss: 0.205091 \tR2: 0.608901\n",
      "Epoch: 3245 \tTraining Loss: 0.224735 \tR2: 0.608901\n",
      "Epoch: 3246 \tTraining Loss: 0.212258 \tR2: 0.608901\n",
      "Epoch: 3247 \tTraining Loss: 0.220454 \tR2: 0.608901\n",
      "Epoch: 3248 \tTraining Loss: 0.196136 \tR2: 0.608901\n",
      "Epoch: 3249 \tTraining Loss: 0.204605 \tR2: 0.608901\n",
      "Epoch: 3250 \tTraining Loss: 0.212795 \tR2: 0.608901\n",
      "Epoch: 3251 \tTraining Loss: 0.209027 \tR2: 0.608901\n",
      "Epoch: 3252 \tTraining Loss: 0.207259 \tR2: 0.608901\n",
      "Epoch: 3253 \tTraining Loss: 0.200287 \tR2: 0.608901\n",
      "Epoch: 3254 \tTraining Loss: 0.229216 \tR2: 0.608901\n",
      "Epoch: 3255 \tTraining Loss: 0.228039 \tR2: 0.608901\n",
      "Epoch: 3256 \tTraining Loss: 0.219180 \tR2: 0.608901\n",
      "Epoch: 3257 \tTraining Loss: 0.215616 \tR2: 0.608901\n",
      "Epoch: 3258 \tTraining Loss: 0.217933 \tR2: 0.608901\n",
      "Epoch: 3259 \tTraining Loss: 0.188344 \tR2: 0.608901\n",
      "Epoch: 3260 \tTraining Loss: 0.203946 \tR2: 0.608901\n",
      "Epoch: 3261 \tTraining Loss: 0.219986 \tR2: 0.608901\n",
      "Epoch: 3262 \tTraining Loss: 0.197258 \tR2: 0.608901\n",
      "Epoch: 3263 \tTraining Loss: 0.214806 \tR2: 0.608901\n",
      "Epoch: 3264 \tTraining Loss: 0.208353 \tR2: 0.608901\n",
      "Epoch: 3265 \tTraining Loss: 0.216525 \tR2: 0.608901\n",
      "Epoch: 3266 \tTraining Loss: 0.221942 \tR2: 0.608901\n",
      "Epoch: 3267 \tTraining Loss: 0.218893 \tR2: 0.608901\n",
      "Epoch: 3268 \tTraining Loss: 0.205865 \tR2: 0.608901\n",
      "Epoch: 3269 \tTraining Loss: 0.210553 \tR2: 0.608901\n",
      "Epoch: 3270 \tTraining Loss: 0.206026 \tR2: 0.608901\n",
      "Epoch: 3271 \tTraining Loss: 0.211734 \tR2: 0.608901\n",
      "Epoch: 3272 \tTraining Loss: 0.203527 \tR2: 0.608901\n",
      "Epoch: 3273 \tTraining Loss: 0.205716 \tR2: 0.608901\n",
      "Epoch: 3274 \tTraining Loss: 0.204259 \tR2: 0.608901\n",
      "Epoch: 3275 \tTraining Loss: 0.211703 \tR2: 0.608901\n",
      "Epoch: 3276 \tTraining Loss: 0.216545 \tR2: 0.608901\n",
      "Epoch: 3277 \tTraining Loss: 0.216851 \tR2: 0.608901\n",
      "Epoch: 3278 \tTraining Loss: 0.200584 \tR2: 0.608901\n",
      "Epoch: 3279 \tTraining Loss: 0.224559 \tR2: 0.608901\n",
      "Epoch: 3280 \tTraining Loss: 0.208538 \tR2: 0.608901\n",
      "Epoch: 3281 \tTraining Loss: 0.195114 \tR2: 0.608901\n",
      "Epoch: 3282 \tTraining Loss: 0.207910 \tR2: 0.608901\n",
      "Epoch: 3283 \tTraining Loss: 0.210572 \tR2: 0.608901\n",
      "Epoch: 3284 \tTraining Loss: 0.220646 \tR2: 0.608901\n",
      "Epoch: 3285 \tTraining Loss: 0.219141 \tR2: 0.608901\n",
      "Epoch: 3286 \tTraining Loss: 0.210251 \tR2: 0.608901\n",
      "Epoch: 3287 \tTraining Loss: 0.217815 \tR2: 0.608901\n",
      "Epoch: 3288 \tTraining Loss: 0.186804 \tR2: 0.608901\n",
      "Epoch: 3289 \tTraining Loss: 0.211302 \tR2: 0.608901\n",
      "Epoch: 3290 \tTraining Loss: 0.201134 \tR2: 0.608901\n",
      "Epoch: 3291 \tTraining Loss: 0.223932 \tR2: 0.608901\n",
      "Epoch: 3292 \tTraining Loss: 0.233860 \tR2: 0.608901\n",
      "Epoch: 3293 \tTraining Loss: 0.213308 \tR2: 0.608901\n",
      "Epoch: 3294 \tTraining Loss: 0.207396 \tR2: 0.608901\n",
      "Epoch: 3295 \tTraining Loss: 0.204022 \tR2: 0.608901\n",
      "Epoch: 3296 \tTraining Loss: 0.217768 \tR2: 0.608901\n",
      "Epoch: 3297 \tTraining Loss: 0.210142 \tR2: 0.608901\n",
      "Epoch: 3298 \tTraining Loss: 0.205508 \tR2: 0.608901\n",
      "Epoch: 3299 \tTraining Loss: 0.219955 \tR2: 0.608901\n",
      "Epoch: 3300 \tTraining Loss: 0.212765 \tR2: 0.706517\n",
      "Epoch: 3301 \tTraining Loss: 0.202704 \tR2: 0.706517\n",
      "Epoch: 3302 \tTraining Loss: 0.211304 \tR2: 0.706517\n",
      "Epoch: 3303 \tTraining Loss: 0.197035 \tR2: 0.706517\n",
      "Epoch: 3304 \tTraining Loss: 0.212183 \tR2: 0.706517\n",
      "Epoch: 3305 \tTraining Loss: 0.202241 \tR2: 0.706517\n",
      "Epoch: 3306 \tTraining Loss: 0.201807 \tR2: 0.706517\n",
      "Epoch: 3307 \tTraining Loss: 0.201094 \tR2: 0.706517\n",
      "Epoch: 3308 \tTraining Loss: 0.212143 \tR2: 0.706517\n",
      "Epoch: 3309 \tTraining Loss: 0.205249 \tR2: 0.706517\n",
      "Epoch: 3310 \tTraining Loss: 0.212835 \tR2: 0.706517\n",
      "Epoch: 3311 \tTraining Loss: 0.243199 \tR2: 0.706517\n",
      "Epoch: 3312 \tTraining Loss: 0.199963 \tR2: 0.706517\n",
      "Epoch: 3313 \tTraining Loss: 0.219259 \tR2: 0.706517\n",
      "Epoch: 3314 \tTraining Loss: 0.205292 \tR2: 0.706517\n",
      "Epoch: 3315 \tTraining Loss: 0.197251 \tR2: 0.706517\n",
      "Epoch: 3316 \tTraining Loss: 0.204558 \tR2: 0.706517\n",
      "Epoch: 3317 \tTraining Loss: 0.208507 \tR2: 0.706517\n",
      "Epoch: 3318 \tTraining Loss: 0.203789 \tR2: 0.706517\n",
      "Epoch: 3319 \tTraining Loss: 0.194306 \tR2: 0.706517\n",
      "Epoch: 3320 \tTraining Loss: 0.201714 \tR2: 0.706517\n",
      "Epoch: 3321 \tTraining Loss: 0.211254 \tR2: 0.706517\n",
      "Epoch: 3322 \tTraining Loss: 0.206559 \tR2: 0.706517\n",
      "Epoch: 3323 \tTraining Loss: 0.261091 \tR2: 0.706517\n",
      "Epoch: 3324 \tTraining Loss: 0.219593 \tR2: 0.706517\n",
      "Epoch: 3325 \tTraining Loss: 0.229383 \tR2: 0.706517\n",
      "Epoch: 3326 \tTraining Loss: 0.188610 \tR2: 0.706517\n",
      "Epoch: 3327 \tTraining Loss: 0.197105 \tR2: 0.706517\n",
      "Epoch: 3328 \tTraining Loss: 0.197243 \tR2: 0.706517\n",
      "Epoch: 3329 \tTraining Loss: 0.210796 \tR2: 0.706517\n",
      "Epoch: 3330 \tTraining Loss: 0.213323 \tR2: 0.706517\n",
      "Epoch: 3331 \tTraining Loss: 0.205462 \tR2: 0.706517\n",
      "Epoch: 3332 \tTraining Loss: 0.221474 \tR2: 0.706517\n",
      "Epoch: 3333 \tTraining Loss: 0.206372 \tR2: 0.706517\n",
      "Epoch: 3334 \tTraining Loss: 0.209701 \tR2: 0.706517\n",
      "Epoch: 3335 \tTraining Loss: 0.215369 \tR2: 0.706517\n",
      "Epoch: 3336 \tTraining Loss: 0.237416 \tR2: 0.706517\n",
      "Epoch: 3337 \tTraining Loss: 0.209698 \tR2: 0.706517\n",
      "Epoch: 3338 \tTraining Loss: 0.228830 \tR2: 0.706517\n",
      "Epoch: 3339 \tTraining Loss: 0.231574 \tR2: 0.706517\n",
      "Epoch: 3340 \tTraining Loss: 0.208270 \tR2: 0.706517\n",
      "Epoch: 3341 \tTraining Loss: 0.216295 \tR2: 0.706517\n",
      "Epoch: 3342 \tTraining Loss: 0.216251 \tR2: 0.706517\n",
      "Epoch: 3343 \tTraining Loss: 0.215220 \tR2: 0.706517\n",
      "Epoch: 3344 \tTraining Loss: 0.218058 \tR2: 0.706517\n",
      "Epoch: 3345 \tTraining Loss: 0.202153 \tR2: 0.706517\n",
      "Epoch: 3346 \tTraining Loss: 0.217241 \tR2: 0.706517\n",
      "Epoch: 3347 \tTraining Loss: 0.196016 \tR2: 0.706517\n",
      "Epoch: 3348 \tTraining Loss: 0.195211 \tR2: 0.706517\n",
      "Epoch: 3349 \tTraining Loss: 0.227877 \tR2: 0.706517\n",
      "Epoch: 3350 \tTraining Loss: 0.208842 \tR2: 0.706517\n",
      "Epoch: 3351 \tTraining Loss: 0.221303 \tR2: 0.706517\n",
      "Epoch: 3352 \tTraining Loss: 0.218105 \tR2: 0.706517\n",
      "Epoch: 3353 \tTraining Loss: 0.218922 \tR2: 0.706517\n",
      "Epoch: 3354 \tTraining Loss: 0.191286 \tR2: 0.706517\n",
      "Epoch: 3355 \tTraining Loss: 0.215804 \tR2: 0.706517\n",
      "Epoch: 3356 \tTraining Loss: 0.213759 \tR2: 0.706517\n",
      "Epoch: 3357 \tTraining Loss: 0.205938 \tR2: 0.706517\n",
      "Epoch: 3358 \tTraining Loss: 0.209688 \tR2: 0.706517\n",
      "Epoch: 3359 \tTraining Loss: 0.215533 \tR2: 0.706517\n",
      "Epoch: 3360 \tTraining Loss: 0.201122 \tR2: 0.706517\n",
      "Epoch: 3361 \tTraining Loss: 0.220173 \tR2: 0.706517\n",
      "Epoch: 3362 \tTraining Loss: 0.185050 \tR2: 0.706517\n",
      "Epoch: 3363 \tTraining Loss: 0.218834 \tR2: 0.706517\n",
      "Epoch: 3364 \tTraining Loss: 0.200082 \tR2: 0.706517\n",
      "Epoch: 3365 \tTraining Loss: 0.214001 \tR2: 0.706517\n",
      "Epoch: 3366 \tTraining Loss: 0.193840 \tR2: 0.706517\n",
      "Epoch: 3367 \tTraining Loss: 0.215352 \tR2: 0.706517\n",
      "Epoch: 3368 \tTraining Loss: 0.221491 \tR2: 0.706517\n",
      "Epoch: 3369 \tTraining Loss: 0.210673 \tR2: 0.706517\n",
      "Epoch: 3370 \tTraining Loss: 0.197953 \tR2: 0.706517\n",
      "Epoch: 3371 \tTraining Loss: 0.210398 \tR2: 0.706517\n",
      "Epoch: 3372 \tTraining Loss: 0.217094 \tR2: 0.706517\n",
      "Epoch: 3373 \tTraining Loss: 0.219714 \tR2: 0.706517\n",
      "Epoch: 3374 \tTraining Loss: 0.205833 \tR2: 0.706517\n",
      "Epoch: 3375 \tTraining Loss: 0.211124 \tR2: 0.706517\n",
      "Epoch: 3376 \tTraining Loss: 0.203843 \tR2: 0.706517\n",
      "Epoch: 3377 \tTraining Loss: 0.209806 \tR2: 0.706517\n",
      "Epoch: 3378 \tTraining Loss: 0.223157 \tR2: 0.706517\n",
      "Epoch: 3379 \tTraining Loss: 0.198544 \tR2: 0.706517\n",
      "Epoch: 3380 \tTraining Loss: 0.207979 \tR2: 0.706517\n",
      "Epoch: 3381 \tTraining Loss: 0.208633 \tR2: 0.706517\n",
      "Epoch: 3382 \tTraining Loss: 0.209295 \tR2: 0.706517\n",
      "Epoch: 3383 \tTraining Loss: 0.203002 \tR2: 0.706517\n",
      "Epoch: 3384 \tTraining Loss: 0.204497 \tR2: 0.706517\n",
      "Epoch: 3385 \tTraining Loss: 0.213853 \tR2: 0.706517\n",
      "Epoch: 3386 \tTraining Loss: 0.197042 \tR2: 0.706517\n",
      "Epoch: 3387 \tTraining Loss: 0.196249 \tR2: 0.706517\n",
      "Epoch: 3388 \tTraining Loss: 0.208720 \tR2: 0.706517\n",
      "Epoch: 3389 \tTraining Loss: 0.201908 \tR2: 0.706517\n",
      "Epoch: 3390 \tTraining Loss: 0.207222 \tR2: 0.706517\n",
      "Epoch: 3391 \tTraining Loss: 0.201480 \tR2: 0.706517\n",
      "Epoch: 3392 \tTraining Loss: 0.214542 \tR2: 0.706517\n",
      "Epoch: 3393 \tTraining Loss: 0.215964 \tR2: 0.706517\n",
      "Epoch: 3394 \tTraining Loss: 0.191710 \tR2: 0.706517\n",
      "Epoch: 3395 \tTraining Loss: 0.208273 \tR2: 0.706517\n",
      "Epoch: 3396 \tTraining Loss: 0.217773 \tR2: 0.706517\n",
      "Epoch: 3397 \tTraining Loss: 0.207294 \tR2: 0.706517\n",
      "Epoch: 3398 \tTraining Loss: 0.189472 \tR2: 0.706517\n",
      "Epoch: 3399 \tTraining Loss: 0.231026 \tR2: 0.706517\n",
      "Epoch: 3400 \tTraining Loss: 0.224852 \tR2: 0.804956\n",
      "Epoch: 3401 \tTraining Loss: 0.218663 \tR2: 0.804956\n",
      "Epoch: 3402 \tTraining Loss: 0.196167 \tR2: 0.804956\n",
      "Epoch: 3403 \tTraining Loss: 0.208535 \tR2: 0.804956\n",
      "Epoch: 3404 \tTraining Loss: 0.202122 \tR2: 0.804956\n",
      "Epoch: 3405 \tTraining Loss: 0.210078 \tR2: 0.804956\n",
      "Epoch: 3406 \tTraining Loss: 0.210324 \tR2: 0.804956\n",
      "Epoch: 3407 \tTraining Loss: 0.201024 \tR2: 0.804956\n",
      "Epoch: 3408 \tTraining Loss: 0.217547 \tR2: 0.804956\n",
      "Epoch: 3409 \tTraining Loss: 0.215401 \tR2: 0.804956\n",
      "Epoch: 3410 \tTraining Loss: 0.201138 \tR2: 0.804956\n",
      "Epoch: 3411 \tTraining Loss: 0.194751 \tR2: 0.804956\n",
      "Epoch: 3412 \tTraining Loss: 0.194990 \tR2: 0.804956\n",
      "Epoch: 3413 \tTraining Loss: 0.217900 \tR2: 0.804956\n",
      "Epoch: 3414 \tTraining Loss: 0.212450 \tR2: 0.804956\n",
      "Epoch: 3415 \tTraining Loss: 0.210058 \tR2: 0.804956\n",
      "Epoch: 3416 \tTraining Loss: 0.207532 \tR2: 0.804956\n",
      "Epoch: 3417 \tTraining Loss: 0.206713 \tR2: 0.804956\n",
      "Epoch: 3418 \tTraining Loss: 0.217519 \tR2: 0.804956\n",
      "Epoch: 3419 \tTraining Loss: 0.204703 \tR2: 0.804956\n",
      "Epoch: 3420 \tTraining Loss: 0.216214 \tR2: 0.804956\n",
      "Epoch: 3421 \tTraining Loss: 0.198545 \tR2: 0.804956\n",
      "Epoch: 3422 \tTraining Loss: 0.193711 \tR2: 0.804956\n",
      "Epoch: 3423 \tTraining Loss: 0.190122 \tR2: 0.804956\n",
      "Epoch: 3424 \tTraining Loss: 0.214256 \tR2: 0.804956\n",
      "Epoch: 3425 \tTraining Loss: 0.210821 \tR2: 0.804956\n",
      "Epoch: 3426 \tTraining Loss: 0.211505 \tR2: 0.804956\n",
      "Epoch: 3427 \tTraining Loss: 0.216908 \tR2: 0.804956\n",
      "Epoch: 3428 \tTraining Loss: 0.228053 \tR2: 0.804956\n",
      "Epoch: 3429 \tTraining Loss: 0.208522 \tR2: 0.804956\n",
      "Epoch: 3430 \tTraining Loss: 0.211025 \tR2: 0.804956\n",
      "Epoch: 3431 \tTraining Loss: 0.227456 \tR2: 0.804956\n",
      "Epoch: 3432 \tTraining Loss: 0.196307 \tR2: 0.804956\n",
      "Epoch: 3433 \tTraining Loss: 0.194372 \tR2: 0.804956\n",
      "Epoch: 3434 \tTraining Loss: 0.223634 \tR2: 0.804956\n",
      "Epoch: 3435 \tTraining Loss: 0.207877 \tR2: 0.804956\n",
      "Epoch: 3436 \tTraining Loss: 0.208139 \tR2: 0.804956\n",
      "Epoch: 3437 \tTraining Loss: 0.246907 \tR2: 0.804956\n",
      "Epoch: 3438 \tTraining Loss: 0.231431 \tR2: 0.804956\n",
      "Epoch: 3439 \tTraining Loss: 0.216397 \tR2: 0.804956\n",
      "Epoch: 3440 \tTraining Loss: 0.203160 \tR2: 0.804956\n",
      "Epoch: 3441 \tTraining Loss: 0.193644 \tR2: 0.804956\n",
      "Epoch: 3442 \tTraining Loss: 0.191521 \tR2: 0.804956\n",
      "Epoch: 3443 \tTraining Loss: 0.224025 \tR2: 0.804956\n",
      "Epoch: 3444 \tTraining Loss: 0.222267 \tR2: 0.804956\n",
      "Epoch: 3445 \tTraining Loss: 0.205282 \tR2: 0.804956\n",
      "Epoch: 3446 \tTraining Loss: 0.211307 \tR2: 0.804956\n",
      "Epoch: 3447 \tTraining Loss: 0.185670 \tR2: 0.804956\n",
      "Epoch: 3448 \tTraining Loss: 0.206394 \tR2: 0.804956\n",
      "Epoch: 3449 \tTraining Loss: 0.205491 \tR2: 0.804956\n",
      "Epoch: 3450 \tTraining Loss: 0.216136 \tR2: 0.804956\n",
      "Epoch: 3451 \tTraining Loss: 0.243428 \tR2: 0.804956\n",
      "Epoch: 3452 \tTraining Loss: 0.226225 \tR2: 0.804956\n",
      "Epoch: 3453 \tTraining Loss: 0.201034 \tR2: 0.804956\n",
      "Epoch: 3454 \tTraining Loss: 0.201846 \tR2: 0.804956\n",
      "Epoch: 3455 \tTraining Loss: 0.195701 \tR2: 0.804956\n",
      "Epoch: 3456 \tTraining Loss: 0.193668 \tR2: 0.804956\n",
      "Epoch: 3457 \tTraining Loss: 0.209349 \tR2: 0.804956\n",
      "Epoch: 3458 \tTraining Loss: 0.217188 \tR2: 0.804956\n",
      "Epoch: 3459 \tTraining Loss: 0.230144 \tR2: 0.804956\n",
      "Epoch: 3460 \tTraining Loss: 0.201916 \tR2: 0.804956\n",
      "Epoch: 3461 \tTraining Loss: 0.200651 \tR2: 0.804956\n",
      "Epoch: 3462 \tTraining Loss: 0.214705 \tR2: 0.804956\n",
      "Epoch: 3463 \tTraining Loss: 0.215388 \tR2: 0.804956\n",
      "Epoch: 3464 \tTraining Loss: 0.205541 \tR2: 0.804956\n",
      "Epoch: 3465 \tTraining Loss: 0.218425 \tR2: 0.804956\n",
      "Epoch: 3466 \tTraining Loss: 0.201035 \tR2: 0.804956\n",
      "Epoch: 3467 \tTraining Loss: 0.211250 \tR2: 0.804956\n",
      "Epoch: 3468 \tTraining Loss: 0.215228 \tR2: 0.804956\n",
      "Epoch: 3469 \tTraining Loss: 0.246110 \tR2: 0.804956\n",
      "Epoch: 3470 \tTraining Loss: 0.216661 \tR2: 0.804956\n",
      "Epoch: 3471 \tTraining Loss: 0.214584 \tR2: 0.804956\n",
      "Epoch: 3472 \tTraining Loss: 0.205265 \tR2: 0.804956\n",
      "Epoch: 3473 \tTraining Loss: 0.227135 \tR2: 0.804956\n",
      "Epoch: 3474 \tTraining Loss: 0.198786 \tR2: 0.804956\n",
      "Epoch: 3475 \tTraining Loss: 0.200781 \tR2: 0.804956\n",
      "Epoch: 3476 \tTraining Loss: 0.200605 \tR2: 0.804956\n",
      "Epoch: 3477 \tTraining Loss: 0.265681 \tR2: 0.804956\n",
      "Epoch: 3478 \tTraining Loss: 0.201972 \tR2: 0.804956\n",
      "Epoch: 3479 \tTraining Loss: 0.213567 \tR2: 0.804956\n",
      "Epoch: 3480 \tTraining Loss: 0.222119 \tR2: 0.804956\n",
      "Epoch: 3481 \tTraining Loss: 0.205786 \tR2: 0.804956\n",
      "Epoch: 3482 \tTraining Loss: 0.216778 \tR2: 0.804956\n",
      "Epoch: 3483 \tTraining Loss: 0.208942 \tR2: 0.804956\n",
      "Epoch: 3484 \tTraining Loss: 0.224626 \tR2: 0.804956\n",
      "Epoch: 3485 \tTraining Loss: 0.200846 \tR2: 0.804956\n",
      "Epoch: 3486 \tTraining Loss: 0.227492 \tR2: 0.804956\n",
      "Epoch: 3487 \tTraining Loss: 0.182167 \tR2: 0.804956\n",
      "Epoch: 3488 \tTraining Loss: 0.196620 \tR2: 0.804956\n",
      "Epoch: 3489 \tTraining Loss: 0.207899 \tR2: 0.804956\n",
      "Epoch: 3490 \tTraining Loss: 0.206316 \tR2: 0.804956\n",
      "Epoch: 3491 \tTraining Loss: 0.203310 \tR2: 0.804956\n",
      "Epoch: 3492 \tTraining Loss: 0.198969 \tR2: 0.804956\n",
      "Epoch: 3493 \tTraining Loss: 0.206422 \tR2: 0.804956\n",
      "Epoch: 3494 \tTraining Loss: 0.205766 \tR2: 0.804956\n",
      "Epoch: 3495 \tTraining Loss: 0.218287 \tR2: 0.804956\n",
      "Epoch: 3496 \tTraining Loss: 0.203546 \tR2: 0.804956\n",
      "Epoch: 3497 \tTraining Loss: 0.218114 \tR2: 0.804956\n",
      "Epoch: 3498 \tTraining Loss: 0.225949 \tR2: 0.804956\n",
      "Epoch: 3499 \tTraining Loss: 0.217703 \tR2: 0.804956\n",
      "Epoch: 3500 \tTraining Loss: 0.205064 \tR2: 0.834397\n",
      "Epoch: 3501 \tTraining Loss: 0.206300 \tR2: 0.834397\n",
      "Epoch: 3502 \tTraining Loss: 0.220063 \tR2: 0.834397\n",
      "Epoch: 3503 \tTraining Loss: 0.195821 \tR2: 0.834397\n",
      "Epoch: 3504 \tTraining Loss: 0.212233 \tR2: 0.834397\n",
      "Epoch: 3505 \tTraining Loss: 0.206341 \tR2: 0.834397\n",
      "Epoch: 3506 \tTraining Loss: 0.204118 \tR2: 0.834397\n",
      "Epoch: 3507 \tTraining Loss: 0.215122 \tR2: 0.834397\n",
      "Epoch: 3508 \tTraining Loss: 0.195046 \tR2: 0.834397\n",
      "Epoch: 3509 \tTraining Loss: 0.215470 \tR2: 0.834397\n",
      "Epoch: 3510 \tTraining Loss: 0.228552 \tR2: 0.834397\n",
      "Epoch: 3511 \tTraining Loss: 0.208740 \tR2: 0.834397\n",
      "Epoch: 3512 \tTraining Loss: 0.213267 \tR2: 0.834397\n",
      "Epoch: 3513 \tTraining Loss: 0.201609 \tR2: 0.834397\n",
      "Epoch: 3514 \tTraining Loss: 0.215310 \tR2: 0.834397\n",
      "Epoch: 3515 \tTraining Loss: 0.202353 \tR2: 0.834397\n",
      "Epoch: 3516 \tTraining Loss: 0.197858 \tR2: 0.834397\n",
      "Epoch: 3517 \tTraining Loss: 0.221346 \tR2: 0.834397\n",
      "Epoch: 3518 \tTraining Loss: 0.210569 \tR2: 0.834397\n",
      "Epoch: 3519 \tTraining Loss: 0.221726 \tR2: 0.834397\n",
      "Epoch: 3520 \tTraining Loss: 0.181571 \tR2: 0.834397\n",
      "Epoch: 3521 \tTraining Loss: 0.197312 \tR2: 0.834397\n",
      "Epoch: 3522 \tTraining Loss: 0.195187 \tR2: 0.834397\n",
      "Epoch: 3523 \tTraining Loss: 0.213655 \tR2: 0.834397\n",
      "Epoch: 3524 \tTraining Loss: 0.196402 \tR2: 0.834397\n",
      "Epoch: 3525 \tTraining Loss: 0.223990 \tR2: 0.834397\n",
      "Epoch: 3526 \tTraining Loss: 0.212888 \tR2: 0.834397\n",
      "Epoch: 3527 \tTraining Loss: 0.225197 \tR2: 0.834397\n",
      "Epoch: 3528 \tTraining Loss: 0.191971 \tR2: 0.834397\n",
      "Epoch: 3529 \tTraining Loss: 0.200035 \tR2: 0.834397\n",
      "Epoch: 3530 \tTraining Loss: 0.196639 \tR2: 0.834397\n",
      "Epoch: 3531 \tTraining Loss: 0.197028 \tR2: 0.834397\n",
      "Epoch: 3532 \tTraining Loss: 0.195995 \tR2: 0.834397\n",
      "Epoch: 3533 \tTraining Loss: 0.209134 \tR2: 0.834397\n",
      "Epoch: 3534 \tTraining Loss: 0.217603 \tR2: 0.834397\n",
      "Epoch: 3535 \tTraining Loss: 0.210989 \tR2: 0.834397\n",
      "Epoch: 3536 \tTraining Loss: 0.214479 \tR2: 0.834397\n",
      "Epoch: 3537 \tTraining Loss: 0.216061 \tR2: 0.834397\n",
      "Epoch: 3538 \tTraining Loss: 0.212998 \tR2: 0.834397\n",
      "Epoch: 3539 \tTraining Loss: 0.207546 \tR2: 0.834397\n",
      "Epoch: 3540 \tTraining Loss: 0.205859 \tR2: 0.834397\n",
      "Epoch: 3541 \tTraining Loss: 0.200426 \tR2: 0.834397\n",
      "Epoch: 3542 \tTraining Loss: 0.213056 \tR2: 0.834397\n",
      "Epoch: 3543 \tTraining Loss: 0.196056 \tR2: 0.834397\n",
      "Epoch: 3544 \tTraining Loss: 0.217322 \tR2: 0.834397\n",
      "Epoch: 3545 \tTraining Loss: 0.203210 \tR2: 0.834397\n",
      "Epoch: 3546 \tTraining Loss: 0.218301 \tR2: 0.834397\n",
      "Epoch: 3547 \tTraining Loss: 0.193515 \tR2: 0.834397\n",
      "Epoch: 3548 \tTraining Loss: 0.209363 \tR2: 0.834397\n",
      "Epoch: 3549 \tTraining Loss: 0.217820 \tR2: 0.834397\n",
      "Epoch: 3550 \tTraining Loss: 0.219608 \tR2: 0.834397\n",
      "Epoch: 3551 \tTraining Loss: 0.193350 \tR2: 0.834397\n",
      "Epoch: 3552 \tTraining Loss: 0.192305 \tR2: 0.834397\n",
      "Epoch: 3553 \tTraining Loss: 0.204471 \tR2: 0.834397\n",
      "Epoch: 3554 \tTraining Loss: 0.219971 \tR2: 0.834397\n",
      "Epoch: 3555 \tTraining Loss: 0.203845 \tR2: 0.834397\n",
      "Epoch: 3556 \tTraining Loss: 0.195236 \tR2: 0.834397\n",
      "Epoch: 3557 \tTraining Loss: 0.216595 \tR2: 0.834397\n",
      "Epoch: 3558 \tTraining Loss: 0.207633 \tR2: 0.834397\n",
      "Epoch: 3559 \tTraining Loss: 0.220701 \tR2: 0.834397\n",
      "Epoch: 3560 \tTraining Loss: 0.204564 \tR2: 0.834397\n",
      "Epoch: 3561 \tTraining Loss: 0.212800 \tR2: 0.834397\n",
      "Epoch: 3562 \tTraining Loss: 0.218102 \tR2: 0.834397\n",
      "Epoch: 3563 \tTraining Loss: 0.204572 \tR2: 0.834397\n",
      "Epoch: 3564 \tTraining Loss: 0.228710 \tR2: 0.834397\n",
      "Epoch: 3565 \tTraining Loss: 0.227405 \tR2: 0.834397\n",
      "Epoch: 3566 \tTraining Loss: 0.208061 \tR2: 0.834397\n",
      "Epoch: 3567 \tTraining Loss: 0.209745 \tR2: 0.834397\n",
      "Epoch: 3568 \tTraining Loss: 0.218234 \tR2: 0.834397\n",
      "Epoch: 3569 \tTraining Loss: 0.213561 \tR2: 0.834397\n",
      "Epoch: 3570 \tTraining Loss: 0.230681 \tR2: 0.834397\n",
      "Epoch: 3571 \tTraining Loss: 0.255156 \tR2: 0.834397\n",
      "Epoch: 3572 \tTraining Loss: 0.218755 \tR2: 0.834397\n",
      "Epoch: 3573 \tTraining Loss: 0.198741 \tR2: 0.834397\n",
      "Epoch: 3574 \tTraining Loss: 0.214021 \tR2: 0.834397\n",
      "Epoch: 3575 \tTraining Loss: 0.203641 \tR2: 0.834397\n",
      "Epoch: 3576 \tTraining Loss: 0.216360 \tR2: 0.834397\n",
      "Epoch: 3577 \tTraining Loss: 0.213683 \tR2: 0.834397\n",
      "Epoch: 3578 \tTraining Loss: 0.201572 \tR2: 0.834397\n",
      "Epoch: 3579 \tTraining Loss: 0.182574 \tR2: 0.834397\n",
      "Epoch: 3580 \tTraining Loss: 0.219838 \tR2: 0.834397\n",
      "Epoch: 3581 \tTraining Loss: 0.196563 \tR2: 0.834397\n",
      "Epoch: 3582 \tTraining Loss: 0.224872 \tR2: 0.834397\n",
      "Epoch: 3583 \tTraining Loss: 0.212103 \tR2: 0.834397\n",
      "Epoch: 3584 \tTraining Loss: 0.194821 \tR2: 0.834397\n",
      "Epoch: 3585 \tTraining Loss: 0.200104 \tR2: 0.834397\n",
      "Epoch: 3586 \tTraining Loss: 0.209891 \tR2: 0.834397\n",
      "Epoch: 3587 \tTraining Loss: 0.200930 \tR2: 0.834397\n",
      "Epoch: 3588 \tTraining Loss: 0.209286 \tR2: 0.834397\n",
      "Epoch: 3589 \tTraining Loss: 0.227370 \tR2: 0.834397\n",
      "Epoch: 3590 \tTraining Loss: 0.203318 \tR2: 0.834397\n",
      "Epoch: 3591 \tTraining Loss: 0.193983 \tR2: 0.834397\n",
      "Epoch: 3592 \tTraining Loss: 0.215255 \tR2: 0.834397\n",
      "Epoch: 3593 \tTraining Loss: 0.201633 \tR2: 0.834397\n",
      "Epoch: 3594 \tTraining Loss: 0.216825 \tR2: 0.834397\n",
      "Epoch: 3595 \tTraining Loss: 0.199186 \tR2: 0.834397\n",
      "Epoch: 3596 \tTraining Loss: 0.199595 \tR2: 0.834397\n",
      "Epoch: 3597 \tTraining Loss: 0.203536 \tR2: 0.834397\n",
      "Epoch: 3598 \tTraining Loss: 0.205401 \tR2: 0.834397\n",
      "Epoch: 3599 \tTraining Loss: 0.210268 \tR2: 0.834397\n",
      "Epoch: 3600 \tTraining Loss: 0.215738 \tR2: 0.659019\n",
      "Epoch: 3601 \tTraining Loss: 0.203704 \tR2: 0.659019\n",
      "Epoch: 3602 \tTraining Loss: 0.207679 \tR2: 0.659019\n",
      "Epoch: 3603 \tTraining Loss: 0.216401 \tR2: 0.659019\n",
      "Epoch: 3604 \tTraining Loss: 0.232646 \tR2: 0.659019\n",
      "Epoch: 3605 \tTraining Loss: 0.221258 \tR2: 0.659019\n",
      "Epoch: 3606 \tTraining Loss: 0.199265 \tR2: 0.659019\n",
      "Epoch: 3607 \tTraining Loss: 0.215141 \tR2: 0.659019\n",
      "Epoch: 3608 \tTraining Loss: 0.203427 \tR2: 0.659019\n",
      "Epoch: 3609 \tTraining Loss: 0.199683 \tR2: 0.659019\n",
      "Epoch: 3610 \tTraining Loss: 0.214862 \tR2: 0.659019\n",
      "Epoch: 3611 \tTraining Loss: 0.236582 \tR2: 0.659019\n",
      "Epoch: 3612 \tTraining Loss: 0.214735 \tR2: 0.659019\n",
      "Epoch: 3613 \tTraining Loss: 0.206589 \tR2: 0.659019\n",
      "Epoch: 3614 \tTraining Loss: 0.211968 \tR2: 0.659019\n",
      "Epoch: 3615 \tTraining Loss: 0.193059 \tR2: 0.659019\n",
      "Epoch: 3616 \tTraining Loss: 0.236612 \tR2: 0.659019\n",
      "Epoch: 3617 \tTraining Loss: 0.220720 \tR2: 0.659019\n",
      "Epoch: 3618 \tTraining Loss: 0.224185 \tR2: 0.659019\n",
      "Epoch: 3619 \tTraining Loss: 0.222962 \tR2: 0.659019\n",
      "Epoch: 3620 \tTraining Loss: 0.197161 \tR2: 0.659019\n",
      "Epoch: 3621 \tTraining Loss: 0.213041 \tR2: 0.659019\n",
      "Epoch: 3622 \tTraining Loss: 0.203219 \tR2: 0.659019\n",
      "Epoch: 3623 \tTraining Loss: 0.201694 \tR2: 0.659019\n",
      "Epoch: 3624 \tTraining Loss: 0.208530 \tR2: 0.659019\n",
      "Epoch: 3625 \tTraining Loss: 0.205586 \tR2: 0.659019\n",
      "Epoch: 3626 \tTraining Loss: 0.225425 \tR2: 0.659019\n",
      "Epoch: 3627 \tTraining Loss: 0.202400 \tR2: 0.659019\n",
      "Epoch: 3628 \tTraining Loss: 0.218359 \tR2: 0.659019\n",
      "Epoch: 3629 \tTraining Loss: 0.211823 \tR2: 0.659019\n",
      "Epoch: 3630 \tTraining Loss: 0.201091 \tR2: 0.659019\n",
      "Epoch: 3631 \tTraining Loss: 0.213574 \tR2: 0.659019\n",
      "Epoch: 3632 \tTraining Loss: 0.202477 \tR2: 0.659019\n",
      "Epoch: 3633 \tTraining Loss: 0.195464 \tR2: 0.659019\n",
      "Epoch: 3634 \tTraining Loss: 0.201474 \tR2: 0.659019\n",
      "Epoch: 3635 \tTraining Loss: 0.208163 \tR2: 0.659019\n",
      "Epoch: 3636 \tTraining Loss: 0.198865 \tR2: 0.659019\n",
      "Epoch: 3637 \tTraining Loss: 0.203091 \tR2: 0.659019\n",
      "Epoch: 3638 \tTraining Loss: 0.203286 \tR2: 0.659019\n",
      "Epoch: 3639 \tTraining Loss: 0.195504 \tR2: 0.659019\n",
      "Epoch: 3640 \tTraining Loss: 0.215352 \tR2: 0.659019\n",
      "Epoch: 3641 \tTraining Loss: 0.193818 \tR2: 0.659019\n",
      "Epoch: 3642 \tTraining Loss: 0.209929 \tR2: 0.659019\n",
      "Epoch: 3643 \tTraining Loss: 0.208833 \tR2: 0.659019\n",
      "Epoch: 3644 \tTraining Loss: 0.196481 \tR2: 0.659019\n",
      "Epoch: 3645 \tTraining Loss: 0.209381 \tR2: 0.659019\n",
      "Epoch: 3646 \tTraining Loss: 0.186538 \tR2: 0.659019\n",
      "Epoch: 3647 \tTraining Loss: 0.203306 \tR2: 0.659019\n",
      "Epoch: 3648 \tTraining Loss: 0.201668 \tR2: 0.659019\n",
      "Epoch: 3649 \tTraining Loss: 0.206396 \tR2: 0.659019\n",
      "Epoch: 3650 \tTraining Loss: 0.210706 \tR2: 0.659019\n",
      "Epoch: 3651 \tTraining Loss: 0.211092 \tR2: 0.659019\n",
      "Epoch: 3652 \tTraining Loss: 0.204881 \tR2: 0.659019\n",
      "Epoch: 3653 \tTraining Loss: 0.200158 \tR2: 0.659019\n",
      "Epoch: 3654 \tTraining Loss: 0.209720 \tR2: 0.659019\n",
      "Epoch: 3655 \tTraining Loss: 0.203624 \tR2: 0.659019\n",
      "Epoch: 3656 \tTraining Loss: 0.196808 \tR2: 0.659019\n",
      "Epoch: 3657 \tTraining Loss: 0.198500 \tR2: 0.659019\n",
      "Epoch: 3658 \tTraining Loss: 0.217863 \tR2: 0.659019\n",
      "Epoch: 3659 \tTraining Loss: 0.201065 \tR2: 0.659019\n",
      "Epoch: 3660 \tTraining Loss: 0.215330 \tR2: 0.659019\n",
      "Epoch: 3661 \tTraining Loss: 0.209045 \tR2: 0.659019\n",
      "Epoch: 3662 \tTraining Loss: 0.198304 \tR2: 0.659019\n",
      "Epoch: 3663 \tTraining Loss: 0.214092 \tR2: 0.659019\n",
      "Epoch: 3664 \tTraining Loss: 0.223391 \tR2: 0.659019\n",
      "Epoch: 3665 \tTraining Loss: 0.206266 \tR2: 0.659019\n",
      "Epoch: 3666 \tTraining Loss: 0.215131 \tR2: 0.659019\n",
      "Epoch: 3667 \tTraining Loss: 0.209926 \tR2: 0.659019\n",
      "Epoch: 3668 \tTraining Loss: 0.205811 \tR2: 0.659019\n",
      "Epoch: 3669 \tTraining Loss: 0.206069 \tR2: 0.659019\n",
      "Epoch: 3670 \tTraining Loss: 0.204250 \tR2: 0.659019\n",
      "Epoch: 3671 \tTraining Loss: 0.215012 \tR2: 0.659019\n",
      "Epoch: 3672 \tTraining Loss: 0.221700 \tR2: 0.659019\n",
      "Epoch: 3673 \tTraining Loss: 0.196409 \tR2: 0.659019\n",
      "Epoch: 3674 \tTraining Loss: 0.195369 \tR2: 0.659019\n",
      "Epoch: 3675 \tTraining Loss: 0.207771 \tR2: 0.659019\n",
      "Epoch: 3676 \tTraining Loss: 0.210701 \tR2: 0.659019\n",
      "Epoch: 3677 \tTraining Loss: 0.213772 \tR2: 0.659019\n",
      "Epoch: 3678 \tTraining Loss: 0.200792 \tR2: 0.659019\n",
      "Epoch: 3679 \tTraining Loss: 0.200813 \tR2: 0.659019\n",
      "Epoch: 3680 \tTraining Loss: 0.196063 \tR2: 0.659019\n",
      "Epoch: 3681 \tTraining Loss: 0.195082 \tR2: 0.659019\n",
      "Epoch: 3682 \tTraining Loss: 0.209449 \tR2: 0.659019\n",
      "Epoch: 3683 \tTraining Loss: 0.211533 \tR2: 0.659019\n",
      "Epoch: 3684 \tTraining Loss: 0.206575 \tR2: 0.659019\n",
      "Epoch: 3685 \tTraining Loss: 0.220894 \tR2: 0.659019\n",
      "Epoch: 3686 \tTraining Loss: 0.225934 \tR2: 0.659019\n",
      "Epoch: 3687 \tTraining Loss: 0.205256 \tR2: 0.659019\n",
      "Epoch: 3688 \tTraining Loss: 0.203473 \tR2: 0.659019\n",
      "Epoch: 3689 \tTraining Loss: 0.205282 \tR2: 0.659019\n",
      "Epoch: 3690 \tTraining Loss: 0.202149 \tR2: 0.659019\n",
      "Epoch: 3691 \tTraining Loss: 0.189710 \tR2: 0.659019\n",
      "Epoch: 3692 \tTraining Loss: 0.226650 \tR2: 0.659019\n",
      "Epoch: 3693 \tTraining Loss: 0.191804 \tR2: 0.659019\n",
      "Epoch: 3694 \tTraining Loss: 0.219311 \tR2: 0.659019\n",
      "Epoch: 3695 \tTraining Loss: 0.198153 \tR2: 0.659019\n",
      "Epoch: 3696 \tTraining Loss: 0.208304 \tR2: 0.659019\n",
      "Epoch: 3697 \tTraining Loss: 0.206065 \tR2: 0.659019\n",
      "Epoch: 3698 \tTraining Loss: 0.209877 \tR2: 0.659019\n",
      "Epoch: 3699 \tTraining Loss: 0.198872 \tR2: 0.659019\n",
      "Epoch: 3700 \tTraining Loss: 0.200799 \tR2: 0.655023\n",
      "Epoch: 3701 \tTraining Loss: 0.202429 \tR2: 0.655023\n",
      "Epoch: 3702 \tTraining Loss: 0.209393 \tR2: 0.655023\n",
      "Epoch: 3703 \tTraining Loss: 0.210545 \tR2: 0.655023\n",
      "Epoch: 3704 \tTraining Loss: 0.211204 \tR2: 0.655023\n",
      "Epoch: 3705 \tTraining Loss: 0.215826 \tR2: 0.655023\n",
      "Epoch: 3706 \tTraining Loss: 0.211905 \tR2: 0.655023\n",
      "Epoch: 3707 \tTraining Loss: 0.199837 \tR2: 0.655023\n",
      "Epoch: 3708 \tTraining Loss: 0.214351 \tR2: 0.655023\n",
      "Epoch: 3709 \tTraining Loss: 0.189476 \tR2: 0.655023\n",
      "Epoch: 3710 \tTraining Loss: 0.201631 \tR2: 0.655023\n",
      "Epoch: 3711 \tTraining Loss: 0.218546 \tR2: 0.655023\n",
      "Epoch: 3712 \tTraining Loss: 0.200962 \tR2: 0.655023\n",
      "Epoch: 3713 \tTraining Loss: 0.206315 \tR2: 0.655023\n",
      "Epoch: 3714 \tTraining Loss: 0.214108 \tR2: 0.655023\n",
      "Epoch: 3715 \tTraining Loss: 0.193139 \tR2: 0.655023\n",
      "Epoch: 3716 \tTraining Loss: 0.212079 \tR2: 0.655023\n",
      "Epoch: 3717 \tTraining Loss: 0.215030 \tR2: 0.655023\n",
      "Epoch: 3718 \tTraining Loss: 0.203974 \tR2: 0.655023\n",
      "Epoch: 3719 \tTraining Loss: 0.194786 \tR2: 0.655023\n",
      "Epoch: 3720 \tTraining Loss: 0.211249 \tR2: 0.655023\n",
      "Epoch: 3721 \tTraining Loss: 0.209105 \tR2: 0.655023\n",
      "Epoch: 3722 \tTraining Loss: 0.227850 \tR2: 0.655023\n",
      "Epoch: 3723 \tTraining Loss: 0.204994 \tR2: 0.655023\n",
      "Epoch: 3724 \tTraining Loss: 0.233452 \tR2: 0.655023\n",
      "Epoch: 3725 \tTraining Loss: 0.231298 \tR2: 0.655023\n",
      "Epoch: 3726 \tTraining Loss: 0.217070 \tR2: 0.655023\n",
      "Epoch: 3727 \tTraining Loss: 0.204875 \tR2: 0.655023\n",
      "Epoch: 3728 \tTraining Loss: 0.222928 \tR2: 0.655023\n",
      "Epoch: 3729 \tTraining Loss: 0.193310 \tR2: 0.655023\n",
      "Epoch: 3730 \tTraining Loss: 0.214069 \tR2: 0.655023\n",
      "Epoch: 3731 \tTraining Loss: 0.220414 \tR2: 0.655023\n",
      "Epoch: 3732 \tTraining Loss: 0.206472 \tR2: 0.655023\n",
      "Epoch: 3733 \tTraining Loss: 0.201082 \tR2: 0.655023\n",
      "Epoch: 3734 \tTraining Loss: 0.222489 \tR2: 0.655023\n",
      "Epoch: 3735 \tTraining Loss: 0.201787 \tR2: 0.655023\n",
      "Epoch: 3736 \tTraining Loss: 0.208575 \tR2: 0.655023\n",
      "Epoch: 3737 \tTraining Loss: 0.215701 \tR2: 0.655023\n",
      "Epoch: 3738 \tTraining Loss: 0.229466 \tR2: 0.655023\n",
      "Epoch: 3739 \tTraining Loss: 0.220471 \tR2: 0.655023\n",
      "Epoch: 3740 \tTraining Loss: 0.225557 \tR2: 0.655023\n",
      "Epoch: 3741 \tTraining Loss: 0.195387 \tR2: 0.655023\n",
      "Epoch: 3742 \tTraining Loss: 0.194777 \tR2: 0.655023\n",
      "Epoch: 3743 \tTraining Loss: 0.199889 \tR2: 0.655023\n",
      "Epoch: 3744 \tTraining Loss: 0.233343 \tR2: 0.655023\n",
      "Epoch: 3745 \tTraining Loss: 0.188402 \tR2: 0.655023\n",
      "Epoch: 3746 \tTraining Loss: 0.221229 \tR2: 0.655023\n",
      "Epoch: 3747 \tTraining Loss: 0.205438 \tR2: 0.655023\n",
      "Epoch: 3748 \tTraining Loss: 0.190299 \tR2: 0.655023\n",
      "Epoch: 3749 \tTraining Loss: 0.216851 \tR2: 0.655023\n",
      "Epoch: 3750 \tTraining Loss: 0.203738 \tR2: 0.655023\n",
      "Epoch: 3751 \tTraining Loss: 0.201498 \tR2: 0.655023\n",
      "Epoch: 3752 \tTraining Loss: 0.211023 \tR2: 0.655023\n",
      "Epoch: 3753 \tTraining Loss: 0.196889 \tR2: 0.655023\n",
      "Epoch: 3754 \tTraining Loss: 0.199706 \tR2: 0.655023\n",
      "Epoch: 3755 \tTraining Loss: 0.208593 \tR2: 0.655023\n",
      "Epoch: 3756 \tTraining Loss: 0.206510 \tR2: 0.655023\n",
      "Epoch: 3757 \tTraining Loss: 0.196421 \tR2: 0.655023\n",
      "Epoch: 3758 \tTraining Loss: 0.201604 \tR2: 0.655023\n",
      "Epoch: 3759 \tTraining Loss: 0.190753 \tR2: 0.655023\n",
      "Epoch: 3760 \tTraining Loss: 0.220717 \tR2: 0.655023\n",
      "Epoch: 3761 \tTraining Loss: 0.205160 \tR2: 0.655023\n",
      "Epoch: 3762 \tTraining Loss: 0.202371 \tR2: 0.655023\n",
      "Epoch: 3763 \tTraining Loss: 0.216835 \tR2: 0.655023\n",
      "Epoch: 3764 \tTraining Loss: 0.212562 \tR2: 0.655023\n",
      "Epoch: 3765 \tTraining Loss: 0.196712 \tR2: 0.655023\n",
      "Epoch: 3766 \tTraining Loss: 0.192409 \tR2: 0.655023\n",
      "Epoch: 3767 \tTraining Loss: 0.213150 \tR2: 0.655023\n",
      "Epoch: 3768 \tTraining Loss: 0.191770 \tR2: 0.655023\n",
      "Epoch: 3769 \tTraining Loss: 0.195477 \tR2: 0.655023\n",
      "Epoch: 3770 \tTraining Loss: 0.190176 \tR2: 0.655023\n",
      "Epoch: 3771 \tTraining Loss: 0.198742 \tR2: 0.655023\n",
      "Epoch: 3772 \tTraining Loss: 0.217909 \tR2: 0.655023\n",
      "Epoch: 3773 \tTraining Loss: 0.235887 \tR2: 0.655023\n",
      "Epoch: 3774 \tTraining Loss: 0.217464 \tR2: 0.655023\n",
      "Epoch: 3775 \tTraining Loss: 0.206745 \tR2: 0.655023\n",
      "Epoch: 3776 \tTraining Loss: 0.197898 \tR2: 0.655023\n",
      "Epoch: 3777 \tTraining Loss: 0.204227 \tR2: 0.655023\n",
      "Epoch: 3778 \tTraining Loss: 0.197199 \tR2: 0.655023\n",
      "Epoch: 3779 \tTraining Loss: 0.203276 \tR2: 0.655023\n",
      "Epoch: 3780 \tTraining Loss: 0.189931 \tR2: 0.655023\n",
      "Epoch: 3781 \tTraining Loss: 0.210379 \tR2: 0.655023\n",
      "Epoch: 3782 \tTraining Loss: 0.219257 \tR2: 0.655023\n",
      "Epoch: 3783 \tTraining Loss: 0.198402 \tR2: 0.655023\n",
      "Epoch: 3784 \tTraining Loss: 0.225150 \tR2: 0.655023\n",
      "Epoch: 3785 \tTraining Loss: 0.199166 \tR2: 0.655023\n",
      "Epoch: 3786 \tTraining Loss: 0.212640 \tR2: 0.655023\n",
      "Epoch: 3787 \tTraining Loss: 0.206293 \tR2: 0.655023\n",
      "Epoch: 3788 \tTraining Loss: 0.201669 \tR2: 0.655023\n",
      "Epoch: 3789 \tTraining Loss: 0.206000 \tR2: 0.655023\n",
      "Epoch: 3790 \tTraining Loss: 0.217313 \tR2: 0.655023\n",
      "Epoch: 3791 \tTraining Loss: 0.199084 \tR2: 0.655023\n",
      "Epoch: 3792 \tTraining Loss: 0.207415 \tR2: 0.655023\n",
      "Epoch: 3793 \tTraining Loss: 0.200393 \tR2: 0.655023\n",
      "Epoch: 3794 \tTraining Loss: 0.212219 \tR2: 0.655023\n",
      "Epoch: 3795 \tTraining Loss: 0.194993 \tR2: 0.655023\n",
      "Epoch: 3796 \tTraining Loss: 0.195597 \tR2: 0.655023\n",
      "Epoch: 3797 \tTraining Loss: 0.196983 \tR2: 0.655023\n",
      "Epoch: 3798 \tTraining Loss: 0.210344 \tR2: 0.655023\n",
      "Epoch: 3799 \tTraining Loss: 0.223986 \tR2: 0.655023\n",
      "Epoch: 3800 \tTraining Loss: 0.204081 \tR2: 0.643351\n",
      "Epoch: 3801 \tTraining Loss: 0.200821 \tR2: 0.643351\n",
      "Epoch: 3802 \tTraining Loss: 0.216876 \tR2: 0.643351\n",
      "Epoch: 3803 \tTraining Loss: 0.207605 \tR2: 0.643351\n",
      "Epoch: 3804 \tTraining Loss: 0.224719 \tR2: 0.643351\n",
      "Epoch: 3805 \tTraining Loss: 0.203575 \tR2: 0.643351\n",
      "Epoch: 3806 \tTraining Loss: 0.208742 \tR2: 0.643351\n",
      "Epoch: 3807 \tTraining Loss: 0.216637 \tR2: 0.643351\n",
      "Epoch: 3808 \tTraining Loss: 0.218810 \tR2: 0.643351\n",
      "Epoch: 3809 \tTraining Loss: 0.228078 \tR2: 0.643351\n",
      "Epoch: 3810 \tTraining Loss: 0.195499 \tR2: 0.643351\n",
      "Epoch: 3811 \tTraining Loss: 0.197596 \tR2: 0.643351\n",
      "Epoch: 3812 \tTraining Loss: 0.209550 \tR2: 0.643351\n",
      "Epoch: 3813 \tTraining Loss: 0.206434 \tR2: 0.643351\n",
      "Epoch: 3814 \tTraining Loss: 0.195086 \tR2: 0.643351\n",
      "Epoch: 3815 \tTraining Loss: 0.198552 \tR2: 0.643351\n",
      "Epoch: 3816 \tTraining Loss: 0.190361 \tR2: 0.643351\n",
      "Epoch: 3817 \tTraining Loss: 0.201155 \tR2: 0.643351\n",
      "Epoch: 3818 \tTraining Loss: 0.209261 \tR2: 0.643351\n",
      "Epoch: 3819 \tTraining Loss: 0.204404 \tR2: 0.643351\n",
      "Epoch: 3820 \tTraining Loss: 0.212740 \tR2: 0.643351\n",
      "Epoch: 3821 \tTraining Loss: 0.201556 \tR2: 0.643351\n",
      "Epoch: 3822 \tTraining Loss: 0.206918 \tR2: 0.643351\n",
      "Epoch: 3823 \tTraining Loss: 0.218753 \tR2: 0.643351\n",
      "Epoch: 3824 \tTraining Loss: 0.196776 \tR2: 0.643351\n",
      "Epoch: 3825 \tTraining Loss: 0.200211 \tR2: 0.643351\n",
      "Epoch: 3826 \tTraining Loss: 0.206298 \tR2: 0.643351\n",
      "Epoch: 3827 \tTraining Loss: 0.184133 \tR2: 0.643351\n",
      "Epoch: 3828 \tTraining Loss: 0.208062 \tR2: 0.643351\n",
      "Epoch: 3829 \tTraining Loss: 0.190284 \tR2: 0.643351\n",
      "Epoch: 3830 \tTraining Loss: 0.211789 \tR2: 0.643351\n",
      "Epoch: 3831 \tTraining Loss: 0.190822 \tR2: 0.643351\n",
      "Epoch: 3832 \tTraining Loss: 0.197539 \tR2: 0.643351\n",
      "Epoch: 3833 \tTraining Loss: 0.189146 \tR2: 0.643351\n",
      "Epoch: 3834 \tTraining Loss: 0.205793 \tR2: 0.643351\n",
      "Epoch: 3835 \tTraining Loss: 0.209876 \tR2: 0.643351\n",
      "Epoch: 3836 \tTraining Loss: 0.190571 \tR2: 0.643351\n",
      "Epoch: 3837 \tTraining Loss: 0.187055 \tR2: 0.643351\n",
      "Epoch: 3838 \tTraining Loss: 0.203979 \tR2: 0.643351\n",
      "Epoch: 3839 \tTraining Loss: 0.205326 \tR2: 0.643351\n",
      "Epoch: 3840 \tTraining Loss: 0.213764 \tR2: 0.643351\n",
      "Epoch: 3841 \tTraining Loss: 0.222424 \tR2: 0.643351\n",
      "Epoch: 3842 \tTraining Loss: 0.194396 \tR2: 0.643351\n",
      "Epoch: 3843 \tTraining Loss: 0.200986 \tR2: 0.643351\n",
      "Epoch: 3844 \tTraining Loss: 0.200182 \tR2: 0.643351\n",
      "Epoch: 3845 \tTraining Loss: 0.227744 \tR2: 0.643351\n",
      "Epoch: 3846 \tTraining Loss: 0.197513 \tR2: 0.643351\n",
      "Epoch: 3847 \tTraining Loss: 0.209906 \tR2: 0.643351\n",
      "Epoch: 3848 \tTraining Loss: 0.198702 \tR2: 0.643351\n",
      "Epoch: 3849 \tTraining Loss: 0.208831 \tR2: 0.643351\n",
      "Epoch: 3850 \tTraining Loss: 0.191030 \tR2: 0.643351\n",
      "Epoch: 3851 \tTraining Loss: 0.195806 \tR2: 0.643351\n",
      "Epoch: 3852 \tTraining Loss: 0.205813 \tR2: 0.643351\n",
      "Epoch: 3853 \tTraining Loss: 0.218023 \tR2: 0.643351\n",
      "Epoch: 3854 \tTraining Loss: 0.202318 \tR2: 0.643351\n",
      "Epoch: 3855 \tTraining Loss: 0.203147 \tR2: 0.643351\n",
      "Epoch: 3856 \tTraining Loss: 0.202069 \tR2: 0.643351\n",
      "Epoch: 3857 \tTraining Loss: 0.198011 \tR2: 0.643351\n",
      "Epoch: 3858 \tTraining Loss: 0.201174 \tR2: 0.643351\n",
      "Epoch: 3859 \tTraining Loss: 0.193292 \tR2: 0.643351\n",
      "Epoch: 3860 \tTraining Loss: 0.225965 \tR2: 0.643351\n",
      "Epoch: 3861 \tTraining Loss: 0.210268 \tR2: 0.643351\n",
      "Epoch: 3862 \tTraining Loss: 0.188149 \tR2: 0.643351\n",
      "Epoch: 3863 \tTraining Loss: 0.201073 \tR2: 0.643351\n",
      "Epoch: 3864 \tTraining Loss: 0.208713 \tR2: 0.643351\n",
      "Epoch: 3865 \tTraining Loss: 0.182994 \tR2: 0.643351\n",
      "Epoch: 3866 \tTraining Loss: 0.178618 \tR2: 0.643351\n",
      "Epoch: 3867 \tTraining Loss: 0.208907 \tR2: 0.643351\n",
      "Epoch: 3868 \tTraining Loss: 0.212756 \tR2: 0.643351\n",
      "Epoch: 3869 \tTraining Loss: 0.211152 \tR2: 0.643351\n",
      "Epoch: 3870 \tTraining Loss: 0.221127 \tR2: 0.643351\n",
      "Epoch: 3871 \tTraining Loss: 0.205705 \tR2: 0.643351\n",
      "Epoch: 3872 \tTraining Loss: 0.201865 \tR2: 0.643351\n",
      "Epoch: 3873 \tTraining Loss: 0.209535 \tR2: 0.643351\n",
      "Epoch: 3874 \tTraining Loss: 0.201948 \tR2: 0.643351\n",
      "Epoch: 3875 \tTraining Loss: 0.215198 \tR2: 0.643351\n",
      "Epoch: 3876 \tTraining Loss: 0.215217 \tR2: 0.643351\n",
      "Epoch: 3877 \tTraining Loss: 0.202309 \tR2: 0.643351\n",
      "Epoch: 3878 \tTraining Loss: 0.219569 \tR2: 0.643351\n",
      "Epoch: 3879 \tTraining Loss: 0.195111 \tR2: 0.643351\n",
      "Epoch: 3880 \tTraining Loss: 0.243086 \tR2: 0.643351\n",
      "Epoch: 3881 \tTraining Loss: 0.194893 \tR2: 0.643351\n",
      "Epoch: 3882 \tTraining Loss: 0.193868 \tR2: 0.643351\n",
      "Epoch: 3883 \tTraining Loss: 0.201179 \tR2: 0.643351\n",
      "Epoch: 3884 \tTraining Loss: 0.204976 \tR2: 0.643351\n",
      "Epoch: 3885 \tTraining Loss: 0.203972 \tR2: 0.643351\n",
      "Epoch: 3886 \tTraining Loss: 0.211667 \tR2: 0.643351\n",
      "Epoch: 3887 \tTraining Loss: 0.200429 \tR2: 0.643351\n",
      "Epoch: 3888 \tTraining Loss: 0.215641 \tR2: 0.643351\n",
      "Epoch: 3889 \tTraining Loss: 0.204078 \tR2: 0.643351\n",
      "Epoch: 3890 \tTraining Loss: 0.208168 \tR2: 0.643351\n",
      "Epoch: 3891 \tTraining Loss: 0.194320 \tR2: 0.643351\n",
      "Epoch: 3892 \tTraining Loss: 0.209638 \tR2: 0.643351\n",
      "Epoch: 3893 \tTraining Loss: 0.200386 \tR2: 0.643351\n",
      "Epoch: 3894 \tTraining Loss: 0.203237 \tR2: 0.643351\n",
      "Epoch: 3895 \tTraining Loss: 0.198906 \tR2: 0.643351\n",
      "Epoch: 3896 \tTraining Loss: 0.207726 \tR2: 0.643351\n",
      "Epoch: 3897 \tTraining Loss: 0.198685 \tR2: 0.643351\n",
      "Epoch: 3898 \tTraining Loss: 0.228393 \tR2: 0.643351\n",
      "Epoch: 3899 \tTraining Loss: 0.207425 \tR2: 0.643351\n",
      "Epoch: 3900 \tTraining Loss: 0.204758 \tR2: 0.747544\n",
      "Epoch: 3901 \tTraining Loss: 0.199040 \tR2: 0.747544\n",
      "Epoch: 3902 \tTraining Loss: 0.200627 \tR2: 0.747544\n",
      "Epoch: 3903 \tTraining Loss: 0.210819 \tR2: 0.747544\n",
      "Epoch: 3904 \tTraining Loss: 0.194936 \tR2: 0.747544\n",
      "Epoch: 3905 \tTraining Loss: 0.236751 \tR2: 0.747544\n",
      "Epoch: 3906 \tTraining Loss: 0.195428 \tR2: 0.747544\n",
      "Epoch: 3907 \tTraining Loss: 0.197300 \tR2: 0.747544\n",
      "Epoch: 3908 \tTraining Loss: 0.212787 \tR2: 0.747544\n",
      "Epoch: 3909 \tTraining Loss: 0.189069 \tR2: 0.747544\n",
      "Epoch: 3910 \tTraining Loss: 0.211866 \tR2: 0.747544\n",
      "Epoch: 3911 \tTraining Loss: 0.193442 \tR2: 0.747544\n",
      "Epoch: 3912 \tTraining Loss: 0.208966 \tR2: 0.747544\n",
      "Epoch: 3913 \tTraining Loss: 0.205451 \tR2: 0.747544\n",
      "Epoch: 3914 \tTraining Loss: 0.184273 \tR2: 0.747544\n",
      "Epoch: 3915 \tTraining Loss: 0.191055 \tR2: 0.747544\n",
      "Epoch: 3916 \tTraining Loss: 0.198036 \tR2: 0.747544\n",
      "Epoch: 3917 \tTraining Loss: 0.197505 \tR2: 0.747544\n",
      "Epoch: 3918 \tTraining Loss: 0.213765 \tR2: 0.747544\n",
      "Epoch: 3919 \tTraining Loss: 0.208150 \tR2: 0.747544\n",
      "Epoch: 3920 \tTraining Loss: 0.202793 \tR2: 0.747544\n",
      "Epoch: 3921 \tTraining Loss: 0.210264 \tR2: 0.747544\n",
      "Epoch: 3922 \tTraining Loss: 0.224195 \tR2: 0.747544\n",
      "Epoch: 3923 \tTraining Loss: 0.187520 \tR2: 0.747544\n",
      "Epoch: 3924 \tTraining Loss: 0.211289 \tR2: 0.747544\n",
      "Epoch: 3925 \tTraining Loss: 0.199999 \tR2: 0.747544\n",
      "Epoch: 3926 \tTraining Loss: 0.209923 \tR2: 0.747544\n",
      "Epoch: 3927 \tTraining Loss: 0.198486 \tR2: 0.747544\n",
      "Epoch: 3928 \tTraining Loss: 0.206972 \tR2: 0.747544\n",
      "Epoch: 3929 \tTraining Loss: 0.208563 \tR2: 0.747544\n",
      "Epoch: 3930 \tTraining Loss: 0.195669 \tR2: 0.747544\n",
      "Epoch: 3931 \tTraining Loss: 0.220775 \tR2: 0.747544\n",
      "Epoch: 3932 \tTraining Loss: 0.207063 \tR2: 0.747544\n",
      "Epoch: 3933 \tTraining Loss: 0.214933 \tR2: 0.747544\n",
      "Epoch: 3934 \tTraining Loss: 0.181747 \tR2: 0.747544\n",
      "Epoch: 3935 \tTraining Loss: 0.213996 \tR2: 0.747544\n",
      "Epoch: 3936 \tTraining Loss: 0.213861 \tR2: 0.747544\n",
      "Epoch: 3937 \tTraining Loss: 0.216415 \tR2: 0.747544\n",
      "Epoch: 3938 \tTraining Loss: 0.203443 \tR2: 0.747544\n",
      "Epoch: 3939 \tTraining Loss: 0.203918 \tR2: 0.747544\n",
      "Epoch: 3940 \tTraining Loss: 0.202744 \tR2: 0.747544\n",
      "Epoch: 3941 \tTraining Loss: 0.209122 \tR2: 0.747544\n",
      "Epoch: 3942 \tTraining Loss: 0.220580 \tR2: 0.747544\n",
      "Epoch: 3943 \tTraining Loss: 0.182898 \tR2: 0.747544\n",
      "Epoch: 3944 \tTraining Loss: 0.200489 \tR2: 0.747544\n",
      "Epoch: 3945 \tTraining Loss: 0.206444 \tR2: 0.747544\n",
      "Epoch: 3946 \tTraining Loss: 0.202162 \tR2: 0.747544\n",
      "Epoch: 3947 \tTraining Loss: 0.187932 \tR2: 0.747544\n",
      "Epoch: 3948 \tTraining Loss: 0.225513 \tR2: 0.747544\n",
      "Epoch: 3949 \tTraining Loss: 0.197406 \tR2: 0.747544\n",
      "Epoch: 3950 \tTraining Loss: 0.200350 \tR2: 0.747544\n",
      "Epoch: 3951 \tTraining Loss: 0.208826 \tR2: 0.747544\n",
      "Epoch: 3952 \tTraining Loss: 0.191191 \tR2: 0.747544\n",
      "Epoch: 3953 \tTraining Loss: 0.189090 \tR2: 0.747544\n",
      "Epoch: 3954 \tTraining Loss: 0.191142 \tR2: 0.747544\n",
      "Epoch: 3955 \tTraining Loss: 0.193051 \tR2: 0.747544\n",
      "Epoch: 3956 \tTraining Loss: 0.214291 \tR2: 0.747544\n",
      "Epoch: 3957 \tTraining Loss: 0.187531 \tR2: 0.747544\n",
      "Epoch: 3958 \tTraining Loss: 0.230678 \tR2: 0.747544\n",
      "Epoch: 3959 \tTraining Loss: 0.207350 \tR2: 0.747544\n",
      "Epoch: 3960 \tTraining Loss: 0.205585 \tR2: 0.747544\n",
      "Epoch: 3961 \tTraining Loss: 0.207871 \tR2: 0.747544\n",
      "Epoch: 3962 \tTraining Loss: 0.186721 \tR2: 0.747544\n",
      "Epoch: 3963 \tTraining Loss: 0.207528 \tR2: 0.747544\n",
      "Epoch: 3964 \tTraining Loss: 0.199283 \tR2: 0.747544\n",
      "Epoch: 3965 \tTraining Loss: 0.215687 \tR2: 0.747544\n",
      "Epoch: 3966 \tTraining Loss: 0.201830 \tR2: 0.747544\n",
      "Epoch: 3967 \tTraining Loss: 0.213633 \tR2: 0.747544\n",
      "Epoch: 3968 \tTraining Loss: 0.198779 \tR2: 0.747544\n",
      "Epoch: 3969 \tTraining Loss: 0.202848 \tR2: 0.747544\n",
      "Epoch: 3970 \tTraining Loss: 0.211210 \tR2: 0.747544\n",
      "Epoch: 3971 \tTraining Loss: 0.212894 \tR2: 0.747544\n",
      "Epoch: 3972 \tTraining Loss: 0.201978 \tR2: 0.747544\n",
      "Epoch: 3973 \tTraining Loss: 0.198484 \tR2: 0.747544\n",
      "Epoch: 3974 \tTraining Loss: 0.226136 \tR2: 0.747544\n",
      "Epoch: 3975 \tTraining Loss: 0.206308 \tR2: 0.747544\n",
      "Epoch: 3976 \tTraining Loss: 0.193364 \tR2: 0.747544\n",
      "Epoch: 3977 \tTraining Loss: 0.201323 \tR2: 0.747544\n",
      "Epoch: 3978 \tTraining Loss: 0.210368 \tR2: 0.747544\n",
      "Epoch: 3979 \tTraining Loss: 0.195123 \tR2: 0.747544\n",
      "Epoch: 3980 \tTraining Loss: 0.179259 \tR2: 0.747544\n",
      "Epoch: 3981 \tTraining Loss: 0.216238 \tR2: 0.747544\n",
      "Epoch: 3982 \tTraining Loss: 0.193650 \tR2: 0.747544\n",
      "Epoch: 3983 \tTraining Loss: 0.193223 \tR2: 0.747544\n",
      "Epoch: 3984 \tTraining Loss: 0.192560 \tR2: 0.747544\n",
      "Epoch: 3985 \tTraining Loss: 0.191323 \tR2: 0.747544\n",
      "Epoch: 3986 \tTraining Loss: 0.193647 \tR2: 0.747544\n",
      "Epoch: 3987 \tTraining Loss: 0.214564 \tR2: 0.747544\n",
      "Epoch: 3988 \tTraining Loss: 0.189498 \tR2: 0.747544\n",
      "Epoch: 3989 \tTraining Loss: 0.185724 \tR2: 0.747544\n",
      "Epoch: 3990 \tTraining Loss: 0.221547 \tR2: 0.747544\n",
      "Epoch: 3991 \tTraining Loss: 0.207708 \tR2: 0.747544\n",
      "Epoch: 3992 \tTraining Loss: 0.205375 \tR2: 0.747544\n",
      "Epoch: 3993 \tTraining Loss: 0.192916 \tR2: 0.747544\n",
      "Epoch: 3994 \tTraining Loss: 0.198487 \tR2: 0.747544\n",
      "Epoch: 3995 \tTraining Loss: 0.192076 \tR2: 0.747544\n",
      "Epoch: 3996 \tTraining Loss: 0.209532 \tR2: 0.747544\n",
      "Epoch: 3997 \tTraining Loss: 0.201857 \tR2: 0.747544\n",
      "Epoch: 3998 \tTraining Loss: 0.192490 \tR2: 0.747544\n",
      "Epoch: 3999 \tTraining Loss: 0.207506 \tR2: 0.747544\n",
      "Epoch: 4000 \tTraining Loss: 0.194656 \tR2: 0.806330\n",
      "Epoch: 4001 \tTraining Loss: 0.194551 \tR2: 0.806330\n",
      "Epoch: 4002 \tTraining Loss: 0.222847 \tR2: 0.806330\n",
      "Epoch: 4003 \tTraining Loss: 0.211919 \tR2: 0.806330\n",
      "Epoch: 4004 \tTraining Loss: 0.181850 \tR2: 0.806330\n",
      "Epoch: 4005 \tTraining Loss: 0.210330 \tR2: 0.806330\n",
      "Epoch: 4006 \tTraining Loss: 0.211841 \tR2: 0.806330\n",
      "Epoch: 4007 \tTraining Loss: 0.229823 \tR2: 0.806330\n",
      "Epoch: 4008 \tTraining Loss: 0.241535 \tR2: 0.806330\n",
      "Epoch: 4009 \tTraining Loss: 0.213386 \tR2: 0.806330\n",
      "Epoch: 4010 \tTraining Loss: 0.203921 \tR2: 0.806330\n",
      "Epoch: 4011 \tTraining Loss: 0.207637 \tR2: 0.806330\n",
      "Epoch: 4012 \tTraining Loss: 0.194551 \tR2: 0.806330\n",
      "Epoch: 4013 \tTraining Loss: 0.208160 \tR2: 0.806330\n",
      "Epoch: 4014 \tTraining Loss: 0.207181 \tR2: 0.806330\n",
      "Epoch: 4015 \tTraining Loss: 0.212943 \tR2: 0.806330\n",
      "Epoch: 4016 \tTraining Loss: 0.210320 \tR2: 0.806330\n",
      "Epoch: 4017 \tTraining Loss: 0.202959 \tR2: 0.806330\n",
      "Epoch: 4018 \tTraining Loss: 0.197927 \tR2: 0.806330\n",
      "Epoch: 4019 \tTraining Loss: 0.200595 \tR2: 0.806330\n",
      "Epoch: 4020 \tTraining Loss: 0.231197 \tR2: 0.806330\n",
      "Epoch: 4021 \tTraining Loss: 0.198858 \tR2: 0.806330\n",
      "Epoch: 4022 \tTraining Loss: 0.218538 \tR2: 0.806330\n",
      "Epoch: 4023 \tTraining Loss: 0.209245 \tR2: 0.806330\n",
      "Epoch: 4024 \tTraining Loss: 0.195691 \tR2: 0.806330\n",
      "Epoch: 4025 \tTraining Loss: 0.211271 \tR2: 0.806330\n",
      "Epoch: 4026 \tTraining Loss: 0.270799 \tR2: 0.806330\n",
      "Epoch: 4027 \tTraining Loss: 0.186421 \tR2: 0.806330\n",
      "Epoch: 4028 \tTraining Loss: 0.199456 \tR2: 0.806330\n",
      "Epoch: 4029 \tTraining Loss: 0.204689 \tR2: 0.806330\n",
      "Epoch: 4030 \tTraining Loss: 0.197587 \tR2: 0.806330\n",
      "Epoch: 4031 \tTraining Loss: 0.194684 \tR2: 0.806330\n",
      "Epoch: 4032 \tTraining Loss: 0.187726 \tR2: 0.806330\n",
      "Epoch: 4033 \tTraining Loss: 0.214683 \tR2: 0.806330\n",
      "Epoch: 4034 \tTraining Loss: 0.202552 \tR2: 0.806330\n",
      "Epoch: 4035 \tTraining Loss: 0.210450 \tR2: 0.806330\n",
      "Epoch: 4036 \tTraining Loss: 0.193661 \tR2: 0.806330\n",
      "Epoch: 4037 \tTraining Loss: 0.231714 \tR2: 0.806330\n",
      "Epoch: 4038 \tTraining Loss: 0.204281 \tR2: 0.806330\n",
      "Epoch: 4039 \tTraining Loss: 0.188829 \tR2: 0.806330\n",
      "Epoch: 4040 \tTraining Loss: 0.202989 \tR2: 0.806330\n",
      "Epoch: 4041 \tTraining Loss: 0.204793 \tR2: 0.806330\n",
      "Epoch: 4042 \tTraining Loss: 0.209933 \tR2: 0.806330\n",
      "Epoch: 4043 \tTraining Loss: 0.207198 \tR2: 0.806330\n",
      "Epoch: 4044 \tTraining Loss: 0.206486 \tR2: 0.806330\n",
      "Epoch: 4045 \tTraining Loss: 0.196963 \tR2: 0.806330\n",
      "Epoch: 4046 \tTraining Loss: 0.205731 \tR2: 0.806330\n",
      "Epoch: 4047 \tTraining Loss: 0.194710 \tR2: 0.806330\n",
      "Epoch: 4048 \tTraining Loss: 0.202286 \tR2: 0.806330\n",
      "Epoch: 4049 \tTraining Loss: 0.220412 \tR2: 0.806330\n",
      "Epoch: 4050 \tTraining Loss: 0.192780 \tR2: 0.806330\n",
      "Epoch: 4051 \tTraining Loss: 0.204207 \tR2: 0.806330\n",
      "Epoch: 4052 \tTraining Loss: 0.184831 \tR2: 0.806330\n",
      "Epoch: 4053 \tTraining Loss: 0.233628 \tR2: 0.806330\n",
      "Epoch: 4054 \tTraining Loss: 0.201962 \tR2: 0.806330\n",
      "Epoch: 4055 \tTraining Loss: 0.204885 \tR2: 0.806330\n",
      "Epoch: 4056 \tTraining Loss: 0.186661 \tR2: 0.806330\n",
      "Epoch: 4057 \tTraining Loss: 0.191137 \tR2: 0.806330\n",
      "Epoch: 4058 \tTraining Loss: 0.196646 \tR2: 0.806330\n",
      "Epoch: 4059 \tTraining Loss: 0.193674 \tR2: 0.806330\n",
      "Epoch: 4060 \tTraining Loss: 0.196954 \tR2: 0.806330\n",
      "Epoch: 4061 \tTraining Loss: 0.214273 \tR2: 0.806330\n",
      "Epoch: 4062 \tTraining Loss: 0.201581 \tR2: 0.806330\n",
      "Epoch: 4063 \tTraining Loss: 0.201678 \tR2: 0.806330\n",
      "Epoch: 4064 \tTraining Loss: 0.209444 \tR2: 0.806330\n",
      "Epoch: 4065 \tTraining Loss: 0.225912 \tR2: 0.806330\n",
      "Epoch: 4066 \tTraining Loss: 0.198686 \tR2: 0.806330\n",
      "Epoch: 4067 \tTraining Loss: 0.214254 \tR2: 0.806330\n",
      "Epoch: 4068 \tTraining Loss: 0.227955 \tR2: 0.806330\n",
      "Epoch: 4069 \tTraining Loss: 0.192247 \tR2: 0.806330\n",
      "Epoch: 4070 \tTraining Loss: 0.197852 \tR2: 0.806330\n",
      "Epoch: 4071 \tTraining Loss: 0.207484 \tR2: 0.806330\n",
      "Epoch: 4072 \tTraining Loss: 0.238804 \tR2: 0.806330\n",
      "Epoch: 4073 \tTraining Loss: 0.240587 \tR2: 0.806330\n",
      "Epoch: 4074 \tTraining Loss: 0.201591 \tR2: 0.806330\n",
      "Epoch: 4075 \tTraining Loss: 0.201415 \tR2: 0.806330\n",
      "Epoch: 4076 \tTraining Loss: 0.193545 \tR2: 0.806330\n",
      "Epoch: 4077 \tTraining Loss: 0.213615 \tR2: 0.806330\n",
      "Epoch: 4078 \tTraining Loss: 0.201603 \tR2: 0.806330\n",
      "Epoch: 4079 \tTraining Loss: 0.204397 \tR2: 0.806330\n",
      "Epoch: 4080 \tTraining Loss: 0.199686 \tR2: 0.806330\n",
      "Epoch: 4081 \tTraining Loss: 0.212924 \tR2: 0.806330\n",
      "Epoch: 4082 \tTraining Loss: 0.200950 \tR2: 0.806330\n",
      "Epoch: 4083 \tTraining Loss: 0.211685 \tR2: 0.806330\n",
      "Epoch: 4084 \tTraining Loss: 0.215432 \tR2: 0.806330\n",
      "Epoch: 4085 \tTraining Loss: 0.211327 \tR2: 0.806330\n",
      "Epoch: 4086 \tTraining Loss: 0.197971 \tR2: 0.806330\n",
      "Epoch: 4087 \tTraining Loss: 0.197102 \tR2: 0.806330\n",
      "Epoch: 4088 \tTraining Loss: 0.218941 \tR2: 0.806330\n",
      "Epoch: 4089 \tTraining Loss: 0.195645 \tR2: 0.806330\n",
      "Epoch: 4090 \tTraining Loss: 0.191208 \tR2: 0.806330\n",
      "Epoch: 4091 \tTraining Loss: 0.202215 \tR2: 0.806330\n",
      "Epoch: 4092 \tTraining Loss: 0.217244 \tR2: 0.806330\n",
      "Epoch: 4093 \tTraining Loss: 0.218078 \tR2: 0.806330\n",
      "Epoch: 4094 \tTraining Loss: 0.209061 \tR2: 0.806330\n",
      "Epoch: 4095 \tTraining Loss: 0.190392 \tR2: 0.806330\n",
      "Epoch: 4096 \tTraining Loss: 0.197151 \tR2: 0.806330\n",
      "Epoch: 4097 \tTraining Loss: 0.199338 \tR2: 0.806330\n",
      "Epoch: 4098 \tTraining Loss: 0.206476 \tR2: 0.806330\n",
      "Epoch: 4099 \tTraining Loss: 0.193134 \tR2: 0.806330\n",
      "Epoch: 4100 \tTraining Loss: 0.195248 \tR2: 0.624281\n",
      "Epoch: 4101 \tTraining Loss: 0.195399 \tR2: 0.624281\n",
      "Epoch: 4102 \tTraining Loss: 0.205497 \tR2: 0.624281\n",
      "Epoch: 4103 \tTraining Loss: 0.201656 \tR2: 0.624281\n",
      "Epoch: 4104 \tTraining Loss: 0.193495 \tR2: 0.624281\n",
      "Epoch: 4105 \tTraining Loss: 0.199256 \tR2: 0.624281\n",
      "Epoch: 4106 \tTraining Loss: 0.210733 \tR2: 0.624281\n",
      "Epoch: 4107 \tTraining Loss: 0.202717 \tR2: 0.624281\n",
      "Epoch: 4108 \tTraining Loss: 0.209792 \tR2: 0.624281\n",
      "Epoch: 4109 \tTraining Loss: 0.197675 \tR2: 0.624281\n",
      "Epoch: 4110 \tTraining Loss: 0.201888 \tR2: 0.624281\n",
      "Epoch: 4111 \tTraining Loss: 0.212939 \tR2: 0.624281\n",
      "Epoch: 4112 \tTraining Loss: 0.207841 \tR2: 0.624281\n",
      "Epoch: 4113 \tTraining Loss: 0.206781 \tR2: 0.624281\n",
      "Epoch: 4114 \tTraining Loss: 0.201259 \tR2: 0.624281\n",
      "Epoch: 4115 \tTraining Loss: 0.196724 \tR2: 0.624281\n",
      "Epoch: 4116 \tTraining Loss: 0.213991 \tR2: 0.624281\n",
      "Epoch: 4117 \tTraining Loss: 0.197728 \tR2: 0.624281\n",
      "Epoch: 4118 \tTraining Loss: 0.207327 \tR2: 0.624281\n",
      "Epoch: 4119 \tTraining Loss: 0.216092 \tR2: 0.624281\n",
      "Epoch: 4120 \tTraining Loss: 0.213862 \tR2: 0.624281\n",
      "Epoch: 4121 \tTraining Loss: 0.199503 \tR2: 0.624281\n",
      "Epoch: 4122 \tTraining Loss: 0.215973 \tR2: 0.624281\n",
      "Epoch: 4123 \tTraining Loss: 0.190734 \tR2: 0.624281\n",
      "Epoch: 4124 \tTraining Loss: 0.207718 \tR2: 0.624281\n",
      "Epoch: 4125 \tTraining Loss: 0.218408 \tR2: 0.624281\n",
      "Epoch: 4126 \tTraining Loss: 0.196847 \tR2: 0.624281\n",
      "Epoch: 4127 \tTraining Loss: 0.195694 \tR2: 0.624281\n",
      "Epoch: 4128 \tTraining Loss: 0.195020 \tR2: 0.624281\n",
      "Epoch: 4129 \tTraining Loss: 0.197621 \tR2: 0.624281\n",
      "Epoch: 4130 \tTraining Loss: 0.214862 \tR2: 0.624281\n",
      "Epoch: 4131 \tTraining Loss: 0.193282 \tR2: 0.624281\n",
      "Epoch: 4132 \tTraining Loss: 0.199834 \tR2: 0.624281\n",
      "Epoch: 4133 \tTraining Loss: 0.213290 \tR2: 0.624281\n",
      "Epoch: 4134 \tTraining Loss: 0.218295 \tR2: 0.624281\n",
      "Epoch: 4135 \tTraining Loss: 0.195650 \tR2: 0.624281\n",
      "Epoch: 4136 \tTraining Loss: 0.205400 \tR2: 0.624281\n",
      "Epoch: 4137 \tTraining Loss: 0.211129 \tR2: 0.624281\n",
      "Epoch: 4138 \tTraining Loss: 0.203096 \tR2: 0.624281\n",
      "Epoch: 4139 \tTraining Loss: 0.199992 \tR2: 0.624281\n",
      "Epoch: 4140 \tTraining Loss: 0.216782 \tR2: 0.624281\n",
      "Epoch: 4141 \tTraining Loss: 0.197420 \tR2: 0.624281\n",
      "Epoch: 4142 \tTraining Loss: 0.199817 \tR2: 0.624281\n",
      "Epoch: 4143 \tTraining Loss: 0.212600 \tR2: 0.624281\n",
      "Epoch: 4144 \tTraining Loss: 0.184861 \tR2: 0.624281\n",
      "Epoch: 4145 \tTraining Loss: 0.211166 \tR2: 0.624281\n",
      "Epoch: 4146 \tTraining Loss: 0.226399 \tR2: 0.624281\n",
      "Epoch: 4147 \tTraining Loss: 0.207111 \tR2: 0.624281\n",
      "Epoch: 4148 \tTraining Loss: 0.210602 \tR2: 0.624281\n",
      "Epoch: 4149 \tTraining Loss: 0.199829 \tR2: 0.624281\n",
      "Epoch: 4150 \tTraining Loss: 0.190994 \tR2: 0.624281\n",
      "Epoch: 4151 \tTraining Loss: 0.194835 \tR2: 0.624281\n",
      "Epoch: 4152 \tTraining Loss: 0.199883 \tR2: 0.624281\n",
      "Epoch: 4153 \tTraining Loss: 0.192791 \tR2: 0.624281\n",
      "Epoch: 4154 \tTraining Loss: 0.200326 \tR2: 0.624281\n",
      "Epoch: 4155 \tTraining Loss: 0.209719 \tR2: 0.624281\n",
      "Epoch: 4156 \tTraining Loss: 0.206776 \tR2: 0.624281\n",
      "Epoch: 4157 \tTraining Loss: 0.194628 \tR2: 0.624281\n",
      "Epoch: 4158 \tTraining Loss: 0.215911 \tR2: 0.624281\n",
      "Epoch: 4159 \tTraining Loss: 0.197946 \tR2: 0.624281\n",
      "Epoch: 4160 \tTraining Loss: 0.191926 \tR2: 0.624281\n",
      "Epoch: 4161 \tTraining Loss: 0.208891 \tR2: 0.624281\n",
      "Epoch: 4162 \tTraining Loss: 0.208938 \tR2: 0.624281\n",
      "Epoch: 4163 \tTraining Loss: 0.212225 \tR2: 0.624281\n",
      "Epoch: 4164 \tTraining Loss: 0.208379 \tR2: 0.624281\n",
      "Epoch: 4165 \tTraining Loss: 0.208074 \tR2: 0.624281\n",
      "Epoch: 4166 \tTraining Loss: 0.200353 \tR2: 0.624281\n",
      "Epoch: 4167 \tTraining Loss: 0.186695 \tR2: 0.624281\n",
      "Epoch: 4168 \tTraining Loss: 0.197979 \tR2: 0.624281\n",
      "Epoch: 4169 \tTraining Loss: 0.190318 \tR2: 0.624281\n",
      "Epoch: 4170 \tTraining Loss: 0.219211 \tR2: 0.624281\n",
      "Epoch: 4171 \tTraining Loss: 0.203189 \tR2: 0.624281\n",
      "Epoch: 4172 \tTraining Loss: 0.201908 \tR2: 0.624281\n",
      "Epoch: 4173 \tTraining Loss: 0.194355 \tR2: 0.624281\n",
      "Epoch: 4174 \tTraining Loss: 0.192034 \tR2: 0.624281\n",
      "Epoch: 4175 \tTraining Loss: 0.227015 \tR2: 0.624281\n",
      "Epoch: 4176 \tTraining Loss: 0.227683 \tR2: 0.624281\n",
      "Epoch: 4177 \tTraining Loss: 0.195251 \tR2: 0.624281\n",
      "Epoch: 4178 \tTraining Loss: 0.194904 \tR2: 0.624281\n",
      "Epoch: 4179 \tTraining Loss: 0.201437 \tR2: 0.624281\n",
      "Epoch: 4180 \tTraining Loss: 0.209947 \tR2: 0.624281\n",
      "Epoch: 4181 \tTraining Loss: 0.201728 \tR2: 0.624281\n",
      "Epoch: 4182 \tTraining Loss: 0.216429 \tR2: 0.624281\n",
      "Epoch: 4183 \tTraining Loss: 0.202186 \tR2: 0.624281\n",
      "Epoch: 4184 \tTraining Loss: 0.207021 \tR2: 0.624281\n",
      "Epoch: 4185 \tTraining Loss: 0.205109 \tR2: 0.624281\n",
      "Epoch: 4186 \tTraining Loss: 0.194134 \tR2: 0.624281\n",
      "Epoch: 4187 \tTraining Loss: 0.212814 \tR2: 0.624281\n",
      "Epoch: 4188 \tTraining Loss: 0.209936 \tR2: 0.624281\n",
      "Epoch: 4189 \tTraining Loss: 0.203861 \tR2: 0.624281\n",
      "Epoch: 4190 \tTraining Loss: 0.207826 \tR2: 0.624281\n",
      "Epoch: 4191 \tTraining Loss: 0.211508 \tR2: 0.624281\n",
      "Epoch: 4192 \tTraining Loss: 0.228244 \tR2: 0.624281\n",
      "Epoch: 4193 \tTraining Loss: 0.181663 \tR2: 0.624281\n",
      "Epoch: 4194 \tTraining Loss: 0.193580 \tR2: 0.624281\n",
      "Epoch: 4195 \tTraining Loss: 0.191489 \tR2: 0.624281\n",
      "Epoch: 4196 \tTraining Loss: 0.209504 \tR2: 0.624281\n",
      "Epoch: 4197 \tTraining Loss: 0.199088 \tR2: 0.624281\n",
      "Epoch: 4198 \tTraining Loss: 0.202319 \tR2: 0.624281\n",
      "Epoch: 4199 \tTraining Loss: 0.196671 \tR2: 0.624281\n",
      "Epoch: 4200 \tTraining Loss: 0.219935 \tR2: 0.740025\n",
      "Epoch: 4201 \tTraining Loss: 0.198172 \tR2: 0.740025\n",
      "Epoch: 4202 \tTraining Loss: 0.196538 \tR2: 0.740025\n",
      "Epoch: 4203 \tTraining Loss: 0.203137 \tR2: 0.740025\n",
      "Epoch: 4204 \tTraining Loss: 0.208859 \tR2: 0.740025\n",
      "Epoch: 4205 \tTraining Loss: 0.207952 \tR2: 0.740025\n",
      "Epoch: 4206 \tTraining Loss: 0.212127 \tR2: 0.740025\n",
      "Epoch: 4207 \tTraining Loss: 0.207054 \tR2: 0.740025\n",
      "Epoch: 4208 \tTraining Loss: 0.206719 \tR2: 0.740025\n",
      "Epoch: 4209 \tTraining Loss: 0.189566 \tR2: 0.740025\n",
      "Epoch: 4210 \tTraining Loss: 0.202186 \tR2: 0.740025\n",
      "Epoch: 4211 \tTraining Loss: 0.206581 \tR2: 0.740025\n",
      "Epoch: 4212 \tTraining Loss: 0.204873 \tR2: 0.740025\n",
      "Epoch: 4213 \tTraining Loss: 0.189414 \tR2: 0.740025\n",
      "Epoch: 4214 \tTraining Loss: 0.197490 \tR2: 0.740025\n",
      "Epoch: 4215 \tTraining Loss: 0.210058 \tR2: 0.740025\n",
      "Epoch: 4216 \tTraining Loss: 0.220260 \tR2: 0.740025\n",
      "Epoch: 4217 \tTraining Loss: 0.189922 \tR2: 0.740025\n",
      "Epoch: 4218 \tTraining Loss: 0.191448 \tR2: 0.740025\n",
      "Epoch: 4219 \tTraining Loss: 0.210831 \tR2: 0.740025\n",
      "Epoch: 4220 \tTraining Loss: 0.218767 \tR2: 0.740025\n",
      "Epoch: 4221 \tTraining Loss: 0.172998 \tR2: 0.740025\n",
      "Epoch: 4222 \tTraining Loss: 0.180907 \tR2: 0.740025\n",
      "Epoch: 4223 \tTraining Loss: 0.203564 \tR2: 0.740025\n",
      "Epoch: 4224 \tTraining Loss: 0.189967 \tR2: 0.740025\n",
      "Epoch: 4225 \tTraining Loss: 0.192823 \tR2: 0.740025\n",
      "Epoch: 4226 \tTraining Loss: 0.188267 \tR2: 0.740025\n",
      "Epoch: 4227 \tTraining Loss: 0.193510 \tR2: 0.740025\n",
      "Epoch: 4228 \tTraining Loss: 0.191010 \tR2: 0.740025\n",
      "Epoch: 4229 \tTraining Loss: 0.201168 \tR2: 0.740025\n",
      "Epoch: 4230 \tTraining Loss: 0.205062 \tR2: 0.740025\n",
      "Epoch: 4231 \tTraining Loss: 0.213007 \tR2: 0.740025\n",
      "Epoch: 4232 \tTraining Loss: 0.197988 \tR2: 0.740025\n",
      "Epoch: 4233 \tTraining Loss: 0.183260 \tR2: 0.740025\n",
      "Epoch: 4234 \tTraining Loss: 0.179601 \tR2: 0.740025\n",
      "Epoch: 4235 \tTraining Loss: 0.201206 \tR2: 0.740025\n",
      "Epoch: 4236 \tTraining Loss: 0.207074 \tR2: 0.740025\n",
      "Epoch: 4237 \tTraining Loss: 0.198703 \tR2: 0.740025\n",
      "Epoch: 4238 \tTraining Loss: 0.204005 \tR2: 0.740025\n",
      "Epoch: 4239 \tTraining Loss: 0.188596 \tR2: 0.740025\n",
      "Epoch: 4240 \tTraining Loss: 0.196679 \tR2: 0.740025\n",
      "Epoch: 4241 \tTraining Loss: 0.213826 \tR2: 0.740025\n",
      "Epoch: 4242 \tTraining Loss: 0.205152 \tR2: 0.740025\n",
      "Epoch: 4243 \tTraining Loss: 0.184443 \tR2: 0.740025\n",
      "Epoch: 4244 \tTraining Loss: 0.216670 \tR2: 0.740025\n",
      "Epoch: 4245 \tTraining Loss: 0.208591 \tR2: 0.740025\n",
      "Epoch: 4246 \tTraining Loss: 0.198611 \tR2: 0.740025\n",
      "Epoch: 4247 \tTraining Loss: 0.184590 \tR2: 0.740025\n",
      "Epoch: 4248 \tTraining Loss: 0.252223 \tR2: 0.740025\n",
      "Epoch: 4249 \tTraining Loss: 0.221912 \tR2: 0.740025\n",
      "Epoch: 4250 \tTraining Loss: 0.197518 \tR2: 0.740025\n",
      "Epoch: 4251 \tTraining Loss: 0.204150 \tR2: 0.740025\n",
      "Epoch: 4252 \tTraining Loss: 0.200335 \tR2: 0.740025\n",
      "Epoch: 4253 \tTraining Loss: 0.223324 \tR2: 0.740025\n",
      "Epoch: 4254 \tTraining Loss: 0.186079 \tR2: 0.740025\n",
      "Epoch: 4255 \tTraining Loss: 0.181214 \tR2: 0.740025\n",
      "Epoch: 4256 \tTraining Loss: 0.198349 \tR2: 0.740025\n",
      "Epoch: 4257 \tTraining Loss: 0.189990 \tR2: 0.740025\n",
      "Epoch: 4258 \tTraining Loss: 0.189699 \tR2: 0.740025\n",
      "Epoch: 4259 \tTraining Loss: 0.221651 \tR2: 0.740025\n",
      "Epoch: 4260 \tTraining Loss: 0.206961 \tR2: 0.740025\n",
      "Epoch: 4261 \tTraining Loss: 0.217487 \tR2: 0.740025\n",
      "Epoch: 4262 \tTraining Loss: 0.191328 \tR2: 0.740025\n",
      "Epoch: 4263 \tTraining Loss: 0.237122 \tR2: 0.740025\n",
      "Epoch: 4264 \tTraining Loss: 0.209864 \tR2: 0.740025\n",
      "Epoch: 4265 \tTraining Loss: 0.180306 \tR2: 0.740025\n",
      "Epoch: 4266 \tTraining Loss: 0.215019 \tR2: 0.740025\n",
      "Epoch: 4267 \tTraining Loss: 0.197912 \tR2: 0.740025\n",
      "Epoch: 4268 \tTraining Loss: 0.217173 \tR2: 0.740025\n",
      "Epoch: 4269 \tTraining Loss: 0.183241 \tR2: 0.740025\n",
      "Epoch: 4270 \tTraining Loss: 0.201315 \tR2: 0.740025\n",
      "Epoch: 4271 \tTraining Loss: 0.201497 \tR2: 0.740025\n",
      "Epoch: 4272 \tTraining Loss: 0.203280 \tR2: 0.740025\n",
      "Epoch: 4273 \tTraining Loss: 0.212007 \tR2: 0.740025\n",
      "Epoch: 4274 \tTraining Loss: 0.196100 \tR2: 0.740025\n",
      "Epoch: 4275 \tTraining Loss: 0.208340 \tR2: 0.740025\n",
      "Epoch: 4276 \tTraining Loss: 0.183550 \tR2: 0.740025\n",
      "Epoch: 4277 \tTraining Loss: 0.200923 \tR2: 0.740025\n",
      "Epoch: 4278 \tTraining Loss: 0.202740 \tR2: 0.740025\n",
      "Epoch: 4279 \tTraining Loss: 0.206743 \tR2: 0.740025\n",
      "Epoch: 4280 \tTraining Loss: 0.201759 \tR2: 0.740025\n",
      "Epoch: 4281 \tTraining Loss: 0.217582 \tR2: 0.740025\n",
      "Epoch: 4282 \tTraining Loss: 0.201882 \tR2: 0.740025\n",
      "Epoch: 4283 \tTraining Loss: 0.197221 \tR2: 0.740025\n",
      "Epoch: 4284 \tTraining Loss: 0.205445 \tR2: 0.740025\n",
      "Epoch: 4285 \tTraining Loss: 0.197330 \tR2: 0.740025\n",
      "Epoch: 4286 \tTraining Loss: 0.214425 \tR2: 0.740025\n",
      "Epoch: 4287 \tTraining Loss: 0.203941 \tR2: 0.740025\n",
      "Epoch: 4288 \tTraining Loss: 0.210796 \tR2: 0.740025\n",
      "Epoch: 4289 \tTraining Loss: 0.200740 \tR2: 0.740025\n",
      "Epoch: 4290 \tTraining Loss: 0.194557 \tR2: 0.740025\n",
      "Epoch: 4291 \tTraining Loss: 0.202880 \tR2: 0.740025\n",
      "Epoch: 4292 \tTraining Loss: 0.198017 \tR2: 0.740025\n",
      "Epoch: 4293 \tTraining Loss: 0.202383 \tR2: 0.740025\n",
      "Epoch: 4294 \tTraining Loss: 0.206274 \tR2: 0.740025\n",
      "Epoch: 4295 \tTraining Loss: 0.197504 \tR2: 0.740025\n",
      "Epoch: 4296 \tTraining Loss: 0.204740 \tR2: 0.740025\n",
      "Epoch: 4297 \tTraining Loss: 0.190929 \tR2: 0.740025\n",
      "Epoch: 4298 \tTraining Loss: 0.198550 \tR2: 0.740025\n",
      "Epoch: 4299 \tTraining Loss: 0.205210 \tR2: 0.740025\n",
      "Epoch: 4300 \tTraining Loss: 0.199469 \tR2: 0.727852\n",
      "Epoch: 4301 \tTraining Loss: 0.203951 \tR2: 0.727852\n",
      "Epoch: 4302 \tTraining Loss: 0.200621 \tR2: 0.727852\n",
      "Epoch: 4303 \tTraining Loss: 0.186846 \tR2: 0.727852\n",
      "Epoch: 4304 \tTraining Loss: 0.207586 \tR2: 0.727852\n",
      "Epoch: 4305 \tTraining Loss: 0.195767 \tR2: 0.727852\n",
      "Epoch: 4306 \tTraining Loss: 0.202751 \tR2: 0.727852\n",
      "Epoch: 4307 \tTraining Loss: 0.215884 \tR2: 0.727852\n",
      "Epoch: 4308 \tTraining Loss: 0.197588 \tR2: 0.727852\n",
      "Epoch: 4309 \tTraining Loss: 0.201119 \tR2: 0.727852\n",
      "Epoch: 4310 \tTraining Loss: 0.182999 \tR2: 0.727852\n",
      "Epoch: 4311 \tTraining Loss: 0.188018 \tR2: 0.727852\n",
      "Epoch: 4312 \tTraining Loss: 0.206170 \tR2: 0.727852\n",
      "Epoch: 4313 \tTraining Loss: 0.190542 \tR2: 0.727852\n",
      "Epoch: 4314 \tTraining Loss: 0.199736 \tR2: 0.727852\n",
      "Epoch: 4315 \tTraining Loss: 0.193884 \tR2: 0.727852\n",
      "Epoch: 4316 \tTraining Loss: 0.213989 \tR2: 0.727852\n",
      "Epoch: 4317 \tTraining Loss: 0.187058 \tR2: 0.727852\n",
      "Epoch: 4318 \tTraining Loss: 0.190769 \tR2: 0.727852\n",
      "Epoch: 4319 \tTraining Loss: 0.193975 \tR2: 0.727852\n",
      "Epoch: 4320 \tTraining Loss: 0.193847 \tR2: 0.727852\n",
      "Epoch: 4321 \tTraining Loss: 0.198067 \tR2: 0.727852\n",
      "Epoch: 4322 \tTraining Loss: 0.183958 \tR2: 0.727852\n",
      "Epoch: 4323 \tTraining Loss: 0.196816 \tR2: 0.727852\n",
      "Epoch: 4324 \tTraining Loss: 0.204883 \tR2: 0.727852\n",
      "Epoch: 4325 \tTraining Loss: 0.204416 \tR2: 0.727852\n",
      "Epoch: 4326 \tTraining Loss: 0.196764 \tR2: 0.727852\n",
      "Epoch: 4327 \tTraining Loss: 0.215127 \tR2: 0.727852\n",
      "Epoch: 4328 \tTraining Loss: 0.192225 \tR2: 0.727852\n",
      "Epoch: 4329 \tTraining Loss: 0.205532 \tR2: 0.727852\n",
      "Epoch: 4330 \tTraining Loss: 0.194192 \tR2: 0.727852\n",
      "Epoch: 4331 \tTraining Loss: 0.208683 \tR2: 0.727852\n",
      "Epoch: 4332 \tTraining Loss: 0.202917 \tR2: 0.727852\n",
      "Epoch: 4333 \tTraining Loss: 0.185755 \tR2: 0.727852\n",
      "Epoch: 4334 \tTraining Loss: 0.195987 \tR2: 0.727852\n",
      "Epoch: 4335 \tTraining Loss: 0.203206 \tR2: 0.727852\n",
      "Epoch: 4336 \tTraining Loss: 0.184620 \tR2: 0.727852\n",
      "Epoch: 4337 \tTraining Loss: 0.205601 \tR2: 0.727852\n",
      "Epoch: 4338 \tTraining Loss: 0.236548 \tR2: 0.727852\n",
      "Epoch: 4339 \tTraining Loss: 0.214334 \tR2: 0.727852\n",
      "Epoch: 4340 \tTraining Loss: 0.225722 \tR2: 0.727852\n",
      "Epoch: 4341 \tTraining Loss: 0.193194 \tR2: 0.727852\n",
      "Epoch: 4342 \tTraining Loss: 0.185091 \tR2: 0.727852\n",
      "Epoch: 4343 \tTraining Loss: 0.197006 \tR2: 0.727852\n",
      "Epoch: 4344 \tTraining Loss: 0.190909 \tR2: 0.727852\n",
      "Epoch: 4345 \tTraining Loss: 0.204932 \tR2: 0.727852\n",
      "Epoch: 4346 \tTraining Loss: 0.184671 \tR2: 0.727852\n",
      "Epoch: 4347 \tTraining Loss: 0.186953 \tR2: 0.727852\n",
      "Epoch: 4348 \tTraining Loss: 0.214916 \tR2: 0.727852\n",
      "Epoch: 4349 \tTraining Loss: 0.207599 \tR2: 0.727852\n",
      "Epoch: 4350 \tTraining Loss: 0.206998 \tR2: 0.727852\n",
      "Epoch: 4351 \tTraining Loss: 0.203411 \tR2: 0.727852\n",
      "Epoch: 4352 \tTraining Loss: 0.199517 \tR2: 0.727852\n",
      "Epoch: 4353 \tTraining Loss: 0.217333 \tR2: 0.727852\n",
      "Epoch: 4354 \tTraining Loss: 0.192225 \tR2: 0.727852\n",
      "Epoch: 4355 \tTraining Loss: 0.205338 \tR2: 0.727852\n",
      "Epoch: 4356 \tTraining Loss: 0.208795 \tR2: 0.727852\n",
      "Epoch: 4357 \tTraining Loss: 0.206237 \tR2: 0.727852\n",
      "Epoch: 4358 \tTraining Loss: 0.210641 \tR2: 0.727852\n",
      "Epoch: 4359 \tTraining Loss: 0.212873 \tR2: 0.727852\n",
      "Epoch: 4360 \tTraining Loss: 0.208463 \tR2: 0.727852\n",
      "Epoch: 4361 \tTraining Loss: 0.204447 \tR2: 0.727852\n",
      "Epoch: 4362 \tTraining Loss: 0.195298 \tR2: 0.727852\n",
      "Epoch: 4363 \tTraining Loss: 0.205527 \tR2: 0.727852\n",
      "Epoch: 4364 \tTraining Loss: 0.191723 \tR2: 0.727852\n",
      "Epoch: 4365 \tTraining Loss: 0.214889 \tR2: 0.727852\n",
      "Epoch: 4366 \tTraining Loss: 0.201392 \tR2: 0.727852\n",
      "Epoch: 4367 \tTraining Loss: 0.198176 \tR2: 0.727852\n",
      "Epoch: 4368 \tTraining Loss: 0.201533 \tR2: 0.727852\n",
      "Epoch: 4369 \tTraining Loss: 0.186677 \tR2: 0.727852\n",
      "Epoch: 4370 \tTraining Loss: 0.220960 \tR2: 0.727852\n",
      "Epoch: 4371 \tTraining Loss: 0.197222 \tR2: 0.727852\n",
      "Epoch: 4372 \tTraining Loss: 0.185213 \tR2: 0.727852\n",
      "Epoch: 4373 \tTraining Loss: 0.215011 \tR2: 0.727852\n",
      "Epoch: 4374 \tTraining Loss: 0.206412 \tR2: 0.727852\n",
      "Epoch: 4375 \tTraining Loss: 0.242840 \tR2: 0.727852\n",
      "Epoch: 4376 \tTraining Loss: 0.194271 \tR2: 0.727852\n",
      "Epoch: 4377 \tTraining Loss: 0.196129 \tR2: 0.727852\n",
      "Epoch: 4378 \tTraining Loss: 0.191858 \tR2: 0.727852\n",
      "Epoch: 4379 \tTraining Loss: 0.203179 \tR2: 0.727852\n",
      "Epoch: 4380 \tTraining Loss: 0.192628 \tR2: 0.727852\n",
      "Epoch: 4381 \tTraining Loss: 0.201267 \tR2: 0.727852\n",
      "Epoch: 4382 \tTraining Loss: 0.211682 \tR2: 0.727852\n",
      "Epoch: 4383 \tTraining Loss: 0.212377 \tR2: 0.727852\n",
      "Epoch: 4384 \tTraining Loss: 0.193725 \tR2: 0.727852\n",
      "Epoch: 4385 \tTraining Loss: 0.185538 \tR2: 0.727852\n",
      "Epoch: 4386 \tTraining Loss: 0.183697 \tR2: 0.727852\n",
      "Epoch: 4387 \tTraining Loss: 0.200629 \tR2: 0.727852\n",
      "Epoch: 4388 \tTraining Loss: 0.205919 \tR2: 0.727852\n",
      "Epoch: 4389 \tTraining Loss: 0.205107 \tR2: 0.727852\n",
      "Epoch: 4390 \tTraining Loss: 0.194795 \tR2: 0.727852\n",
      "Epoch: 4391 \tTraining Loss: 0.187863 \tR2: 0.727852\n",
      "Epoch: 4392 \tTraining Loss: 0.178553 \tR2: 0.727852\n",
      "Epoch: 4393 \tTraining Loss: 0.212380 \tR2: 0.727852\n",
      "Epoch: 4394 \tTraining Loss: 0.225413 \tR2: 0.727852\n",
      "Epoch: 4395 \tTraining Loss: 0.203815 \tR2: 0.727852\n",
      "Epoch: 4396 \tTraining Loss: 0.211090 \tR2: 0.727852\n",
      "Epoch: 4397 \tTraining Loss: 0.213154 \tR2: 0.727852\n",
      "Epoch: 4398 \tTraining Loss: 0.232892 \tR2: 0.727852\n",
      "Epoch: 4399 \tTraining Loss: 0.197059 \tR2: 0.727852\n",
      "Epoch: 4400 \tTraining Loss: 0.199617 \tR2: 0.747091\n",
      "Epoch: 4401 \tTraining Loss: 0.198976 \tR2: 0.747091\n",
      "Epoch: 4402 \tTraining Loss: 0.193734 \tR2: 0.747091\n",
      "Epoch: 4403 \tTraining Loss: 0.210465 \tR2: 0.747091\n",
      "Epoch: 4404 \tTraining Loss: 0.186982 \tR2: 0.747091\n",
      "Epoch: 4405 \tTraining Loss: 0.190034 \tR2: 0.747091\n",
      "Epoch: 4406 \tTraining Loss: 0.231736 \tR2: 0.747091\n",
      "Epoch: 4407 \tTraining Loss: 0.198998 \tR2: 0.747091\n",
      "Epoch: 4408 \tTraining Loss: 0.199512 \tR2: 0.747091\n",
      "Epoch: 4409 \tTraining Loss: 0.215593 \tR2: 0.747091\n",
      "Epoch: 4410 \tTraining Loss: 0.203524 \tR2: 0.747091\n",
      "Epoch: 4411 \tTraining Loss: 0.196064 \tR2: 0.747091\n",
      "Epoch: 4412 \tTraining Loss: 0.216353 \tR2: 0.747091\n",
      "Epoch: 4413 \tTraining Loss: 0.197519 \tR2: 0.747091\n",
      "Epoch: 4414 \tTraining Loss: 0.199874 \tR2: 0.747091\n",
      "Epoch: 4415 \tTraining Loss: 0.202664 \tR2: 0.747091\n",
      "Epoch: 4416 \tTraining Loss: 0.210475 \tR2: 0.747091\n",
      "Epoch: 4417 \tTraining Loss: 0.183497 \tR2: 0.747091\n",
      "Epoch: 4418 \tTraining Loss: 0.198552 \tR2: 0.747091\n",
      "Epoch: 4419 \tTraining Loss: 0.197099 \tR2: 0.747091\n",
      "Epoch: 4420 \tTraining Loss: 0.188567 \tR2: 0.747091\n",
      "Epoch: 4421 \tTraining Loss: 0.215519 \tR2: 0.747091\n",
      "Epoch: 4422 \tTraining Loss: 0.200653 \tR2: 0.747091\n",
      "Epoch: 4423 \tTraining Loss: 0.214872 \tR2: 0.747091\n",
      "Epoch: 4424 \tTraining Loss: 0.198356 \tR2: 0.747091\n",
      "Epoch: 4425 \tTraining Loss: 0.201830 \tR2: 0.747091\n",
      "Epoch: 4426 \tTraining Loss: 0.194860 \tR2: 0.747091\n",
      "Epoch: 4427 \tTraining Loss: 0.191353 \tR2: 0.747091\n",
      "Epoch: 4428 \tTraining Loss: 0.183401 \tR2: 0.747091\n",
      "Epoch: 4429 \tTraining Loss: 0.196972 \tR2: 0.747091\n",
      "Epoch: 4430 \tTraining Loss: 0.208759 \tR2: 0.747091\n",
      "Epoch: 4431 \tTraining Loss: 0.208070 \tR2: 0.747091\n",
      "Epoch: 4432 \tTraining Loss: 0.202363 \tR2: 0.747091\n",
      "Epoch: 4433 \tTraining Loss: 0.187434 \tR2: 0.747091\n",
      "Epoch: 4434 \tTraining Loss: 0.205826 \tR2: 0.747091\n",
      "Epoch: 4435 \tTraining Loss: 0.210893 \tR2: 0.747091\n",
      "Epoch: 4436 \tTraining Loss: 0.187201 \tR2: 0.747091\n",
      "Epoch: 4437 \tTraining Loss: 0.189720 \tR2: 0.747091\n",
      "Epoch: 4438 \tTraining Loss: 0.234871 \tR2: 0.747091\n",
      "Epoch: 4439 \tTraining Loss: 0.207571 \tR2: 0.747091\n",
      "Epoch: 4440 \tTraining Loss: 0.198838 \tR2: 0.747091\n",
      "Epoch: 4441 \tTraining Loss: 0.211079 \tR2: 0.747091\n",
      "Epoch: 4442 \tTraining Loss: 0.207906 \tR2: 0.747091\n",
      "Epoch: 4443 \tTraining Loss: 0.190555 \tR2: 0.747091\n",
      "Epoch: 4444 \tTraining Loss: 0.187829 \tR2: 0.747091\n",
      "Epoch: 4445 \tTraining Loss: 0.210006 \tR2: 0.747091\n",
      "Epoch: 4446 \tTraining Loss: 0.209057 \tR2: 0.747091\n",
      "Epoch: 4447 \tTraining Loss: 0.192980 \tR2: 0.747091\n",
      "Epoch: 4448 \tTraining Loss: 0.182933 \tR2: 0.747091\n",
      "Epoch: 4449 \tTraining Loss: 0.231303 \tR2: 0.747091\n",
      "Epoch: 4450 \tTraining Loss: 0.185968 \tR2: 0.747091\n",
      "Epoch: 4451 \tTraining Loss: 0.192938 \tR2: 0.747091\n",
      "Epoch: 4452 \tTraining Loss: 0.208009 \tR2: 0.747091\n",
      "Epoch: 4453 \tTraining Loss: 0.195745 \tR2: 0.747091\n",
      "Epoch: 4454 \tTraining Loss: 0.193278 \tR2: 0.747091\n",
      "Epoch: 4455 \tTraining Loss: 0.211213 \tR2: 0.747091\n",
      "Epoch: 4456 \tTraining Loss: 0.205597 \tR2: 0.747091\n",
      "Epoch: 4457 \tTraining Loss: 0.197963 \tR2: 0.747091\n",
      "Epoch: 4458 \tTraining Loss: 0.208928 \tR2: 0.747091\n",
      "Epoch: 4459 \tTraining Loss: 0.203737 \tR2: 0.747091\n",
      "Epoch: 4460 \tTraining Loss: 0.201388 \tR2: 0.747091\n",
      "Epoch: 4461 \tTraining Loss: 0.187105 \tR2: 0.747091\n",
      "Epoch: 4462 \tTraining Loss: 0.203967 \tR2: 0.747091\n",
      "Epoch: 4463 \tTraining Loss: 0.205140 \tR2: 0.747091\n",
      "Epoch: 4464 \tTraining Loss: 0.206330 \tR2: 0.747091\n",
      "Epoch: 4465 \tTraining Loss: 0.186511 \tR2: 0.747091\n",
      "Epoch: 4466 \tTraining Loss: 0.229043 \tR2: 0.747091\n",
      "Epoch: 4467 \tTraining Loss: 0.196358 \tR2: 0.747091\n",
      "Epoch: 4468 \tTraining Loss: 0.196543 \tR2: 0.747091\n",
      "Epoch: 4469 \tTraining Loss: 0.203719 \tR2: 0.747091\n",
      "Epoch: 4470 \tTraining Loss: 0.189346 \tR2: 0.747091\n",
      "Epoch: 4471 \tTraining Loss: 0.184324 \tR2: 0.747091\n",
      "Epoch: 4472 \tTraining Loss: 0.213661 \tR2: 0.747091\n",
      "Epoch: 4473 \tTraining Loss: 0.191980 \tR2: 0.747091\n",
      "Epoch: 4474 \tTraining Loss: 0.205312 \tR2: 0.747091\n",
      "Epoch: 4475 \tTraining Loss: 0.195472 \tR2: 0.747091\n",
      "Epoch: 4476 \tTraining Loss: 0.194194 \tR2: 0.747091\n",
      "Epoch: 4477 \tTraining Loss: 0.212479 \tR2: 0.747091\n",
      "Epoch: 4478 \tTraining Loss: 0.194140 \tR2: 0.747091\n",
      "Epoch: 4479 \tTraining Loss: 0.187643 \tR2: 0.747091\n",
      "Epoch: 4480 \tTraining Loss: 0.199500 \tR2: 0.747091\n",
      "Epoch: 4481 \tTraining Loss: 0.199017 \tR2: 0.747091\n",
      "Epoch: 4482 \tTraining Loss: 0.218934 \tR2: 0.747091\n",
      "Epoch: 4483 \tTraining Loss: 0.198582 \tR2: 0.747091\n",
      "Epoch: 4484 \tTraining Loss: 0.200488 \tR2: 0.747091\n",
      "Epoch: 4485 \tTraining Loss: 0.187720 \tR2: 0.747091\n",
      "Epoch: 4486 \tTraining Loss: 0.196883 \tR2: 0.747091\n",
      "Epoch: 4487 \tTraining Loss: 0.247251 \tR2: 0.747091\n",
      "Epoch: 4488 \tTraining Loss: 0.208656 \tR2: 0.747091\n",
      "Epoch: 4489 \tTraining Loss: 0.215948 \tR2: 0.747091\n",
      "Epoch: 4490 \tTraining Loss: 0.193171 \tR2: 0.747091\n",
      "Epoch: 4491 \tTraining Loss: 0.187270 \tR2: 0.747091\n",
      "Epoch: 4492 \tTraining Loss: 0.227934 \tR2: 0.747091\n",
      "Epoch: 4493 \tTraining Loss: 0.211961 \tR2: 0.747091\n",
      "Epoch: 4494 \tTraining Loss: 0.219708 \tR2: 0.747091\n",
      "Epoch: 4495 \tTraining Loss: 0.197691 \tR2: 0.747091\n",
      "Epoch: 4496 \tTraining Loss: 0.178270 \tR2: 0.747091\n",
      "Epoch: 4497 \tTraining Loss: 0.197159 \tR2: 0.747091\n",
      "Epoch: 4498 \tTraining Loss: 0.199109 \tR2: 0.747091\n",
      "Epoch: 4499 \tTraining Loss: 0.200494 \tR2: 0.747091\n",
      "Epoch: 4500 \tTraining Loss: 0.191638 \tR2: 0.717473\n",
      "Epoch: 4501 \tTraining Loss: 0.195799 \tR2: 0.717473\n",
      "Epoch: 4502 \tTraining Loss: 0.195644 \tR2: 0.717473\n",
      "Epoch: 4503 \tTraining Loss: 0.210171 \tR2: 0.717473\n",
      "Epoch: 4504 \tTraining Loss: 0.220936 \tR2: 0.717473\n",
      "Epoch: 4505 \tTraining Loss: 0.190127 \tR2: 0.717473\n",
      "Epoch: 4506 \tTraining Loss: 0.209338 \tR2: 0.717473\n",
      "Epoch: 4507 \tTraining Loss: 0.185095 \tR2: 0.717473\n",
      "Epoch: 4508 \tTraining Loss: 0.198877 \tR2: 0.717473\n",
      "Epoch: 4509 \tTraining Loss: 0.198490 \tR2: 0.717473\n",
      "Epoch: 4510 \tTraining Loss: 0.210751 \tR2: 0.717473\n",
      "Epoch: 4511 \tTraining Loss: 0.198773 \tR2: 0.717473\n",
      "Epoch: 4512 \tTraining Loss: 0.198439 \tR2: 0.717473\n",
      "Epoch: 4513 \tTraining Loss: 0.189952 \tR2: 0.717473\n",
      "Epoch: 4514 \tTraining Loss: 0.207801 \tR2: 0.717473\n",
      "Epoch: 4515 \tTraining Loss: 0.217646 \tR2: 0.717473\n",
      "Epoch: 4516 \tTraining Loss: 0.210714 \tR2: 0.717473\n",
      "Epoch: 4517 \tTraining Loss: 0.196784 \tR2: 0.717473\n",
      "Epoch: 4518 \tTraining Loss: 0.199763 \tR2: 0.717473\n",
      "Epoch: 4519 \tTraining Loss: 0.214356 \tR2: 0.717473\n",
      "Epoch: 4520 \tTraining Loss: 0.201512 \tR2: 0.717473\n",
      "Epoch: 4521 \tTraining Loss: 0.192407 \tR2: 0.717473\n",
      "Epoch: 4522 \tTraining Loss: 0.175624 \tR2: 0.717473\n",
      "Epoch: 4523 \tTraining Loss: 0.209160 \tR2: 0.717473\n",
      "Epoch: 4524 \tTraining Loss: 0.191495 \tR2: 0.717473\n",
      "Epoch: 4525 \tTraining Loss: 0.201708 \tR2: 0.717473\n",
      "Epoch: 4526 \tTraining Loss: 0.216423 \tR2: 0.717473\n",
      "Epoch: 4527 \tTraining Loss: 0.217720 \tR2: 0.717473\n",
      "Epoch: 4528 \tTraining Loss: 0.186078 \tR2: 0.717473\n",
      "Epoch: 4529 \tTraining Loss: 0.197281 \tR2: 0.717473\n",
      "Epoch: 4530 \tTraining Loss: 0.205962 \tR2: 0.717473\n",
      "Epoch: 4531 \tTraining Loss: 0.200801 \tR2: 0.717473\n",
      "Epoch: 4532 \tTraining Loss: 0.201899 \tR2: 0.717473\n",
      "Epoch: 4533 \tTraining Loss: 0.194030 \tR2: 0.717473\n",
      "Epoch: 4534 \tTraining Loss: 0.207183 \tR2: 0.717473\n",
      "Epoch: 4535 \tTraining Loss: 0.189591 \tR2: 0.717473\n",
      "Epoch: 4536 \tTraining Loss: 0.204053 \tR2: 0.717473\n",
      "Epoch: 4537 \tTraining Loss: 0.176818 \tR2: 0.717473\n",
      "Epoch: 4538 \tTraining Loss: 0.217782 \tR2: 0.717473\n",
      "Epoch: 4539 \tTraining Loss: 0.184492 \tR2: 0.717473\n",
      "Epoch: 4540 \tTraining Loss: 0.219326 \tR2: 0.717473\n",
      "Epoch: 4541 \tTraining Loss: 0.184577 \tR2: 0.717473\n",
      "Epoch: 4542 \tTraining Loss: 0.192860 \tR2: 0.717473\n",
      "Epoch: 4543 \tTraining Loss: 0.194940 \tR2: 0.717473\n",
      "Epoch: 4544 \tTraining Loss: 0.191820 \tR2: 0.717473\n",
      "Epoch: 4545 \tTraining Loss: 0.189906 \tR2: 0.717473\n",
      "Epoch: 4546 \tTraining Loss: 0.204562 \tR2: 0.717473\n",
      "Epoch: 4547 \tTraining Loss: 0.214356 \tR2: 0.717473\n",
      "Epoch: 4548 \tTraining Loss: 0.204828 \tR2: 0.717473\n",
      "Epoch: 4549 \tTraining Loss: 0.186160 \tR2: 0.717473\n",
      "Epoch: 4550 \tTraining Loss: 0.214849 \tR2: 0.717473\n",
      "Epoch: 4551 \tTraining Loss: 0.195346 \tR2: 0.717473\n",
      "Epoch: 4552 \tTraining Loss: 0.205678 \tR2: 0.717473\n",
      "Epoch: 4553 \tTraining Loss: 0.215991 \tR2: 0.717473\n",
      "Epoch: 4554 \tTraining Loss: 0.204036 \tR2: 0.717473\n",
      "Epoch: 4555 \tTraining Loss: 0.213014 \tR2: 0.717473\n",
      "Epoch: 4556 \tTraining Loss: 0.201284 \tR2: 0.717473\n",
      "Epoch: 4557 \tTraining Loss: 0.213147 \tR2: 0.717473\n",
      "Epoch: 4558 \tTraining Loss: 0.208437 \tR2: 0.717473\n",
      "Epoch: 4559 \tTraining Loss: 0.199229 \tR2: 0.717473\n",
      "Epoch: 4560 \tTraining Loss: 0.195233 \tR2: 0.717473\n",
      "Epoch: 4561 \tTraining Loss: 0.207901 \tR2: 0.717473\n",
      "Epoch: 4562 \tTraining Loss: 0.208179 \tR2: 0.717473\n",
      "Epoch: 4563 \tTraining Loss: 0.203722 \tR2: 0.717473\n",
      "Epoch: 4564 \tTraining Loss: 0.205704 \tR2: 0.717473\n",
      "Epoch: 4565 \tTraining Loss: 0.195202 \tR2: 0.717473\n",
      "Epoch: 4566 \tTraining Loss: 0.203341 \tR2: 0.717473\n",
      "Epoch: 4567 \tTraining Loss: 0.212488 \tR2: 0.717473\n",
      "Epoch: 4568 \tTraining Loss: 0.196077 \tR2: 0.717473\n",
      "Epoch: 4569 \tTraining Loss: 0.185236 \tR2: 0.717473\n",
      "Epoch: 4570 \tTraining Loss: 0.202297 \tR2: 0.717473\n",
      "Epoch: 4571 \tTraining Loss: 0.202816 \tR2: 0.717473\n",
      "Epoch: 4572 \tTraining Loss: 0.180755 \tR2: 0.717473\n",
      "Epoch: 4573 \tTraining Loss: 0.199554 \tR2: 0.717473\n",
      "Epoch: 4574 \tTraining Loss: 0.182961 \tR2: 0.717473\n",
      "Epoch: 4575 \tTraining Loss: 0.194762 \tR2: 0.717473\n",
      "Epoch: 4576 \tTraining Loss: 0.186861 \tR2: 0.717473\n",
      "Epoch: 4577 \tTraining Loss: 0.210798 \tR2: 0.717473\n",
      "Epoch: 4578 \tTraining Loss: 0.184127 \tR2: 0.717473\n",
      "Epoch: 4579 \tTraining Loss: 0.187224 \tR2: 0.717473\n",
      "Epoch: 4580 \tTraining Loss: 0.216987 \tR2: 0.717473\n",
      "Epoch: 4581 \tTraining Loss: 0.214249 \tR2: 0.717473\n",
      "Epoch: 4582 \tTraining Loss: 0.199609 \tR2: 0.717473\n",
      "Epoch: 4583 \tTraining Loss: 0.191650 \tR2: 0.717473\n",
      "Epoch: 4584 \tTraining Loss: 0.226466 \tR2: 0.717473\n",
      "Epoch: 4585 \tTraining Loss: 0.208716 \tR2: 0.717473\n",
      "Epoch: 4586 \tTraining Loss: 0.195997 \tR2: 0.717473\n",
      "Epoch: 4587 \tTraining Loss: 0.198487 \tR2: 0.717473\n",
      "Epoch: 4588 \tTraining Loss: 0.187624 \tR2: 0.717473\n",
      "Epoch: 4589 \tTraining Loss: 0.210003 \tR2: 0.717473\n",
      "Epoch: 4590 \tTraining Loss: 0.199135 \tR2: 0.717473\n",
      "Epoch: 4591 \tTraining Loss: 0.198861 \tR2: 0.717473\n",
      "Epoch: 4592 \tTraining Loss: 0.185285 \tR2: 0.717473\n",
      "Epoch: 4593 \tTraining Loss: 0.215374 \tR2: 0.717473\n",
      "Epoch: 4594 \tTraining Loss: 0.201366 \tR2: 0.717473\n",
      "Epoch: 4595 \tTraining Loss: 0.199626 \tR2: 0.717473\n",
      "Epoch: 4596 \tTraining Loss: 0.193456 \tR2: 0.717473\n",
      "Epoch: 4597 \tTraining Loss: 0.185874 \tR2: 0.717473\n",
      "Epoch: 4598 \tTraining Loss: 0.209456 \tR2: 0.717473\n",
      "Epoch: 4599 \tTraining Loss: 0.214926 \tR2: 0.717473\n",
      "Epoch: 4600 \tTraining Loss: 0.199204 \tR2: 0.735378\n",
      "Epoch: 4601 \tTraining Loss: 0.185636 \tR2: 0.735378\n",
      "Epoch: 4602 \tTraining Loss: 0.205589 \tR2: 0.735378\n",
      "Epoch: 4603 \tTraining Loss: 0.178526 \tR2: 0.735378\n",
      "Epoch: 4604 \tTraining Loss: 0.188055 \tR2: 0.735378\n",
      "Epoch: 4605 \tTraining Loss: 0.204425 \tR2: 0.735378\n",
      "Epoch: 4606 \tTraining Loss: 0.202322 \tR2: 0.735378\n",
      "Epoch: 4607 \tTraining Loss: 0.221780 \tR2: 0.735378\n",
      "Epoch: 4608 \tTraining Loss: 0.187607 \tR2: 0.735378\n",
      "Epoch: 4609 \tTraining Loss: 0.208033 \tR2: 0.735378\n",
      "Epoch: 4610 \tTraining Loss: 0.198660 \tR2: 0.735378\n",
      "Epoch: 4611 \tTraining Loss: 0.205618 \tR2: 0.735378\n",
      "Epoch: 4612 \tTraining Loss: 0.196615 \tR2: 0.735378\n",
      "Epoch: 4613 \tTraining Loss: 0.199722 \tR2: 0.735378\n",
      "Epoch: 4614 \tTraining Loss: 0.192460 \tR2: 0.735378\n",
      "Epoch: 4615 \tTraining Loss: 0.192158 \tR2: 0.735378\n",
      "Epoch: 4616 \tTraining Loss: 0.175563 \tR2: 0.735378\n",
      "Epoch: 4617 \tTraining Loss: 0.200516 \tR2: 0.735378\n",
      "Epoch: 4618 \tTraining Loss: 0.195448 \tR2: 0.735378\n",
      "Epoch: 4619 \tTraining Loss: 0.221588 \tR2: 0.735378\n",
      "Epoch: 4620 \tTraining Loss: 0.206212 \tR2: 0.735378\n",
      "Epoch: 4621 \tTraining Loss: 0.221394 \tR2: 0.735378\n",
      "Epoch: 4622 \tTraining Loss: 0.207559 \tR2: 0.735378\n",
      "Epoch: 4623 \tTraining Loss: 0.200797 \tR2: 0.735378\n",
      "Epoch: 4624 \tTraining Loss: 0.199311 \tR2: 0.735378\n",
      "Epoch: 4625 \tTraining Loss: 0.216158 \tR2: 0.735378\n",
      "Epoch: 4626 \tTraining Loss: 0.213075 \tR2: 0.735378\n",
      "Epoch: 4627 \tTraining Loss: 0.199258 \tR2: 0.735378\n",
      "Epoch: 4628 \tTraining Loss: 0.197728 \tR2: 0.735378\n",
      "Epoch: 4629 \tTraining Loss: 0.194914 \tR2: 0.735378\n",
      "Epoch: 4630 \tTraining Loss: 0.191551 \tR2: 0.735378\n",
      "Epoch: 4631 \tTraining Loss: 0.197875 \tR2: 0.735378\n",
      "Epoch: 4632 \tTraining Loss: 0.192206 \tR2: 0.735378\n",
      "Epoch: 4633 \tTraining Loss: 0.203958 \tR2: 0.735378\n",
      "Epoch: 4634 \tTraining Loss: 0.196035 \tR2: 0.735378\n",
      "Epoch: 4635 \tTraining Loss: 0.214161 \tR2: 0.735378\n",
      "Epoch: 4636 \tTraining Loss: 0.187642 \tR2: 0.735378\n",
      "Epoch: 4637 \tTraining Loss: 0.185226 \tR2: 0.735378\n",
      "Epoch: 4638 \tTraining Loss: 0.202189 \tR2: 0.735378\n",
      "Epoch: 4639 \tTraining Loss: 0.199130 \tR2: 0.735378\n",
      "Epoch: 4640 \tTraining Loss: 0.188318 \tR2: 0.735378\n",
      "Epoch: 4641 \tTraining Loss: 0.196344 \tR2: 0.735378\n",
      "Epoch: 4642 \tTraining Loss: 0.201629 \tR2: 0.735378\n",
      "Epoch: 4643 \tTraining Loss: 0.204907 \tR2: 0.735378\n",
      "Epoch: 4644 \tTraining Loss: 0.206353 \tR2: 0.735378\n",
      "Epoch: 4645 \tTraining Loss: 0.192664 \tR2: 0.735378\n",
      "Epoch: 4646 \tTraining Loss: 0.190671 \tR2: 0.735378\n",
      "Epoch: 4647 \tTraining Loss: 0.199675 \tR2: 0.735378\n",
      "Epoch: 4648 \tTraining Loss: 0.191152 \tR2: 0.735378\n",
      "Epoch: 4649 \tTraining Loss: 0.214252 \tR2: 0.735378\n",
      "Epoch: 4650 \tTraining Loss: 0.205225 \tR2: 0.735378\n",
      "Epoch: 4651 \tTraining Loss: 0.200746 \tR2: 0.735378\n",
      "Epoch: 4652 \tTraining Loss: 0.188845 \tR2: 0.735378\n",
      "Epoch: 4653 \tTraining Loss: 0.183616 \tR2: 0.735378\n",
      "Epoch: 4654 \tTraining Loss: 0.201331 \tR2: 0.735378\n",
      "Epoch: 4655 \tTraining Loss: 0.191513 \tR2: 0.735378\n",
      "Epoch: 4656 \tTraining Loss: 0.196523 \tR2: 0.735378\n",
      "Epoch: 4657 \tTraining Loss: 0.192939 \tR2: 0.735378\n",
      "Epoch: 4658 \tTraining Loss: 0.195451 \tR2: 0.735378\n",
      "Epoch: 4659 \tTraining Loss: 0.179911 \tR2: 0.735378\n",
      "Epoch: 4660 \tTraining Loss: 0.190855 \tR2: 0.735378\n",
      "Epoch: 4661 \tTraining Loss: 0.199945 \tR2: 0.735378\n",
      "Epoch: 4662 \tTraining Loss: 0.200665 \tR2: 0.735378\n",
      "Epoch: 4663 \tTraining Loss: 0.185719 \tR2: 0.735378\n",
      "Epoch: 4664 \tTraining Loss: 0.177604 \tR2: 0.735378\n",
      "Epoch: 4665 \tTraining Loss: 0.205577 \tR2: 0.735378\n",
      "Epoch: 4666 \tTraining Loss: 0.196006 \tR2: 0.735378\n",
      "Epoch: 4667 \tTraining Loss: 0.184799 \tR2: 0.735378\n",
      "Epoch: 4668 \tTraining Loss: 0.208046 \tR2: 0.735378\n",
      "Epoch: 4669 \tTraining Loss: 0.190885 \tR2: 0.735378\n",
      "Epoch: 4670 \tTraining Loss: 0.208028 \tR2: 0.735378\n",
      "Epoch: 4671 \tTraining Loss: 0.187121 \tR2: 0.735378\n",
      "Epoch: 4672 \tTraining Loss: 0.196651 \tR2: 0.735378\n",
      "Epoch: 4673 \tTraining Loss: 0.204887 \tR2: 0.735378\n",
      "Epoch: 4674 \tTraining Loss: 0.207356 \tR2: 0.735378\n",
      "Epoch: 4675 \tTraining Loss: 0.195824 \tR2: 0.735378\n",
      "Epoch: 4676 \tTraining Loss: 0.199419 \tR2: 0.735378\n",
      "Epoch: 4677 \tTraining Loss: 0.199621 \tR2: 0.735378\n",
      "Epoch: 4678 \tTraining Loss: 0.203830 \tR2: 0.735378\n",
      "Epoch: 4679 \tTraining Loss: 0.194835 \tR2: 0.735378\n",
      "Epoch: 4680 \tTraining Loss: 0.203444 \tR2: 0.735378\n",
      "Epoch: 4681 \tTraining Loss: 0.208100 \tR2: 0.735378\n",
      "Epoch: 4682 \tTraining Loss: 0.201866 \tR2: 0.735378\n",
      "Epoch: 4683 \tTraining Loss: 0.227614 \tR2: 0.735378\n",
      "Epoch: 4684 \tTraining Loss: 0.206850 \tR2: 0.735378\n",
      "Epoch: 4685 \tTraining Loss: 0.186497 \tR2: 0.735378\n",
      "Epoch: 4686 \tTraining Loss: 0.211792 \tR2: 0.735378\n",
      "Epoch: 4687 \tTraining Loss: 0.196816 \tR2: 0.735378\n",
      "Epoch: 4688 \tTraining Loss: 0.182499 \tR2: 0.735378\n",
      "Epoch: 4689 \tTraining Loss: 0.214286 \tR2: 0.735378\n",
      "Epoch: 4690 \tTraining Loss: 0.198897 \tR2: 0.735378\n",
      "Epoch: 4691 \tTraining Loss: 0.188455 \tR2: 0.735378\n",
      "Epoch: 4692 \tTraining Loss: 0.190075 \tR2: 0.735378\n",
      "Epoch: 4693 \tTraining Loss: 0.186482 \tR2: 0.735378\n",
      "Epoch: 4694 \tTraining Loss: 0.200201 \tR2: 0.735378\n",
      "Epoch: 4695 \tTraining Loss: 0.211502 \tR2: 0.735378\n",
      "Epoch: 4696 \tTraining Loss: 0.192499 \tR2: 0.735378\n",
      "Epoch: 4697 \tTraining Loss: 0.189882 \tR2: 0.735378\n",
      "Epoch: 4698 \tTraining Loss: 0.207790 \tR2: 0.735378\n",
      "Epoch: 4699 \tTraining Loss: 0.224008 \tR2: 0.735378\n",
      "Epoch: 4700 \tTraining Loss: 0.186551 \tR2: 0.815294\n",
      "Epoch: 4701 \tTraining Loss: 0.177663 \tR2: 0.815294\n",
      "Epoch: 4702 \tTraining Loss: 0.189163 \tR2: 0.815294\n",
      "Epoch: 4703 \tTraining Loss: 0.199970 \tR2: 0.815294\n",
      "Epoch: 4704 \tTraining Loss: 0.191861 \tR2: 0.815294\n",
      "Epoch: 4705 \tTraining Loss: 0.190086 \tR2: 0.815294\n",
      "Epoch: 4706 \tTraining Loss: 0.197675 \tR2: 0.815294\n",
      "Epoch: 4707 \tTraining Loss: 0.207991 \tR2: 0.815294\n",
      "Epoch: 4708 \tTraining Loss: 0.188596 \tR2: 0.815294\n",
      "Epoch: 4709 \tTraining Loss: 0.202008 \tR2: 0.815294\n",
      "Epoch: 4710 \tTraining Loss: 0.201017 \tR2: 0.815294\n",
      "Epoch: 4711 \tTraining Loss: 0.218924 \tR2: 0.815294\n",
      "Epoch: 4712 \tTraining Loss: 0.230577 \tR2: 0.815294\n",
      "Epoch: 4713 \tTraining Loss: 0.205054 \tR2: 0.815294\n",
      "Epoch: 4714 \tTraining Loss: 0.202449 \tR2: 0.815294\n",
      "Epoch: 4715 \tTraining Loss: 0.190087 \tR2: 0.815294\n",
      "Epoch: 4716 \tTraining Loss: 0.191425 \tR2: 0.815294\n",
      "Epoch: 4717 \tTraining Loss: 0.213278 \tR2: 0.815294\n",
      "Epoch: 4718 \tTraining Loss: 0.192935 \tR2: 0.815294\n",
      "Epoch: 4719 \tTraining Loss: 0.206947 \tR2: 0.815294\n",
      "Epoch: 4720 \tTraining Loss: 0.200598 \tR2: 0.815294\n",
      "Epoch: 4721 \tTraining Loss: 0.194148 \tR2: 0.815294\n",
      "Epoch: 4722 \tTraining Loss: 0.207998 \tR2: 0.815294\n",
      "Epoch: 4723 \tTraining Loss: 0.200095 \tR2: 0.815294\n",
      "Epoch: 4724 \tTraining Loss: 0.193098 \tR2: 0.815294\n",
      "Epoch: 4725 \tTraining Loss: 0.205791 \tR2: 0.815294\n",
      "Epoch: 4726 \tTraining Loss: 0.213241 \tR2: 0.815294\n",
      "Epoch: 4727 \tTraining Loss: 0.196569 \tR2: 0.815294\n",
      "Epoch: 4728 \tTraining Loss: 0.208077 \tR2: 0.815294\n",
      "Epoch: 4729 \tTraining Loss: 0.201746 \tR2: 0.815294\n",
      "Epoch: 4730 \tTraining Loss: 0.190649 \tR2: 0.815294\n",
      "Epoch: 4731 \tTraining Loss: 0.216065 \tR2: 0.815294\n",
      "Epoch: 4732 \tTraining Loss: 0.191907 \tR2: 0.815294\n",
      "Epoch: 4733 \tTraining Loss: 0.189543 \tR2: 0.815294\n",
      "Epoch: 4734 \tTraining Loss: 0.193558 \tR2: 0.815294\n",
      "Epoch: 4735 \tTraining Loss: 0.212569 \tR2: 0.815294\n",
      "Epoch: 4736 \tTraining Loss: 0.210519 \tR2: 0.815294\n",
      "Epoch: 4737 \tTraining Loss: 0.188394 \tR2: 0.815294\n",
      "Epoch: 4738 \tTraining Loss: 0.209153 \tR2: 0.815294\n",
      "Epoch: 4739 \tTraining Loss: 0.184021 \tR2: 0.815294\n",
      "Epoch: 4740 \tTraining Loss: 0.184012 \tR2: 0.815294\n",
      "Epoch: 4741 \tTraining Loss: 0.168790 \tR2: 0.815294\n",
      "Epoch: 4742 \tTraining Loss: 0.206129 \tR2: 0.815294\n",
      "Epoch: 4743 \tTraining Loss: 0.193082 \tR2: 0.815294\n",
      "Epoch: 4744 \tTraining Loss: 0.210970 \tR2: 0.815294\n",
      "Epoch: 4745 \tTraining Loss: 0.203166 \tR2: 0.815294\n",
      "Epoch: 4746 \tTraining Loss: 0.195075 \tR2: 0.815294\n",
      "Epoch: 4747 \tTraining Loss: 0.251847 \tR2: 0.815294\n",
      "Epoch: 4748 \tTraining Loss: 0.222844 \tR2: 0.815294\n",
      "Epoch: 4749 \tTraining Loss: 0.188735 \tR2: 0.815294\n",
      "Epoch: 4750 \tTraining Loss: 0.190992 \tR2: 0.815294\n",
      "Epoch: 4751 \tTraining Loss: 0.193627 \tR2: 0.815294\n",
      "Epoch: 4752 \tTraining Loss: 0.212394 \tR2: 0.815294\n",
      "Epoch: 4753 \tTraining Loss: 0.212580 \tR2: 0.815294\n",
      "Epoch: 4754 \tTraining Loss: 0.174641 \tR2: 0.815294\n",
      "Epoch: 4755 \tTraining Loss: 0.197350 \tR2: 0.815294\n",
      "Epoch: 4756 \tTraining Loss: 0.185300 \tR2: 0.815294\n",
      "Epoch: 4757 \tTraining Loss: 0.214664 \tR2: 0.815294\n",
      "Epoch: 4758 \tTraining Loss: 0.197369 \tR2: 0.815294\n",
      "Epoch: 4759 \tTraining Loss: 0.185999 \tR2: 0.815294\n",
      "Epoch: 4760 \tTraining Loss: 0.193457 \tR2: 0.815294\n",
      "Epoch: 4761 \tTraining Loss: 0.195769 \tR2: 0.815294\n",
      "Epoch: 4762 \tTraining Loss: 0.203493 \tR2: 0.815294\n",
      "Epoch: 4763 \tTraining Loss: 0.190643 \tR2: 0.815294\n",
      "Epoch: 4764 \tTraining Loss: 0.184919 \tR2: 0.815294\n",
      "Epoch: 4765 \tTraining Loss: 0.190924 \tR2: 0.815294\n",
      "Epoch: 4766 \tTraining Loss: 0.208604 \tR2: 0.815294\n",
      "Epoch: 4767 \tTraining Loss: 0.207557 \tR2: 0.815294\n",
      "Epoch: 4768 \tTraining Loss: 0.189222 \tR2: 0.815294\n",
      "Epoch: 4769 \tTraining Loss: 0.218968 \tR2: 0.815294\n",
      "Epoch: 4770 \tTraining Loss: 0.183865 \tR2: 0.815294\n",
      "Epoch: 4771 \tTraining Loss: 0.195754 \tR2: 0.815294\n",
      "Epoch: 4772 \tTraining Loss: 0.196954 \tR2: 0.815294\n",
      "Epoch: 4773 \tTraining Loss: 0.194402 \tR2: 0.815294\n",
      "Epoch: 4774 \tTraining Loss: 0.205061 \tR2: 0.815294\n",
      "Epoch: 4775 \tTraining Loss: 0.193137 \tR2: 0.815294\n",
      "Epoch: 4776 \tTraining Loss: 0.198736 \tR2: 0.815294\n",
      "Epoch: 4777 \tTraining Loss: 0.194646 \tR2: 0.815294\n",
      "Epoch: 4778 \tTraining Loss: 0.195179 \tR2: 0.815294\n",
      "Epoch: 4779 \tTraining Loss: 0.194484 \tR2: 0.815294\n",
      "Epoch: 4780 \tTraining Loss: 0.189385 \tR2: 0.815294\n",
      "Epoch: 4781 \tTraining Loss: 0.205076 \tR2: 0.815294\n",
      "Epoch: 4782 \tTraining Loss: 0.200996 \tR2: 0.815294\n",
      "Epoch: 4783 \tTraining Loss: 0.194764 \tR2: 0.815294\n",
      "Epoch: 4784 \tTraining Loss: 0.191468 \tR2: 0.815294\n",
      "Epoch: 4785 \tTraining Loss: 0.200942 \tR2: 0.815294\n",
      "Epoch: 4786 \tTraining Loss: 0.187878 \tR2: 0.815294\n",
      "Epoch: 4787 \tTraining Loss: 0.202591 \tR2: 0.815294\n",
      "Epoch: 4788 \tTraining Loss: 0.200497 \tR2: 0.815294\n",
      "Epoch: 4789 \tTraining Loss: 0.196065 \tR2: 0.815294\n",
      "Epoch: 4790 \tTraining Loss: 0.196955 \tR2: 0.815294\n",
      "Epoch: 4791 \tTraining Loss: 0.188800 \tR2: 0.815294\n",
      "Epoch: 4792 \tTraining Loss: 0.209451 \tR2: 0.815294\n",
      "Epoch: 4793 \tTraining Loss: 0.225531 \tR2: 0.815294\n",
      "Epoch: 4794 \tTraining Loss: 0.187455 \tR2: 0.815294\n",
      "Epoch: 4795 \tTraining Loss: 0.202960 \tR2: 0.815294\n",
      "Epoch: 4796 \tTraining Loss: 0.187099 \tR2: 0.815294\n",
      "Epoch: 4797 \tTraining Loss: 0.205872 \tR2: 0.815294\n",
      "Epoch: 4798 \tTraining Loss: 0.188081 \tR2: 0.815294\n",
      "Epoch: 4799 \tTraining Loss: 0.194192 \tR2: 0.815294\n",
      "Epoch: 4800 \tTraining Loss: 0.255612 \tR2: 0.465379\n",
      "Epoch: 4801 \tTraining Loss: 0.211521 \tR2: 0.465379\n",
      "Epoch: 4802 \tTraining Loss: 0.209091 \tR2: 0.465379\n",
      "Epoch: 4803 \tTraining Loss: 0.205815 \tR2: 0.465379\n",
      "Epoch: 4804 \tTraining Loss: 0.218110 \tR2: 0.465379\n",
      "Epoch: 4805 \tTraining Loss: 0.191232 \tR2: 0.465379\n",
      "Epoch: 4806 \tTraining Loss: 0.197070 \tR2: 0.465379\n",
      "Epoch: 4807 \tTraining Loss: 0.200345 \tR2: 0.465379\n",
      "Epoch: 4808 \tTraining Loss: 0.200349 \tR2: 0.465379\n",
      "Epoch: 4809 \tTraining Loss: 0.187140 \tR2: 0.465379\n",
      "Epoch: 4810 \tTraining Loss: 0.196340 \tR2: 0.465379\n",
      "Epoch: 4811 \tTraining Loss: 0.186740 \tR2: 0.465379\n",
      "Epoch: 4812 \tTraining Loss: 0.183598 \tR2: 0.465379\n",
      "Epoch: 4813 \tTraining Loss: 0.188657 \tR2: 0.465379\n",
      "Epoch: 4814 \tTraining Loss: 0.204116 \tR2: 0.465379\n",
      "Epoch: 4815 \tTraining Loss: 0.206147 \tR2: 0.465379\n",
      "Epoch: 4816 \tTraining Loss: 0.199027 \tR2: 0.465379\n",
      "Epoch: 4817 \tTraining Loss: 0.187158 \tR2: 0.465379\n",
      "Epoch: 4818 \tTraining Loss: 0.219918 \tR2: 0.465379\n",
      "Epoch: 4819 \tTraining Loss: 0.192009 \tR2: 0.465379\n",
      "Epoch: 4820 \tTraining Loss: 0.196757 \tR2: 0.465379\n",
      "Epoch: 4821 \tTraining Loss: 0.209333 \tR2: 0.465379\n",
      "Epoch: 4822 \tTraining Loss: 0.209771 \tR2: 0.465379\n",
      "Epoch: 4823 \tTraining Loss: 0.209017 \tR2: 0.465379\n",
      "Epoch: 4824 \tTraining Loss: 0.202049 \tR2: 0.465379\n",
      "Epoch: 4825 \tTraining Loss: 0.191850 \tR2: 0.465379\n",
      "Epoch: 4826 \tTraining Loss: 0.213164 \tR2: 0.465379\n",
      "Epoch: 4827 \tTraining Loss: 0.171802 \tR2: 0.465379\n",
      "Epoch: 4828 \tTraining Loss: 0.212113 \tR2: 0.465379\n",
      "Epoch: 4829 \tTraining Loss: 0.192420 \tR2: 0.465379\n",
      "Epoch: 4830 \tTraining Loss: 0.196255 \tR2: 0.465379\n",
      "Epoch: 4831 \tTraining Loss: 0.193734 \tR2: 0.465379\n",
      "Epoch: 4832 \tTraining Loss: 0.198453 \tR2: 0.465379\n",
      "Epoch: 4833 \tTraining Loss: 0.198953 \tR2: 0.465379\n",
      "Epoch: 4834 \tTraining Loss: 0.195228 \tR2: 0.465379\n",
      "Epoch: 4835 \tTraining Loss: 0.191609 \tR2: 0.465379\n",
      "Epoch: 4836 \tTraining Loss: 0.196603 \tR2: 0.465379\n",
      "Epoch: 4837 \tTraining Loss: 0.196260 \tR2: 0.465379\n",
      "Epoch: 4838 \tTraining Loss: 0.185625 \tR2: 0.465379\n",
      "Epoch: 4839 \tTraining Loss: 0.200945 \tR2: 0.465379\n",
      "Epoch: 4840 \tTraining Loss: 0.197123 \tR2: 0.465379\n",
      "Epoch: 4841 \tTraining Loss: 0.181433 \tR2: 0.465379\n",
      "Epoch: 4842 \tTraining Loss: 0.195731 \tR2: 0.465379\n",
      "Epoch: 4843 \tTraining Loss: 0.205232 \tR2: 0.465379\n",
      "Epoch: 4844 \tTraining Loss: 0.183943 \tR2: 0.465379\n",
      "Epoch: 4845 \tTraining Loss: 0.206128 \tR2: 0.465379\n",
      "Epoch: 4846 \tTraining Loss: 0.199771 \tR2: 0.465379\n",
      "Epoch: 4847 \tTraining Loss: 0.183582 \tR2: 0.465379\n",
      "Epoch: 4848 \tTraining Loss: 0.190325 \tR2: 0.465379\n",
      "Epoch: 4849 \tTraining Loss: 0.216553 \tR2: 0.465379\n",
      "Epoch: 4850 \tTraining Loss: 0.198641 \tR2: 0.465379\n",
      "Epoch: 4851 \tTraining Loss: 0.185544 \tR2: 0.465379\n",
      "Epoch: 4852 \tTraining Loss: 0.205285 \tR2: 0.465379\n",
      "Epoch: 4853 \tTraining Loss: 0.196997 \tR2: 0.465379\n",
      "Epoch: 4854 \tTraining Loss: 0.202158 \tR2: 0.465379\n",
      "Epoch: 4855 \tTraining Loss: 0.202671 \tR2: 0.465379\n",
      "Epoch: 4856 \tTraining Loss: 0.201870 \tR2: 0.465379\n",
      "Epoch: 4857 \tTraining Loss: 0.185674 \tR2: 0.465379\n",
      "Epoch: 4858 \tTraining Loss: 0.197440 \tR2: 0.465379\n",
      "Epoch: 4859 \tTraining Loss: 0.218428 \tR2: 0.465379\n",
      "Epoch: 4860 \tTraining Loss: 0.200306 \tR2: 0.465379\n",
      "Epoch: 4861 \tTraining Loss: 0.193370 \tR2: 0.465379\n",
      "Epoch: 4862 \tTraining Loss: 0.212416 \tR2: 0.465379\n",
      "Epoch: 4863 \tTraining Loss: 0.185578 \tR2: 0.465379\n",
      "Epoch: 4864 \tTraining Loss: 0.194701 \tR2: 0.465379\n",
      "Epoch: 4865 \tTraining Loss: 0.206805 \tR2: 0.465379\n",
      "Epoch: 4866 \tTraining Loss: 0.198439 \tR2: 0.465379\n",
      "Epoch: 4867 \tTraining Loss: 0.205130 \tR2: 0.465379\n",
      "Epoch: 4868 \tTraining Loss: 0.203995 \tR2: 0.465379\n",
      "Epoch: 4869 \tTraining Loss: 0.192706 \tR2: 0.465379\n",
      "Epoch: 4870 \tTraining Loss: 0.194252 \tR2: 0.465379\n",
      "Epoch: 4871 \tTraining Loss: 0.180549 \tR2: 0.465379\n",
      "Epoch: 4872 \tTraining Loss: 0.203874 \tR2: 0.465379\n",
      "Epoch: 4873 \tTraining Loss: 0.195420 \tR2: 0.465379\n",
      "Epoch: 4874 \tTraining Loss: 0.187888 \tR2: 0.465379\n",
      "Epoch: 4875 \tTraining Loss: 0.191903 \tR2: 0.465379\n",
      "Epoch: 4876 \tTraining Loss: 0.189592 \tR2: 0.465379\n",
      "Epoch: 4877 \tTraining Loss: 0.245572 \tR2: 0.465379\n",
      "Epoch: 4878 \tTraining Loss: 0.182924 \tR2: 0.465379\n",
      "Epoch: 4879 \tTraining Loss: 0.211816 \tR2: 0.465379\n",
      "Epoch: 4880 \tTraining Loss: 0.201594 \tR2: 0.465379\n",
      "Epoch: 4881 \tTraining Loss: 0.227902 \tR2: 0.465379\n",
      "Epoch: 4882 \tTraining Loss: 0.202459 \tR2: 0.465379\n",
      "Epoch: 4883 \tTraining Loss: 0.205241 \tR2: 0.465379\n",
      "Epoch: 4884 \tTraining Loss: 0.214218 \tR2: 0.465379\n",
      "Epoch: 4885 \tTraining Loss: 0.207740 \tR2: 0.465379\n",
      "Epoch: 4886 \tTraining Loss: 0.214878 \tR2: 0.465379\n",
      "Epoch: 4887 \tTraining Loss: 0.187739 \tR2: 0.465379\n",
      "Epoch: 4888 \tTraining Loss: 0.194154 \tR2: 0.465379\n",
      "Epoch: 4889 \tTraining Loss: 0.200704 \tR2: 0.465379\n",
      "Epoch: 4890 \tTraining Loss: 0.198841 \tR2: 0.465379\n",
      "Epoch: 4891 \tTraining Loss: 0.185823 \tR2: 0.465379\n",
      "Epoch: 4892 \tTraining Loss: 0.207968 \tR2: 0.465379\n",
      "Epoch: 4893 \tTraining Loss: 0.190054 \tR2: 0.465379\n",
      "Epoch: 4894 \tTraining Loss: 0.197546 \tR2: 0.465379\n",
      "Epoch: 4895 \tTraining Loss: 0.182369 \tR2: 0.465379\n",
      "Epoch: 4896 \tTraining Loss: 0.198571 \tR2: 0.465379\n",
      "Epoch: 4897 \tTraining Loss: 0.199990 \tR2: 0.465379\n",
      "Epoch: 4898 \tTraining Loss: 0.197405 \tR2: 0.465379\n",
      "Epoch: 4899 \tTraining Loss: 0.207809 \tR2: 0.465379\n",
      "Epoch: 4900 \tTraining Loss: 0.194268 \tR2: 0.811481\n",
      "Epoch: 4901 \tTraining Loss: 0.193920 \tR2: 0.811481\n",
      "Epoch: 4902 \tTraining Loss: 0.198210 \tR2: 0.811481\n",
      "Epoch: 4903 \tTraining Loss: 0.199774 \tR2: 0.811481\n",
      "Epoch: 4904 \tTraining Loss: 0.193870 \tR2: 0.811481\n",
      "Epoch: 4905 \tTraining Loss: 0.196597 \tR2: 0.811481\n",
      "Epoch: 4906 \tTraining Loss: 0.201404 \tR2: 0.811481\n",
      "Epoch: 4907 \tTraining Loss: 0.185290 \tR2: 0.811481\n",
      "Epoch: 4908 \tTraining Loss: 0.207173 \tR2: 0.811481\n",
      "Epoch: 4909 \tTraining Loss: 0.181492 \tR2: 0.811481\n",
      "Epoch: 4910 \tTraining Loss: 0.193893 \tR2: 0.811481\n",
      "Epoch: 4911 \tTraining Loss: 0.199177 \tR2: 0.811481\n",
      "Epoch: 4912 \tTraining Loss: 0.200259 \tR2: 0.811481\n",
      "Epoch: 4913 \tTraining Loss: 0.189626 \tR2: 0.811481\n",
      "Epoch: 4914 \tTraining Loss: 0.196693 \tR2: 0.811481\n",
      "Epoch: 4915 \tTraining Loss: 0.201457 \tR2: 0.811481\n",
      "Epoch: 4916 \tTraining Loss: 0.189308 \tR2: 0.811481\n",
      "Epoch: 4917 \tTraining Loss: 0.193327 \tR2: 0.811481\n",
      "Epoch: 4918 \tTraining Loss: 0.195726 \tR2: 0.811481\n",
      "Epoch: 4919 \tTraining Loss: 0.204984 \tR2: 0.811481\n",
      "Epoch: 4920 \tTraining Loss: 0.205069 \tR2: 0.811481\n",
      "Epoch: 4921 \tTraining Loss: 0.195736 \tR2: 0.811481\n",
      "Epoch: 4922 \tTraining Loss: 0.196966 \tR2: 0.811481\n",
      "Epoch: 4923 \tTraining Loss: 0.202329 \tR2: 0.811481\n",
      "Epoch: 4924 \tTraining Loss: 0.198508 \tR2: 0.811481\n",
      "Epoch: 4925 \tTraining Loss: 0.210642 \tR2: 0.811481\n",
      "Epoch: 4926 \tTraining Loss: 0.194954 \tR2: 0.811481\n",
      "Epoch: 4927 \tTraining Loss: 0.187029 \tR2: 0.811481\n",
      "Epoch: 4928 \tTraining Loss: 0.190868 \tR2: 0.811481\n",
      "Epoch: 4929 \tTraining Loss: 0.196499 \tR2: 0.811481\n",
      "Epoch: 4930 \tTraining Loss: 0.217917 \tR2: 0.811481\n",
      "Epoch: 4931 \tTraining Loss: 0.204602 \tR2: 0.811481\n",
      "Epoch: 4932 \tTraining Loss: 0.198891 \tR2: 0.811481\n",
      "Epoch: 4933 \tTraining Loss: 0.222041 \tR2: 0.811481\n",
      "Epoch: 4934 \tTraining Loss: 0.182909 \tR2: 0.811481\n",
      "Epoch: 4935 \tTraining Loss: 0.205473 \tR2: 0.811481\n",
      "Epoch: 4936 \tTraining Loss: 0.188936 \tR2: 0.811481\n",
      "Epoch: 4937 \tTraining Loss: 0.191467 \tR2: 0.811481\n",
      "Epoch: 4938 \tTraining Loss: 0.197826 \tR2: 0.811481\n",
      "Epoch: 4939 \tTraining Loss: 0.204913 \tR2: 0.811481\n",
      "Epoch: 4940 \tTraining Loss: 0.201722 \tR2: 0.811481\n",
      "Epoch: 4941 \tTraining Loss: 0.200045 \tR2: 0.811481\n",
      "Epoch: 4942 \tTraining Loss: 0.210938 \tR2: 0.811481\n",
      "Epoch: 4943 \tTraining Loss: 0.196538 \tR2: 0.811481\n",
      "Epoch: 4944 \tTraining Loss: 0.238447 \tR2: 0.811481\n",
      "Epoch: 4945 \tTraining Loss: 0.205048 \tR2: 0.811481\n",
      "Epoch: 4946 \tTraining Loss: 0.208344 \tR2: 0.811481\n",
      "Epoch: 4947 \tTraining Loss: 0.202526 \tR2: 0.811481\n",
      "Epoch: 4948 \tTraining Loss: 0.188571 \tR2: 0.811481\n",
      "Epoch: 4949 \tTraining Loss: 0.197323 \tR2: 0.811481\n",
      "Epoch: 4950 \tTraining Loss: 0.207079 \tR2: 0.811481\n",
      "Epoch: 4951 \tTraining Loss: 0.192296 \tR2: 0.811481\n",
      "Epoch: 4952 \tTraining Loss: 0.193230 \tR2: 0.811481\n",
      "Epoch: 4953 \tTraining Loss: 0.195224 \tR2: 0.811481\n",
      "Epoch: 4954 \tTraining Loss: 0.201699 \tR2: 0.811481\n",
      "Epoch: 4955 \tTraining Loss: 0.180689 \tR2: 0.811481\n",
      "Epoch: 4956 \tTraining Loss: 0.193179 \tR2: 0.811481\n",
      "Epoch: 4957 \tTraining Loss: 0.189449 \tR2: 0.811481\n",
      "Epoch: 4958 \tTraining Loss: 0.213653 \tR2: 0.811481\n",
      "Epoch: 4959 \tTraining Loss: 0.187694 \tR2: 0.811481\n",
      "Epoch: 4960 \tTraining Loss: 0.187896 \tR2: 0.811481\n",
      "Epoch: 4961 \tTraining Loss: 0.198832 \tR2: 0.811481\n",
      "Epoch: 4962 \tTraining Loss: 0.195482 \tR2: 0.811481\n",
      "Epoch: 4963 \tTraining Loss: 0.216245 \tR2: 0.811481\n",
      "Epoch: 4964 \tTraining Loss: 0.178048 \tR2: 0.811481\n",
      "Epoch: 4965 \tTraining Loss: 0.188041 \tR2: 0.811481\n",
      "Epoch: 4966 \tTraining Loss: 0.212755 \tR2: 0.811481\n",
      "Epoch: 4967 \tTraining Loss: 0.202548 \tR2: 0.811481\n",
      "Epoch: 4968 \tTraining Loss: 0.210931 \tR2: 0.811481\n",
      "Epoch: 4969 \tTraining Loss: 0.189250 \tR2: 0.811481\n",
      "Epoch: 4970 \tTraining Loss: 0.195460 \tR2: 0.811481\n",
      "Epoch: 4971 \tTraining Loss: 0.211875 \tR2: 0.811481\n",
      "Epoch: 4972 \tTraining Loss: 0.211883 \tR2: 0.811481\n",
      "Epoch: 4973 \tTraining Loss: 0.211108 \tR2: 0.811481\n",
      "Epoch: 4974 \tTraining Loss: 0.199765 \tR2: 0.811481\n",
      "Epoch: 4975 \tTraining Loss: 0.197933 \tR2: 0.811481\n",
      "Epoch: 4976 \tTraining Loss: 0.222758 \tR2: 0.811481\n",
      "Epoch: 4977 \tTraining Loss: 0.213683 \tR2: 0.811481\n",
      "Epoch: 4978 \tTraining Loss: 0.200629 \tR2: 0.811481\n",
      "Epoch: 4979 \tTraining Loss: 0.173822 \tR2: 0.811481\n",
      "Epoch: 4980 \tTraining Loss: 0.183475 \tR2: 0.811481\n",
      "Epoch: 4981 \tTraining Loss: 0.194836 \tR2: 0.811481\n",
      "Epoch: 4982 \tTraining Loss: 0.213566 \tR2: 0.811481\n",
      "Epoch: 4983 \tTraining Loss: 0.206755 \tR2: 0.811481\n",
      "Epoch: 4984 \tTraining Loss: 0.184599 \tR2: 0.811481\n",
      "Epoch: 4985 \tTraining Loss: 0.185999 \tR2: 0.811481\n",
      "Epoch: 4986 \tTraining Loss: 0.252213 \tR2: 0.811481\n",
      "Epoch: 4987 \tTraining Loss: 0.205517 \tR2: 0.811481\n",
      "Epoch: 4988 \tTraining Loss: 0.198393 \tR2: 0.811481\n",
      "Epoch: 4989 \tTraining Loss: 0.210214 \tR2: 0.811481\n",
      "Epoch: 4990 \tTraining Loss: 0.214146 \tR2: 0.811481\n",
      "Epoch: 4991 \tTraining Loss: 0.193547 \tR2: 0.811481\n",
      "Epoch: 4992 \tTraining Loss: 0.176885 \tR2: 0.811481\n",
      "Epoch: 4993 \tTraining Loss: 0.187453 \tR2: 0.811481\n",
      "Epoch: 4994 \tTraining Loss: 0.201549 \tR2: 0.811481\n",
      "Epoch: 4995 \tTraining Loss: 0.207374 \tR2: 0.811481\n",
      "Epoch: 4996 \tTraining Loss: 0.193052 \tR2: 0.811481\n",
      "Epoch: 4997 \tTraining Loss: 0.208015 \tR2: 0.811481\n",
      "Epoch: 4998 \tTraining Loss: 0.206581 \tR2: 0.811481\n",
      "Epoch: 4999 \tTraining Loss: 0.223015 \tR2: 0.811481\n",
      "Epoch: 5000 \tTraining Loss: 0.198614 \tR2: 0.741634\n",
      "Epoch: 5001 \tTraining Loss: 0.204082 \tR2: 0.741634\n",
      "Epoch: 5002 \tTraining Loss: 0.178310 \tR2: 0.741634\n",
      "Epoch: 5003 \tTraining Loss: 0.216122 \tR2: 0.741634\n",
      "Epoch: 5004 \tTraining Loss: 0.209021 \tR2: 0.741634\n",
      "Epoch: 5005 \tTraining Loss: 0.195214 \tR2: 0.741634\n",
      "Epoch: 5006 \tTraining Loss: 0.202686 \tR2: 0.741634\n",
      "Epoch: 5007 \tTraining Loss: 0.196827 \tR2: 0.741634\n",
      "Epoch: 5008 \tTraining Loss: 0.187045 \tR2: 0.741634\n",
      "Epoch: 5009 \tTraining Loss: 0.213382 \tR2: 0.741634\n",
      "Epoch: 5010 \tTraining Loss: 0.192417 \tR2: 0.741634\n",
      "Epoch: 5011 \tTraining Loss: 0.196699 \tR2: 0.741634\n",
      "Epoch: 5012 \tTraining Loss: 0.202498 \tR2: 0.741634\n",
      "Epoch: 5013 \tTraining Loss: 0.203438 \tR2: 0.741634\n",
      "Epoch: 5014 \tTraining Loss: 0.206353 \tR2: 0.741634\n",
      "Epoch: 5015 \tTraining Loss: 0.188443 \tR2: 0.741634\n",
      "Epoch: 5016 \tTraining Loss: 0.208272 \tR2: 0.741634\n",
      "Epoch: 5017 \tTraining Loss: 0.226870 \tR2: 0.741634\n",
      "Epoch: 5018 \tTraining Loss: 0.199864 \tR2: 0.741634\n",
      "Epoch: 5019 \tTraining Loss: 0.191490 \tR2: 0.741634\n",
      "Epoch: 5020 \tTraining Loss: 0.197688 \tR2: 0.741634\n",
      "Epoch: 5021 \tTraining Loss: 0.203184 \tR2: 0.741634\n",
      "Epoch: 5022 \tTraining Loss: 0.192181 \tR2: 0.741634\n",
      "Epoch: 5023 \tTraining Loss: 0.203634 \tR2: 0.741634\n",
      "Epoch: 5024 \tTraining Loss: 0.191163 \tR2: 0.741634\n",
      "Epoch: 5025 \tTraining Loss: 0.197530 \tR2: 0.741634\n",
      "Epoch: 5026 \tTraining Loss: 0.210082 \tR2: 0.741634\n",
      "Epoch: 5027 \tTraining Loss: 0.188342 \tR2: 0.741634\n",
      "Epoch: 5028 \tTraining Loss: 0.190469 \tR2: 0.741634\n",
      "Epoch: 5029 \tTraining Loss: 0.192342 \tR2: 0.741634\n",
      "Epoch: 5030 \tTraining Loss: 0.194682 \tR2: 0.741634\n",
      "Epoch: 5031 \tTraining Loss: 0.193255 \tR2: 0.741634\n",
      "Epoch: 5032 \tTraining Loss: 0.193712 \tR2: 0.741634\n",
      "Epoch: 5033 \tTraining Loss: 0.209241 \tR2: 0.741634\n",
      "Epoch: 5034 \tTraining Loss: 0.219319 \tR2: 0.741634\n",
      "Epoch: 5035 \tTraining Loss: 0.190788 \tR2: 0.741634\n",
      "Epoch: 5036 \tTraining Loss: 0.182739 \tR2: 0.741634\n",
      "Epoch: 5037 \tTraining Loss: 0.208629 \tR2: 0.741634\n",
      "Epoch: 5038 \tTraining Loss: 0.169000 \tR2: 0.741634\n",
      "Epoch: 5039 \tTraining Loss: 0.190985 \tR2: 0.741634\n",
      "Epoch: 5040 \tTraining Loss: 0.196197 \tR2: 0.741634\n",
      "Epoch: 5041 \tTraining Loss: 0.195297 \tR2: 0.741634\n",
      "Epoch: 5042 \tTraining Loss: 0.187621 \tR2: 0.741634\n",
      "Epoch: 5043 \tTraining Loss: 0.192251 \tR2: 0.741634\n",
      "Epoch: 5044 \tTraining Loss: 0.186583 \tR2: 0.741634\n",
      "Epoch: 5045 \tTraining Loss: 0.199766 \tR2: 0.741634\n",
      "Epoch: 5046 \tTraining Loss: 0.192314 \tR2: 0.741634\n",
      "Epoch: 5047 \tTraining Loss: 0.198189 \tR2: 0.741634\n",
      "Epoch: 5048 \tTraining Loss: 0.208469 \tR2: 0.741634\n",
      "Epoch: 5049 \tTraining Loss: 0.182131 \tR2: 0.741634\n",
      "Epoch: 5050 \tTraining Loss: 0.205452 \tR2: 0.741634\n",
      "Epoch: 5051 \tTraining Loss: 0.183018 \tR2: 0.741634\n",
      "Epoch: 5052 \tTraining Loss: 0.193683 \tR2: 0.741634\n",
      "Epoch: 5053 \tTraining Loss: 0.192814 \tR2: 0.741634\n",
      "Epoch: 5054 \tTraining Loss: 0.194767 \tR2: 0.741634\n",
      "Epoch: 5055 \tTraining Loss: 0.197656 \tR2: 0.741634\n",
      "Epoch: 5056 \tTraining Loss: 0.220523 \tR2: 0.741634\n",
      "Epoch: 5057 \tTraining Loss: 0.204489 \tR2: 0.741634\n",
      "Epoch: 5058 \tTraining Loss: 0.211714 \tR2: 0.741634\n",
      "Epoch: 5059 \tTraining Loss: 0.196488 \tR2: 0.741634\n",
      "Epoch: 5060 \tTraining Loss: 0.224185 \tR2: 0.741634\n",
      "Epoch: 5061 \tTraining Loss: 0.206864 \tR2: 0.741634\n",
      "Epoch: 5062 \tTraining Loss: 0.199282 \tR2: 0.741634\n",
      "Epoch: 5063 \tTraining Loss: 0.200380 \tR2: 0.741634\n",
      "Epoch: 5064 \tTraining Loss: 0.214314 \tR2: 0.741634\n",
      "Epoch: 5065 \tTraining Loss: 0.201481 \tR2: 0.741634\n",
      "Epoch: 5066 \tTraining Loss: 0.209983 \tR2: 0.741634\n",
      "Epoch: 5067 \tTraining Loss: 0.180382 \tR2: 0.741634\n",
      "Epoch: 5068 \tTraining Loss: 0.219955 \tR2: 0.741634\n",
      "Epoch: 5069 \tTraining Loss: 0.198871 \tR2: 0.741634\n",
      "Epoch: 5070 \tTraining Loss: 0.190795 \tR2: 0.741634\n",
      "Epoch: 5071 \tTraining Loss: 0.206802 \tR2: 0.741634\n",
      "Epoch: 5072 \tTraining Loss: 0.195032 \tR2: 0.741634\n",
      "Epoch: 5073 \tTraining Loss: 0.198411 \tR2: 0.741634\n",
      "Epoch: 5074 \tTraining Loss: 0.209807 \tR2: 0.741634\n",
      "Epoch: 5075 \tTraining Loss: 0.195230 \tR2: 0.741634\n",
      "Epoch: 5076 \tTraining Loss: 0.198986 \tR2: 0.741634\n",
      "Epoch: 5077 \tTraining Loss: 0.205177 \tR2: 0.741634\n",
      "Epoch: 5078 \tTraining Loss: 0.199692 \tR2: 0.741634\n",
      "Epoch: 5079 \tTraining Loss: 0.201931 \tR2: 0.741634\n",
      "Epoch: 5080 \tTraining Loss: 0.268052 \tR2: 0.741634\n",
      "Epoch: 5081 \tTraining Loss: 0.209598 \tR2: 0.741634\n",
      "Epoch: 5082 \tTraining Loss: 0.192405 \tR2: 0.741634\n",
      "Epoch: 5083 \tTraining Loss: 0.198464 \tR2: 0.741634\n",
      "Epoch: 5084 \tTraining Loss: 0.180285 \tR2: 0.741634\n",
      "Epoch: 5085 \tTraining Loss: 0.199001 \tR2: 0.741634\n",
      "Epoch: 5086 \tTraining Loss: 0.193133 \tR2: 0.741634\n",
      "Epoch: 5087 \tTraining Loss: 0.193974 \tR2: 0.741634\n",
      "Epoch: 5088 \tTraining Loss: 0.193936 \tR2: 0.741634\n",
      "Epoch: 5089 \tTraining Loss: 0.198479 \tR2: 0.741634\n",
      "Epoch: 5090 \tTraining Loss: 0.201795 \tR2: 0.741634\n",
      "Epoch: 5091 \tTraining Loss: 0.306954 \tR2: 0.741634\n",
      "Epoch: 5092 \tTraining Loss: 0.229281 \tR2: 0.741634\n",
      "Epoch: 5093 \tTraining Loss: 0.198004 \tR2: 0.741634\n",
      "Epoch: 5094 \tTraining Loss: 0.186063 \tR2: 0.741634\n",
      "Epoch: 5095 \tTraining Loss: 0.227438 \tR2: 0.741634\n",
      "Epoch: 5096 \tTraining Loss: 0.207773 \tR2: 0.741634\n",
      "Epoch: 5097 \tTraining Loss: 0.195084 \tR2: 0.741634\n",
      "Epoch: 5098 \tTraining Loss: 0.210052 \tR2: 0.741634\n",
      "Epoch: 5099 \tTraining Loss: 0.199928 \tR2: 0.741634\n",
      "Epoch: 5100 \tTraining Loss: 0.185359 \tR2: 0.804488\n",
      "Epoch: 5101 \tTraining Loss: 0.190299 \tR2: 0.804488\n",
      "Epoch: 5102 \tTraining Loss: 0.207528 \tR2: 0.804488\n",
      "Epoch: 5103 \tTraining Loss: 0.195588 \tR2: 0.804488\n",
      "Epoch: 5104 \tTraining Loss: 0.205481 \tR2: 0.804488\n",
      "Epoch: 5105 \tTraining Loss: 0.198848 \tR2: 0.804488\n",
      "Epoch: 5106 \tTraining Loss: 0.198147 \tR2: 0.804488\n",
      "Epoch: 5107 \tTraining Loss: 0.197003 \tR2: 0.804488\n",
      "Epoch: 5108 \tTraining Loss: 0.185607 \tR2: 0.804488\n",
      "Epoch: 5109 \tTraining Loss: 0.191561 \tR2: 0.804488\n",
      "Epoch: 5110 \tTraining Loss: 0.190823 \tR2: 0.804488\n",
      "Epoch: 5111 \tTraining Loss: 0.173482 \tR2: 0.804488\n",
      "Epoch: 5112 \tTraining Loss: 0.204129 \tR2: 0.804488\n",
      "Epoch: 5113 \tTraining Loss: 0.194448 \tR2: 0.804488\n",
      "Epoch: 5114 \tTraining Loss: 0.179602 \tR2: 0.804488\n",
      "Epoch: 5115 \tTraining Loss: 0.210957 \tR2: 0.804488\n",
      "Epoch: 5116 \tTraining Loss: 0.237803 \tR2: 0.804488\n",
      "Epoch: 5117 \tTraining Loss: 0.197568 \tR2: 0.804488\n",
      "Epoch: 5118 \tTraining Loss: 0.209270 \tR2: 0.804488\n",
      "Epoch: 5119 \tTraining Loss: 0.200777 \tR2: 0.804488\n",
      "Epoch: 5120 \tTraining Loss: 0.202602 \tR2: 0.804488\n",
      "Epoch: 5121 \tTraining Loss: 0.200292 \tR2: 0.804488\n",
      "Epoch: 5122 \tTraining Loss: 0.214282 \tR2: 0.804488\n",
      "Epoch: 5123 \tTraining Loss: 0.172401 \tR2: 0.804488\n",
      "Epoch: 5124 \tTraining Loss: 0.186104 \tR2: 0.804488\n",
      "Epoch: 5125 \tTraining Loss: 0.202299 \tR2: 0.804488\n",
      "Epoch: 5126 \tTraining Loss: 0.208680 \tR2: 0.804488\n",
      "Epoch: 5127 \tTraining Loss: 0.179995 \tR2: 0.804488\n",
      "Epoch: 5128 \tTraining Loss: 0.212124 \tR2: 0.804488\n",
      "Epoch: 5129 \tTraining Loss: 0.208156 \tR2: 0.804488\n",
      "Epoch: 5130 \tTraining Loss: 0.215968 \tR2: 0.804488\n",
      "Epoch: 5131 \tTraining Loss: 0.195851 \tR2: 0.804488\n",
      "Epoch: 5132 \tTraining Loss: 0.192327 \tR2: 0.804488\n",
      "Epoch: 5133 \tTraining Loss: 0.182386 \tR2: 0.804488\n",
      "Epoch: 5134 \tTraining Loss: 0.192857 \tR2: 0.804488\n",
      "Epoch: 5135 \tTraining Loss: 0.205408 \tR2: 0.804488\n",
      "Epoch: 5136 \tTraining Loss: 0.204835 \tR2: 0.804488\n",
      "Epoch: 5137 \tTraining Loss: 0.183163 \tR2: 0.804488\n",
      "Epoch: 5138 \tTraining Loss: 0.206867 \tR2: 0.804488\n",
      "Epoch: 5139 \tTraining Loss: 0.215509 \tR2: 0.804488\n",
      "Epoch: 5140 \tTraining Loss: 0.201809 \tR2: 0.804488\n",
      "Epoch: 5141 \tTraining Loss: 0.199222 \tR2: 0.804488\n",
      "Epoch: 5142 \tTraining Loss: 0.174100 \tR2: 0.804488\n",
      "Epoch: 5143 \tTraining Loss: 0.191141 \tR2: 0.804488\n",
      "Epoch: 5144 \tTraining Loss: 0.199354 \tR2: 0.804488\n",
      "Epoch: 5145 \tTraining Loss: 0.210027 \tR2: 0.804488\n",
      "Epoch: 5146 \tTraining Loss: 0.192217 \tR2: 0.804488\n",
      "Epoch: 5147 \tTraining Loss: 0.201453 \tR2: 0.804488\n",
      "Epoch: 5148 \tTraining Loss: 0.191942 \tR2: 0.804488\n",
      "Epoch: 5149 \tTraining Loss: 0.210982 \tR2: 0.804488\n",
      "Epoch: 5150 \tTraining Loss: 0.195539 \tR2: 0.804488\n",
      "Epoch: 5151 \tTraining Loss: 0.211353 \tR2: 0.804488\n",
      "Epoch: 5152 \tTraining Loss: 0.184054 \tR2: 0.804488\n",
      "Epoch: 5153 \tTraining Loss: 0.188241 \tR2: 0.804488\n",
      "Epoch: 5154 \tTraining Loss: 0.201210 \tR2: 0.804488\n",
      "Epoch: 5155 \tTraining Loss: 0.191822 \tR2: 0.804488\n",
      "Epoch: 5156 \tTraining Loss: 0.189289 \tR2: 0.804488\n",
      "Epoch: 5157 \tTraining Loss: 0.187939 \tR2: 0.804488\n",
      "Epoch: 5158 \tTraining Loss: 0.180801 \tR2: 0.804488\n",
      "Epoch: 5159 \tTraining Loss: 0.186257 \tR2: 0.804488\n",
      "Epoch: 5160 \tTraining Loss: 0.189800 \tR2: 0.804488\n",
      "Epoch: 5161 \tTraining Loss: 0.207283 \tR2: 0.804488\n",
      "Epoch: 5162 \tTraining Loss: 0.192208 \tR2: 0.804488\n",
      "Epoch: 5163 \tTraining Loss: 0.197409 \tR2: 0.804488\n",
      "Epoch: 5164 \tTraining Loss: 0.195900 \tR2: 0.804488\n",
      "Epoch: 5165 \tTraining Loss: 0.180570 \tR2: 0.804488\n",
      "Epoch: 5166 \tTraining Loss: 0.181745 \tR2: 0.804488\n",
      "Epoch: 5167 \tTraining Loss: 0.176701 \tR2: 0.804488\n",
      "Epoch: 5168 \tTraining Loss: 0.214891 \tR2: 0.804488\n",
      "Epoch: 5169 \tTraining Loss: 0.195409 \tR2: 0.804488\n",
      "Epoch: 5170 \tTraining Loss: 0.186271 \tR2: 0.804488\n",
      "Epoch: 5171 \tTraining Loss: 0.198949 \tR2: 0.804488\n",
      "Epoch: 5172 \tTraining Loss: 0.196747 \tR2: 0.804488\n",
      "Epoch: 5173 \tTraining Loss: 0.187107 \tR2: 0.804488\n",
      "Epoch: 5174 \tTraining Loss: 0.208984 \tR2: 0.804488\n",
      "Epoch: 5175 \tTraining Loss: 0.185188 \tR2: 0.804488\n",
      "Epoch: 5176 \tTraining Loss: 0.210041 \tR2: 0.804488\n",
      "Epoch: 5177 \tTraining Loss: 0.189827 \tR2: 0.804488\n",
      "Epoch: 5178 \tTraining Loss: 0.180801 \tR2: 0.804488\n",
      "Epoch: 5179 \tTraining Loss: 0.200946 \tR2: 0.804488\n",
      "Epoch: 5180 \tTraining Loss: 0.216914 \tR2: 0.804488\n",
      "Epoch: 5181 \tTraining Loss: 0.199642 \tR2: 0.804488\n",
      "Epoch: 5182 \tTraining Loss: 0.190756 \tR2: 0.804488\n",
      "Epoch: 5183 \tTraining Loss: 0.205272 \tR2: 0.804488\n",
      "Epoch: 5184 \tTraining Loss: 0.177914 \tR2: 0.804488\n",
      "Epoch: 5185 \tTraining Loss: 0.182876 \tR2: 0.804488\n",
      "Epoch: 5186 \tTraining Loss: 0.180159 \tR2: 0.804488\n",
      "Epoch: 5187 \tTraining Loss: 0.189334 \tR2: 0.804488\n",
      "Epoch: 5188 \tTraining Loss: 0.192344 \tR2: 0.804488\n",
      "Epoch: 5189 \tTraining Loss: 0.200026 \tR2: 0.804488\n",
      "Epoch: 5190 \tTraining Loss: 0.194340 \tR2: 0.804488\n",
      "Epoch: 5191 \tTraining Loss: 0.194745 \tR2: 0.804488\n",
      "Epoch: 5192 \tTraining Loss: 0.192191 \tR2: 0.804488\n",
      "Epoch: 5193 \tTraining Loss: 0.213520 \tR2: 0.804488\n",
      "Epoch: 5194 \tTraining Loss: 0.194411 \tR2: 0.804488\n",
      "Epoch: 5195 \tTraining Loss: 0.194567 \tR2: 0.804488\n",
      "Epoch: 5196 \tTraining Loss: 0.205189 \tR2: 0.804488\n",
      "Epoch: 5197 \tTraining Loss: 0.197121 \tR2: 0.804488\n",
      "Epoch: 5198 \tTraining Loss: 0.197494 \tR2: 0.804488\n",
      "Epoch: 5199 \tTraining Loss: 0.206014 \tR2: 0.804488\n",
      "Epoch: 5200 \tTraining Loss: 0.192923 \tR2: 0.734979\n",
      "Epoch: 5201 \tTraining Loss: 0.185249 \tR2: 0.734979\n",
      "Epoch: 5202 \tTraining Loss: 0.199668 \tR2: 0.734979\n",
      "Epoch: 5203 \tTraining Loss: 0.189537 \tR2: 0.734979\n",
      "Epoch: 5204 \tTraining Loss: 0.215971 \tR2: 0.734979\n",
      "Epoch: 5205 \tTraining Loss: 0.188240 \tR2: 0.734979\n",
      "Epoch: 5206 \tTraining Loss: 0.203725 \tR2: 0.734979\n",
      "Epoch: 5207 \tTraining Loss: 0.205092 \tR2: 0.734979\n",
      "Epoch: 5208 \tTraining Loss: 0.211205 \tR2: 0.734979\n",
      "Epoch: 5209 \tTraining Loss: 0.209099 \tR2: 0.734979\n",
      "Epoch: 5210 \tTraining Loss: 0.177821 \tR2: 0.734979\n",
      "Epoch: 5211 \tTraining Loss: 0.197218 \tR2: 0.734979\n",
      "Epoch: 5212 \tTraining Loss: 0.214001 \tR2: 0.734979\n",
      "Epoch: 5213 \tTraining Loss: 0.203037 \tR2: 0.734979\n",
      "Epoch: 5214 \tTraining Loss: 0.201595 \tR2: 0.734979\n",
      "Epoch: 5215 \tTraining Loss: 0.189215 \tR2: 0.734979\n",
      "Epoch: 5216 \tTraining Loss: 0.179737 \tR2: 0.734979\n",
      "Epoch: 5217 \tTraining Loss: 0.204381 \tR2: 0.734979\n",
      "Epoch: 5218 \tTraining Loss: 0.197489 \tR2: 0.734979\n",
      "Epoch: 5219 \tTraining Loss: 0.179315 \tR2: 0.734979\n",
      "Epoch: 5220 \tTraining Loss: 0.186277 \tR2: 0.734979\n",
      "Epoch: 5221 \tTraining Loss: 0.190585 \tR2: 0.734979\n",
      "Epoch: 5222 \tTraining Loss: 0.191126 \tR2: 0.734979\n",
      "Epoch: 5223 \tTraining Loss: 0.186610 \tR2: 0.734979\n",
      "Epoch: 5224 \tTraining Loss: 0.187391 \tR2: 0.734979\n",
      "Epoch: 5225 \tTraining Loss: 0.186317 \tR2: 0.734979\n",
      "Epoch: 5226 \tTraining Loss: 0.185013 \tR2: 0.734979\n",
      "Epoch: 5227 \tTraining Loss: 0.203111 \tR2: 0.734979\n",
      "Epoch: 5228 \tTraining Loss: 0.188659 \tR2: 0.734979\n",
      "Epoch: 5229 \tTraining Loss: 0.193493 \tR2: 0.734979\n",
      "Epoch: 5230 \tTraining Loss: 0.192748 \tR2: 0.734979\n",
      "Epoch: 5231 \tTraining Loss: 0.181881 \tR2: 0.734979\n",
      "Epoch: 5232 \tTraining Loss: 0.189939 \tR2: 0.734979\n",
      "Epoch: 5233 \tTraining Loss: 0.189059 \tR2: 0.734979\n",
      "Epoch: 5234 \tTraining Loss: 0.204262 \tR2: 0.734979\n",
      "Epoch: 5235 \tTraining Loss: 0.192767 \tR2: 0.734979\n",
      "Epoch: 5236 \tTraining Loss: 0.193819 \tR2: 0.734979\n",
      "Epoch: 5237 \tTraining Loss: 0.195901 \tR2: 0.734979\n",
      "Epoch: 5238 \tTraining Loss: 0.200807 \tR2: 0.734979\n",
      "Epoch: 5239 \tTraining Loss: 0.178408 \tR2: 0.734979\n",
      "Epoch: 5240 \tTraining Loss: 0.187712 \tR2: 0.734979\n",
      "Epoch: 5241 \tTraining Loss: 0.201864 \tR2: 0.734979\n",
      "Epoch: 5242 \tTraining Loss: 0.182737 \tR2: 0.734979\n",
      "Epoch: 5243 \tTraining Loss: 0.193274 \tR2: 0.734979\n",
      "Epoch: 5244 \tTraining Loss: 0.201202 \tR2: 0.734979\n",
      "Epoch: 5245 \tTraining Loss: 0.204931 \tR2: 0.734979\n",
      "Epoch: 5246 \tTraining Loss: 0.175280 \tR2: 0.734979\n",
      "Epoch: 5247 \tTraining Loss: 0.198647 \tR2: 0.734979\n",
      "Epoch: 5248 \tTraining Loss: 0.217852 \tR2: 0.734979\n",
      "Epoch: 5249 \tTraining Loss: 0.202356 \tR2: 0.734979\n",
      "Epoch: 5250 \tTraining Loss: 0.191862 \tR2: 0.734979\n",
      "Epoch: 5251 \tTraining Loss: 0.202601 \tR2: 0.734979\n",
      "Epoch: 5252 \tTraining Loss: 0.191866 \tR2: 0.734979\n",
      "Epoch: 5253 \tTraining Loss: 0.207261 \tR2: 0.734979\n",
      "Epoch: 5254 \tTraining Loss: 0.195473 \tR2: 0.734979\n",
      "Epoch: 5255 \tTraining Loss: 0.203711 \tR2: 0.734979\n",
      "Epoch: 5256 \tTraining Loss: 0.204642 \tR2: 0.734979\n",
      "Epoch: 5257 \tTraining Loss: 0.199528 \tR2: 0.734979\n",
      "Epoch: 5258 \tTraining Loss: 0.216483 \tR2: 0.734979\n",
      "Epoch: 5259 \tTraining Loss: 0.194325 \tR2: 0.734979\n",
      "Epoch: 5260 \tTraining Loss: 0.198751 \tR2: 0.734979\n",
      "Epoch: 5261 \tTraining Loss: 0.192309 \tR2: 0.734979\n",
      "Epoch: 5262 \tTraining Loss: 0.211059 \tR2: 0.734979\n",
      "Epoch: 5263 \tTraining Loss: 0.209038 \tR2: 0.734979\n",
      "Epoch: 5264 \tTraining Loss: 0.199659 \tR2: 0.734979\n",
      "Epoch: 5265 \tTraining Loss: 0.166960 \tR2: 0.734979\n",
      "Epoch: 5266 \tTraining Loss: 0.203775 \tR2: 0.734979\n",
      "Epoch: 5267 \tTraining Loss: 0.194932 \tR2: 0.734979\n",
      "Epoch: 5268 \tTraining Loss: 0.204931 \tR2: 0.734979\n",
      "Epoch: 5269 \tTraining Loss: 0.194355 \tR2: 0.734979\n",
      "Epoch: 5270 \tTraining Loss: 0.175849 \tR2: 0.734979\n",
      "Epoch: 5271 \tTraining Loss: 0.218876 \tR2: 0.734979\n",
      "Epoch: 5272 \tTraining Loss: 0.208153 \tR2: 0.734979\n",
      "Epoch: 5273 \tTraining Loss: 0.203903 \tR2: 0.734979\n",
      "Epoch: 5274 \tTraining Loss: 0.209466 \tR2: 0.734979\n",
      "Epoch: 5275 \tTraining Loss: 0.191607 \tR2: 0.734979\n",
      "Epoch: 5276 \tTraining Loss: 0.198349 \tR2: 0.734979\n",
      "Epoch: 5277 \tTraining Loss: 0.184313 \tR2: 0.734979\n",
      "Epoch: 5278 \tTraining Loss: 0.190194 \tR2: 0.734979\n",
      "Epoch: 5279 \tTraining Loss: 0.191004 \tR2: 0.734979\n",
      "Epoch: 5280 \tTraining Loss: 0.197228 \tR2: 0.734979\n",
      "Epoch: 5281 \tTraining Loss: 0.199391 \tR2: 0.734979\n",
      "Epoch: 5282 \tTraining Loss: 0.183210 \tR2: 0.734979\n",
      "Epoch: 5283 \tTraining Loss: 0.192766 \tR2: 0.734979\n",
      "Epoch: 5284 \tTraining Loss: 0.199962 \tR2: 0.734979\n",
      "Epoch: 5285 \tTraining Loss: 0.180820 \tR2: 0.734979\n",
      "Epoch: 5286 \tTraining Loss: 0.205950 \tR2: 0.734979\n",
      "Epoch: 5287 \tTraining Loss: 0.224614 \tR2: 0.734979\n",
      "Epoch: 5288 \tTraining Loss: 0.187426 \tR2: 0.734979\n",
      "Epoch: 5289 \tTraining Loss: 0.196498 \tR2: 0.734979\n",
      "Epoch: 5290 \tTraining Loss: 0.194348 \tR2: 0.734979\n",
      "Epoch: 5291 \tTraining Loss: 0.182723 \tR2: 0.734979\n",
      "Epoch: 5292 \tTraining Loss: 0.199845 \tR2: 0.734979\n",
      "Epoch: 5293 \tTraining Loss: 0.183078 \tR2: 0.734979\n",
      "Epoch: 5294 \tTraining Loss: 0.197296 \tR2: 0.734979\n",
      "Epoch: 5295 \tTraining Loss: 0.177899 \tR2: 0.734979\n",
      "Epoch: 5296 \tTraining Loss: 0.188977 \tR2: 0.734979\n",
      "Epoch: 5297 \tTraining Loss: 0.207486 \tR2: 0.734979\n",
      "Epoch: 5298 \tTraining Loss: 0.207108 \tR2: 0.734979\n",
      "Epoch: 5299 \tTraining Loss: 0.213384 \tR2: 0.734979\n",
      "Epoch: 5300 \tTraining Loss: 0.200674 \tR2: 0.700413\n",
      "Epoch: 5301 \tTraining Loss: 0.193094 \tR2: 0.700413\n",
      "Epoch: 5302 \tTraining Loss: 0.199803 \tR2: 0.700413\n",
      "Epoch: 5303 \tTraining Loss: 0.200471 \tR2: 0.700413\n",
      "Epoch: 5304 \tTraining Loss: 0.181080 \tR2: 0.700413\n",
      "Epoch: 5305 \tTraining Loss: 0.187705 \tR2: 0.700413\n",
      "Epoch: 5306 \tTraining Loss: 0.180733 \tR2: 0.700413\n",
      "Epoch: 5307 \tTraining Loss: 0.184779 \tR2: 0.700413\n",
      "Epoch: 5308 \tTraining Loss: 0.187278 \tR2: 0.700413\n",
      "Epoch: 5309 \tTraining Loss: 0.184559 \tR2: 0.700413\n",
      "Epoch: 5310 \tTraining Loss: 0.202704 \tR2: 0.700413\n",
      "Epoch: 5311 \tTraining Loss: 0.199628 \tR2: 0.700413\n",
      "Epoch: 5312 \tTraining Loss: 0.191916 \tR2: 0.700413\n",
      "Epoch: 5313 \tTraining Loss: 0.215270 \tR2: 0.700413\n",
      "Epoch: 5314 \tTraining Loss: 0.200341 \tR2: 0.700413\n",
      "Epoch: 5315 \tTraining Loss: 0.191287 \tR2: 0.700413\n",
      "Epoch: 5316 \tTraining Loss: 0.189836 \tR2: 0.700413\n",
      "Epoch: 5317 \tTraining Loss: 0.204687 \tR2: 0.700413\n",
      "Epoch: 5318 \tTraining Loss: 0.196231 \tR2: 0.700413\n",
      "Epoch: 5319 \tTraining Loss: 0.197683 \tR2: 0.700413\n",
      "Epoch: 5320 \tTraining Loss: 0.183955 \tR2: 0.700413\n",
      "Epoch: 5321 \tTraining Loss: 0.189204 \tR2: 0.700413\n",
      "Epoch: 5322 \tTraining Loss: 0.194328 \tR2: 0.700413\n",
      "Epoch: 5323 \tTraining Loss: 0.191292 \tR2: 0.700413\n",
      "Epoch: 5324 \tTraining Loss: 0.183638 \tR2: 0.700413\n",
      "Epoch: 5325 \tTraining Loss: 0.184341 \tR2: 0.700413\n",
      "Epoch: 5326 \tTraining Loss: 0.195030 \tR2: 0.700413\n",
      "Epoch: 5327 \tTraining Loss: 0.194546 \tR2: 0.700413\n",
      "Epoch: 5328 \tTraining Loss: 0.190793 \tR2: 0.700413\n",
      "Epoch: 5329 \tTraining Loss: 0.198916 \tR2: 0.700413\n",
      "Epoch: 5330 \tTraining Loss: 0.214502 \tR2: 0.700413\n",
      "Epoch: 5331 \tTraining Loss: 0.190481 \tR2: 0.700413\n",
      "Epoch: 5332 \tTraining Loss: 0.194379 \tR2: 0.700413\n",
      "Epoch: 5333 \tTraining Loss: 0.204702 \tR2: 0.700413\n",
      "Epoch: 5334 \tTraining Loss: 0.190228 \tR2: 0.700413\n",
      "Epoch: 5335 \tTraining Loss: 0.188408 \tR2: 0.700413\n",
      "Epoch: 5336 \tTraining Loss: 0.200204 \tR2: 0.700413\n",
      "Epoch: 5337 \tTraining Loss: 0.191620 \tR2: 0.700413\n",
      "Epoch: 5338 \tTraining Loss: 0.177469 \tR2: 0.700413\n",
      "Epoch: 5339 \tTraining Loss: 0.203338 \tR2: 0.700413\n",
      "Epoch: 5340 \tTraining Loss: 0.190357 \tR2: 0.700413\n",
      "Epoch: 5341 \tTraining Loss: 0.204812 \tR2: 0.700413\n",
      "Epoch: 5342 \tTraining Loss: 0.187614 \tR2: 0.700413\n",
      "Epoch: 5343 \tTraining Loss: 0.188275 \tR2: 0.700413\n",
      "Epoch: 5344 \tTraining Loss: 0.185993 \tR2: 0.700413\n",
      "Epoch: 5345 \tTraining Loss: 0.184141 \tR2: 0.700413\n",
      "Epoch: 5346 \tTraining Loss: 0.188303 \tR2: 0.700413\n",
      "Epoch: 5347 \tTraining Loss: 0.198588 \tR2: 0.700413\n",
      "Epoch: 5348 \tTraining Loss: 0.208120 \tR2: 0.700413\n",
      "Epoch: 5349 \tTraining Loss: 0.182115 \tR2: 0.700413\n",
      "Epoch: 5350 \tTraining Loss: 0.200345 \tR2: 0.700413\n",
      "Epoch: 5351 \tTraining Loss: 0.202852 \tR2: 0.700413\n",
      "Epoch: 5352 \tTraining Loss: 0.188391 \tR2: 0.700413\n",
      "Epoch: 5353 \tTraining Loss: 0.211115 \tR2: 0.700413\n",
      "Epoch: 5354 \tTraining Loss: 0.189536 \tR2: 0.700413\n",
      "Epoch: 5355 \tTraining Loss: 0.211319 \tR2: 0.700413\n",
      "Epoch: 5356 \tTraining Loss: 0.220069 \tR2: 0.700413\n",
      "Epoch: 5357 \tTraining Loss: 0.188632 \tR2: 0.700413\n",
      "Epoch: 5358 \tTraining Loss: 0.203773 \tR2: 0.700413\n",
      "Epoch: 5359 \tTraining Loss: 0.193990 \tR2: 0.700413\n",
      "Epoch: 5360 \tTraining Loss: 0.187639 \tR2: 0.700413\n",
      "Epoch: 5361 \tTraining Loss: 0.182750 \tR2: 0.700413\n",
      "Epoch: 5362 \tTraining Loss: 0.190541 \tR2: 0.700413\n",
      "Epoch: 5363 \tTraining Loss: 0.187753 \tR2: 0.700413\n",
      "Epoch: 5364 \tTraining Loss: 0.181378 \tR2: 0.700413\n",
      "Epoch: 5365 \tTraining Loss: 0.199959 \tR2: 0.700413\n",
      "Epoch: 5366 \tTraining Loss: 0.183855 \tR2: 0.700413\n",
      "Epoch: 5367 \tTraining Loss: 0.211421 \tR2: 0.700413\n",
      "Epoch: 5368 \tTraining Loss: 0.187730 \tR2: 0.700413\n",
      "Epoch: 5369 \tTraining Loss: 0.206493 \tR2: 0.700413\n",
      "Epoch: 5370 \tTraining Loss: 0.185402 \tR2: 0.700413\n",
      "Epoch: 5371 \tTraining Loss: 0.190296 \tR2: 0.700413\n",
      "Epoch: 5372 \tTraining Loss: 0.190322 \tR2: 0.700413\n",
      "Epoch: 5373 \tTraining Loss: 0.185419 \tR2: 0.700413\n",
      "Epoch: 5374 \tTraining Loss: 0.185256 \tR2: 0.700413\n",
      "Epoch: 5375 \tTraining Loss: 0.197539 \tR2: 0.700413\n",
      "Epoch: 5376 \tTraining Loss: 0.210335 \tR2: 0.700413\n",
      "Epoch: 5377 \tTraining Loss: 0.211630 \tR2: 0.700413\n",
      "Epoch: 5378 \tTraining Loss: 0.190768 \tR2: 0.700413\n",
      "Epoch: 5379 \tTraining Loss: 0.193456 \tR2: 0.700413\n",
      "Epoch: 5380 \tTraining Loss: 0.183123 \tR2: 0.700413\n",
      "Epoch: 5381 \tTraining Loss: 0.190971 \tR2: 0.700413\n",
      "Epoch: 5382 \tTraining Loss: 0.185196 \tR2: 0.700413\n",
      "Epoch: 5383 \tTraining Loss: 0.175104 \tR2: 0.700413\n",
      "Epoch: 5384 \tTraining Loss: 0.177401 \tR2: 0.700413\n",
      "Epoch: 5385 \tTraining Loss: 0.195784 \tR2: 0.700413\n",
      "Epoch: 5386 \tTraining Loss: 0.175799 \tR2: 0.700413\n",
      "Epoch: 5387 \tTraining Loss: 0.203331 \tR2: 0.700413\n",
      "Epoch: 5388 \tTraining Loss: 0.183553 \tR2: 0.700413\n",
      "Epoch: 5389 \tTraining Loss: 0.207496 \tR2: 0.700413\n",
      "Epoch: 5390 \tTraining Loss: 0.184379 \tR2: 0.700413\n",
      "Epoch: 5391 \tTraining Loss: 0.188770 \tR2: 0.700413\n",
      "Epoch: 5392 \tTraining Loss: 0.204378 \tR2: 0.700413\n",
      "Epoch: 5393 \tTraining Loss: 0.194472 \tR2: 0.700413\n",
      "Epoch: 5394 \tTraining Loss: 0.197047 \tR2: 0.700413\n",
      "Epoch: 5395 \tTraining Loss: 0.193820 \tR2: 0.700413\n",
      "Epoch: 5396 \tTraining Loss: 0.198393 \tR2: 0.700413\n",
      "Epoch: 5397 \tTraining Loss: 0.188995 \tR2: 0.700413\n",
      "Epoch: 5398 \tTraining Loss: 0.206282 \tR2: 0.700413\n",
      "Epoch: 5399 \tTraining Loss: 0.233683 \tR2: 0.700413\n",
      "Epoch: 5400 \tTraining Loss: 0.188889 \tR2: 0.648346\n",
      "Epoch: 5401 \tTraining Loss: 0.186487 \tR2: 0.648346\n",
      "Epoch: 5402 \tTraining Loss: 0.196515 \tR2: 0.648346\n",
      "Epoch: 5403 \tTraining Loss: 0.207800 \tR2: 0.648346\n",
      "Epoch: 5404 \tTraining Loss: 0.203520 \tR2: 0.648346\n",
      "Epoch: 5405 \tTraining Loss: 0.187494 \tR2: 0.648346\n",
      "Epoch: 5406 \tTraining Loss: 0.217926 \tR2: 0.648346\n",
      "Epoch: 5407 \tTraining Loss: 0.187700 \tR2: 0.648346\n",
      "Epoch: 5408 \tTraining Loss: 0.198873 \tR2: 0.648346\n",
      "Epoch: 5409 \tTraining Loss: 0.194808 \tR2: 0.648346\n",
      "Epoch: 5410 \tTraining Loss: 0.191549 \tR2: 0.648346\n",
      "Epoch: 5411 \tTraining Loss: 0.202605 \tR2: 0.648346\n",
      "Epoch: 5412 \tTraining Loss: 0.189756 \tR2: 0.648346\n",
      "Epoch: 5413 \tTraining Loss: 0.200966 \tR2: 0.648346\n",
      "Epoch: 5414 \tTraining Loss: 0.199032 \tR2: 0.648346\n",
      "Epoch: 5415 \tTraining Loss: 0.181495 \tR2: 0.648346\n",
      "Epoch: 5416 \tTraining Loss: 0.198305 \tR2: 0.648346\n",
      "Epoch: 5417 \tTraining Loss: 0.182884 \tR2: 0.648346\n",
      "Epoch: 5418 \tTraining Loss: 0.201325 \tR2: 0.648346\n",
      "Epoch: 5419 \tTraining Loss: 0.193309 \tR2: 0.648346\n",
      "Epoch: 5420 \tTraining Loss: 0.178965 \tR2: 0.648346\n",
      "Epoch: 5421 \tTraining Loss: 0.185901 \tR2: 0.648346\n",
      "Epoch: 5422 \tTraining Loss: 0.179399 \tR2: 0.648346\n",
      "Epoch: 5423 \tTraining Loss: 0.191055 \tR2: 0.648346\n",
      "Epoch: 5424 \tTraining Loss: 0.204782 \tR2: 0.648346\n",
      "Epoch: 5425 \tTraining Loss: 0.189609 \tR2: 0.648346\n",
      "Epoch: 5426 \tTraining Loss: 0.184650 \tR2: 0.648346\n",
      "Epoch: 5427 \tTraining Loss: 0.201120 \tR2: 0.648346\n",
      "Epoch: 5428 \tTraining Loss: 0.198489 \tR2: 0.648346\n",
      "Epoch: 5429 \tTraining Loss: 0.166906 \tR2: 0.648346\n",
      "Epoch: 5430 \tTraining Loss: 0.187439 \tR2: 0.648346\n",
      "Epoch: 5431 \tTraining Loss: 0.179968 \tR2: 0.648346\n",
      "Epoch: 5432 \tTraining Loss: 0.192415 \tR2: 0.648346\n",
      "Epoch: 5433 \tTraining Loss: 0.184212 \tR2: 0.648346\n",
      "Epoch: 5434 \tTraining Loss: 0.205872 \tR2: 0.648346\n",
      "Epoch: 5435 \tTraining Loss: 0.190788 \tR2: 0.648346\n",
      "Epoch: 5436 \tTraining Loss: 0.183571 \tR2: 0.648346\n",
      "Epoch: 5437 \tTraining Loss: 0.188919 \tR2: 0.648346\n",
      "Epoch: 5438 \tTraining Loss: 0.180846 \tR2: 0.648346\n",
      "Epoch: 5439 \tTraining Loss: 0.211984 \tR2: 0.648346\n",
      "Epoch: 5440 \tTraining Loss: 0.196584 \tR2: 0.648346\n",
      "Epoch: 5441 \tTraining Loss: 0.217537 \tR2: 0.648346\n",
      "Epoch: 5442 \tTraining Loss: 0.189034 \tR2: 0.648346\n",
      "Epoch: 5443 \tTraining Loss: 0.203514 \tR2: 0.648346\n",
      "Epoch: 5444 \tTraining Loss: 0.195046 \tR2: 0.648346\n",
      "Epoch: 5445 \tTraining Loss: 0.197728 \tR2: 0.648346\n",
      "Epoch: 5446 \tTraining Loss: 0.196825 \tR2: 0.648346\n",
      "Epoch: 5447 \tTraining Loss: 0.203791 \tR2: 0.648346\n",
      "Epoch: 5448 \tTraining Loss: 0.192369 \tR2: 0.648346\n",
      "Epoch: 5449 \tTraining Loss: 0.194133 \tR2: 0.648346\n",
      "Epoch: 5450 \tTraining Loss: 0.207883 \tR2: 0.648346\n",
      "Epoch: 5451 \tTraining Loss: 0.179233 \tR2: 0.648346\n",
      "Epoch: 5452 \tTraining Loss: 0.168059 \tR2: 0.648346\n",
      "Epoch: 5453 \tTraining Loss: 0.189303 \tR2: 0.648346\n",
      "Epoch: 5454 \tTraining Loss: 0.186175 \tR2: 0.648346\n",
      "Epoch: 5455 \tTraining Loss: 0.197624 \tR2: 0.648346\n",
      "Epoch: 5456 \tTraining Loss: 0.192049 \tR2: 0.648346\n",
      "Epoch: 5457 \tTraining Loss: 0.196075 \tR2: 0.648346\n",
      "Epoch: 5458 \tTraining Loss: 0.184140 \tR2: 0.648346\n",
      "Epoch: 5459 \tTraining Loss: 0.190896 \tR2: 0.648346\n",
      "Epoch: 5460 \tTraining Loss: 0.204467 \tR2: 0.648346\n",
      "Epoch: 5461 \tTraining Loss: 0.186261 \tR2: 0.648346\n",
      "Epoch: 5462 \tTraining Loss: 0.196726 \tR2: 0.648346\n",
      "Epoch: 5463 \tTraining Loss: 0.186202 \tR2: 0.648346\n",
      "Epoch: 5464 \tTraining Loss: 0.177428 \tR2: 0.648346\n",
      "Epoch: 5465 \tTraining Loss: 0.208741 \tR2: 0.648346\n",
      "Epoch: 5466 \tTraining Loss: 0.208811 \tR2: 0.648346\n",
      "Epoch: 5467 \tTraining Loss: 0.180991 \tR2: 0.648346\n",
      "Epoch: 5468 \tTraining Loss: 0.210327 \tR2: 0.648346\n",
      "Epoch: 5469 \tTraining Loss: 0.187610 \tR2: 0.648346\n",
      "Epoch: 5470 \tTraining Loss: 0.187434 \tR2: 0.648346\n",
      "Epoch: 5471 \tTraining Loss: 0.206811 \tR2: 0.648346\n",
      "Epoch: 5472 \tTraining Loss: 0.201519 \tR2: 0.648346\n",
      "Epoch: 5473 \tTraining Loss: 0.215340 \tR2: 0.648346\n",
      "Epoch: 5474 \tTraining Loss: 0.200483 \tR2: 0.648346\n",
      "Epoch: 5475 \tTraining Loss: 0.208196 \tR2: 0.648346\n",
      "Epoch: 5476 \tTraining Loss: 0.191868 \tR2: 0.648346\n",
      "Epoch: 5477 \tTraining Loss: 0.174302 \tR2: 0.648346\n",
      "Epoch: 5478 \tTraining Loss: 0.190330 \tR2: 0.648346\n",
      "Epoch: 5479 \tTraining Loss: 0.222780 \tR2: 0.648346\n",
      "Epoch: 5480 \tTraining Loss: 0.211236 \tR2: 0.648346\n",
      "Epoch: 5481 \tTraining Loss: 0.219034 \tR2: 0.648346\n",
      "Epoch: 5482 \tTraining Loss: 0.186523 \tR2: 0.648346\n",
      "Epoch: 5483 \tTraining Loss: 0.186986 \tR2: 0.648346\n",
      "Epoch: 5484 \tTraining Loss: 0.195680 \tR2: 0.648346\n",
      "Epoch: 5485 \tTraining Loss: 0.213200 \tR2: 0.648346\n",
      "Epoch: 5486 \tTraining Loss: 0.200437 \tR2: 0.648346\n",
      "Epoch: 5487 \tTraining Loss: 0.201119 \tR2: 0.648346\n",
      "Epoch: 5488 \tTraining Loss: 0.192741 \tR2: 0.648346\n",
      "Epoch: 5489 \tTraining Loss: 0.189484 \tR2: 0.648346\n",
      "Epoch: 5490 \tTraining Loss: 0.207901 \tR2: 0.648346\n",
      "Epoch: 5491 \tTraining Loss: 0.197025 \tR2: 0.648346\n",
      "Epoch: 5492 \tTraining Loss: 0.194427 \tR2: 0.648346\n",
      "Epoch: 5493 \tTraining Loss: 0.191372 \tR2: 0.648346\n",
      "Epoch: 5494 \tTraining Loss: 0.190273 \tR2: 0.648346\n",
      "Epoch: 5495 \tTraining Loss: 0.186318 \tR2: 0.648346\n",
      "Epoch: 5496 \tTraining Loss: 0.180056 \tR2: 0.648346\n",
      "Epoch: 5497 \tTraining Loss: 0.187966 \tR2: 0.648346\n",
      "Epoch: 5498 \tTraining Loss: 0.195170 \tR2: 0.648346\n",
      "Epoch: 5499 \tTraining Loss: 0.205171 \tR2: 0.648346\n",
      "Epoch: 5500 \tTraining Loss: 0.201555 \tR2: 0.836777\n",
      "Epoch: 5501 \tTraining Loss: 0.180731 \tR2: 0.836777\n",
      "Epoch: 5502 \tTraining Loss: 0.185156 \tR2: 0.836777\n",
      "Epoch: 5503 \tTraining Loss: 0.190617 \tR2: 0.836777\n",
      "Epoch: 5504 \tTraining Loss: 0.216957 \tR2: 0.836777\n",
      "Epoch: 5505 \tTraining Loss: 0.201138 \tR2: 0.836777\n",
      "Epoch: 5506 \tTraining Loss: 0.194780 \tR2: 0.836777\n",
      "Epoch: 5507 \tTraining Loss: 0.208480 \tR2: 0.836777\n",
      "Epoch: 5508 \tTraining Loss: 0.186636 \tR2: 0.836777\n",
      "Epoch: 5509 \tTraining Loss: 0.190729 \tR2: 0.836777\n",
      "Epoch: 5510 \tTraining Loss: 0.195767 \tR2: 0.836777\n",
      "Epoch: 5511 \tTraining Loss: 0.194207 \tR2: 0.836777\n",
      "Epoch: 5512 \tTraining Loss: 0.180471 \tR2: 0.836777\n",
      "Epoch: 5513 \tTraining Loss: 0.190767 \tR2: 0.836777\n",
      "Epoch: 5514 \tTraining Loss: 0.211434 \tR2: 0.836777\n",
      "Epoch: 5515 \tTraining Loss: 0.197660 \tR2: 0.836777\n",
      "Epoch: 5516 \tTraining Loss: 0.192105 \tR2: 0.836777\n",
      "Epoch: 5517 \tTraining Loss: 0.193582 \tR2: 0.836777\n",
      "Epoch: 5518 \tTraining Loss: 0.203139 \tR2: 0.836777\n",
      "Epoch: 5519 \tTraining Loss: 0.211837 \tR2: 0.836777\n",
      "Epoch: 5520 \tTraining Loss: 0.183791 \tR2: 0.836777\n",
      "Epoch: 5521 \tTraining Loss: 0.191200 \tR2: 0.836777\n",
      "Epoch: 5522 \tTraining Loss: 0.203586 \tR2: 0.836777\n",
      "Epoch: 5523 \tTraining Loss: 0.202548 \tR2: 0.836777\n",
      "Epoch: 5524 \tTraining Loss: 0.201904 \tR2: 0.836777\n",
      "Epoch: 5525 \tTraining Loss: 0.245727 \tR2: 0.836777\n",
      "Epoch: 5526 \tTraining Loss: 0.192979 \tR2: 0.836777\n",
      "Epoch: 5527 \tTraining Loss: 0.180784 \tR2: 0.836777\n",
      "Epoch: 5528 \tTraining Loss: 0.192380 \tR2: 0.836777\n",
      "Epoch: 5529 \tTraining Loss: 0.182788 \tR2: 0.836777\n",
      "Epoch: 5530 \tTraining Loss: 0.188389 \tR2: 0.836777\n",
      "Epoch: 5531 \tTraining Loss: 0.195979 \tR2: 0.836777\n",
      "Epoch: 5532 \tTraining Loss: 0.190067 \tR2: 0.836777\n",
      "Epoch: 5533 \tTraining Loss: 0.190114 \tR2: 0.836777\n",
      "Epoch: 5534 \tTraining Loss: 0.186843 \tR2: 0.836777\n",
      "Epoch: 5535 \tTraining Loss: 0.199097 \tR2: 0.836777\n",
      "Epoch: 5536 \tTraining Loss: 0.215468 \tR2: 0.836777\n",
      "Epoch: 5537 \tTraining Loss: 0.195464 \tR2: 0.836777\n",
      "Epoch: 5538 \tTraining Loss: 0.191011 \tR2: 0.836777\n",
      "Epoch: 5539 \tTraining Loss: 0.194011 \tR2: 0.836777\n",
      "Epoch: 5540 \tTraining Loss: 0.174406 \tR2: 0.836777\n",
      "Epoch: 5541 \tTraining Loss: 0.189326 \tR2: 0.836777\n",
      "Epoch: 5542 \tTraining Loss: 0.206596 \tR2: 0.836777\n",
      "Epoch: 5543 \tTraining Loss: 0.184977 \tR2: 0.836777\n",
      "Epoch: 5544 \tTraining Loss: 0.185722 \tR2: 0.836777\n",
      "Epoch: 5545 \tTraining Loss: 0.182824 \tR2: 0.836777\n",
      "Epoch: 5546 \tTraining Loss: 0.197629 \tR2: 0.836777\n",
      "Epoch: 5547 \tTraining Loss: 0.183738 \tR2: 0.836777\n",
      "Epoch: 5548 \tTraining Loss: 0.188589 \tR2: 0.836777\n",
      "Epoch: 5549 \tTraining Loss: 0.194902 \tR2: 0.836777\n",
      "Epoch: 5550 \tTraining Loss: 0.197514 \tR2: 0.836777\n",
      "Epoch: 5551 \tTraining Loss: 0.186036 \tR2: 0.836777\n",
      "Epoch: 5552 \tTraining Loss: 0.204152 \tR2: 0.836777\n",
      "Epoch: 5553 \tTraining Loss: 0.189348 \tR2: 0.836777\n",
      "Epoch: 5554 \tTraining Loss: 0.195859 \tR2: 0.836777\n",
      "Epoch: 5555 \tTraining Loss: 0.198649 \tR2: 0.836777\n",
      "Epoch: 5556 \tTraining Loss: 0.193958 \tR2: 0.836777\n",
      "Epoch: 5557 \tTraining Loss: 0.187675 \tR2: 0.836777\n",
      "Epoch: 5558 \tTraining Loss: 0.181085 \tR2: 0.836777\n",
      "Epoch: 5559 \tTraining Loss: 0.193227 \tR2: 0.836777\n",
      "Epoch: 5560 \tTraining Loss: 0.177824 \tR2: 0.836777\n",
      "Epoch: 5561 \tTraining Loss: 0.190986 \tR2: 0.836777\n",
      "Epoch: 5562 \tTraining Loss: 0.172966 \tR2: 0.836777\n",
      "Epoch: 5563 \tTraining Loss: 0.192985 \tR2: 0.836777\n",
      "Epoch: 5564 \tTraining Loss: 0.192939 \tR2: 0.836777\n",
      "Epoch: 5565 \tTraining Loss: 0.197124 \tR2: 0.836777\n",
      "Epoch: 5566 \tTraining Loss: 0.192265 \tR2: 0.836777\n",
      "Epoch: 5567 \tTraining Loss: 0.198965 \tR2: 0.836777\n",
      "Epoch: 5568 \tTraining Loss: 0.192409 \tR2: 0.836777\n",
      "Epoch: 5569 \tTraining Loss: 0.194447 \tR2: 0.836777\n",
      "Epoch: 5570 \tTraining Loss: 0.172037 \tR2: 0.836777\n",
      "Epoch: 5571 \tTraining Loss: 0.200649 \tR2: 0.836777\n",
      "Epoch: 5572 \tTraining Loss: 0.201994 \tR2: 0.836777\n",
      "Epoch: 5573 \tTraining Loss: 0.187638 \tR2: 0.836777\n",
      "Epoch: 5574 \tTraining Loss: 0.190143 \tR2: 0.836777\n",
      "Epoch: 5575 \tTraining Loss: 0.198703 \tR2: 0.836777\n",
      "Epoch: 5576 \tTraining Loss: 0.184605 \tR2: 0.836777\n",
      "Epoch: 5577 \tTraining Loss: 0.199448 \tR2: 0.836777\n",
      "Epoch: 5578 \tTraining Loss: 0.192037 \tR2: 0.836777\n",
      "Epoch: 5579 \tTraining Loss: 0.186488 \tR2: 0.836777\n",
      "Epoch: 5580 \tTraining Loss: 0.191328 \tR2: 0.836777\n",
      "Epoch: 5581 \tTraining Loss: 0.200393 \tR2: 0.836777\n",
      "Epoch: 5582 \tTraining Loss: 0.183211 \tR2: 0.836777\n",
      "Epoch: 5583 \tTraining Loss: 0.187346 \tR2: 0.836777\n",
      "Epoch: 5584 \tTraining Loss: 0.198097 \tR2: 0.836777\n",
      "Epoch: 5585 \tTraining Loss: 0.184262 \tR2: 0.836777\n",
      "Epoch: 5586 \tTraining Loss: 0.174884 \tR2: 0.836777\n",
      "Epoch: 5587 \tTraining Loss: 0.202361 \tR2: 0.836777\n",
      "Epoch: 5588 \tTraining Loss: 0.204256 \tR2: 0.836777\n",
      "Epoch: 5589 \tTraining Loss: 0.199644 \tR2: 0.836777\n",
      "Epoch: 5590 \tTraining Loss: 0.210533 \tR2: 0.836777\n",
      "Epoch: 5591 \tTraining Loss: 0.196144 \tR2: 0.836777\n",
      "Epoch: 5592 \tTraining Loss: 0.187542 \tR2: 0.836777\n",
      "Epoch: 5593 \tTraining Loss: 0.202945 \tR2: 0.836777\n",
      "Epoch: 5594 \tTraining Loss: 0.189164 \tR2: 0.836777\n",
      "Epoch: 5595 \tTraining Loss: 0.197358 \tR2: 0.836777\n",
      "Epoch: 5596 \tTraining Loss: 0.186212 \tR2: 0.836777\n",
      "Epoch: 5597 \tTraining Loss: 0.194912 \tR2: 0.836777\n",
      "Epoch: 5598 \tTraining Loss: 0.206013 \tR2: 0.836777\n",
      "Epoch: 5599 \tTraining Loss: 0.194575 \tR2: 0.836777\n",
      "Epoch: 5600 \tTraining Loss: 0.197157 \tR2: 0.854012\n",
      "Epoch: 5601 \tTraining Loss: 0.173886 \tR2: 0.854012\n",
      "Epoch: 5602 \tTraining Loss: 0.194966 \tR2: 0.854012\n",
      "Epoch: 5603 \tTraining Loss: 0.188519 \tR2: 0.854012\n",
      "Epoch: 5604 \tTraining Loss: 0.184353 \tR2: 0.854012\n",
      "Epoch: 5605 \tTraining Loss: 0.184632 \tR2: 0.854012\n",
      "Epoch: 5606 \tTraining Loss: 0.186074 \tR2: 0.854012\n",
      "Epoch: 5607 \tTraining Loss: 0.181458 \tR2: 0.854012\n",
      "Epoch: 5608 \tTraining Loss: 0.199426 \tR2: 0.854012\n",
      "Epoch: 5609 \tTraining Loss: 0.173984 \tR2: 0.854012\n",
      "Epoch: 5610 \tTraining Loss: 0.193608 \tR2: 0.854012\n",
      "Epoch: 5611 \tTraining Loss: 0.178778 \tR2: 0.854012\n",
      "Epoch: 5612 \tTraining Loss: 0.180890 \tR2: 0.854012\n",
      "Epoch: 5613 \tTraining Loss: 0.192590 \tR2: 0.854012\n",
      "Epoch: 5614 \tTraining Loss: 0.184801 \tR2: 0.854012\n",
      "Epoch: 5615 \tTraining Loss: 0.206702 \tR2: 0.854012\n",
      "Epoch: 5616 \tTraining Loss: 0.195147 \tR2: 0.854012\n",
      "Epoch: 5617 \tTraining Loss: 0.183175 \tR2: 0.854012\n",
      "Epoch: 5618 \tTraining Loss: 0.197668 \tR2: 0.854012\n",
      "Epoch: 5619 \tTraining Loss: 0.190577 \tR2: 0.854012\n",
      "Epoch: 5620 \tTraining Loss: 0.183881 \tR2: 0.854012\n",
      "Epoch: 5621 \tTraining Loss: 0.187769 \tR2: 0.854012\n",
      "Epoch: 5622 \tTraining Loss: 0.200486 \tR2: 0.854012\n",
      "Epoch: 5623 \tTraining Loss: 0.185476 \tR2: 0.854012\n",
      "Epoch: 5624 \tTraining Loss: 0.198794 \tR2: 0.854012\n",
      "Epoch: 5625 \tTraining Loss: 0.203128 \tR2: 0.854012\n",
      "Epoch: 5626 \tTraining Loss: 0.195910 \tR2: 0.854012\n",
      "Epoch: 5627 \tTraining Loss: 0.196488 \tR2: 0.854012\n",
      "Epoch: 5628 \tTraining Loss: 0.196593 \tR2: 0.854012\n",
      "Epoch: 5629 \tTraining Loss: 0.195090 \tR2: 0.854012\n",
      "Epoch: 5630 \tTraining Loss: 0.181428 \tR2: 0.854012\n",
      "Epoch: 5631 \tTraining Loss: 0.183173 \tR2: 0.854012\n",
      "Epoch: 5632 \tTraining Loss: 0.176172 \tR2: 0.854012\n",
      "Epoch: 5633 \tTraining Loss: 0.201995 \tR2: 0.854012\n",
      "Epoch: 5634 \tTraining Loss: 0.200658 \tR2: 0.854012\n",
      "Epoch: 5635 \tTraining Loss: 0.195101 \tR2: 0.854012\n",
      "Epoch: 5636 \tTraining Loss: 0.195463 \tR2: 0.854012\n",
      "Epoch: 5637 \tTraining Loss: 0.225915 \tR2: 0.854012\n",
      "Epoch: 5638 \tTraining Loss: 0.194903 \tR2: 0.854012\n",
      "Epoch: 5639 \tTraining Loss: 0.178944 \tR2: 0.854012\n",
      "Epoch: 5640 \tTraining Loss: 0.197536 \tR2: 0.854012\n",
      "Epoch: 5641 \tTraining Loss: 0.196059 \tR2: 0.854012\n",
      "Epoch: 5642 \tTraining Loss: 0.190903 \tR2: 0.854012\n",
      "Epoch: 5643 \tTraining Loss: 0.213755 \tR2: 0.854012\n",
      "Epoch: 5644 \tTraining Loss: 0.195765 \tR2: 0.854012\n",
      "Epoch: 5645 \tTraining Loss: 0.198161 \tR2: 0.854012\n",
      "Epoch: 5646 \tTraining Loss: 0.194041 \tR2: 0.854012\n",
      "Epoch: 5647 \tTraining Loss: 0.188631 \tR2: 0.854012\n",
      "Epoch: 5648 \tTraining Loss: 0.197617 \tR2: 0.854012\n",
      "Epoch: 5649 \tTraining Loss: 0.201272 \tR2: 0.854012\n",
      "Epoch: 5650 \tTraining Loss: 0.184215 \tR2: 0.854012\n",
      "Epoch: 5651 \tTraining Loss: 0.201274 \tR2: 0.854012\n",
      "Epoch: 5652 \tTraining Loss: 0.205492 \tR2: 0.854012\n",
      "Epoch: 5653 \tTraining Loss: 0.211548 \tR2: 0.854012\n",
      "Epoch: 5654 \tTraining Loss: 0.196729 \tR2: 0.854012\n",
      "Epoch: 5655 \tTraining Loss: 0.194137 \tR2: 0.854012\n",
      "Epoch: 5656 \tTraining Loss: 0.206643 \tR2: 0.854012\n",
      "Epoch: 5657 \tTraining Loss: 0.210160 \tR2: 0.854012\n",
      "Epoch: 5658 \tTraining Loss: 0.192861 \tR2: 0.854012\n",
      "Epoch: 5659 \tTraining Loss: 0.191786 \tR2: 0.854012\n",
      "Epoch: 5660 \tTraining Loss: 0.215274 \tR2: 0.854012\n",
      "Epoch: 5661 \tTraining Loss: 0.183867 \tR2: 0.854012\n",
      "Epoch: 5662 \tTraining Loss: 0.201704 \tR2: 0.854012\n",
      "Epoch: 5663 \tTraining Loss: 0.186992 \tR2: 0.854012\n",
      "Epoch: 5664 \tTraining Loss: 0.183133 \tR2: 0.854012\n",
      "Epoch: 5665 \tTraining Loss: 0.208801 \tR2: 0.854012\n",
      "Epoch: 5666 \tTraining Loss: 0.170098 \tR2: 0.854012\n",
      "Epoch: 5667 \tTraining Loss: 0.202871 \tR2: 0.854012\n",
      "Epoch: 5668 \tTraining Loss: 0.188907 \tR2: 0.854012\n",
      "Epoch: 5669 \tTraining Loss: 0.184481 \tR2: 0.854012\n",
      "Epoch: 5670 \tTraining Loss: 0.193365 \tR2: 0.854012\n",
      "Epoch: 5671 \tTraining Loss: 0.204672 \tR2: 0.854012\n",
      "Epoch: 5672 \tTraining Loss: 0.198912 \tR2: 0.854012\n",
      "Epoch: 5673 \tTraining Loss: 0.204262 \tR2: 0.854012\n",
      "Epoch: 5674 \tTraining Loss: 0.202477 \tR2: 0.854012\n",
      "Epoch: 5675 \tTraining Loss: 0.190315 \tR2: 0.854012\n",
      "Epoch: 5676 \tTraining Loss: 0.171467 \tR2: 0.854012\n",
      "Epoch: 5677 \tTraining Loss: 0.189731 \tR2: 0.854012\n",
      "Epoch: 5678 \tTraining Loss: 0.182458 \tR2: 0.854012\n",
      "Epoch: 5679 \tTraining Loss: 0.189716 \tR2: 0.854012\n",
      "Epoch: 5680 \tTraining Loss: 0.186308 \tR2: 0.854012\n",
      "Epoch: 5681 \tTraining Loss: 0.214891 \tR2: 0.854012\n",
      "Epoch: 5682 \tTraining Loss: 0.181776 \tR2: 0.854012\n",
      "Epoch: 5683 \tTraining Loss: 0.201523 \tR2: 0.854012\n",
      "Epoch: 5684 \tTraining Loss: 0.194397 \tR2: 0.854012\n",
      "Epoch: 5685 \tTraining Loss: 0.197672 \tR2: 0.854012\n",
      "Epoch: 5686 \tTraining Loss: 0.185637 \tR2: 0.854012\n",
      "Epoch: 5687 \tTraining Loss: 0.259100 \tR2: 0.854012\n",
      "Epoch: 5688 \tTraining Loss: 0.289569 \tR2: 0.854012\n",
      "Epoch: 5689 \tTraining Loss: 0.220210 \tR2: 0.854012\n",
      "Epoch: 5690 \tTraining Loss: 0.203144 \tR2: 0.854012\n",
      "Epoch: 5691 \tTraining Loss: 0.209828 \tR2: 0.854012\n",
      "Epoch: 5692 \tTraining Loss: 0.220918 \tR2: 0.854012\n",
      "Epoch: 5693 \tTraining Loss: 0.221425 \tR2: 0.854012\n",
      "Epoch: 5694 \tTraining Loss: 0.198970 \tR2: 0.854012\n",
      "Epoch: 5695 \tTraining Loss: 0.205832 \tR2: 0.854012\n",
      "Epoch: 5696 \tTraining Loss: 0.199257 \tR2: 0.854012\n",
      "Epoch: 5697 \tTraining Loss: 0.197429 \tR2: 0.854012\n",
      "Epoch: 5698 \tTraining Loss: 0.196041 \tR2: 0.854012\n",
      "Epoch: 5699 \tTraining Loss: 0.206419 \tR2: 0.854012\n",
      "Epoch: 5700 \tTraining Loss: 0.198743 \tR2: 0.786006\n",
      "Epoch: 5701 \tTraining Loss: 0.173296 \tR2: 0.786006\n",
      "Epoch: 5702 \tTraining Loss: 0.203149 \tR2: 0.786006\n",
      "Epoch: 5703 \tTraining Loss: 0.196313 \tR2: 0.786006\n",
      "Epoch: 5704 \tTraining Loss: 0.208552 \tR2: 0.786006\n",
      "Epoch: 5705 \tTraining Loss: 0.199221 \tR2: 0.786006\n",
      "Epoch: 5706 \tTraining Loss: 0.200465 \tR2: 0.786006\n",
      "Epoch: 5707 \tTraining Loss: 0.193831 \tR2: 0.786006\n",
      "Epoch: 5708 \tTraining Loss: 0.192970 \tR2: 0.786006\n",
      "Epoch: 5709 \tTraining Loss: 0.179647 \tR2: 0.786006\n",
      "Epoch: 5710 \tTraining Loss: 0.174437 \tR2: 0.786006\n",
      "Epoch: 5711 \tTraining Loss: 0.184559 \tR2: 0.786006\n",
      "Epoch: 5712 \tTraining Loss: 0.209466 \tR2: 0.786006\n",
      "Epoch: 5713 \tTraining Loss: 0.210237 \tR2: 0.786006\n",
      "Epoch: 5714 \tTraining Loss: 0.168007 \tR2: 0.786006\n",
      "Epoch: 5715 \tTraining Loss: 0.186371 \tR2: 0.786006\n",
      "Epoch: 5716 \tTraining Loss: 0.198263 \tR2: 0.786006\n",
      "Epoch: 5717 \tTraining Loss: 0.190344 \tR2: 0.786006\n",
      "Epoch: 5718 \tTraining Loss: 0.214087 \tR2: 0.786006\n",
      "Epoch: 5719 \tTraining Loss: 0.201826 \tR2: 0.786006\n",
      "Epoch: 5720 \tTraining Loss: 0.211907 \tR2: 0.786006\n",
      "Epoch: 5721 \tTraining Loss: 0.207378 \tR2: 0.786006\n",
      "Epoch: 5722 \tTraining Loss: 0.196845 \tR2: 0.786006\n",
      "Epoch: 5723 \tTraining Loss: 0.181981 \tR2: 0.786006\n",
      "Epoch: 5724 \tTraining Loss: 0.197916 \tR2: 0.786006\n",
      "Epoch: 5725 \tTraining Loss: 0.179811 \tR2: 0.786006\n",
      "Epoch: 5726 \tTraining Loss: 0.183100 \tR2: 0.786006\n",
      "Epoch: 5727 \tTraining Loss: 0.177459 \tR2: 0.786006\n",
      "Epoch: 5728 \tTraining Loss: 0.224700 \tR2: 0.786006\n",
      "Epoch: 5729 \tTraining Loss: 0.194613 \tR2: 0.786006\n",
      "Epoch: 5730 \tTraining Loss: 0.168885 \tR2: 0.786006\n",
      "Epoch: 5731 \tTraining Loss: 0.182502 \tR2: 0.786006\n",
      "Epoch: 5732 \tTraining Loss: 0.186915 \tR2: 0.786006\n",
      "Epoch: 5733 \tTraining Loss: 0.188281 \tR2: 0.786006\n",
      "Epoch: 5734 \tTraining Loss: 0.194673 \tR2: 0.786006\n",
      "Epoch: 5735 \tTraining Loss: 0.192604 \tR2: 0.786006\n",
      "Epoch: 5736 \tTraining Loss: 0.187918 \tR2: 0.786006\n",
      "Epoch: 5737 \tTraining Loss: 0.177182 \tR2: 0.786006\n",
      "Epoch: 5738 \tTraining Loss: 0.203141 \tR2: 0.786006\n",
      "Epoch: 5739 \tTraining Loss: 0.207426 \tR2: 0.786006\n",
      "Epoch: 5740 \tTraining Loss: 0.181297 \tR2: 0.786006\n",
      "Epoch: 5741 \tTraining Loss: 0.189506 \tR2: 0.786006\n",
      "Epoch: 5742 \tTraining Loss: 0.197700 \tR2: 0.786006\n",
      "Epoch: 5743 \tTraining Loss: 0.172190 \tR2: 0.786006\n",
      "Epoch: 5744 \tTraining Loss: 0.202849 \tR2: 0.786006\n",
      "Epoch: 5745 \tTraining Loss: 0.191236 \tR2: 0.786006\n",
      "Epoch: 5746 \tTraining Loss: 0.175113 \tR2: 0.786006\n",
      "Epoch: 5747 \tTraining Loss: 0.202871 \tR2: 0.786006\n",
      "Epoch: 5748 \tTraining Loss: 0.221409 \tR2: 0.786006\n",
      "Epoch: 5749 \tTraining Loss: 0.189375 \tR2: 0.786006\n",
      "Epoch: 5750 \tTraining Loss: 0.194972 \tR2: 0.786006\n",
      "Epoch: 5751 \tTraining Loss: 0.179372 \tR2: 0.786006\n",
      "Epoch: 5752 \tTraining Loss: 0.185708 \tR2: 0.786006\n",
      "Epoch: 5753 \tTraining Loss: 0.171581 \tR2: 0.786006\n",
      "Epoch: 5754 \tTraining Loss: 0.180074 \tR2: 0.786006\n",
      "Epoch: 5755 \tTraining Loss: 0.194536 \tR2: 0.786006\n",
      "Epoch: 5756 \tTraining Loss: 0.168011 \tR2: 0.786006\n",
      "Epoch: 5757 \tTraining Loss: 0.183924 \tR2: 0.786006\n",
      "Epoch: 5758 \tTraining Loss: 0.201246 \tR2: 0.786006\n",
      "Epoch: 5759 \tTraining Loss: 0.194753 \tR2: 0.786006\n",
      "Epoch: 5760 \tTraining Loss: 0.211337 \tR2: 0.786006\n",
      "Epoch: 5761 \tTraining Loss: 0.184633 \tR2: 0.786006\n",
      "Epoch: 5762 \tTraining Loss: 0.191669 \tR2: 0.786006\n",
      "Epoch: 5763 \tTraining Loss: 0.204449 \tR2: 0.786006\n",
      "Epoch: 5764 \tTraining Loss: 0.195599 \tR2: 0.786006\n",
      "Epoch: 5765 \tTraining Loss: 0.191607 \tR2: 0.786006\n",
      "Epoch: 5766 \tTraining Loss: 0.185968 \tR2: 0.786006\n",
      "Epoch: 5767 \tTraining Loss: 0.192387 \tR2: 0.786006\n",
      "Epoch: 5768 \tTraining Loss: 0.230113 \tR2: 0.786006\n",
      "Epoch: 5769 \tTraining Loss: 0.204129 \tR2: 0.786006\n",
      "Epoch: 5770 \tTraining Loss: 0.181261 \tR2: 0.786006\n",
      "Epoch: 5771 \tTraining Loss: 0.175454 \tR2: 0.786006\n",
      "Epoch: 5772 \tTraining Loss: 0.194995 \tR2: 0.786006\n",
      "Epoch: 5773 \tTraining Loss: 0.179046 \tR2: 0.786006\n",
      "Epoch: 5774 \tTraining Loss: 0.212514 \tR2: 0.786006\n",
      "Epoch: 5775 \tTraining Loss: 0.196269 \tR2: 0.786006\n",
      "Epoch: 5776 \tTraining Loss: 0.184897 \tR2: 0.786006\n",
      "Epoch: 5777 \tTraining Loss: 0.201402 \tR2: 0.786006\n",
      "Epoch: 5778 \tTraining Loss: 0.173217 \tR2: 0.786006\n",
      "Epoch: 5779 \tTraining Loss: 0.194840 \tR2: 0.786006\n",
      "Epoch: 5780 \tTraining Loss: 0.191955 \tR2: 0.786006\n",
      "Epoch: 5781 \tTraining Loss: 0.196132 \tR2: 0.786006\n",
      "Epoch: 5782 \tTraining Loss: 0.195543 \tR2: 0.786006\n",
      "Epoch: 5783 \tTraining Loss: 0.195741 \tR2: 0.786006\n",
      "Epoch: 5784 \tTraining Loss: 0.177511 \tR2: 0.786006\n",
      "Epoch: 5785 \tTraining Loss: 0.194188 \tR2: 0.786006\n",
      "Epoch: 5786 \tTraining Loss: 0.194753 \tR2: 0.786006\n",
      "Epoch: 5787 \tTraining Loss: 0.176462 \tR2: 0.786006\n",
      "Epoch: 5788 \tTraining Loss: 0.188280 \tR2: 0.786006\n",
      "Epoch: 5789 \tTraining Loss: 0.202787 \tR2: 0.786006\n",
      "Epoch: 5790 \tTraining Loss: 0.179237 \tR2: 0.786006\n",
      "Epoch: 5791 \tTraining Loss: 0.185040 \tR2: 0.786006\n",
      "Epoch: 5792 \tTraining Loss: 0.204243 \tR2: 0.786006\n",
      "Epoch: 5793 \tTraining Loss: 0.195858 \tR2: 0.786006\n",
      "Epoch: 5794 \tTraining Loss: 0.177091 \tR2: 0.786006\n",
      "Epoch: 5795 \tTraining Loss: 0.180484 \tR2: 0.786006\n",
      "Epoch: 5796 \tTraining Loss: 0.188794 \tR2: 0.786006\n",
      "Epoch: 5797 \tTraining Loss: 0.194085 \tR2: 0.786006\n",
      "Epoch: 5798 \tTraining Loss: 0.190858 \tR2: 0.786006\n",
      "Epoch: 5799 \tTraining Loss: 0.205782 \tR2: 0.786006\n",
      "Epoch: 5800 \tTraining Loss: 0.228701 \tR2: 0.661206\n",
      "Epoch: 5801 \tTraining Loss: 0.213909 \tR2: 0.661206\n",
      "Epoch: 5802 \tTraining Loss: 0.186280 \tR2: 0.661206\n",
      "Epoch: 5803 \tTraining Loss: 0.183857 \tR2: 0.661206\n",
      "Epoch: 5804 \tTraining Loss: 0.198471 \tR2: 0.661206\n",
      "Epoch: 5805 \tTraining Loss: 0.209937 \tR2: 0.661206\n",
      "Epoch: 5806 \tTraining Loss: 0.187108 \tR2: 0.661206\n",
      "Epoch: 5807 \tTraining Loss: 0.201081 \tR2: 0.661206\n",
      "Epoch: 5808 \tTraining Loss: 0.210091 \tR2: 0.661206\n",
      "Epoch: 5809 \tTraining Loss: 0.186082 \tR2: 0.661206\n",
      "Epoch: 5810 \tTraining Loss: 0.179506 \tR2: 0.661206\n",
      "Epoch: 5811 \tTraining Loss: 0.197239 \tR2: 0.661206\n",
      "Epoch: 5812 \tTraining Loss: 0.192072 \tR2: 0.661206\n",
      "Epoch: 5813 \tTraining Loss: 0.183983 \tR2: 0.661206\n",
      "Epoch: 5814 \tTraining Loss: 0.209635 \tR2: 0.661206\n",
      "Epoch: 5815 \tTraining Loss: 0.205710 \tR2: 0.661206\n",
      "Epoch: 5816 \tTraining Loss: 0.203014 \tR2: 0.661206\n",
      "Epoch: 5817 \tTraining Loss: 0.197721 \tR2: 0.661206\n",
      "Epoch: 5818 \tTraining Loss: 0.194634 \tR2: 0.661206\n",
      "Epoch: 5819 \tTraining Loss: 0.201360 \tR2: 0.661206\n",
      "Epoch: 5820 \tTraining Loss: 0.178656 \tR2: 0.661206\n",
      "Epoch: 5821 \tTraining Loss: 0.208801 \tR2: 0.661206\n",
      "Epoch: 5822 \tTraining Loss: 0.195860 \tR2: 0.661206\n",
      "Epoch: 5823 \tTraining Loss: 0.179861 \tR2: 0.661206\n",
      "Epoch: 5824 \tTraining Loss: 0.196241 \tR2: 0.661206\n",
      "Epoch: 5825 \tTraining Loss: 0.202705 \tR2: 0.661206\n",
      "Epoch: 5826 \tTraining Loss: 0.185771 \tR2: 0.661206\n",
      "Epoch: 5827 \tTraining Loss: 0.184295 \tR2: 0.661206\n",
      "Epoch: 5828 \tTraining Loss: 0.191127 \tR2: 0.661206\n",
      "Epoch: 5829 \tTraining Loss: 0.188982 \tR2: 0.661206\n",
      "Epoch: 5830 \tTraining Loss: 0.213206 \tR2: 0.661206\n",
      "Epoch: 5831 \tTraining Loss: 0.182329 \tR2: 0.661206\n",
      "Epoch: 5832 \tTraining Loss: 0.217054 \tR2: 0.661206\n",
      "Epoch: 5833 \tTraining Loss: 0.181773 \tR2: 0.661206\n",
      "Epoch: 5834 \tTraining Loss: 0.185363 \tR2: 0.661206\n",
      "Epoch: 5835 \tTraining Loss: 0.189913 \tR2: 0.661206\n",
      "Epoch: 5836 \tTraining Loss: 0.186830 \tR2: 0.661206\n",
      "Epoch: 5837 \tTraining Loss: 0.196515 \tR2: 0.661206\n",
      "Epoch: 5838 \tTraining Loss: 0.191595 \tR2: 0.661206\n",
      "Epoch: 5839 \tTraining Loss: 0.186574 \tR2: 0.661206\n",
      "Epoch: 5840 \tTraining Loss: 0.192137 \tR2: 0.661206\n",
      "Epoch: 5841 \tTraining Loss: 0.195530 \tR2: 0.661206\n",
      "Epoch: 5842 \tTraining Loss: 0.182626 \tR2: 0.661206\n",
      "Epoch: 5843 \tTraining Loss: 0.193833 \tR2: 0.661206\n",
      "Epoch: 5844 \tTraining Loss: 0.181836 \tR2: 0.661206\n",
      "Epoch: 5845 \tTraining Loss: 0.191896 \tR2: 0.661206\n",
      "Epoch: 5846 \tTraining Loss: 0.207615 \tR2: 0.661206\n",
      "Epoch: 5847 \tTraining Loss: 0.198211 \tR2: 0.661206\n",
      "Epoch: 5848 \tTraining Loss: 0.189615 \tR2: 0.661206\n",
      "Epoch: 5849 \tTraining Loss: 0.185687 \tR2: 0.661206\n",
      "Epoch: 5850 \tTraining Loss: 0.221566 \tR2: 0.661206\n",
      "Epoch: 5851 \tTraining Loss: 0.202073 \tR2: 0.661206\n",
      "Epoch: 5852 \tTraining Loss: 0.188004 \tR2: 0.661206\n",
      "Epoch: 5853 \tTraining Loss: 0.193505 \tR2: 0.661206\n",
      "Epoch: 5854 \tTraining Loss: 0.187941 \tR2: 0.661206\n",
      "Epoch: 5855 \tTraining Loss: 0.188192 \tR2: 0.661206\n",
      "Epoch: 5856 \tTraining Loss: 0.173621 \tR2: 0.661206\n",
      "Epoch: 5857 \tTraining Loss: 0.189297 \tR2: 0.661206\n",
      "Epoch: 5858 \tTraining Loss: 0.176083 \tR2: 0.661206\n",
      "Epoch: 5859 \tTraining Loss: 0.201009 \tR2: 0.661206\n",
      "Epoch: 5860 \tTraining Loss: 0.189816 \tR2: 0.661206\n",
      "Epoch: 5861 \tTraining Loss: 0.187983 \tR2: 0.661206\n",
      "Epoch: 5862 \tTraining Loss: 0.196069 \tR2: 0.661206\n",
      "Epoch: 5863 \tTraining Loss: 0.189723 \tR2: 0.661206\n",
      "Epoch: 5864 \tTraining Loss: 0.183252 \tR2: 0.661206\n",
      "Epoch: 5865 \tTraining Loss: 0.205201 \tR2: 0.661206\n",
      "Epoch: 5866 \tTraining Loss: 0.202517 \tR2: 0.661206\n",
      "Epoch: 5867 \tTraining Loss: 0.203126 \tR2: 0.661206\n",
      "Epoch: 5868 \tTraining Loss: 0.201928 \tR2: 0.661206\n",
      "Epoch: 5869 \tTraining Loss: 0.259682 \tR2: 0.661206\n",
      "Epoch: 5870 \tTraining Loss: 0.199317 \tR2: 0.661206\n",
      "Epoch: 5871 \tTraining Loss: 0.181264 \tR2: 0.661206\n",
      "Epoch: 5872 \tTraining Loss: 0.202002 \tR2: 0.661206\n",
      "Epoch: 5873 \tTraining Loss: 0.173912 \tR2: 0.661206\n",
      "Epoch: 5874 \tTraining Loss: 0.218673 \tR2: 0.661206\n",
      "Epoch: 5875 \tTraining Loss: 0.178480 \tR2: 0.661206\n",
      "Epoch: 5876 \tTraining Loss: 0.198029 \tR2: 0.661206\n",
      "Epoch: 5877 \tTraining Loss: 0.208518 \tR2: 0.661206\n",
      "Epoch: 5878 \tTraining Loss: 0.198853 \tR2: 0.661206\n",
      "Epoch: 5879 \tTraining Loss: 0.180144 \tR2: 0.661206\n",
      "Epoch: 5880 \tTraining Loss: 0.204823 \tR2: 0.661206\n",
      "Epoch: 5881 \tTraining Loss: 0.184763 \tR2: 0.661206\n",
      "Epoch: 5882 \tTraining Loss: 0.191093 \tR2: 0.661206\n",
      "Epoch: 5883 \tTraining Loss: 0.187741 \tR2: 0.661206\n",
      "Epoch: 5884 \tTraining Loss: 0.191213 \tR2: 0.661206\n",
      "Epoch: 5885 \tTraining Loss: 0.174308 \tR2: 0.661206\n",
      "Epoch: 5886 \tTraining Loss: 0.190286 \tR2: 0.661206\n",
      "Epoch: 5887 \tTraining Loss: 0.198565 \tR2: 0.661206\n",
      "Epoch: 5888 \tTraining Loss: 0.176263 \tR2: 0.661206\n",
      "Epoch: 5889 \tTraining Loss: 0.210237 \tR2: 0.661206\n",
      "Epoch: 5890 \tTraining Loss: 0.201325 \tR2: 0.661206\n",
      "Epoch: 5891 \tTraining Loss: 0.192275 \tR2: 0.661206\n",
      "Epoch: 5892 \tTraining Loss: 0.175418 \tR2: 0.661206\n",
      "Epoch: 5893 \tTraining Loss: 0.198396 \tR2: 0.661206\n",
      "Epoch: 5894 \tTraining Loss: 0.217144 \tR2: 0.661206\n",
      "Epoch: 5895 \tTraining Loss: 0.194947 \tR2: 0.661206\n",
      "Epoch: 5896 \tTraining Loss: 0.190227 \tR2: 0.661206\n",
      "Epoch: 5897 \tTraining Loss: 0.185706 \tR2: 0.661206\n",
      "Epoch: 5898 \tTraining Loss: 0.210024 \tR2: 0.661206\n",
      "Epoch: 5899 \tTraining Loss: 0.172769 \tR2: 0.661206\n",
      "Epoch: 5900 \tTraining Loss: 0.181493 \tR2: 0.792001\n",
      "Epoch: 5901 \tTraining Loss: 0.180448 \tR2: 0.792001\n",
      "Epoch: 5902 \tTraining Loss: 0.174864 \tR2: 0.792001\n",
      "Epoch: 5903 \tTraining Loss: 0.186115 \tR2: 0.792001\n",
      "Epoch: 5904 \tTraining Loss: 0.189958 \tR2: 0.792001\n",
      "Epoch: 5905 \tTraining Loss: 0.193571 \tR2: 0.792001\n",
      "Epoch: 5906 \tTraining Loss: 0.191118 \tR2: 0.792001\n",
      "Epoch: 5907 \tTraining Loss: 0.207294 \tR2: 0.792001\n",
      "Epoch: 5908 \tTraining Loss: 0.174975 \tR2: 0.792001\n",
      "Epoch: 5909 \tTraining Loss: 0.198413 \tR2: 0.792001\n",
      "Epoch: 5910 \tTraining Loss: 0.203645 \tR2: 0.792001\n",
      "Epoch: 5911 \tTraining Loss: 0.192792 \tR2: 0.792001\n",
      "Epoch: 5912 \tTraining Loss: 0.205822 \tR2: 0.792001\n",
      "Epoch: 5913 \tTraining Loss: 0.185249 \tR2: 0.792001\n",
      "Epoch: 5914 \tTraining Loss: 0.180105 \tR2: 0.792001\n",
      "Epoch: 5915 \tTraining Loss: 0.191325 \tR2: 0.792001\n",
      "Epoch: 5916 \tTraining Loss: 0.195523 \tR2: 0.792001\n",
      "Epoch: 5917 \tTraining Loss: 0.191010 \tR2: 0.792001\n",
      "Epoch: 5918 \tTraining Loss: 0.185826 \tR2: 0.792001\n",
      "Epoch: 5919 \tTraining Loss: 0.190154 \tR2: 0.792001\n",
      "Epoch: 5920 \tTraining Loss: 0.175105 \tR2: 0.792001\n",
      "Epoch: 5921 \tTraining Loss: 0.200708 \tR2: 0.792001\n",
      "Epoch: 5922 \tTraining Loss: 0.185725 \tR2: 0.792001\n",
      "Epoch: 5923 \tTraining Loss: 0.196721 \tR2: 0.792001\n",
      "Epoch: 5924 \tTraining Loss: 0.186388 \tR2: 0.792001\n",
      "Epoch: 5925 \tTraining Loss: 0.223492 \tR2: 0.792001\n",
      "Epoch: 5926 \tTraining Loss: 0.187441 \tR2: 0.792001\n",
      "Epoch: 5927 \tTraining Loss: 0.179210 \tR2: 0.792001\n",
      "Epoch: 5928 \tTraining Loss: 0.187119 \tR2: 0.792001\n",
      "Epoch: 5929 \tTraining Loss: 0.209745 \tR2: 0.792001\n",
      "Epoch: 5930 \tTraining Loss: 0.184214 \tR2: 0.792001\n",
      "Epoch: 5931 \tTraining Loss: 0.189320 \tR2: 0.792001\n",
      "Epoch: 5932 \tTraining Loss: 0.187009 \tR2: 0.792001\n",
      "Epoch: 5933 \tTraining Loss: 0.195220 \tR2: 0.792001\n",
      "Epoch: 5934 \tTraining Loss: 0.198477 \tR2: 0.792001\n",
      "Epoch: 5935 \tTraining Loss: 0.190400 \tR2: 0.792001\n",
      "Epoch: 5936 \tTraining Loss: 0.213425 \tR2: 0.792001\n",
      "Epoch: 5937 \tTraining Loss: 0.183329 \tR2: 0.792001\n",
      "Epoch: 5938 \tTraining Loss: 0.187260 \tR2: 0.792001\n",
      "Epoch: 5939 \tTraining Loss: 0.201031 \tR2: 0.792001\n",
      "Epoch: 5940 \tTraining Loss: 0.209733 \tR2: 0.792001\n",
      "Epoch: 5941 \tTraining Loss: 0.229872 \tR2: 0.792001\n",
      "Epoch: 5942 \tTraining Loss: 0.209798 \tR2: 0.792001\n",
      "Epoch: 5943 \tTraining Loss: 0.194226 \tR2: 0.792001\n",
      "Epoch: 5944 \tTraining Loss: 0.184483 \tR2: 0.792001\n",
      "Epoch: 5945 \tTraining Loss: 0.196722 \tR2: 0.792001\n",
      "Epoch: 5946 \tTraining Loss: 0.188422 \tR2: 0.792001\n",
      "Epoch: 5947 \tTraining Loss: 0.195232 \tR2: 0.792001\n",
      "Epoch: 5948 \tTraining Loss: 0.186251 \tR2: 0.792001\n",
      "Epoch: 5949 \tTraining Loss: 0.200388 \tR2: 0.792001\n",
      "Epoch: 5950 \tTraining Loss: 0.185296 \tR2: 0.792001\n",
      "Epoch: 5951 \tTraining Loss: 0.190443 \tR2: 0.792001\n",
      "Epoch: 5952 \tTraining Loss: 0.189810 \tR2: 0.792001\n",
      "Epoch: 5953 \tTraining Loss: 0.195233 \tR2: 0.792001\n",
      "Epoch: 5954 \tTraining Loss: 0.195426 \tR2: 0.792001\n",
      "Epoch: 5955 \tTraining Loss: 0.194257 \tR2: 0.792001\n",
      "Epoch: 5956 \tTraining Loss: 0.202263 \tR2: 0.792001\n",
      "Epoch: 5957 \tTraining Loss: 0.188772 \tR2: 0.792001\n",
      "Epoch: 5958 \tTraining Loss: 0.202856 \tR2: 0.792001\n",
      "Epoch: 5959 \tTraining Loss: 0.180227 \tR2: 0.792001\n",
      "Epoch: 5960 \tTraining Loss: 0.181569 \tR2: 0.792001\n",
      "Epoch: 5961 \tTraining Loss: 0.190555 \tR2: 0.792001\n",
      "Epoch: 5962 \tTraining Loss: 0.190384 \tR2: 0.792001\n",
      "Epoch: 5963 \tTraining Loss: 0.175620 \tR2: 0.792001\n",
      "Epoch: 5964 \tTraining Loss: 0.208080 \tR2: 0.792001\n",
      "Epoch: 5965 \tTraining Loss: 0.210215 \tR2: 0.792001\n",
      "Epoch: 5966 \tTraining Loss: 0.183595 \tR2: 0.792001\n",
      "Epoch: 5967 \tTraining Loss: 0.182187 \tR2: 0.792001\n",
      "Epoch: 5968 \tTraining Loss: 0.203255 \tR2: 0.792001\n",
      "Epoch: 5969 \tTraining Loss: 0.175505 \tR2: 0.792001\n",
      "Epoch: 5970 \tTraining Loss: 0.188001 \tR2: 0.792001\n",
      "Epoch: 5971 \tTraining Loss: 0.179132 \tR2: 0.792001\n",
      "Epoch: 5972 \tTraining Loss: 0.214013 \tR2: 0.792001\n",
      "Epoch: 5973 \tTraining Loss: 0.188367 \tR2: 0.792001\n",
      "Epoch: 5974 \tTraining Loss: 0.178124 \tR2: 0.792001\n",
      "Epoch: 5975 \tTraining Loss: 0.176441 \tR2: 0.792001\n",
      "Epoch: 5976 \tTraining Loss: 0.183847 \tR2: 0.792001\n",
      "Epoch: 5977 \tTraining Loss: 0.201454 \tR2: 0.792001\n",
      "Epoch: 5978 \tTraining Loss: 0.189649 \tR2: 0.792001\n",
      "Epoch: 5979 \tTraining Loss: 0.188219 \tR2: 0.792001\n",
      "Epoch: 5980 \tTraining Loss: 0.187326 \tR2: 0.792001\n",
      "Epoch: 5981 \tTraining Loss: 0.182626 \tR2: 0.792001\n",
      "Epoch: 5982 \tTraining Loss: 0.200241 \tR2: 0.792001\n",
      "Epoch: 5983 \tTraining Loss: 0.179275 \tR2: 0.792001\n",
      "Epoch: 5984 \tTraining Loss: 0.202235 \tR2: 0.792001\n",
      "Epoch: 5985 \tTraining Loss: 0.198788 \tR2: 0.792001\n",
      "Epoch: 5986 \tTraining Loss: 0.200491 \tR2: 0.792001\n",
      "Epoch: 5987 \tTraining Loss: 0.205158 \tR2: 0.792001\n",
      "Epoch: 5988 \tTraining Loss: 0.176925 \tR2: 0.792001\n",
      "Epoch: 5989 \tTraining Loss: 0.177904 \tR2: 0.792001\n",
      "Epoch: 5990 \tTraining Loss: 0.202071 \tR2: 0.792001\n",
      "Epoch: 5991 \tTraining Loss: 0.218271 \tR2: 0.792001\n",
      "Epoch: 5992 \tTraining Loss: 0.185710 \tR2: 0.792001\n",
      "Epoch: 5993 \tTraining Loss: 0.211266 \tR2: 0.792001\n",
      "Epoch: 5994 \tTraining Loss: 0.209929 \tR2: 0.792001\n",
      "Epoch: 5995 \tTraining Loss: 0.190006 \tR2: 0.792001\n",
      "Epoch: 5996 \tTraining Loss: 0.194242 \tR2: 0.792001\n",
      "Epoch: 5997 \tTraining Loss: 0.194198 \tR2: 0.792001\n",
      "Epoch: 5998 \tTraining Loss: 0.177979 \tR2: 0.792001\n",
      "Epoch: 5999 \tTraining Loss: 0.181561 \tR2: 0.792001\n",
      "Epoch: 6000 \tTraining Loss: 0.186830 \tR2: 0.889288\n",
      "Epoch: 6001 \tTraining Loss: 0.188026 \tR2: 0.889288\n",
      "Epoch: 6002 \tTraining Loss: 0.175476 \tR2: 0.889288\n",
      "Epoch: 6003 \tTraining Loss: 0.195374 \tR2: 0.889288\n",
      "Epoch: 6004 \tTraining Loss: 0.189166 \tR2: 0.889288\n",
      "Epoch: 6005 \tTraining Loss: 0.189585 \tR2: 0.889288\n",
      "Epoch: 6006 \tTraining Loss: 0.188845 \tR2: 0.889288\n",
      "Epoch: 6007 \tTraining Loss: 0.178523 \tR2: 0.889288\n",
      "Epoch: 6008 \tTraining Loss: 0.199146 \tR2: 0.889288\n",
      "Epoch: 6009 \tTraining Loss: 0.193187 \tR2: 0.889288\n",
      "Epoch: 6010 \tTraining Loss: 0.195441 \tR2: 0.889288\n",
      "Epoch: 6011 \tTraining Loss: 0.197141 \tR2: 0.889288\n",
      "Epoch: 6012 \tTraining Loss: 0.202780 \tR2: 0.889288\n",
      "Epoch: 6013 \tTraining Loss: 0.184050 \tR2: 0.889288\n",
      "Epoch: 6014 \tTraining Loss: 0.186912 \tR2: 0.889288\n",
      "Epoch: 6015 \tTraining Loss: 0.181498 \tR2: 0.889288\n",
      "Epoch: 6016 \tTraining Loss: 0.193533 \tR2: 0.889288\n",
      "Epoch: 6017 \tTraining Loss: 0.205274 \tR2: 0.889288\n",
      "Epoch: 6018 \tTraining Loss: 0.186170 \tR2: 0.889288\n",
      "Epoch: 6019 \tTraining Loss: 0.191988 \tR2: 0.889288\n",
      "Epoch: 6020 \tTraining Loss: 0.197422 \tR2: 0.889288\n",
      "Epoch: 6021 \tTraining Loss: 0.188263 \tR2: 0.889288\n",
      "Epoch: 6022 \tTraining Loss: 0.219280 \tR2: 0.889288\n",
      "Epoch: 6023 \tTraining Loss: 0.200008 \tR2: 0.889288\n",
      "Epoch: 6024 \tTraining Loss: 0.192720 \tR2: 0.889288\n",
      "Epoch: 6025 \tTraining Loss: 0.195343 \tR2: 0.889288\n",
      "Epoch: 6026 \tTraining Loss: 0.182295 \tR2: 0.889288\n",
      "Epoch: 6027 \tTraining Loss: 0.193717 \tR2: 0.889288\n",
      "Epoch: 6028 \tTraining Loss: 0.191349 \tR2: 0.889288\n",
      "Epoch: 6029 \tTraining Loss: 0.180150 \tR2: 0.889288\n",
      "Epoch: 6030 \tTraining Loss: 0.210060 \tR2: 0.889288\n",
      "Epoch: 6031 \tTraining Loss: 0.198080 \tR2: 0.889288\n",
      "Epoch: 6032 \tTraining Loss: 0.195239 \tR2: 0.889288\n",
      "Epoch: 6033 \tTraining Loss: 0.199952 \tR2: 0.889288\n",
      "Epoch: 6034 \tTraining Loss: 0.201086 \tR2: 0.889288\n",
      "Epoch: 6035 \tTraining Loss: 0.188371 \tR2: 0.889288\n",
      "Epoch: 6036 \tTraining Loss: 0.195409 \tR2: 0.889288\n",
      "Epoch: 6037 \tTraining Loss: 0.183885 \tR2: 0.889288\n",
      "Epoch: 6038 \tTraining Loss: 0.195853 \tR2: 0.889288\n",
      "Epoch: 6039 \tTraining Loss: 0.201632 \tR2: 0.889288\n",
      "Epoch: 6040 \tTraining Loss: 0.199284 \tR2: 0.889288\n",
      "Epoch: 6041 \tTraining Loss: 0.200952 \tR2: 0.889288\n",
      "Epoch: 6042 \tTraining Loss: 0.233195 \tR2: 0.889288\n",
      "Epoch: 6043 \tTraining Loss: 0.203070 \tR2: 0.889288\n",
      "Epoch: 6044 \tTraining Loss: 0.186582 \tR2: 0.889288\n",
      "Epoch: 6045 \tTraining Loss: 0.184817 \tR2: 0.889288\n",
      "Epoch: 6046 \tTraining Loss: 0.192488 \tR2: 0.889288\n",
      "Epoch: 6047 \tTraining Loss: 0.181886 \tR2: 0.889288\n",
      "Epoch: 6048 \tTraining Loss: 0.183033 \tR2: 0.889288\n",
      "Epoch: 6049 \tTraining Loss: 0.207899 \tR2: 0.889288\n",
      "Epoch: 6050 \tTraining Loss: 0.182850 \tR2: 0.889288\n",
      "Epoch: 6051 \tTraining Loss: 0.186419 \tR2: 0.889288\n",
      "Epoch: 6052 \tTraining Loss: 0.180590 \tR2: 0.889288\n",
      "Epoch: 6053 \tTraining Loss: 0.182286 \tR2: 0.889288\n",
      "Epoch: 6054 \tTraining Loss: 0.202741 \tR2: 0.889288\n",
      "Epoch: 6055 \tTraining Loss: 0.178225 \tR2: 0.889288\n",
      "Epoch: 6056 \tTraining Loss: 0.196847 \tR2: 0.889288\n",
      "Epoch: 6057 \tTraining Loss: 0.189587 \tR2: 0.889288\n",
      "Epoch: 6058 \tTraining Loss: 0.194104 \tR2: 0.889288\n",
      "Epoch: 6059 \tTraining Loss: 0.197455 \tR2: 0.889288\n",
      "Epoch: 6060 \tTraining Loss: 0.210422 \tR2: 0.889288\n",
      "Epoch: 6061 \tTraining Loss: 0.210177 \tR2: 0.889288\n",
      "Epoch: 6062 \tTraining Loss: 0.196918 \tR2: 0.889288\n",
      "Epoch: 6063 \tTraining Loss: 0.192693 \tR2: 0.889288\n",
      "Epoch: 6064 \tTraining Loss: 0.177056 \tR2: 0.889288\n",
      "Epoch: 6065 \tTraining Loss: 0.184923 \tR2: 0.889288\n",
      "Epoch: 6066 \tTraining Loss: 0.186278 \tR2: 0.889288\n",
      "Epoch: 6067 \tTraining Loss: 0.186573 \tR2: 0.889288\n",
      "Epoch: 6068 \tTraining Loss: 0.175105 \tR2: 0.889288\n",
      "Epoch: 6069 \tTraining Loss: 0.189040 \tR2: 0.889288\n",
      "Epoch: 6070 \tTraining Loss: 0.190392 \tR2: 0.889288\n",
      "Epoch: 6071 \tTraining Loss: 0.201730 \tR2: 0.889288\n",
      "Epoch: 6072 \tTraining Loss: 0.211809 \tR2: 0.889288\n",
      "Epoch: 6073 \tTraining Loss: 0.188365 \tR2: 0.889288\n",
      "Epoch: 6074 \tTraining Loss: 0.198590 \tR2: 0.889288\n",
      "Epoch: 6075 \tTraining Loss: 0.183581 \tR2: 0.889288\n",
      "Epoch: 6076 \tTraining Loss: 0.201039 \tR2: 0.889288\n",
      "Epoch: 6077 \tTraining Loss: 0.218777 \tR2: 0.889288\n",
      "Epoch: 6078 \tTraining Loss: 0.186979 \tR2: 0.889288\n",
      "Epoch: 6079 \tTraining Loss: 0.186194 \tR2: 0.889288\n",
      "Epoch: 6080 \tTraining Loss: 0.191044 \tR2: 0.889288\n",
      "Epoch: 6081 \tTraining Loss: 0.196507 \tR2: 0.889288\n",
      "Epoch: 6082 \tTraining Loss: 0.190966 \tR2: 0.889288\n",
      "Epoch: 6083 \tTraining Loss: 0.195813 \tR2: 0.889288\n",
      "Epoch: 6084 \tTraining Loss: 0.181407 \tR2: 0.889288\n",
      "Epoch: 6085 \tTraining Loss: 0.209111 \tR2: 0.889288\n",
      "Epoch: 6086 \tTraining Loss: 0.181449 \tR2: 0.889288\n",
      "Epoch: 6087 \tTraining Loss: 0.179224 \tR2: 0.889288\n",
      "Epoch: 6088 \tTraining Loss: 0.218448 \tR2: 0.889288\n",
      "Epoch: 6089 \tTraining Loss: 0.202361 \tR2: 0.889288\n",
      "Epoch: 6090 \tTraining Loss: 0.199273 \tR2: 0.889288\n",
      "Epoch: 6091 \tTraining Loss: 0.180627 \tR2: 0.889288\n",
      "Epoch: 6092 \tTraining Loss: 0.189687 \tR2: 0.889288\n",
      "Epoch: 6093 \tTraining Loss: 0.185850 \tR2: 0.889288\n",
      "Epoch: 6094 \tTraining Loss: 0.176779 \tR2: 0.889288\n",
      "Epoch: 6095 \tTraining Loss: 0.201414 \tR2: 0.889288\n",
      "Epoch: 6096 \tTraining Loss: 0.189556 \tR2: 0.889288\n",
      "Epoch: 6097 \tTraining Loss: 0.188173 \tR2: 0.889288\n",
      "Epoch: 6098 \tTraining Loss: 0.197899 \tR2: 0.889288\n",
      "Epoch: 6099 \tTraining Loss: 0.218363 \tR2: 0.889288\n",
      "Epoch: 6100 \tTraining Loss: 0.188407 \tR2: 0.728891\n",
      "Epoch: 6101 \tTraining Loss: 0.193497 \tR2: 0.728891\n",
      "Epoch: 6102 \tTraining Loss: 0.180643 \tR2: 0.728891\n",
      "Epoch: 6103 \tTraining Loss: 0.197871 \tR2: 0.728891\n",
      "Epoch: 6104 \tTraining Loss: 0.182730 \tR2: 0.728891\n",
      "Epoch: 6105 \tTraining Loss: 0.202279 \tR2: 0.728891\n",
      "Epoch: 6106 \tTraining Loss: 0.193053 \tR2: 0.728891\n",
      "Epoch: 6107 \tTraining Loss: 0.201507 \tR2: 0.728891\n",
      "Epoch: 6108 \tTraining Loss: 0.203399 \tR2: 0.728891\n",
      "Epoch: 6109 \tTraining Loss: 0.182730 \tR2: 0.728891\n",
      "Epoch: 6110 \tTraining Loss: 0.179862 \tR2: 0.728891\n",
      "Epoch: 6111 \tTraining Loss: 0.194889 \tR2: 0.728891\n",
      "Epoch: 6112 \tTraining Loss: 0.197081 \tR2: 0.728891\n",
      "Epoch: 6113 \tTraining Loss: 0.188365 \tR2: 0.728891\n",
      "Epoch: 6114 \tTraining Loss: 0.159834 \tR2: 0.728891\n",
      "Epoch: 6115 \tTraining Loss: 0.216836 \tR2: 0.728891\n",
      "Epoch: 6116 \tTraining Loss: 0.196496 \tR2: 0.728891\n",
      "Epoch: 6117 \tTraining Loss: 0.203015 \tR2: 0.728891\n",
      "Epoch: 6118 \tTraining Loss: 0.204085 \tR2: 0.728891\n",
      "Epoch: 6119 \tTraining Loss: 0.193345 \tR2: 0.728891\n",
      "Epoch: 6120 \tTraining Loss: 0.202350 \tR2: 0.728891\n",
      "Epoch: 6121 \tTraining Loss: 0.187897 \tR2: 0.728891\n",
      "Epoch: 6122 \tTraining Loss: 0.199198 \tR2: 0.728891\n",
      "Epoch: 6123 \tTraining Loss: 0.199224 \tR2: 0.728891\n",
      "Epoch: 6124 \tTraining Loss: 0.206974 \tR2: 0.728891\n",
      "Epoch: 6125 \tTraining Loss: 0.187564 \tR2: 0.728891\n",
      "Epoch: 6126 \tTraining Loss: 0.189507 \tR2: 0.728891\n",
      "Epoch: 6127 \tTraining Loss: 0.186606 \tR2: 0.728891\n",
      "Epoch: 6128 \tTraining Loss: 0.198997 \tR2: 0.728891\n",
      "Epoch: 6129 \tTraining Loss: 0.189982 \tR2: 0.728891\n",
      "Epoch: 6130 \tTraining Loss: 0.176999 \tR2: 0.728891\n",
      "Epoch: 6131 \tTraining Loss: 0.179553 \tR2: 0.728891\n",
      "Epoch: 6132 \tTraining Loss: 0.184485 \tR2: 0.728891\n",
      "Epoch: 6133 \tTraining Loss: 0.204757 \tR2: 0.728891\n",
      "Epoch: 6134 \tTraining Loss: 0.177649 \tR2: 0.728891\n",
      "Epoch: 6135 \tTraining Loss: 0.189563 \tR2: 0.728891\n",
      "Epoch: 6136 \tTraining Loss: 0.203047 \tR2: 0.728891\n",
      "Epoch: 6137 \tTraining Loss: 0.202707 \tR2: 0.728891\n",
      "Epoch: 6138 \tTraining Loss: 0.178981 \tR2: 0.728891\n",
      "Epoch: 6139 \tTraining Loss: 0.211556 \tR2: 0.728891\n",
      "Epoch: 6140 \tTraining Loss: 0.195133 \tR2: 0.728891\n",
      "Epoch: 6141 \tTraining Loss: 0.191915 \tR2: 0.728891\n",
      "Epoch: 6142 \tTraining Loss: 0.202419 \tR2: 0.728891\n",
      "Epoch: 6143 \tTraining Loss: 0.198553 \tR2: 0.728891\n",
      "Epoch: 6144 \tTraining Loss: 0.189894 \tR2: 0.728891\n",
      "Epoch: 6145 \tTraining Loss: 0.201368 \tR2: 0.728891\n",
      "Epoch: 6146 \tTraining Loss: 0.199887 \tR2: 0.728891\n",
      "Epoch: 6147 \tTraining Loss: 0.212143 \tR2: 0.728891\n",
      "Epoch: 6148 \tTraining Loss: 0.187049 \tR2: 0.728891\n",
      "Epoch: 6149 \tTraining Loss: 0.204256 \tR2: 0.728891\n",
      "Epoch: 6150 \tTraining Loss: 0.195279 \tR2: 0.728891\n",
      "Epoch: 6151 \tTraining Loss: 0.189307 \tR2: 0.728891\n",
      "Epoch: 6152 \tTraining Loss: 0.197260 \tR2: 0.728891\n",
      "Epoch: 6153 \tTraining Loss: 0.205738 \tR2: 0.728891\n",
      "Epoch: 6154 \tTraining Loss: 0.189538 \tR2: 0.728891\n",
      "Epoch: 6155 \tTraining Loss: 0.183219 \tR2: 0.728891\n",
      "Epoch: 6156 \tTraining Loss: 0.192072 \tR2: 0.728891\n",
      "Epoch: 6157 \tTraining Loss: 0.192734 \tR2: 0.728891\n",
      "Epoch: 6158 \tTraining Loss: 0.182819 \tR2: 0.728891\n",
      "Epoch: 6159 \tTraining Loss: 0.198457 \tR2: 0.728891\n",
      "Epoch: 6160 \tTraining Loss: 0.212931 \tR2: 0.728891\n",
      "Epoch: 6161 \tTraining Loss: 0.203131 \tR2: 0.728891\n",
      "Epoch: 6162 \tTraining Loss: 0.189615 \tR2: 0.728891\n",
      "Epoch: 6163 \tTraining Loss: 0.195413 \tR2: 0.728891\n",
      "Epoch: 6164 \tTraining Loss: 0.179860 \tR2: 0.728891\n",
      "Epoch: 6165 \tTraining Loss: 0.194415 \tR2: 0.728891\n",
      "Epoch: 6166 \tTraining Loss: 0.211739 \tR2: 0.728891\n",
      "Epoch: 6167 \tTraining Loss: 0.186138 \tR2: 0.728891\n",
      "Epoch: 6168 \tTraining Loss: 0.183094 \tR2: 0.728891\n",
      "Epoch: 6169 \tTraining Loss: 0.204360 \tR2: 0.728891\n",
      "Epoch: 6170 \tTraining Loss: 0.194712 \tR2: 0.728891\n",
      "Epoch: 6171 \tTraining Loss: 0.203892 \tR2: 0.728891\n",
      "Epoch: 6172 \tTraining Loss: 0.182271 \tR2: 0.728891\n",
      "Epoch: 6173 \tTraining Loss: 0.191034 \tR2: 0.728891\n",
      "Epoch: 6174 \tTraining Loss: 0.193128 \tR2: 0.728891\n",
      "Epoch: 6175 \tTraining Loss: 0.200866 \tR2: 0.728891\n",
      "Epoch: 6176 \tTraining Loss: 0.188024 \tR2: 0.728891\n",
      "Epoch: 6177 \tTraining Loss: 0.192556 \tR2: 0.728891\n",
      "Epoch: 6178 \tTraining Loss: 0.186808 \tR2: 0.728891\n",
      "Epoch: 6179 \tTraining Loss: 0.201243 \tR2: 0.728891\n",
      "Epoch: 6180 \tTraining Loss: 0.204618 \tR2: 0.728891\n",
      "Epoch: 6181 \tTraining Loss: 0.182647 \tR2: 0.728891\n",
      "Epoch: 6182 \tTraining Loss: 0.205395 \tR2: 0.728891\n",
      "Epoch: 6183 \tTraining Loss: 0.203262 \tR2: 0.728891\n",
      "Epoch: 6184 \tTraining Loss: 0.186750 \tR2: 0.728891\n",
      "Epoch: 6185 \tTraining Loss: 0.181189 \tR2: 0.728891\n",
      "Epoch: 6186 \tTraining Loss: 0.218929 \tR2: 0.728891\n",
      "Epoch: 6187 \tTraining Loss: 0.188830 \tR2: 0.728891\n",
      "Epoch: 6188 \tTraining Loss: 0.179058 \tR2: 0.728891\n",
      "Epoch: 6189 \tTraining Loss: 0.201607 \tR2: 0.728891\n",
      "Epoch: 6190 \tTraining Loss: 0.196790 \tR2: 0.728891\n",
      "Epoch: 6191 \tTraining Loss: 0.186338 \tR2: 0.728891\n",
      "Epoch: 6192 \tTraining Loss: 0.177230 \tR2: 0.728891\n",
      "Epoch: 6193 \tTraining Loss: 0.196773 \tR2: 0.728891\n",
      "Epoch: 6194 \tTraining Loss: 0.184730 \tR2: 0.728891\n",
      "Epoch: 6195 \tTraining Loss: 0.193768 \tR2: 0.728891\n",
      "Epoch: 6196 \tTraining Loss: 0.188159 \tR2: 0.728891\n",
      "Epoch: 6197 \tTraining Loss: 0.180034 \tR2: 0.728891\n",
      "Epoch: 6198 \tTraining Loss: 0.193622 \tR2: 0.728891\n",
      "Epoch: 6199 \tTraining Loss: 0.201090 \tR2: 0.728891\n",
      "Epoch: 6200 \tTraining Loss: 0.182054 \tR2: 0.808010\n",
      "Epoch: 6201 \tTraining Loss: 0.199278 \tR2: 0.808010\n",
      "Epoch: 6202 \tTraining Loss: 0.186955 \tR2: 0.808010\n",
      "Epoch: 6203 \tTraining Loss: 0.173290 \tR2: 0.808010\n",
      "Epoch: 6204 \tTraining Loss: 0.187375 \tR2: 0.808010\n",
      "Epoch: 6205 \tTraining Loss: 0.182550 \tR2: 0.808010\n",
      "Epoch: 6206 \tTraining Loss: 0.217040 \tR2: 0.808010\n",
      "Epoch: 6207 \tTraining Loss: 0.201191 \tR2: 0.808010\n",
      "Epoch: 6208 \tTraining Loss: 0.188710 \tR2: 0.808010\n",
      "Epoch: 6209 \tTraining Loss: 0.188565 \tR2: 0.808010\n",
      "Epoch: 6210 \tTraining Loss: 0.186441 \tR2: 0.808010\n",
      "Epoch: 6211 \tTraining Loss: 0.189483 \tR2: 0.808010\n",
      "Epoch: 6212 \tTraining Loss: 0.201681 \tR2: 0.808010\n",
      "Epoch: 6213 \tTraining Loss: 0.192820 \tR2: 0.808010\n",
      "Epoch: 6214 \tTraining Loss: 0.193576 \tR2: 0.808010\n",
      "Epoch: 6215 \tTraining Loss: 0.190322 \tR2: 0.808010\n",
      "Epoch: 6216 \tTraining Loss: 0.188964 \tR2: 0.808010\n",
      "Epoch: 6217 \tTraining Loss: 0.223668 \tR2: 0.808010\n",
      "Epoch: 6218 \tTraining Loss: 0.185700 \tR2: 0.808010\n",
      "Epoch: 6219 \tTraining Loss: 0.188599 \tR2: 0.808010\n",
      "Epoch: 6220 \tTraining Loss: 0.182879 \tR2: 0.808010\n",
      "Epoch: 6221 \tTraining Loss: 0.194266 \tR2: 0.808010\n",
      "Epoch: 6222 \tTraining Loss: 0.183430 \tR2: 0.808010\n",
      "Epoch: 6223 \tTraining Loss: 0.195284 \tR2: 0.808010\n",
      "Epoch: 6224 \tTraining Loss: 0.182770 \tR2: 0.808010\n",
      "Epoch: 6225 \tTraining Loss: 0.205282 \tR2: 0.808010\n",
      "Epoch: 6226 \tTraining Loss: 0.184848 \tR2: 0.808010\n",
      "Epoch: 6227 \tTraining Loss: 0.226535 \tR2: 0.808010\n",
      "Epoch: 6228 \tTraining Loss: 0.209456 \tR2: 0.808010\n",
      "Epoch: 6229 \tTraining Loss: 0.171343 \tR2: 0.808010\n",
      "Epoch: 6230 \tTraining Loss: 0.202582 \tR2: 0.808010\n",
      "Epoch: 6231 \tTraining Loss: 0.175649 \tR2: 0.808010\n",
      "Epoch: 6232 \tTraining Loss: 0.194270 \tR2: 0.808010\n",
      "Epoch: 6233 \tTraining Loss: 0.197476 \tR2: 0.808010\n",
      "Epoch: 6234 \tTraining Loss: 0.188003 \tR2: 0.808010\n",
      "Epoch: 6235 \tTraining Loss: 0.188799 \tR2: 0.808010\n",
      "Epoch: 6236 \tTraining Loss: 0.199157 \tR2: 0.808010\n",
      "Epoch: 6237 \tTraining Loss: 0.182884 \tR2: 0.808010\n",
      "Epoch: 6238 \tTraining Loss: 0.188033 \tR2: 0.808010\n",
      "Epoch: 6239 \tTraining Loss: 0.191791 \tR2: 0.808010\n",
      "Epoch: 6240 \tTraining Loss: 0.209791 \tR2: 0.808010\n",
      "Epoch: 6241 \tTraining Loss: 0.194101 \tR2: 0.808010\n",
      "Epoch: 6242 \tTraining Loss: 0.185335 \tR2: 0.808010\n",
      "Epoch: 6243 \tTraining Loss: 0.186201 \tR2: 0.808010\n",
      "Epoch: 6244 \tTraining Loss: 0.179021 \tR2: 0.808010\n",
      "Epoch: 6245 \tTraining Loss: 0.204775 \tR2: 0.808010\n",
      "Epoch: 6246 \tTraining Loss: 0.167025 \tR2: 0.808010\n",
      "Epoch: 6247 \tTraining Loss: 0.185668 \tR2: 0.808010\n",
      "Epoch: 6248 \tTraining Loss: 0.206640 \tR2: 0.808010\n",
      "Epoch: 6249 \tTraining Loss: 0.198549 \tR2: 0.808010\n",
      "Epoch: 6250 \tTraining Loss: 0.198784 \tR2: 0.808010\n",
      "Epoch: 6251 \tTraining Loss: 0.177974 \tR2: 0.808010\n",
      "Epoch: 6252 \tTraining Loss: 0.197389 \tR2: 0.808010\n",
      "Epoch: 6253 \tTraining Loss: 0.210158 \tR2: 0.808010\n",
      "Epoch: 6254 \tTraining Loss: 0.180630 \tR2: 0.808010\n",
      "Epoch: 6255 \tTraining Loss: 0.174052 \tR2: 0.808010\n",
      "Epoch: 6256 \tTraining Loss: 0.185480 \tR2: 0.808010\n",
      "Epoch: 6257 \tTraining Loss: 0.209894 \tR2: 0.808010\n",
      "Epoch: 6258 \tTraining Loss: 0.205662 \tR2: 0.808010\n",
      "Epoch: 6259 \tTraining Loss: 0.182776 \tR2: 0.808010\n",
      "Epoch: 6260 \tTraining Loss: 0.208877 \tR2: 0.808010\n",
      "Epoch: 6261 \tTraining Loss: 0.187821 \tR2: 0.808010\n",
      "Epoch: 6262 \tTraining Loss: 0.198674 \tR2: 0.808010\n",
      "Epoch: 6263 \tTraining Loss: 0.189225 \tR2: 0.808010\n",
      "Epoch: 6264 \tTraining Loss: 0.208033 \tR2: 0.808010\n",
      "Epoch: 6265 \tTraining Loss: 0.173107 \tR2: 0.808010\n",
      "Epoch: 6266 \tTraining Loss: 0.187166 \tR2: 0.808010\n",
      "Epoch: 6267 \tTraining Loss: 0.189391 \tR2: 0.808010\n",
      "Epoch: 6268 \tTraining Loss: 0.178579 \tR2: 0.808010\n",
      "Epoch: 6269 \tTraining Loss: 0.184573 \tR2: 0.808010\n",
      "Epoch: 6270 \tTraining Loss: 0.186549 \tR2: 0.808010\n",
      "Epoch: 6271 \tTraining Loss: 0.196840 \tR2: 0.808010\n",
      "Epoch: 6272 \tTraining Loss: 0.198128 \tR2: 0.808010\n",
      "Epoch: 6273 \tTraining Loss: 0.190758 \tR2: 0.808010\n",
      "Epoch: 6274 \tTraining Loss: 0.182058 \tR2: 0.808010\n",
      "Epoch: 6275 \tTraining Loss: 0.186750 \tR2: 0.808010\n",
      "Epoch: 6276 \tTraining Loss: 0.181442 \tR2: 0.808010\n",
      "Epoch: 6277 \tTraining Loss: 0.173645 \tR2: 0.808010\n",
      "Epoch: 6278 \tTraining Loss: 0.192610 \tR2: 0.808010\n",
      "Epoch: 6279 \tTraining Loss: 0.176597 \tR2: 0.808010\n",
      "Epoch: 6280 \tTraining Loss: 0.201645 \tR2: 0.808010\n",
      "Epoch: 6281 \tTraining Loss: 0.190410 \tR2: 0.808010\n",
      "Epoch: 6282 \tTraining Loss: 0.208083 \tR2: 0.808010\n",
      "Epoch: 6283 \tTraining Loss: 0.185898 \tR2: 0.808010\n",
      "Epoch: 6284 \tTraining Loss: 0.194591 \tR2: 0.808010\n",
      "Epoch: 6285 \tTraining Loss: 0.188858 \tR2: 0.808010\n",
      "Epoch: 6286 \tTraining Loss: 0.181769 \tR2: 0.808010\n",
      "Epoch: 6287 \tTraining Loss: 0.185358 \tR2: 0.808010\n",
      "Epoch: 6288 \tTraining Loss: 0.180460 \tR2: 0.808010\n",
      "Epoch: 6289 \tTraining Loss: 0.190547 \tR2: 0.808010\n",
      "Epoch: 6290 \tTraining Loss: 0.173712 \tR2: 0.808010\n",
      "Epoch: 6291 \tTraining Loss: 0.188759 \tR2: 0.808010\n",
      "Epoch: 6292 \tTraining Loss: 0.188670 \tR2: 0.808010\n",
      "Epoch: 6293 \tTraining Loss: 0.182484 \tR2: 0.808010\n",
      "Epoch: 6294 \tTraining Loss: 0.171920 \tR2: 0.808010\n",
      "Epoch: 6295 \tTraining Loss: 0.196759 \tR2: 0.808010\n",
      "Epoch: 6296 \tTraining Loss: 0.213363 \tR2: 0.808010\n",
      "Epoch: 6297 \tTraining Loss: 0.185026 \tR2: 0.808010\n",
      "Epoch: 6298 \tTraining Loss: 0.187072 \tR2: 0.808010\n",
      "Epoch: 6299 \tTraining Loss: 0.206195 \tR2: 0.808010\n",
      "Epoch: 6300 \tTraining Loss: 0.213091 \tR2: 0.872624\n",
      "Epoch: 6301 \tTraining Loss: 0.195974 \tR2: 0.872624\n",
      "Epoch: 6302 \tTraining Loss: 0.192314 \tR2: 0.872624\n",
      "Epoch: 6303 \tTraining Loss: 0.187612 \tR2: 0.872624\n",
      "Epoch: 6304 \tTraining Loss: 0.214059 \tR2: 0.872624\n",
      "Epoch: 6305 \tTraining Loss: 0.209938 \tR2: 0.872624\n",
      "Epoch: 6306 \tTraining Loss: 0.192933 \tR2: 0.872624\n",
      "Epoch: 6307 \tTraining Loss: 0.180066 \tR2: 0.872624\n",
      "Epoch: 6308 \tTraining Loss: 0.175516 \tR2: 0.872624\n",
      "Epoch: 6309 \tTraining Loss: 0.165503 \tR2: 0.872624\n",
      "Epoch: 6310 \tTraining Loss: 0.197025 \tR2: 0.872624\n",
      "Epoch: 6311 \tTraining Loss: 0.201832 \tR2: 0.872624\n",
      "Epoch: 6312 \tTraining Loss: 0.169728 \tR2: 0.872624\n",
      "Epoch: 6313 \tTraining Loss: 0.196231 \tR2: 0.872624\n",
      "Epoch: 6314 \tTraining Loss: 0.196023 \tR2: 0.872624\n",
      "Epoch: 6315 \tTraining Loss: 0.211656 \tR2: 0.872624\n",
      "Epoch: 6316 \tTraining Loss: 0.200711 \tR2: 0.872624\n",
      "Epoch: 6317 \tTraining Loss: 0.174623 \tR2: 0.872624\n",
      "Epoch: 6318 \tTraining Loss: 0.214423 \tR2: 0.872624\n",
      "Epoch: 6319 \tTraining Loss: 0.176235 \tR2: 0.872624\n",
      "Epoch: 6320 \tTraining Loss: 0.183529 \tR2: 0.872624\n",
      "Epoch: 6321 \tTraining Loss: 0.172889 \tR2: 0.872624\n",
      "Epoch: 6322 \tTraining Loss: 0.183773 \tR2: 0.872624\n",
      "Epoch: 6323 \tTraining Loss: 0.189491 \tR2: 0.872624\n",
      "Epoch: 6324 \tTraining Loss: 0.200422 \tR2: 0.872624\n",
      "Epoch: 6325 \tTraining Loss: 0.211025 \tR2: 0.872624\n",
      "Epoch: 6326 \tTraining Loss: 0.185096 \tR2: 0.872624\n",
      "Epoch: 6327 \tTraining Loss: 0.183493 \tR2: 0.872624\n",
      "Epoch: 6328 \tTraining Loss: 0.188413 \tR2: 0.872624\n",
      "Epoch: 6329 \tTraining Loss: 0.194652 \tR2: 0.872624\n",
      "Epoch: 6330 \tTraining Loss: 0.195761 \tR2: 0.872624\n",
      "Epoch: 6331 \tTraining Loss: 0.164589 \tR2: 0.872624\n",
      "Epoch: 6332 \tTraining Loss: 0.209880 \tR2: 0.872624\n",
      "Epoch: 6333 \tTraining Loss: 0.186854 \tR2: 0.872624\n",
      "Epoch: 6334 \tTraining Loss: 0.181343 \tR2: 0.872624\n",
      "Epoch: 6335 \tTraining Loss: 0.205957 \tR2: 0.872624\n",
      "Epoch: 6336 \tTraining Loss: 0.188763 \tR2: 0.872624\n",
      "Epoch: 6337 \tTraining Loss: 0.199711 \tR2: 0.872624\n",
      "Epoch: 6338 \tTraining Loss: 0.187999 \tR2: 0.872624\n",
      "Epoch: 6339 \tTraining Loss: 0.194105 \tR2: 0.872624\n",
      "Epoch: 6340 \tTraining Loss: 0.191263 \tR2: 0.872624\n",
      "Epoch: 6341 \tTraining Loss: 0.196528 \tR2: 0.872624\n",
      "Epoch: 6342 \tTraining Loss: 0.181821 \tR2: 0.872624\n",
      "Epoch: 6343 \tTraining Loss: 0.179419 \tR2: 0.872624\n",
      "Epoch: 6344 \tTraining Loss: 0.194008 \tR2: 0.872624\n",
      "Epoch: 6345 \tTraining Loss: 0.199302 \tR2: 0.872624\n",
      "Epoch: 6346 \tTraining Loss: 0.197643 \tR2: 0.872624\n",
      "Epoch: 6347 \tTraining Loss: 0.195167 \tR2: 0.872624\n",
      "Epoch: 6348 \tTraining Loss: 0.194647 \tR2: 0.872624\n",
      "Epoch: 6349 \tTraining Loss: 0.208701 \tR2: 0.872624\n",
      "Epoch: 6350 \tTraining Loss: 0.208831 \tR2: 0.872624\n",
      "Epoch: 6351 \tTraining Loss: 0.217601 \tR2: 0.872624\n",
      "Epoch: 6352 \tTraining Loss: 0.198940 \tR2: 0.872624\n",
      "Epoch: 6353 \tTraining Loss: 0.172361 \tR2: 0.872624\n",
      "Epoch: 6354 \tTraining Loss: 0.191641 \tR2: 0.872624\n",
      "Epoch: 6355 \tTraining Loss: 0.181963 \tR2: 0.872624\n",
      "Epoch: 6356 \tTraining Loss: 0.183595 \tR2: 0.872624\n",
      "Epoch: 6357 \tTraining Loss: 0.187285 \tR2: 0.872624\n",
      "Epoch: 6358 \tTraining Loss: 0.205760 \tR2: 0.872624\n",
      "Epoch: 6359 \tTraining Loss: 0.188762 \tR2: 0.872624\n",
      "Epoch: 6360 \tTraining Loss: 0.190714 \tR2: 0.872624\n",
      "Epoch: 6361 \tTraining Loss: 0.207258 \tR2: 0.872624\n",
      "Epoch: 6362 \tTraining Loss: 0.184223 \tR2: 0.872624\n",
      "Epoch: 6363 \tTraining Loss: 0.204129 \tR2: 0.872624\n",
      "Epoch: 6364 \tTraining Loss: 0.201835 \tR2: 0.872624\n",
      "Epoch: 6365 \tTraining Loss: 0.191508 \tR2: 0.872624\n",
      "Epoch: 6366 \tTraining Loss: 0.194135 \tR2: 0.872624\n",
      "Epoch: 6367 \tTraining Loss: 0.188103 \tR2: 0.872624\n",
      "Epoch: 6368 \tTraining Loss: 0.186416 \tR2: 0.872624\n",
      "Epoch: 6369 \tTraining Loss: 0.221228 \tR2: 0.872624\n",
      "Epoch: 6370 \tTraining Loss: 0.201377 \tR2: 0.872624\n",
      "Epoch: 6371 \tTraining Loss: 0.188739 \tR2: 0.872624\n",
      "Epoch: 6372 \tTraining Loss: 0.212540 \tR2: 0.872624\n",
      "Epoch: 6373 \tTraining Loss: 0.193584 \tR2: 0.872624\n",
      "Epoch: 6374 \tTraining Loss: 0.192621 \tR2: 0.872624\n",
      "Epoch: 6375 \tTraining Loss: 0.184468 \tR2: 0.872624\n",
      "Epoch: 6376 \tTraining Loss: 0.188773 \tR2: 0.872624\n",
      "Epoch: 6377 \tTraining Loss: 0.186812 \tR2: 0.872624\n",
      "Epoch: 6378 \tTraining Loss: 0.181795 \tR2: 0.872624\n",
      "Epoch: 6379 \tTraining Loss: 0.195200 \tR2: 0.872624\n",
      "Epoch: 6380 \tTraining Loss: 0.177831 \tR2: 0.872624\n",
      "Epoch: 6381 \tTraining Loss: 0.201495 \tR2: 0.872624\n",
      "Epoch: 6382 \tTraining Loss: 0.165931 \tR2: 0.872624\n",
      "Epoch: 6383 \tTraining Loss: 0.182483 \tR2: 0.872624\n",
      "Epoch: 6384 \tTraining Loss: 0.175390 \tR2: 0.872624\n",
      "Epoch: 6385 \tTraining Loss: 0.206755 \tR2: 0.872624\n",
      "Epoch: 6386 \tTraining Loss: 0.158708 \tR2: 0.872624\n",
      "Epoch: 6387 \tTraining Loss: 0.212152 \tR2: 0.872624\n",
      "Epoch: 6388 \tTraining Loss: 0.176153 \tR2: 0.872624\n",
      "Epoch: 6389 \tTraining Loss: 0.192220 \tR2: 0.872624\n",
      "Epoch: 6390 \tTraining Loss: 0.190926 \tR2: 0.872624\n",
      "Epoch: 6391 \tTraining Loss: 0.209025 \tR2: 0.872624\n",
      "Epoch: 6392 \tTraining Loss: 0.188807 \tR2: 0.872624\n",
      "Epoch: 6393 \tTraining Loss: 0.190012 \tR2: 0.872624\n",
      "Epoch: 6394 \tTraining Loss: 0.190086 \tR2: 0.872624\n",
      "Epoch: 6395 \tTraining Loss: 0.184470 \tR2: 0.872624\n",
      "Epoch: 6396 \tTraining Loss: 0.197120 \tR2: 0.872624\n",
      "Epoch: 6397 \tTraining Loss: 0.192179 \tR2: 0.872624\n",
      "Epoch: 6398 \tTraining Loss: 0.186840 \tR2: 0.872624\n",
      "Epoch: 6399 \tTraining Loss: 0.177761 \tR2: 0.872624\n",
      "Epoch: 6400 \tTraining Loss: 0.200574 \tR2: 0.666748\n",
      "Epoch: 6401 \tTraining Loss: 0.178918 \tR2: 0.666748\n",
      "Epoch: 6402 \tTraining Loss: 0.191450 \tR2: 0.666748\n",
      "Epoch: 6403 \tTraining Loss: 0.178477 \tR2: 0.666748\n",
      "Epoch: 6404 \tTraining Loss: 0.181062 \tR2: 0.666748\n",
      "Epoch: 6405 \tTraining Loss: 0.183663 \tR2: 0.666748\n",
      "Epoch: 6406 \tTraining Loss: 0.190027 \tR2: 0.666748\n",
      "Epoch: 6407 \tTraining Loss: 0.217874 \tR2: 0.666748\n",
      "Epoch: 6408 \tTraining Loss: 0.192772 \tR2: 0.666748\n",
      "Epoch: 6409 \tTraining Loss: 0.183905 \tR2: 0.666748\n",
      "Epoch: 6410 \tTraining Loss: 0.195475 \tR2: 0.666748\n",
      "Epoch: 6411 \tTraining Loss: 0.230070 \tR2: 0.666748\n",
      "Epoch: 6412 \tTraining Loss: 0.186631 \tR2: 0.666748\n",
      "Epoch: 6413 \tTraining Loss: 0.194702 \tR2: 0.666748\n",
      "Epoch: 6414 \tTraining Loss: 0.197113 \tR2: 0.666748\n",
      "Epoch: 6415 \tTraining Loss: 0.182429 \tR2: 0.666748\n",
      "Epoch: 6416 \tTraining Loss: 0.184817 \tR2: 0.666748\n",
      "Epoch: 6417 \tTraining Loss: 0.191207 \tR2: 0.666748\n",
      "Epoch: 6418 \tTraining Loss: 0.195703 \tR2: 0.666748\n",
      "Epoch: 6419 \tTraining Loss: 0.187434 \tR2: 0.666748\n",
      "Epoch: 6420 \tTraining Loss: 0.207959 \tR2: 0.666748\n",
      "Epoch: 6421 \tTraining Loss: 0.179536 \tR2: 0.666748\n",
      "Epoch: 6422 \tTraining Loss: 0.197160 \tR2: 0.666748\n",
      "Epoch: 6423 \tTraining Loss: 0.187972 \tR2: 0.666748\n",
      "Epoch: 6424 \tTraining Loss: 0.195614 \tR2: 0.666748\n",
      "Epoch: 6425 \tTraining Loss: 0.186098 \tR2: 0.666748\n",
      "Epoch: 6426 \tTraining Loss: 0.184267 \tR2: 0.666748\n",
      "Epoch: 6427 \tTraining Loss: 0.169163 \tR2: 0.666748\n",
      "Epoch: 6428 \tTraining Loss: 0.196336 \tR2: 0.666748\n",
      "Epoch: 6429 \tTraining Loss: 0.182949 \tR2: 0.666748\n",
      "Epoch: 6430 \tTraining Loss: 0.189003 \tR2: 0.666748\n",
      "Epoch: 6431 \tTraining Loss: 0.174034 \tR2: 0.666748\n",
      "Epoch: 6432 \tTraining Loss: 0.187565 \tR2: 0.666748\n",
      "Epoch: 6433 \tTraining Loss: 0.187458 \tR2: 0.666748\n",
      "Epoch: 6434 \tTraining Loss: 0.187727 \tR2: 0.666748\n",
      "Epoch: 6435 \tTraining Loss: 0.182603 \tR2: 0.666748\n",
      "Epoch: 6436 \tTraining Loss: 0.195766 \tR2: 0.666748\n",
      "Epoch: 6437 \tTraining Loss: 0.208854 \tR2: 0.666748\n",
      "Epoch: 6438 \tTraining Loss: 0.188073 \tR2: 0.666748\n",
      "Epoch: 6439 \tTraining Loss: 0.187889 \tR2: 0.666748\n",
      "Epoch: 6440 \tTraining Loss: 0.194585 \tR2: 0.666748\n",
      "Epoch: 6441 \tTraining Loss: 0.177012 \tR2: 0.666748\n",
      "Epoch: 6442 \tTraining Loss: 0.198225 \tR2: 0.666748\n",
      "Epoch: 6443 \tTraining Loss: 0.187689 \tR2: 0.666748\n",
      "Epoch: 6444 \tTraining Loss: 0.187680 \tR2: 0.666748\n",
      "Epoch: 6445 \tTraining Loss: 0.169669 \tR2: 0.666748\n",
      "Epoch: 6446 \tTraining Loss: 0.190479 \tR2: 0.666748\n",
      "Epoch: 6447 \tTraining Loss: 0.178600 \tR2: 0.666748\n",
      "Epoch: 6448 \tTraining Loss: 0.200302 \tR2: 0.666748\n",
      "Epoch: 6449 \tTraining Loss: 0.184845 \tR2: 0.666748\n",
      "Epoch: 6450 \tTraining Loss: 0.190166 \tR2: 0.666748\n",
      "Epoch: 6451 \tTraining Loss: 0.172105 \tR2: 0.666748\n",
      "Epoch: 6452 \tTraining Loss: 0.197259 \tR2: 0.666748\n",
      "Epoch: 6453 \tTraining Loss: 0.184920 \tR2: 0.666748\n",
      "Epoch: 6454 \tTraining Loss: 0.212533 \tR2: 0.666748\n",
      "Epoch: 6455 \tTraining Loss: 0.184263 \tR2: 0.666748\n",
      "Epoch: 6456 \tTraining Loss: 0.181126 \tR2: 0.666748\n",
      "Epoch: 6457 \tTraining Loss: 0.183269 \tR2: 0.666748\n",
      "Epoch: 6458 \tTraining Loss: 0.184998 \tR2: 0.666748\n",
      "Epoch: 6459 \tTraining Loss: 0.193834 \tR2: 0.666748\n",
      "Epoch: 6460 \tTraining Loss: 0.179573 \tR2: 0.666748\n",
      "Epoch: 6461 \tTraining Loss: 0.177296 \tR2: 0.666748\n",
      "Epoch: 6462 \tTraining Loss: 0.183782 \tR2: 0.666748\n",
      "Epoch: 6463 \tTraining Loss: 0.190416 \tR2: 0.666748\n",
      "Epoch: 6464 \tTraining Loss: 0.180375 \tR2: 0.666748\n",
      "Epoch: 6465 \tTraining Loss: 0.199658 \tR2: 0.666748\n",
      "Epoch: 6466 \tTraining Loss: 0.198973 \tR2: 0.666748\n",
      "Epoch: 6467 \tTraining Loss: 0.199734 \tR2: 0.666748\n",
      "Epoch: 6468 \tTraining Loss: 0.194283 \tR2: 0.666748\n",
      "Epoch: 6469 \tTraining Loss: 0.185930 \tR2: 0.666748\n",
      "Epoch: 6470 \tTraining Loss: 0.189440 \tR2: 0.666748\n",
      "Epoch: 6471 \tTraining Loss: 0.189054 \tR2: 0.666748\n",
      "Epoch: 6472 \tTraining Loss: 0.175752 \tR2: 0.666748\n",
      "Epoch: 6473 \tTraining Loss: 0.208833 \tR2: 0.666748\n",
      "Epoch: 6474 \tTraining Loss: 0.213823 \tR2: 0.666748\n",
      "Epoch: 6475 \tTraining Loss: 0.193449 \tR2: 0.666748\n",
      "Epoch: 6476 \tTraining Loss: 0.188514 \tR2: 0.666748\n",
      "Epoch: 6477 \tTraining Loss: 0.199209 \tR2: 0.666748\n",
      "Epoch: 6478 \tTraining Loss: 0.183695 \tR2: 0.666748\n",
      "Epoch: 6479 \tTraining Loss: 0.197893 \tR2: 0.666748\n",
      "Epoch: 6480 \tTraining Loss: 0.199100 \tR2: 0.666748\n",
      "Epoch: 6481 \tTraining Loss: 0.187801 \tR2: 0.666748\n",
      "Epoch: 6482 \tTraining Loss: 0.181381 \tR2: 0.666748\n",
      "Epoch: 6483 \tTraining Loss: 0.178699 \tR2: 0.666748\n",
      "Epoch: 6484 \tTraining Loss: 0.194247 \tR2: 0.666748\n",
      "Epoch: 6485 \tTraining Loss: 0.182981 \tR2: 0.666748\n",
      "Epoch: 6486 \tTraining Loss: 0.185228 \tR2: 0.666748\n",
      "Epoch: 6487 \tTraining Loss: 0.174222 \tR2: 0.666748\n",
      "Epoch: 6488 \tTraining Loss: 0.186854 \tR2: 0.666748\n",
      "Epoch: 6489 \tTraining Loss: 0.177191 \tR2: 0.666748\n",
      "Epoch: 6490 \tTraining Loss: 0.184048 \tR2: 0.666748\n",
      "Epoch: 6491 \tTraining Loss: 0.188700 \tR2: 0.666748\n",
      "Epoch: 6492 \tTraining Loss: 0.176198 \tR2: 0.666748\n",
      "Epoch: 6493 \tTraining Loss: 0.183372 \tR2: 0.666748\n",
      "Epoch: 6494 \tTraining Loss: 0.192734 \tR2: 0.666748\n",
      "Epoch: 6495 \tTraining Loss: 0.176724 \tR2: 0.666748\n",
      "Epoch: 6496 \tTraining Loss: 0.196843 \tR2: 0.666748\n",
      "Epoch: 6497 \tTraining Loss: 0.176942 \tR2: 0.666748\n",
      "Epoch: 6498 \tTraining Loss: 0.176808 \tR2: 0.666748\n",
      "Epoch: 6499 \tTraining Loss: 0.191749 \tR2: 0.666748\n",
      "Epoch: 6500 \tTraining Loss: 0.180052 \tR2: 0.845221\n",
      "Epoch: 6501 \tTraining Loss: 0.186595 \tR2: 0.845221\n",
      "Epoch: 6502 \tTraining Loss: 0.192450 \tR2: 0.845221\n",
      "Epoch: 6503 \tTraining Loss: 0.177977 \tR2: 0.845221\n",
      "Epoch: 6504 \tTraining Loss: 0.202788 \tR2: 0.845221\n",
      "Epoch: 6505 \tTraining Loss: 0.227344 \tR2: 0.845221\n",
      "Epoch: 6506 \tTraining Loss: 0.215109 \tR2: 0.845221\n",
      "Epoch: 6507 \tTraining Loss: 0.226761 \tR2: 0.845221\n",
      "Epoch: 6508 \tTraining Loss: 0.204150 \tR2: 0.845221\n",
      "Epoch: 6509 \tTraining Loss: 0.199208 \tR2: 0.845221\n",
      "Epoch: 6510 \tTraining Loss: 0.189545 \tR2: 0.845221\n",
      "Epoch: 6511 \tTraining Loss: 0.228638 \tR2: 0.845221\n",
      "Epoch: 6512 \tTraining Loss: 0.184458 \tR2: 0.845221\n",
      "Epoch: 6513 \tTraining Loss: 0.205813 \tR2: 0.845221\n",
      "Epoch: 6514 \tTraining Loss: 0.188753 \tR2: 0.845221\n",
      "Epoch: 6515 \tTraining Loss: 0.186197 \tR2: 0.845221\n",
      "Epoch: 6516 \tTraining Loss: 0.173812 \tR2: 0.845221\n",
      "Epoch: 6517 \tTraining Loss: 0.187052 \tR2: 0.845221\n",
      "Epoch: 6518 \tTraining Loss: 0.206136 \tR2: 0.845221\n",
      "Epoch: 6519 \tTraining Loss: 0.190878 \tR2: 0.845221\n",
      "Epoch: 6520 \tTraining Loss: 0.198621 \tR2: 0.845221\n",
      "Epoch: 6521 \tTraining Loss: 0.193304 \tR2: 0.845221\n",
      "Epoch: 6522 \tTraining Loss: 0.204419 \tR2: 0.845221\n",
      "Epoch: 6523 \tTraining Loss: 0.173964 \tR2: 0.845221\n",
      "Epoch: 6524 \tTraining Loss: 0.193406 \tR2: 0.845221\n",
      "Epoch: 6525 \tTraining Loss: 0.200856 \tR2: 0.845221\n",
      "Epoch: 6526 \tTraining Loss: 0.189401 \tR2: 0.845221\n",
      "Epoch: 6527 \tTraining Loss: 0.199807 \tR2: 0.845221\n",
      "Epoch: 6528 \tTraining Loss: 0.198322 \tR2: 0.845221\n",
      "Epoch: 6529 \tTraining Loss: 0.194468 \tR2: 0.845221\n",
      "Epoch: 6530 \tTraining Loss: 0.192917 \tR2: 0.845221\n",
      "Epoch: 6531 \tTraining Loss: 0.195645 \tR2: 0.845221\n",
      "Epoch: 6532 \tTraining Loss: 0.200386 \tR2: 0.845221\n",
      "Epoch: 6533 \tTraining Loss: 0.181788 \tR2: 0.845221\n",
      "Epoch: 6534 \tTraining Loss: 0.203488 \tR2: 0.845221\n",
      "Epoch: 6535 \tTraining Loss: 0.187283 \tR2: 0.845221\n",
      "Epoch: 6536 \tTraining Loss: 0.182257 \tR2: 0.845221\n",
      "Epoch: 6537 \tTraining Loss: 0.182564 \tR2: 0.845221\n",
      "Epoch: 6538 \tTraining Loss: 0.214687 \tR2: 0.845221\n",
      "Epoch: 6539 \tTraining Loss: 0.190333 \tR2: 0.845221\n",
      "Epoch: 6540 \tTraining Loss: 0.192908 \tR2: 0.845221\n",
      "Epoch: 6541 \tTraining Loss: 0.170824 \tR2: 0.845221\n",
      "Epoch: 6542 \tTraining Loss: 0.206215 \tR2: 0.845221\n",
      "Epoch: 6543 \tTraining Loss: 0.201941 \tR2: 0.845221\n",
      "Epoch: 6544 \tTraining Loss: 0.184618 \tR2: 0.845221\n",
      "Epoch: 6545 \tTraining Loss: 0.204731 \tR2: 0.845221\n",
      "Epoch: 6546 \tTraining Loss: 0.192826 \tR2: 0.845221\n",
      "Epoch: 6547 \tTraining Loss: 0.180741 \tR2: 0.845221\n",
      "Epoch: 6548 \tTraining Loss: 0.189730 \tR2: 0.845221\n",
      "Epoch: 6549 \tTraining Loss: 0.185780 \tR2: 0.845221\n",
      "Epoch: 6550 \tTraining Loss: 0.187255 \tR2: 0.845221\n",
      "Epoch: 6551 \tTraining Loss: 0.193252 \tR2: 0.845221\n",
      "Epoch: 6552 \tTraining Loss: 0.207894 \tR2: 0.845221\n",
      "Epoch: 6553 \tTraining Loss: 0.176517 \tR2: 0.845221\n",
      "Epoch: 6554 \tTraining Loss: 0.223691 \tR2: 0.845221\n",
      "Epoch: 6555 \tTraining Loss: 0.210604 \tR2: 0.845221\n",
      "Epoch: 6556 \tTraining Loss: 0.196959 \tR2: 0.845221\n",
      "Epoch: 6557 \tTraining Loss: 0.182150 \tR2: 0.845221\n",
      "Epoch: 6558 \tTraining Loss: 0.175660 \tR2: 0.845221\n",
      "Epoch: 6559 \tTraining Loss: 0.188467 \tR2: 0.845221\n",
      "Epoch: 6560 \tTraining Loss: 0.187906 \tR2: 0.845221\n",
      "Epoch: 6561 \tTraining Loss: 0.185977 \tR2: 0.845221\n",
      "Epoch: 6562 \tTraining Loss: 0.212181 \tR2: 0.845221\n",
      "Epoch: 6563 \tTraining Loss: 0.187666 \tR2: 0.845221\n",
      "Epoch: 6564 \tTraining Loss: 0.176023 \tR2: 0.845221\n",
      "Epoch: 6565 \tTraining Loss: 0.185869 \tR2: 0.845221\n",
      "Epoch: 6566 \tTraining Loss: 0.179303 \tR2: 0.845221\n",
      "Epoch: 6567 \tTraining Loss: 0.175527 \tR2: 0.845221\n",
      "Epoch: 6568 \tTraining Loss: 0.176885 \tR2: 0.845221\n",
      "Epoch: 6569 \tTraining Loss: 0.179214 \tR2: 0.845221\n",
      "Epoch: 6570 \tTraining Loss: 0.171378 \tR2: 0.845221\n",
      "Epoch: 6571 \tTraining Loss: 0.196857 \tR2: 0.845221\n",
      "Epoch: 6572 \tTraining Loss: 0.212961 \tR2: 0.845221\n",
      "Epoch: 6573 \tTraining Loss: 0.186316 \tR2: 0.845221\n",
      "Epoch: 6574 \tTraining Loss: 0.201807 \tR2: 0.845221\n",
      "Epoch: 6575 \tTraining Loss: 0.202984 \tR2: 0.845221\n",
      "Epoch: 6576 \tTraining Loss: 0.199024 \tR2: 0.845221\n",
      "Epoch: 6577 \tTraining Loss: 0.173811 \tR2: 0.845221\n",
      "Epoch: 6578 \tTraining Loss: 0.182383 \tR2: 0.845221\n",
      "Epoch: 6579 \tTraining Loss: 0.192527 \tR2: 0.845221\n",
      "Epoch: 6580 \tTraining Loss: 0.184895 \tR2: 0.845221\n",
      "Epoch: 6581 \tTraining Loss: 0.180795 \tR2: 0.845221\n",
      "Epoch: 6582 \tTraining Loss: 0.174000 \tR2: 0.845221\n",
      "Epoch: 6583 \tTraining Loss: 0.195174 \tR2: 0.845221\n",
      "Epoch: 6584 \tTraining Loss: 0.193596 \tR2: 0.845221\n",
      "Epoch: 6585 \tTraining Loss: 0.176448 \tR2: 0.845221\n",
      "Epoch: 6586 \tTraining Loss: 0.184944 \tR2: 0.845221\n",
      "Epoch: 6587 \tTraining Loss: 0.202818 \tR2: 0.845221\n",
      "Epoch: 6588 \tTraining Loss: 0.204281 \tR2: 0.845221\n",
      "Epoch: 6589 \tTraining Loss: 0.182173 \tR2: 0.845221\n",
      "Epoch: 6590 \tTraining Loss: 0.167350 \tR2: 0.845221\n",
      "Epoch: 6591 \tTraining Loss: 0.192125 \tR2: 0.845221\n",
      "Epoch: 6592 \tTraining Loss: 0.196185 \tR2: 0.845221\n",
      "Epoch: 6593 \tTraining Loss: 0.205321 \tR2: 0.845221\n",
      "Epoch: 6594 \tTraining Loss: 0.183710 \tR2: 0.845221\n",
      "Epoch: 6595 \tTraining Loss: 0.197157 \tR2: 0.845221\n",
      "Epoch: 6596 \tTraining Loss: 0.188928 \tR2: 0.845221\n",
      "Epoch: 6597 \tTraining Loss: 0.180918 \tR2: 0.845221\n",
      "Epoch: 6598 \tTraining Loss: 0.186091 \tR2: 0.845221\n",
      "Epoch: 6599 \tTraining Loss: 0.177269 \tR2: 0.845221\n",
      "Epoch: 6600 \tTraining Loss: 0.196738 \tR2: 0.701805\n",
      "Epoch: 6601 \tTraining Loss: 0.195492 \tR2: 0.701805\n",
      "Epoch: 6602 \tTraining Loss: 0.195642 \tR2: 0.701805\n",
      "Epoch: 6603 \tTraining Loss: 0.201169 \tR2: 0.701805\n",
      "Epoch: 6604 \tTraining Loss: 0.192602 \tR2: 0.701805\n",
      "Epoch: 6605 \tTraining Loss: 0.187809 \tR2: 0.701805\n",
      "Epoch: 6606 \tTraining Loss: 0.190908 \tR2: 0.701805\n",
      "Epoch: 6607 \tTraining Loss: 0.185715 \tR2: 0.701805\n",
      "Epoch: 6608 \tTraining Loss: 0.182840 \tR2: 0.701805\n",
      "Epoch: 6609 \tTraining Loss: 0.183802 \tR2: 0.701805\n",
      "Epoch: 6610 \tTraining Loss: 0.189981 \tR2: 0.701805\n",
      "Epoch: 6611 \tTraining Loss: 0.193450 \tR2: 0.701805\n",
      "Epoch: 6612 \tTraining Loss: 0.199057 \tR2: 0.701805\n",
      "Epoch: 6613 \tTraining Loss: 0.173968 \tR2: 0.701805\n",
      "Epoch: 6614 \tTraining Loss: 0.169428 \tR2: 0.701805\n",
      "Epoch: 6615 \tTraining Loss: 0.204437 \tR2: 0.701805\n",
      "Epoch: 6616 \tTraining Loss: 0.190906 \tR2: 0.701805\n",
      "Epoch: 6617 \tTraining Loss: 0.185988 \tR2: 0.701805\n",
      "Epoch: 6618 \tTraining Loss: 0.200494 \tR2: 0.701805\n",
      "Epoch: 6619 \tTraining Loss: 0.180862 \tR2: 0.701805\n",
      "Epoch: 6620 \tTraining Loss: 0.199113 \tR2: 0.701805\n",
      "Epoch: 6621 \tTraining Loss: 0.200332 \tR2: 0.701805\n",
      "Epoch: 6622 \tTraining Loss: 0.178985 \tR2: 0.701805\n",
      "Epoch: 6623 \tTraining Loss: 0.193636 \tR2: 0.701805\n",
      "Epoch: 6624 \tTraining Loss: 0.192547 \tR2: 0.701805\n",
      "Epoch: 6625 \tTraining Loss: 0.193639 \tR2: 0.701805\n",
      "Epoch: 6626 \tTraining Loss: 0.205416 \tR2: 0.701805\n",
      "Epoch: 6627 \tTraining Loss: 0.187965 \tR2: 0.701805\n",
      "Epoch: 6628 \tTraining Loss: 0.165947 \tR2: 0.701805\n",
      "Epoch: 6629 \tTraining Loss: 0.186569 \tR2: 0.701805\n",
      "Epoch: 6630 \tTraining Loss: 0.175600 \tR2: 0.701805\n",
      "Epoch: 6631 \tTraining Loss: 0.180512 \tR2: 0.701805\n",
      "Epoch: 6632 \tTraining Loss: 0.185993 \tR2: 0.701805\n",
      "Epoch: 6633 \tTraining Loss: 0.201735 \tR2: 0.701805\n",
      "Epoch: 6634 \tTraining Loss: 0.182371 \tR2: 0.701805\n",
      "Epoch: 6635 \tTraining Loss: 0.188551 \tR2: 0.701805\n",
      "Epoch: 6636 \tTraining Loss: 0.172167 \tR2: 0.701805\n",
      "Epoch: 6637 \tTraining Loss: 0.181808 \tR2: 0.701805\n",
      "Epoch: 6638 \tTraining Loss: 0.219782 \tR2: 0.701805\n",
      "Epoch: 6639 \tTraining Loss: 0.211784 \tR2: 0.701805\n",
      "Epoch: 6640 \tTraining Loss: 0.187876 \tR2: 0.701805\n",
      "Epoch: 6641 \tTraining Loss: 0.190991 \tR2: 0.701805\n",
      "Epoch: 6642 \tTraining Loss: 0.185685 \tR2: 0.701805\n",
      "Epoch: 6643 \tTraining Loss: 0.183725 \tR2: 0.701805\n",
      "Epoch: 6644 \tTraining Loss: 0.171370 \tR2: 0.701805\n",
      "Epoch: 6645 \tTraining Loss: 0.172853 \tR2: 0.701805\n",
      "Epoch: 6646 \tTraining Loss: 0.188156 \tR2: 0.701805\n",
      "Epoch: 6647 \tTraining Loss: 0.183243 \tR2: 0.701805\n",
      "Epoch: 6648 \tTraining Loss: 0.203593 \tR2: 0.701805\n",
      "Epoch: 6649 \tTraining Loss: 0.190357 \tR2: 0.701805\n",
      "Epoch: 6650 \tTraining Loss: 0.186331 \tR2: 0.701805\n",
      "Epoch: 6651 \tTraining Loss: 0.171832 \tR2: 0.701805\n",
      "Epoch: 6652 \tTraining Loss: 0.170798 \tR2: 0.701805\n",
      "Epoch: 6653 \tTraining Loss: 0.203814 \tR2: 0.701805\n",
      "Epoch: 6654 \tTraining Loss: 0.201195 \tR2: 0.701805\n",
      "Epoch: 6655 \tTraining Loss: 0.181819 \tR2: 0.701805\n",
      "Epoch: 6656 \tTraining Loss: 0.176614 \tR2: 0.701805\n",
      "Epoch: 6657 \tTraining Loss: 0.200842 \tR2: 0.701805\n",
      "Epoch: 6658 \tTraining Loss: 0.187844 \tR2: 0.701805\n",
      "Epoch: 6659 \tTraining Loss: 0.202052 \tR2: 0.701805\n",
      "Epoch: 6660 \tTraining Loss: 0.195017 \tR2: 0.701805\n",
      "Epoch: 6661 \tTraining Loss: 0.182271 \tR2: 0.701805\n",
      "Epoch: 6662 \tTraining Loss: 0.189190 \tR2: 0.701805\n",
      "Epoch: 6663 \tTraining Loss: 0.195002 \tR2: 0.701805\n",
      "Epoch: 6664 \tTraining Loss: 0.174671 \tR2: 0.701805\n",
      "Epoch: 6665 \tTraining Loss: 0.190764 \tR2: 0.701805\n",
      "Epoch: 6666 \tTraining Loss: 0.179840 \tR2: 0.701805\n",
      "Epoch: 6667 \tTraining Loss: 0.188502 \tR2: 0.701805\n",
      "Epoch: 6668 \tTraining Loss: 0.176342 \tR2: 0.701805\n",
      "Epoch: 6669 \tTraining Loss: 0.184236 \tR2: 0.701805\n",
      "Epoch: 6670 \tTraining Loss: 0.199322 \tR2: 0.701805\n",
      "Epoch: 6671 \tTraining Loss: 0.187551 \tR2: 0.701805\n",
      "Epoch: 6672 \tTraining Loss: 0.179238 \tR2: 0.701805\n",
      "Epoch: 6673 \tTraining Loss: 0.192956 \tR2: 0.701805\n",
      "Epoch: 6674 \tTraining Loss: 0.201721 \tR2: 0.701805\n",
      "Epoch: 6675 \tTraining Loss: 0.179508 \tR2: 0.701805\n",
      "Epoch: 6676 \tTraining Loss: 0.192676 \tR2: 0.701805\n",
      "Epoch: 6677 \tTraining Loss: 0.192305 \tR2: 0.701805\n",
      "Epoch: 6678 \tTraining Loss: 0.177441 \tR2: 0.701805\n",
      "Epoch: 6679 \tTraining Loss: 0.193796 \tR2: 0.701805\n",
      "Epoch: 6680 \tTraining Loss: 0.187017 \tR2: 0.701805\n",
      "Epoch: 6681 \tTraining Loss: 0.178945 \tR2: 0.701805\n",
      "Epoch: 6682 \tTraining Loss: 0.183315 \tR2: 0.701805\n",
      "Epoch: 6683 \tTraining Loss: 0.193909 \tR2: 0.701805\n",
      "Epoch: 6684 \tTraining Loss: 0.207087 \tR2: 0.701805\n",
      "Epoch: 6685 \tTraining Loss: 0.178996 \tR2: 0.701805\n",
      "Epoch: 6686 \tTraining Loss: 0.191613 \tR2: 0.701805\n",
      "Epoch: 6687 \tTraining Loss: 0.215301 \tR2: 0.701805\n",
      "Epoch: 6688 \tTraining Loss: 0.189495 \tR2: 0.701805\n",
      "Epoch: 6689 \tTraining Loss: 0.188495 \tR2: 0.701805\n",
      "Epoch: 6690 \tTraining Loss: 0.185386 \tR2: 0.701805\n",
      "Epoch: 6691 \tTraining Loss: 0.190277 \tR2: 0.701805\n",
      "Epoch: 6692 \tTraining Loss: 0.202495 \tR2: 0.701805\n",
      "Epoch: 6693 \tTraining Loss: 0.185897 \tR2: 0.701805\n",
      "Epoch: 6694 \tTraining Loss: 0.197334 \tR2: 0.701805\n",
      "Epoch: 6695 \tTraining Loss: 0.176247 \tR2: 0.701805\n",
      "Epoch: 6696 \tTraining Loss: 0.175874 \tR2: 0.701805\n",
      "Epoch: 6697 \tTraining Loss: 0.196209 \tR2: 0.701805\n",
      "Epoch: 6698 \tTraining Loss: 0.197661 \tR2: 0.701805\n",
      "Epoch: 6699 \tTraining Loss: 0.202434 \tR2: 0.701805\n",
      "Epoch: 6700 \tTraining Loss: 0.179426 \tR2: 0.730864\n",
      "Epoch: 6701 \tTraining Loss: 0.186463 \tR2: 0.730864\n",
      "Epoch: 6702 \tTraining Loss: 0.182520 \tR2: 0.730864\n",
      "Epoch: 6703 \tTraining Loss: 0.184503 \tR2: 0.730864\n",
      "Epoch: 6704 \tTraining Loss: 0.178491 \tR2: 0.730864\n",
      "Epoch: 6705 \tTraining Loss: 0.199571 \tR2: 0.730864\n",
      "Epoch: 6706 \tTraining Loss: 0.186667 \tR2: 0.730864\n",
      "Epoch: 6707 \tTraining Loss: 0.178631 \tR2: 0.730864\n",
      "Epoch: 6708 \tTraining Loss: 0.186902 \tR2: 0.730864\n",
      "Epoch: 6709 \tTraining Loss: 0.186907 \tR2: 0.730864\n",
      "Epoch: 6710 \tTraining Loss: 0.200651 \tR2: 0.730864\n",
      "Epoch: 6711 \tTraining Loss: 0.200139 \tR2: 0.730864\n",
      "Epoch: 6712 \tTraining Loss: 0.200027 \tR2: 0.730864\n",
      "Epoch: 6713 \tTraining Loss: 0.188284 \tR2: 0.730864\n",
      "Epoch: 6714 \tTraining Loss: 0.194641 \tR2: 0.730864\n",
      "Epoch: 6715 \tTraining Loss: 0.210865 \tR2: 0.730864\n",
      "Epoch: 6716 \tTraining Loss: 0.192743 \tR2: 0.730864\n",
      "Epoch: 6717 \tTraining Loss: 0.210388 \tR2: 0.730864\n",
      "Epoch: 6718 \tTraining Loss: 0.185103 \tR2: 0.730864\n",
      "Epoch: 6719 \tTraining Loss: 0.197362 \tR2: 0.730864\n",
      "Epoch: 6720 \tTraining Loss: 0.180989 \tR2: 0.730864\n",
      "Epoch: 6721 \tTraining Loss: 0.179583 \tR2: 0.730864\n",
      "Epoch: 6722 \tTraining Loss: 0.218680 \tR2: 0.730864\n",
      "Epoch: 6723 \tTraining Loss: 0.184228 \tR2: 0.730864\n",
      "Epoch: 6724 \tTraining Loss: 0.185316 \tR2: 0.730864\n",
      "Epoch: 6725 \tTraining Loss: 0.187731 \tR2: 0.730864\n",
      "Epoch: 6726 \tTraining Loss: 0.182698 \tR2: 0.730864\n",
      "Epoch: 6727 \tTraining Loss: 0.191142 \tR2: 0.730864\n",
      "Epoch: 6728 \tTraining Loss: 0.185746 \tR2: 0.730864\n",
      "Epoch: 6729 \tTraining Loss: 0.198672 \tR2: 0.730864\n",
      "Epoch: 6730 \tTraining Loss: 0.178846 \tR2: 0.730864\n",
      "Epoch: 6731 \tTraining Loss: 0.175891 \tR2: 0.730864\n",
      "Epoch: 6732 \tTraining Loss: 0.184332 \tR2: 0.730864\n",
      "Epoch: 6733 \tTraining Loss: 0.185265 \tR2: 0.730864\n",
      "Epoch: 6734 \tTraining Loss: 0.200451 \tR2: 0.730864\n",
      "Epoch: 6735 \tTraining Loss: 0.210888 \tR2: 0.730864\n",
      "Epoch: 6736 \tTraining Loss: 0.169439 \tR2: 0.730864\n",
      "Epoch: 6737 \tTraining Loss: 0.176084 \tR2: 0.730864\n",
      "Epoch: 6738 \tTraining Loss: 0.188996 \tR2: 0.730864\n",
      "Epoch: 6739 \tTraining Loss: 0.192760 \tR2: 0.730864\n",
      "Epoch: 6740 \tTraining Loss: 0.211145 \tR2: 0.730864\n",
      "Epoch: 6741 \tTraining Loss: 0.191832 \tR2: 0.730864\n",
      "Epoch: 6742 \tTraining Loss: 0.188201 \tR2: 0.730864\n",
      "Epoch: 6743 \tTraining Loss: 0.185489 \tR2: 0.730864\n",
      "Epoch: 6744 \tTraining Loss: 0.198179 \tR2: 0.730864\n",
      "Epoch: 6745 \tTraining Loss: 0.178276 \tR2: 0.730864\n",
      "Epoch: 6746 \tTraining Loss: 0.217140 \tR2: 0.730864\n",
      "Epoch: 6747 \tTraining Loss: 0.183158 \tR2: 0.730864\n",
      "Epoch: 6748 \tTraining Loss: 0.208560 \tR2: 0.730864\n",
      "Epoch: 6749 \tTraining Loss: 0.190697 \tR2: 0.730864\n",
      "Epoch: 6750 \tTraining Loss: 0.171855 \tR2: 0.730864\n",
      "Epoch: 6751 \tTraining Loss: 0.199873 \tR2: 0.730864\n",
      "Epoch: 6752 \tTraining Loss: 0.177085 \tR2: 0.730864\n",
      "Epoch: 6753 \tTraining Loss: 0.189579 \tR2: 0.730864\n",
      "Epoch: 6754 \tTraining Loss: 0.213118 \tR2: 0.730864\n",
      "Epoch: 6755 \tTraining Loss: 0.186143 \tR2: 0.730864\n",
      "Epoch: 6756 \tTraining Loss: 0.189671 \tR2: 0.730864\n",
      "Epoch: 6757 \tTraining Loss: 0.200420 \tR2: 0.730864\n",
      "Epoch: 6758 \tTraining Loss: 0.190665 \tR2: 0.730864\n",
      "Epoch: 6759 \tTraining Loss: 0.190093 \tR2: 0.730864\n",
      "Epoch: 6760 \tTraining Loss: 0.191136 \tR2: 0.730864\n",
      "Epoch: 6761 \tTraining Loss: 0.190191 \tR2: 0.730864\n",
      "Epoch: 6762 \tTraining Loss: 0.192354 \tR2: 0.730864\n",
      "Epoch: 6763 \tTraining Loss: 0.186078 \tR2: 0.730864\n",
      "Epoch: 6764 \tTraining Loss: 0.176034 \tR2: 0.730864\n",
      "Epoch: 6765 \tTraining Loss: 0.196364 \tR2: 0.730864\n",
      "Epoch: 6766 \tTraining Loss: 0.195183 \tR2: 0.730864\n",
      "Epoch: 6767 \tTraining Loss: 0.194116 \tR2: 0.730864\n",
      "Epoch: 6768 \tTraining Loss: 0.185327 \tR2: 0.730864\n",
      "Epoch: 6769 \tTraining Loss: 0.180021 \tR2: 0.730864\n",
      "Epoch: 6770 \tTraining Loss: 0.191432 \tR2: 0.730864\n",
      "Epoch: 6771 \tTraining Loss: 0.188352 \tR2: 0.730864\n",
      "Epoch: 6772 \tTraining Loss: 0.184927 \tR2: 0.730864\n",
      "Epoch: 6773 \tTraining Loss: 0.187850 \tR2: 0.730864\n",
      "Epoch: 6774 \tTraining Loss: 0.176145 \tR2: 0.730864\n",
      "Epoch: 6775 \tTraining Loss: 0.190153 \tR2: 0.730864\n",
      "Epoch: 6776 \tTraining Loss: 0.180701 \tR2: 0.730864\n",
      "Epoch: 6777 \tTraining Loss: 0.183275 \tR2: 0.730864\n",
      "Epoch: 6778 \tTraining Loss: 0.201322 \tR2: 0.730864\n",
      "Epoch: 6779 \tTraining Loss: 0.187942 \tR2: 0.730864\n",
      "Epoch: 6780 \tTraining Loss: 0.202964 \tR2: 0.730864\n",
      "Epoch: 6781 \tTraining Loss: 0.182590 \tR2: 0.730864\n",
      "Epoch: 6782 \tTraining Loss: 0.168865 \tR2: 0.730864\n",
      "Epoch: 6783 \tTraining Loss: 0.183260 \tR2: 0.730864\n",
      "Epoch: 6784 \tTraining Loss: 0.172277 \tR2: 0.730864\n",
      "Epoch: 6785 \tTraining Loss: 0.215394 \tR2: 0.730864\n",
      "Epoch: 6786 \tTraining Loss: 0.185909 \tR2: 0.730864\n",
      "Epoch: 6787 \tTraining Loss: 0.181510 \tR2: 0.730864\n",
      "Epoch: 6788 \tTraining Loss: 0.185280 \tR2: 0.730864\n",
      "Epoch: 6789 \tTraining Loss: 0.179635 \tR2: 0.730864\n",
      "Epoch: 6790 \tTraining Loss: 0.176779 \tR2: 0.730864\n",
      "Epoch: 6791 \tTraining Loss: 0.187556 \tR2: 0.730864\n",
      "Epoch: 6792 \tTraining Loss: 0.206147 \tR2: 0.730864\n",
      "Epoch: 6793 \tTraining Loss: 0.175932 \tR2: 0.730864\n",
      "Epoch: 6794 \tTraining Loss: 0.184368 \tR2: 0.730864\n",
      "Epoch: 6795 \tTraining Loss: 0.180333 \tR2: 0.730864\n",
      "Epoch: 6796 \tTraining Loss: 0.198921 \tR2: 0.730864\n",
      "Epoch: 6797 \tTraining Loss: 0.179186 \tR2: 0.730864\n",
      "Epoch: 6798 \tTraining Loss: 0.185946 \tR2: 0.730864\n",
      "Epoch: 6799 \tTraining Loss: 0.185666 \tR2: 0.730864\n",
      "Epoch: 6800 \tTraining Loss: 0.198497 \tR2: 0.839623\n",
      "Epoch: 6801 \tTraining Loss: 0.203212 \tR2: 0.839623\n",
      "Epoch: 6802 \tTraining Loss: 0.180801 \tR2: 0.839623\n",
      "Epoch: 6803 \tTraining Loss: 0.188851 \tR2: 0.839623\n",
      "Epoch: 6804 \tTraining Loss: 0.188263 \tR2: 0.839623\n",
      "Epoch: 6805 \tTraining Loss: 0.190137 \tR2: 0.839623\n",
      "Epoch: 6806 \tTraining Loss: 0.186472 \tR2: 0.839623\n",
      "Epoch: 6807 \tTraining Loss: 0.192543 \tR2: 0.839623\n",
      "Epoch: 6808 \tTraining Loss: 0.185549 \tR2: 0.839623\n",
      "Epoch: 6809 \tTraining Loss: 0.191063 \tR2: 0.839623\n",
      "Epoch: 6810 \tTraining Loss: 0.184503 \tR2: 0.839623\n",
      "Epoch: 6811 \tTraining Loss: 0.179709 \tR2: 0.839623\n",
      "Epoch: 6812 \tTraining Loss: 0.188525 \tR2: 0.839623\n",
      "Epoch: 6813 \tTraining Loss: 0.187335 \tR2: 0.839623\n",
      "Epoch: 6814 \tTraining Loss: 0.179080 \tR2: 0.839623\n",
      "Epoch: 6815 \tTraining Loss: 0.185740 \tR2: 0.839623\n",
      "Epoch: 6816 \tTraining Loss: 0.205405 \tR2: 0.839623\n",
      "Epoch: 6817 \tTraining Loss: 0.196712 \tR2: 0.839623\n",
      "Epoch: 6818 \tTraining Loss: 0.193815 \tR2: 0.839623\n",
      "Epoch: 6819 \tTraining Loss: 0.185294 \tR2: 0.839623\n",
      "Epoch: 6820 \tTraining Loss: 0.187237 \tR2: 0.839623\n",
      "Epoch: 6821 \tTraining Loss: 0.176152 \tR2: 0.839623\n",
      "Epoch: 6822 \tTraining Loss: 0.188755 \tR2: 0.839623\n",
      "Epoch: 6823 \tTraining Loss: 0.191967 \tR2: 0.839623\n",
      "Epoch: 6824 \tTraining Loss: 0.175552 \tR2: 0.839623\n",
      "Epoch: 6825 \tTraining Loss: 0.182264 \tR2: 0.839623\n",
      "Epoch: 6826 \tTraining Loss: 0.187021 \tR2: 0.839623\n",
      "Epoch: 6827 \tTraining Loss: 0.168814 \tR2: 0.839623\n",
      "Epoch: 6828 \tTraining Loss: 0.171187 \tR2: 0.839623\n",
      "Epoch: 6829 \tTraining Loss: 0.205234 \tR2: 0.839623\n",
      "Epoch: 6830 \tTraining Loss: 0.198915 \tR2: 0.839623\n",
      "Epoch: 6831 \tTraining Loss: 0.183489 \tR2: 0.839623\n",
      "Epoch: 6832 \tTraining Loss: 0.187251 \tR2: 0.839623\n",
      "Epoch: 6833 \tTraining Loss: 0.200065 \tR2: 0.839623\n",
      "Epoch: 6834 \tTraining Loss: 0.203345 \tR2: 0.839623\n",
      "Epoch: 6835 \tTraining Loss: 0.192569 \tR2: 0.839623\n",
      "Epoch: 6836 \tTraining Loss: 0.176151 \tR2: 0.839623\n",
      "Epoch: 6837 \tTraining Loss: 0.181899 \tR2: 0.839623\n",
      "Epoch: 6838 \tTraining Loss: 0.189417 \tR2: 0.839623\n",
      "Epoch: 6839 \tTraining Loss: 0.200268 \tR2: 0.839623\n",
      "Epoch: 6840 \tTraining Loss: 0.181082 \tR2: 0.839623\n",
      "Epoch: 6841 \tTraining Loss: 0.204267 \tR2: 0.839623\n",
      "Epoch: 6842 \tTraining Loss: 0.188351 \tR2: 0.839623\n",
      "Epoch: 6843 \tTraining Loss: 0.184798 \tR2: 0.839623\n",
      "Epoch: 6844 \tTraining Loss: 0.184621 \tR2: 0.839623\n",
      "Epoch: 6845 \tTraining Loss: 0.175746 \tR2: 0.839623\n",
      "Epoch: 6846 \tTraining Loss: 0.192210 \tR2: 0.839623\n",
      "Epoch: 6847 \tTraining Loss: 0.202780 \tR2: 0.839623\n",
      "Epoch: 6848 \tTraining Loss: 0.194008 \tR2: 0.839623\n",
      "Epoch: 6849 \tTraining Loss: 0.189511 \tR2: 0.839623\n",
      "Epoch: 6850 \tTraining Loss: 0.187402 \tR2: 0.839623\n",
      "Epoch: 6851 \tTraining Loss: 0.181828 \tR2: 0.839623\n",
      "Epoch: 6852 \tTraining Loss: 0.198974 \tR2: 0.839623\n",
      "Epoch: 6853 \tTraining Loss: 0.188064 \tR2: 0.839623\n",
      "Epoch: 6854 \tTraining Loss: 0.190176 \tR2: 0.839623\n",
      "Epoch: 6855 \tTraining Loss: 0.191170 \tR2: 0.839623\n",
      "Epoch: 6856 \tTraining Loss: 0.189315 \tR2: 0.839623\n",
      "Epoch: 6857 \tTraining Loss: 0.190052 \tR2: 0.839623\n",
      "Epoch: 6858 \tTraining Loss: 0.215888 \tR2: 0.839623\n",
      "Epoch: 6859 \tTraining Loss: 0.198997 \tR2: 0.839623\n",
      "Epoch: 6860 \tTraining Loss: 0.195688 \tR2: 0.839623\n",
      "Epoch: 6861 \tTraining Loss: 0.173504 \tR2: 0.839623\n",
      "Epoch: 6862 \tTraining Loss: 0.195265 \tR2: 0.839623\n",
      "Epoch: 6863 \tTraining Loss: 0.186854 \tR2: 0.839623\n",
      "Epoch: 6864 \tTraining Loss: 0.174955 \tR2: 0.839623\n",
      "Epoch: 6865 \tTraining Loss: 0.200558 \tR2: 0.839623\n",
      "Epoch: 6866 \tTraining Loss: 0.187336 \tR2: 0.839623\n",
      "Epoch: 6867 \tTraining Loss: 0.177288 \tR2: 0.839623\n",
      "Epoch: 6868 \tTraining Loss: 0.186510 \tR2: 0.839623\n",
      "Epoch: 6869 \tTraining Loss: 0.193053 \tR2: 0.839623\n",
      "Epoch: 6870 \tTraining Loss: 0.184483 \tR2: 0.839623\n",
      "Epoch: 6871 \tTraining Loss: 0.189654 \tR2: 0.839623\n",
      "Epoch: 6872 \tTraining Loss: 0.185335 \tR2: 0.839623\n",
      "Epoch: 6873 \tTraining Loss: 0.184410 \tR2: 0.839623\n",
      "Epoch: 6874 \tTraining Loss: 0.196227 \tR2: 0.839623\n",
      "Epoch: 6875 \tTraining Loss: 0.187230 \tR2: 0.839623\n",
      "Epoch: 6876 \tTraining Loss: 0.170411 \tR2: 0.839623\n",
      "Epoch: 6877 \tTraining Loss: 0.180557 \tR2: 0.839623\n",
      "Epoch: 6878 \tTraining Loss: 0.192762 \tR2: 0.839623\n",
      "Epoch: 6879 \tTraining Loss: 0.191156 \tR2: 0.839623\n",
      "Epoch: 6880 \tTraining Loss: 0.198686 \tR2: 0.839623\n",
      "Epoch: 6881 \tTraining Loss: 0.187219 \tR2: 0.839623\n",
      "Epoch: 6882 \tTraining Loss: 0.177031 \tR2: 0.839623\n",
      "Epoch: 6883 \tTraining Loss: 0.179924 \tR2: 0.839623\n",
      "Epoch: 6884 \tTraining Loss: 0.194438 \tR2: 0.839623\n",
      "Epoch: 6885 \tTraining Loss: 0.205971 \tR2: 0.839623\n",
      "Epoch: 6886 \tTraining Loss: 0.193088 \tR2: 0.839623\n",
      "Epoch: 6887 \tTraining Loss: 0.179352 \tR2: 0.839623\n",
      "Epoch: 6888 \tTraining Loss: 0.208636 \tR2: 0.839623\n",
      "Epoch: 6889 \tTraining Loss: 0.194199 \tR2: 0.839623\n",
      "Epoch: 6890 \tTraining Loss: 0.195199 \tR2: 0.839623\n",
      "Epoch: 6891 \tTraining Loss: 0.174933 \tR2: 0.839623\n",
      "Epoch: 6892 \tTraining Loss: 0.180977 \tR2: 0.839623\n",
      "Epoch: 6893 \tTraining Loss: 0.180935 \tR2: 0.839623\n",
      "Epoch: 6894 \tTraining Loss: 0.167706 \tR2: 0.839623\n",
      "Epoch: 6895 \tTraining Loss: 0.177765 \tR2: 0.839623\n",
      "Epoch: 6896 \tTraining Loss: 0.184649 \tR2: 0.839623\n",
      "Epoch: 6897 \tTraining Loss: 0.185494 \tR2: 0.839623\n",
      "Epoch: 6898 \tTraining Loss: 0.193981 \tR2: 0.839623\n",
      "Epoch: 6899 \tTraining Loss: 0.185043 \tR2: 0.839623\n",
      "Epoch: 6900 \tTraining Loss: 0.177739 \tR2: 0.621087\n",
      "Epoch: 6901 \tTraining Loss: 0.192757 \tR2: 0.621087\n",
      "Epoch: 6902 \tTraining Loss: 0.191804 \tR2: 0.621087\n",
      "Epoch: 6903 \tTraining Loss: 0.205523 \tR2: 0.621087\n",
      "Epoch: 6904 \tTraining Loss: 0.203684 \tR2: 0.621087\n",
      "Epoch: 6905 \tTraining Loss: 0.204054 \tR2: 0.621087\n",
      "Epoch: 6906 \tTraining Loss: 0.202343 \tR2: 0.621087\n",
      "Epoch: 6907 \tTraining Loss: 0.164147 \tR2: 0.621087\n",
      "Epoch: 6908 \tTraining Loss: 0.179753 \tR2: 0.621087\n",
      "Epoch: 6909 \tTraining Loss: 0.197912 \tR2: 0.621087\n",
      "Epoch: 6910 \tTraining Loss: 0.189134 \tR2: 0.621087\n",
      "Epoch: 6911 \tTraining Loss: 0.193392 \tR2: 0.621087\n",
      "Epoch: 6912 \tTraining Loss: 0.194735 \tR2: 0.621087\n",
      "Epoch: 6913 \tTraining Loss: 0.197969 \tR2: 0.621087\n",
      "Epoch: 6914 \tTraining Loss: 0.181804 \tR2: 0.621087\n",
      "Epoch: 6915 \tTraining Loss: 0.191092 \tR2: 0.621087\n",
      "Epoch: 6916 \tTraining Loss: 0.182441 \tR2: 0.621087\n",
      "Epoch: 6917 \tTraining Loss: 0.189192 \tR2: 0.621087\n",
      "Epoch: 6918 \tTraining Loss: 0.178948 \tR2: 0.621087\n",
      "Epoch: 6919 \tTraining Loss: 0.191547 \tR2: 0.621087\n",
      "Epoch: 6920 \tTraining Loss: 0.190887 \tR2: 0.621087\n",
      "Epoch: 6921 \tTraining Loss: 0.202456 \tR2: 0.621087\n",
      "Epoch: 6922 \tTraining Loss: 0.173780 \tR2: 0.621087\n",
      "Epoch: 6923 \tTraining Loss: 0.172971 \tR2: 0.621087\n",
      "Epoch: 6924 \tTraining Loss: 0.184987 \tR2: 0.621087\n",
      "Epoch: 6925 \tTraining Loss: 0.183190 \tR2: 0.621087\n",
      "Epoch: 6926 \tTraining Loss: 0.171107 \tR2: 0.621087\n",
      "Epoch: 6927 \tTraining Loss: 0.182083 \tR2: 0.621087\n",
      "Epoch: 6928 \tTraining Loss: 0.167425 \tR2: 0.621087\n",
      "Epoch: 6929 \tTraining Loss: 0.183312 \tR2: 0.621087\n",
      "Epoch: 6930 \tTraining Loss: 0.199242 \tR2: 0.621087\n",
      "Epoch: 6931 \tTraining Loss: 0.178662 \tR2: 0.621087\n",
      "Epoch: 6932 \tTraining Loss: 0.170826 \tR2: 0.621087\n",
      "Epoch: 6933 \tTraining Loss: 0.185750 \tR2: 0.621087\n",
      "Epoch: 6934 \tTraining Loss: 0.188565 \tR2: 0.621087\n",
      "Epoch: 6935 \tTraining Loss: 0.200400 \tR2: 0.621087\n",
      "Epoch: 6936 \tTraining Loss: 0.184722 \tR2: 0.621087\n",
      "Epoch: 6937 \tTraining Loss: 0.180348 \tR2: 0.621087\n",
      "Epoch: 6938 \tTraining Loss: 0.189502 \tR2: 0.621087\n",
      "Epoch: 6939 \tTraining Loss: 0.189994 \tR2: 0.621087\n",
      "Epoch: 6940 \tTraining Loss: 0.180816 \tR2: 0.621087\n",
      "Epoch: 6941 \tTraining Loss: 0.183265 \tR2: 0.621087\n",
      "Epoch: 6942 \tTraining Loss: 0.189071 \tR2: 0.621087\n",
      "Epoch: 6943 \tTraining Loss: 0.209978 \tR2: 0.621087\n",
      "Epoch: 6944 \tTraining Loss: 0.188674 \tR2: 0.621087\n",
      "Epoch: 6945 \tTraining Loss: 0.195058 \tR2: 0.621087\n",
      "Epoch: 6946 \tTraining Loss: 0.171832 \tR2: 0.621087\n",
      "Epoch: 6947 \tTraining Loss: 0.206872 \tR2: 0.621087\n",
      "Epoch: 6948 \tTraining Loss: 0.198304 \tR2: 0.621087\n",
      "Epoch: 6949 \tTraining Loss: 0.179396 \tR2: 0.621087\n",
      "Epoch: 6950 \tTraining Loss: 0.189202 \tR2: 0.621087\n",
      "Epoch: 6951 \tTraining Loss: 0.179741 \tR2: 0.621087\n",
      "Epoch: 6952 \tTraining Loss: 0.181029 \tR2: 0.621087\n",
      "Epoch: 6953 \tTraining Loss: 0.187649 \tR2: 0.621087\n",
      "Epoch: 6954 \tTraining Loss: 0.189101 \tR2: 0.621087\n",
      "Epoch: 6955 \tTraining Loss: 0.182187 \tR2: 0.621087\n",
      "Epoch: 6956 \tTraining Loss: 0.196509 \tR2: 0.621087\n",
      "Epoch: 6957 \tTraining Loss: 0.195146 \tR2: 0.621087\n",
      "Epoch: 6958 \tTraining Loss: 0.200632 \tR2: 0.621087\n",
      "Epoch: 6959 \tTraining Loss: 0.209146 \tR2: 0.621087\n",
      "Epoch: 6960 \tTraining Loss: 0.195123 \tR2: 0.621087\n",
      "Epoch: 6961 \tTraining Loss: 0.202752 \tR2: 0.621087\n",
      "Epoch: 6962 \tTraining Loss: 0.186171 \tR2: 0.621087\n",
      "Epoch: 6963 \tTraining Loss: 0.171447 \tR2: 0.621087\n",
      "Epoch: 6964 \tTraining Loss: 0.191822 \tR2: 0.621087\n",
      "Epoch: 6965 \tTraining Loss: 0.178366 \tR2: 0.621087\n",
      "Epoch: 6966 \tTraining Loss: 0.192828 \tR2: 0.621087\n",
      "Epoch: 6967 \tTraining Loss: 0.175671 \tR2: 0.621087\n",
      "Epoch: 6968 \tTraining Loss: 0.191142 \tR2: 0.621087\n",
      "Epoch: 6969 \tTraining Loss: 0.206654 \tR2: 0.621087\n",
      "Epoch: 6970 \tTraining Loss: 0.179400 \tR2: 0.621087\n",
      "Epoch: 6971 \tTraining Loss: 0.183622 \tR2: 0.621087\n",
      "Epoch: 6972 \tTraining Loss: 0.197972 \tR2: 0.621087\n",
      "Epoch: 6973 \tTraining Loss: 0.182808 \tR2: 0.621087\n",
      "Epoch: 6974 \tTraining Loss: 0.212007 \tR2: 0.621087\n",
      "Epoch: 6975 \tTraining Loss: 0.179377 \tR2: 0.621087\n",
      "Epoch: 6976 \tTraining Loss: 0.177429 \tR2: 0.621087\n",
      "Epoch: 6977 \tTraining Loss: 0.199645 \tR2: 0.621087\n",
      "Epoch: 6978 \tTraining Loss: 0.199468 \tR2: 0.621087\n",
      "Epoch: 6979 \tTraining Loss: 0.195817 \tR2: 0.621087\n",
      "Epoch: 6980 \tTraining Loss: 0.189857 \tR2: 0.621087\n",
      "Epoch: 6981 \tTraining Loss: 0.177454 \tR2: 0.621087\n",
      "Epoch: 6982 \tTraining Loss: 0.176936 \tR2: 0.621087\n",
      "Epoch: 6983 \tTraining Loss: 0.178876 \tR2: 0.621087\n",
      "Epoch: 6984 \tTraining Loss: 0.187259 \tR2: 0.621087\n",
      "Epoch: 6985 \tTraining Loss: 0.178771 \tR2: 0.621087\n",
      "Epoch: 6986 \tTraining Loss: 0.190093 \tR2: 0.621087\n",
      "Epoch: 6987 \tTraining Loss: 0.194806 \tR2: 0.621087\n",
      "Epoch: 6988 \tTraining Loss: 0.187759 \tR2: 0.621087\n",
      "Epoch: 6989 \tTraining Loss: 0.176982 \tR2: 0.621087\n",
      "Epoch: 6990 \tTraining Loss: 0.192887 \tR2: 0.621087\n",
      "Epoch: 6991 \tTraining Loss: 0.194372 \tR2: 0.621087\n",
      "Epoch: 6992 \tTraining Loss: 0.178357 \tR2: 0.621087\n",
      "Epoch: 6993 \tTraining Loss: 0.182928 \tR2: 0.621087\n",
      "Epoch: 6994 \tTraining Loss: 0.188679 \tR2: 0.621087\n",
      "Epoch: 6995 \tTraining Loss: 0.211299 \tR2: 0.621087\n",
      "Epoch: 6996 \tTraining Loss: 0.189605 \tR2: 0.621087\n",
      "Epoch: 6997 \tTraining Loss: 0.188936 \tR2: 0.621087\n",
      "Epoch: 6998 \tTraining Loss: 0.200066 \tR2: 0.621087\n",
      "Epoch: 6999 \tTraining Loss: 0.187901 \tR2: 0.621087\n",
      "Epoch: 7000 \tTraining Loss: 0.183235 \tR2: 0.779587\n",
      "Epoch: 7001 \tTraining Loss: 0.184785 \tR2: 0.779587\n",
      "Epoch: 7002 \tTraining Loss: 0.205300 \tR2: 0.779587\n",
      "Epoch: 7003 \tTraining Loss: 0.195662 \tR2: 0.779587\n",
      "Epoch: 7004 \tTraining Loss: 0.198685 \tR2: 0.779587\n",
      "Epoch: 7005 \tTraining Loss: 0.186031 \tR2: 0.779587\n",
      "Epoch: 7006 \tTraining Loss: 0.190162 \tR2: 0.779587\n",
      "Epoch: 7007 \tTraining Loss: 0.187307 \tR2: 0.779587\n",
      "Epoch: 7008 \tTraining Loss: 0.195893 \tR2: 0.779587\n",
      "Epoch: 7009 \tTraining Loss: 0.191117 \tR2: 0.779587\n",
      "Epoch: 7010 \tTraining Loss: 0.180987 \tR2: 0.779587\n",
      "Epoch: 7011 \tTraining Loss: 0.189947 \tR2: 0.779587\n",
      "Epoch: 7012 \tTraining Loss: 0.176589 \tR2: 0.779587\n",
      "Epoch: 7013 \tTraining Loss: 0.192781 \tR2: 0.779587\n",
      "Epoch: 7014 \tTraining Loss: 0.188361 \tR2: 0.779587\n",
      "Epoch: 7015 \tTraining Loss: 0.201558 \tR2: 0.779587\n",
      "Epoch: 7016 \tTraining Loss: 0.190899 \tR2: 0.779587\n",
      "Epoch: 7017 \tTraining Loss: 0.168822 \tR2: 0.779587\n",
      "Epoch: 7018 \tTraining Loss: 0.177636 \tR2: 0.779587\n",
      "Epoch: 7019 \tTraining Loss: 0.185611 \tR2: 0.779587\n",
      "Epoch: 7020 \tTraining Loss: 0.187316 \tR2: 0.779587\n",
      "Epoch: 7021 \tTraining Loss: 0.178401 \tR2: 0.779587\n",
      "Epoch: 7022 \tTraining Loss: 0.199540 \tR2: 0.779587\n",
      "Epoch: 7023 \tTraining Loss: 0.183174 \tR2: 0.779587\n",
      "Epoch: 7024 \tTraining Loss: 0.178872 \tR2: 0.779587\n",
      "Epoch: 7025 \tTraining Loss: 0.187661 \tR2: 0.779587\n",
      "Epoch: 7026 \tTraining Loss: 0.188505 \tR2: 0.779587\n",
      "Epoch: 7027 \tTraining Loss: 0.186274 \tR2: 0.779587\n",
      "Epoch: 7028 \tTraining Loss: 0.206398 \tR2: 0.779587\n",
      "Epoch: 7029 \tTraining Loss: 0.181784 \tR2: 0.779587\n",
      "Epoch: 7030 \tTraining Loss: 0.185685 \tR2: 0.779587\n",
      "Epoch: 7031 \tTraining Loss: 0.207413 \tR2: 0.779587\n",
      "Epoch: 7032 \tTraining Loss: 0.196846 \tR2: 0.779587\n",
      "Epoch: 7033 \tTraining Loss: 0.184103 \tR2: 0.779587\n",
      "Epoch: 7034 \tTraining Loss: 0.177172 \tR2: 0.779587\n",
      "Epoch: 7035 \tTraining Loss: 0.223279 \tR2: 0.779587\n",
      "Epoch: 7036 \tTraining Loss: 0.216605 \tR2: 0.779587\n",
      "Epoch: 7037 \tTraining Loss: 0.182604 \tR2: 0.779587\n",
      "Epoch: 7038 \tTraining Loss: 0.194778 \tR2: 0.779587\n",
      "Epoch: 7039 \tTraining Loss: 0.185959 \tR2: 0.779587\n",
      "Epoch: 7040 \tTraining Loss: 0.191487 \tR2: 0.779587\n",
      "Epoch: 7041 \tTraining Loss: 0.179174 \tR2: 0.779587\n",
      "Epoch: 7042 \tTraining Loss: 0.197256 \tR2: 0.779587\n",
      "Epoch: 7043 \tTraining Loss: 0.192008 \tR2: 0.779587\n",
      "Epoch: 7044 \tTraining Loss: 0.192359 \tR2: 0.779587\n",
      "Epoch: 7045 \tTraining Loss: 0.190684 \tR2: 0.779587\n",
      "Epoch: 7046 \tTraining Loss: 0.190691 \tR2: 0.779587\n",
      "Epoch: 7047 \tTraining Loss: 0.184880 \tR2: 0.779587\n",
      "Epoch: 7048 \tTraining Loss: 0.177120 \tR2: 0.779587\n",
      "Epoch: 7049 \tTraining Loss: 0.182632 \tR2: 0.779587\n",
      "Epoch: 7050 \tTraining Loss: 0.196932 \tR2: 0.779587\n",
      "Epoch: 7051 \tTraining Loss: 0.184671 \tR2: 0.779587\n",
      "Epoch: 7052 \tTraining Loss: 0.190799 \tR2: 0.779587\n",
      "Epoch: 7053 \tTraining Loss: 0.186661 \tR2: 0.779587\n",
      "Epoch: 7054 \tTraining Loss: 0.177135 \tR2: 0.779587\n",
      "Epoch: 7055 \tTraining Loss: 0.176223 \tR2: 0.779587\n",
      "Epoch: 7056 \tTraining Loss: 0.187682 \tR2: 0.779587\n",
      "Epoch: 7057 \tTraining Loss: 0.168101 \tR2: 0.779587\n",
      "Epoch: 7058 \tTraining Loss: 0.187473 \tR2: 0.779587\n",
      "Epoch: 7059 \tTraining Loss: 0.187035 \tR2: 0.779587\n",
      "Epoch: 7060 \tTraining Loss: 0.181404 \tR2: 0.779587\n",
      "Epoch: 7061 \tTraining Loss: 0.180082 \tR2: 0.779587\n",
      "Epoch: 7062 \tTraining Loss: 0.194512 \tR2: 0.779587\n",
      "Epoch: 7063 \tTraining Loss: 0.199586 \tR2: 0.779587\n",
      "Epoch: 7064 \tTraining Loss: 0.193686 \tR2: 0.779587\n",
      "Epoch: 7065 \tTraining Loss: 0.185717 \tR2: 0.779587\n",
      "Epoch: 7066 \tTraining Loss: 0.180334 \tR2: 0.779587\n",
      "Epoch: 7067 \tTraining Loss: 0.181654 \tR2: 0.779587\n",
      "Epoch: 7068 \tTraining Loss: 0.197662 \tR2: 0.779587\n",
      "Epoch: 7069 \tTraining Loss: 0.210045 \tR2: 0.779587\n",
      "Epoch: 7070 \tTraining Loss: 0.198649 \tR2: 0.779587\n",
      "Epoch: 7071 \tTraining Loss: 0.180558 \tR2: 0.779587\n",
      "Epoch: 7072 \tTraining Loss: 0.187202 \tR2: 0.779587\n",
      "Epoch: 7073 \tTraining Loss: 0.197720 \tR2: 0.779587\n",
      "Epoch: 7074 \tTraining Loss: 0.183030 \tR2: 0.779587\n",
      "Epoch: 7075 \tTraining Loss: 0.181420 \tR2: 0.779587\n",
      "Epoch: 7076 \tTraining Loss: 0.181212 \tR2: 0.779587\n",
      "Epoch: 7077 \tTraining Loss: 0.188442 \tR2: 0.779587\n",
      "Epoch: 7078 \tTraining Loss: 0.202352 \tR2: 0.779587\n",
      "Epoch: 7079 \tTraining Loss: 0.188622 \tR2: 0.779587\n",
      "Epoch: 7080 \tTraining Loss: 0.185847 \tR2: 0.779587\n",
      "Epoch: 7081 \tTraining Loss: 0.197247 \tR2: 0.779587\n",
      "Epoch: 7082 \tTraining Loss: 0.186118 \tR2: 0.779587\n",
      "Epoch: 7083 \tTraining Loss: 0.188141 \tR2: 0.779587\n",
      "Epoch: 7084 \tTraining Loss: 0.180038 \tR2: 0.779587\n",
      "Epoch: 7085 \tTraining Loss: 0.192642 \tR2: 0.779587\n",
      "Epoch: 7086 \tTraining Loss: 0.189473 \tR2: 0.779587\n",
      "Epoch: 7087 \tTraining Loss: 0.171977 \tR2: 0.779587\n",
      "Epoch: 7088 \tTraining Loss: 0.189901 \tR2: 0.779587\n",
      "Epoch: 7089 \tTraining Loss: 0.177608 \tR2: 0.779587\n",
      "Epoch: 7090 \tTraining Loss: 0.190799 \tR2: 0.779587\n",
      "Epoch: 7091 \tTraining Loss: 0.168499 \tR2: 0.779587\n",
      "Epoch: 7092 \tTraining Loss: 0.188646 \tR2: 0.779587\n",
      "Epoch: 7093 \tTraining Loss: 0.179744 \tR2: 0.779587\n",
      "Epoch: 7094 \tTraining Loss: 0.177568 \tR2: 0.779587\n",
      "Epoch: 7095 \tTraining Loss: 0.197899 \tR2: 0.779587\n",
      "Epoch: 7096 \tTraining Loss: 0.176963 \tR2: 0.779587\n",
      "Epoch: 7097 \tTraining Loss: 0.194909 \tR2: 0.779587\n",
      "Epoch: 7098 \tTraining Loss: 0.183917 \tR2: 0.779587\n",
      "Epoch: 7099 \tTraining Loss: 0.179400 \tR2: 0.779587\n",
      "Epoch: 7100 \tTraining Loss: 0.190423 \tR2: 0.823688\n",
      "Epoch: 7101 \tTraining Loss: 0.165204 \tR2: 0.823688\n",
      "Epoch: 7102 \tTraining Loss: 0.222071 \tR2: 0.823688\n",
      "Epoch: 7103 \tTraining Loss: 0.191976 \tR2: 0.823688\n",
      "Epoch: 7104 \tTraining Loss: 0.178383 \tR2: 0.823688\n",
      "Epoch: 7105 \tTraining Loss: 0.185442 \tR2: 0.823688\n",
      "Epoch: 7106 \tTraining Loss: 0.191725 \tR2: 0.823688\n",
      "Epoch: 7107 \tTraining Loss: 0.184509 \tR2: 0.823688\n",
      "Epoch: 7108 \tTraining Loss: 0.183794 \tR2: 0.823688\n",
      "Epoch: 7109 \tTraining Loss: 0.180843 \tR2: 0.823688\n",
      "Epoch: 7110 \tTraining Loss: 0.203143 \tR2: 0.823688\n",
      "Epoch: 7111 \tTraining Loss: 0.189243 \tR2: 0.823688\n",
      "Epoch: 7112 \tTraining Loss: 0.188625 \tR2: 0.823688\n",
      "Epoch: 7113 \tTraining Loss: 0.198544 \tR2: 0.823688\n",
      "Epoch: 7114 \tTraining Loss: 0.182269 \tR2: 0.823688\n",
      "Epoch: 7115 \tTraining Loss: 0.188246 \tR2: 0.823688\n",
      "Epoch: 7116 \tTraining Loss: 0.189461 \tR2: 0.823688\n",
      "Epoch: 7117 \tTraining Loss: 0.172440 \tR2: 0.823688\n",
      "Epoch: 7118 \tTraining Loss: 0.182536 \tR2: 0.823688\n",
      "Epoch: 7119 \tTraining Loss: 0.180820 \tR2: 0.823688\n",
      "Epoch: 7120 \tTraining Loss: 0.179162 \tR2: 0.823688\n",
      "Epoch: 7121 \tTraining Loss: 0.183507 \tR2: 0.823688\n",
      "Epoch: 7122 \tTraining Loss: 0.188275 \tR2: 0.823688\n",
      "Epoch: 7123 \tTraining Loss: 0.173379 \tR2: 0.823688\n",
      "Epoch: 7124 \tTraining Loss: 0.190078 \tR2: 0.823688\n",
      "Epoch: 7125 \tTraining Loss: 0.198968 \tR2: 0.823688\n",
      "Epoch: 7126 \tTraining Loss: 0.179312 \tR2: 0.823688\n",
      "Epoch: 7127 \tTraining Loss: 0.179863 \tR2: 0.823688\n",
      "Epoch: 7128 \tTraining Loss: 0.172282 \tR2: 0.823688\n",
      "Epoch: 7129 \tTraining Loss: 0.171786 \tR2: 0.823688\n",
      "Epoch: 7130 \tTraining Loss: 0.195968 \tR2: 0.823688\n",
      "Epoch: 7131 \tTraining Loss: 0.179604 \tR2: 0.823688\n",
      "Epoch: 7132 \tTraining Loss: 0.173289 \tR2: 0.823688\n",
      "Epoch: 7133 \tTraining Loss: 0.206767 \tR2: 0.823688\n",
      "Epoch: 7134 \tTraining Loss: 0.190402 \tR2: 0.823688\n",
      "Epoch: 7135 \tTraining Loss: 0.226815 \tR2: 0.823688\n",
      "Epoch: 7136 \tTraining Loss: 0.178584 \tR2: 0.823688\n",
      "Epoch: 7137 \tTraining Loss: 0.175849 \tR2: 0.823688\n",
      "Epoch: 7138 \tTraining Loss: 0.202987 \tR2: 0.823688\n",
      "Epoch: 7139 \tTraining Loss: 0.212592 \tR2: 0.823688\n",
      "Epoch: 7140 \tTraining Loss: 0.193012 \tR2: 0.823688\n",
      "Epoch: 7141 \tTraining Loss: 0.182772 \tR2: 0.823688\n",
      "Epoch: 7142 \tTraining Loss: 0.186542 \tR2: 0.823688\n",
      "Epoch: 7143 \tTraining Loss: 0.188797 \tR2: 0.823688\n",
      "Epoch: 7144 \tTraining Loss: 0.179348 \tR2: 0.823688\n",
      "Epoch: 7145 \tTraining Loss: 0.186862 \tR2: 0.823688\n",
      "Epoch: 7146 \tTraining Loss: 0.186156 \tR2: 0.823688\n",
      "Epoch: 7147 \tTraining Loss: 0.200017 \tR2: 0.823688\n",
      "Epoch: 7148 \tTraining Loss: 0.182974 \tR2: 0.823688\n",
      "Epoch: 7149 \tTraining Loss: 0.180767 \tR2: 0.823688\n",
      "Epoch: 7150 \tTraining Loss: 0.183614 \tR2: 0.823688\n",
      "Epoch: 7151 \tTraining Loss: 0.188617 \tR2: 0.823688\n",
      "Epoch: 7152 \tTraining Loss: 0.185405 \tR2: 0.823688\n",
      "Epoch: 7153 \tTraining Loss: 0.209174 \tR2: 0.823688\n",
      "Epoch: 7154 \tTraining Loss: 0.211844 \tR2: 0.823688\n",
      "Epoch: 7155 \tTraining Loss: 0.186179 \tR2: 0.823688\n",
      "Epoch: 7156 \tTraining Loss: 0.177331 \tR2: 0.823688\n",
      "Epoch: 7157 \tTraining Loss: 0.196742 \tR2: 0.823688\n",
      "Epoch: 7158 \tTraining Loss: 0.203536 \tR2: 0.823688\n",
      "Epoch: 7159 \tTraining Loss: 0.186342 \tR2: 0.823688\n",
      "Epoch: 7160 \tTraining Loss: 0.190302 \tR2: 0.823688\n",
      "Epoch: 7161 \tTraining Loss: 0.184100 \tR2: 0.823688\n",
      "Epoch: 7162 \tTraining Loss: 0.182917 \tR2: 0.823688\n",
      "Epoch: 7163 \tTraining Loss: 0.202036 \tR2: 0.823688\n",
      "Epoch: 7164 \tTraining Loss: 0.177262 \tR2: 0.823688\n",
      "Epoch: 7165 \tTraining Loss: 0.178737 \tR2: 0.823688\n",
      "Epoch: 7166 \tTraining Loss: 0.173796 \tR2: 0.823688\n",
      "Epoch: 7167 \tTraining Loss: 0.183684 \tR2: 0.823688\n",
      "Epoch: 7168 \tTraining Loss: 0.180657 \tR2: 0.823688\n",
      "Epoch: 7169 \tTraining Loss: 0.176090 \tR2: 0.823688\n",
      "Epoch: 7170 \tTraining Loss: 0.177233 \tR2: 0.823688\n",
      "Epoch: 7171 \tTraining Loss: 0.191241 \tR2: 0.823688\n",
      "Epoch: 7172 \tTraining Loss: 0.177306 \tR2: 0.823688\n",
      "Epoch: 7173 \tTraining Loss: 0.172860 \tR2: 0.823688\n",
      "Epoch: 7174 \tTraining Loss: 0.178081 \tR2: 0.823688\n",
      "Epoch: 7175 \tTraining Loss: 0.208921 \tR2: 0.823688\n",
      "Epoch: 7176 \tTraining Loss: 0.185960 \tR2: 0.823688\n",
      "Epoch: 7177 \tTraining Loss: 0.193927 \tR2: 0.823688\n",
      "Epoch: 7178 \tTraining Loss: 0.179714 \tR2: 0.823688\n",
      "Epoch: 7179 \tTraining Loss: 0.177224 \tR2: 0.823688\n",
      "Epoch: 7180 \tTraining Loss: 0.196449 \tR2: 0.823688\n",
      "Epoch: 7181 \tTraining Loss: 0.183733 \tR2: 0.823688\n",
      "Epoch: 7182 \tTraining Loss: 0.171026 \tR2: 0.823688\n",
      "Epoch: 7183 \tTraining Loss: 0.181614 \tR2: 0.823688\n",
      "Epoch: 7184 \tTraining Loss: 0.188313 \tR2: 0.823688\n",
      "Epoch: 7185 \tTraining Loss: 0.181949 \tR2: 0.823688\n",
      "Epoch: 7186 \tTraining Loss: 0.185743 \tR2: 0.823688\n",
      "Epoch: 7187 \tTraining Loss: 0.192325 \tR2: 0.823688\n",
      "Epoch: 7188 \tTraining Loss: 0.183916 \tR2: 0.823688\n",
      "Epoch: 7189 \tTraining Loss: 0.165299 \tR2: 0.823688\n",
      "Epoch: 7190 \tTraining Loss: 0.184954 \tR2: 0.823688\n",
      "Epoch: 7191 \tTraining Loss: 0.222416 \tR2: 0.823688\n",
      "Epoch: 7192 \tTraining Loss: 0.197576 \tR2: 0.823688\n",
      "Epoch: 7193 \tTraining Loss: 0.175797 \tR2: 0.823688\n",
      "Epoch: 7194 \tTraining Loss: 0.176244 \tR2: 0.823688\n",
      "Epoch: 7195 \tTraining Loss: 0.174551 \tR2: 0.823688\n",
      "Epoch: 7196 \tTraining Loss: 0.193483 \tR2: 0.823688\n",
      "Epoch: 7197 \tTraining Loss: 0.200921 \tR2: 0.823688\n",
      "Epoch: 7198 \tTraining Loss: 0.177929 \tR2: 0.823688\n",
      "Epoch: 7199 \tTraining Loss: 0.171100 \tR2: 0.823688\n",
      "Epoch: 7200 \tTraining Loss: 0.188672 \tR2: 0.767294\n",
      "Epoch: 7201 \tTraining Loss: 0.199064 \tR2: 0.767294\n",
      "Epoch: 7202 \tTraining Loss: 0.176195 \tR2: 0.767294\n",
      "Epoch: 7203 \tTraining Loss: 0.181017 \tR2: 0.767294\n",
      "Epoch: 7204 \tTraining Loss: 0.186571 \tR2: 0.767294\n",
      "Epoch: 7205 \tTraining Loss: 0.193927 \tR2: 0.767294\n",
      "Epoch: 7206 \tTraining Loss: 0.182003 \tR2: 0.767294\n",
      "Epoch: 7207 \tTraining Loss: 0.163006 \tR2: 0.767294\n",
      "Epoch: 7208 \tTraining Loss: 0.184928 \tR2: 0.767294\n",
      "Epoch: 7209 \tTraining Loss: 0.195081 \tR2: 0.767294\n",
      "Epoch: 7210 \tTraining Loss: 0.188822 \tR2: 0.767294\n",
      "Epoch: 7211 \tTraining Loss: 0.194186 \tR2: 0.767294\n",
      "Epoch: 7212 \tTraining Loss: 0.198805 \tR2: 0.767294\n",
      "Epoch: 7213 \tTraining Loss: 0.185557 \tR2: 0.767294\n",
      "Epoch: 7214 \tTraining Loss: 0.174947 \tR2: 0.767294\n",
      "Epoch: 7215 \tTraining Loss: 0.179403 \tR2: 0.767294\n",
      "Epoch: 7216 \tTraining Loss: 0.188925 \tR2: 0.767294\n",
      "Epoch: 7217 \tTraining Loss: 0.186740 \tR2: 0.767294\n",
      "Epoch: 7218 \tTraining Loss: 0.186449 \tR2: 0.767294\n",
      "Epoch: 7219 \tTraining Loss: 0.195499 \tR2: 0.767294\n",
      "Epoch: 7220 \tTraining Loss: 0.197329 \tR2: 0.767294\n",
      "Epoch: 7221 \tTraining Loss: 0.193611 \tR2: 0.767294\n",
      "Epoch: 7222 \tTraining Loss: 0.187967 \tR2: 0.767294\n",
      "Epoch: 7223 \tTraining Loss: 0.197959 \tR2: 0.767294\n",
      "Epoch: 7224 \tTraining Loss: 0.179342 \tR2: 0.767294\n",
      "Epoch: 7225 \tTraining Loss: 0.172888 \tR2: 0.767294\n",
      "Epoch: 7226 \tTraining Loss: 0.175944 \tR2: 0.767294\n",
      "Epoch: 7227 \tTraining Loss: 0.191512 \tR2: 0.767294\n",
      "Epoch: 7228 \tTraining Loss: 0.192743 \tR2: 0.767294\n",
      "Epoch: 7229 \tTraining Loss: 0.211556 \tR2: 0.767294\n",
      "Epoch: 7230 \tTraining Loss: 0.184308 \tR2: 0.767294\n",
      "Epoch: 7231 \tTraining Loss: 0.195977 \tR2: 0.767294\n",
      "Epoch: 7232 \tTraining Loss: 0.175948 \tR2: 0.767294\n",
      "Epoch: 7233 \tTraining Loss: 0.186188 \tR2: 0.767294\n",
      "Epoch: 7234 \tTraining Loss: 0.175196 \tR2: 0.767294\n",
      "Epoch: 7235 \tTraining Loss: 0.196201 \tR2: 0.767294\n",
      "Epoch: 7236 \tTraining Loss: 0.172642 \tR2: 0.767294\n",
      "Epoch: 7237 \tTraining Loss: 0.186038 \tR2: 0.767294\n",
      "Epoch: 7238 \tTraining Loss: 0.181599 \tR2: 0.767294\n",
      "Epoch: 7239 \tTraining Loss: 0.203017 \tR2: 0.767294\n",
      "Epoch: 7240 \tTraining Loss: 0.181745 \tR2: 0.767294\n",
      "Epoch: 7241 \tTraining Loss: 0.191830 \tR2: 0.767294\n",
      "Epoch: 7242 \tTraining Loss: 0.198411 \tR2: 0.767294\n",
      "Epoch: 7243 \tTraining Loss: 0.185083 \tR2: 0.767294\n",
      "Epoch: 7244 \tTraining Loss: 0.168356 \tR2: 0.767294\n",
      "Epoch: 7245 \tTraining Loss: 0.204331 \tR2: 0.767294\n",
      "Epoch: 7246 \tTraining Loss: 0.204806 \tR2: 0.767294\n",
      "Epoch: 7247 \tTraining Loss: 0.219235 \tR2: 0.767294\n",
      "Epoch: 7248 \tTraining Loss: 0.189966 \tR2: 0.767294\n",
      "Epoch: 7249 \tTraining Loss: 0.182030 \tR2: 0.767294\n",
      "Epoch: 7250 \tTraining Loss: 0.189711 \tR2: 0.767294\n",
      "Epoch: 7251 \tTraining Loss: 0.174685 \tR2: 0.767294\n",
      "Epoch: 7252 \tTraining Loss: 0.186509 \tR2: 0.767294\n",
      "Epoch: 7253 \tTraining Loss: 0.177011 \tR2: 0.767294\n",
      "Epoch: 7254 \tTraining Loss: 0.187392 \tR2: 0.767294\n",
      "Epoch: 7255 \tTraining Loss: 0.188649 \tR2: 0.767294\n",
      "Epoch: 7256 \tTraining Loss: 0.184876 \tR2: 0.767294\n",
      "Epoch: 7257 \tTraining Loss: 0.180286 \tR2: 0.767294\n",
      "Epoch: 7258 \tTraining Loss: 0.180528 \tR2: 0.767294\n",
      "Epoch: 7259 \tTraining Loss: 0.177079 \tR2: 0.767294\n",
      "Epoch: 7260 \tTraining Loss: 0.197631 \tR2: 0.767294\n",
      "Epoch: 7261 \tTraining Loss: 0.186649 \tR2: 0.767294\n",
      "Epoch: 7262 \tTraining Loss: 0.196795 \tR2: 0.767294\n",
      "Epoch: 7263 \tTraining Loss: 0.167043 \tR2: 0.767294\n",
      "Epoch: 7264 \tTraining Loss: 0.177453 \tR2: 0.767294\n",
      "Epoch: 7265 \tTraining Loss: 0.167055 \tR2: 0.767294\n",
      "Epoch: 7266 \tTraining Loss: 0.185296 \tR2: 0.767294\n",
      "Epoch: 7267 \tTraining Loss: 0.194690 \tR2: 0.767294\n",
      "Epoch: 7268 \tTraining Loss: 0.196170 \tR2: 0.767294\n",
      "Epoch: 7269 \tTraining Loss: 0.180937 \tR2: 0.767294\n",
      "Epoch: 7270 \tTraining Loss: 0.176630 \tR2: 0.767294\n",
      "Epoch: 7271 \tTraining Loss: 0.194576 \tR2: 0.767294\n",
      "Epoch: 7272 \tTraining Loss: 0.202699 \tR2: 0.767294\n",
      "Epoch: 7273 \tTraining Loss: 0.204884 \tR2: 0.767294\n",
      "Epoch: 7274 \tTraining Loss: 0.182692 \tR2: 0.767294\n",
      "Epoch: 7275 \tTraining Loss: 0.169516 \tR2: 0.767294\n",
      "Epoch: 7276 \tTraining Loss: 0.187746 \tR2: 0.767294\n",
      "Epoch: 7277 \tTraining Loss: 0.184894 \tR2: 0.767294\n",
      "Epoch: 7278 \tTraining Loss: 0.189851 \tR2: 0.767294\n",
      "Epoch: 7279 \tTraining Loss: 0.190801 \tR2: 0.767294\n",
      "Epoch: 7280 \tTraining Loss: 0.195466 \tR2: 0.767294\n",
      "Epoch: 7281 \tTraining Loss: 0.181637 \tR2: 0.767294\n",
      "Epoch: 7282 \tTraining Loss: 0.180009 \tR2: 0.767294\n",
      "Epoch: 7283 \tTraining Loss: 0.172149 \tR2: 0.767294\n",
      "Epoch: 7284 \tTraining Loss: 0.177163 \tR2: 0.767294\n",
      "Epoch: 7285 \tTraining Loss: 0.166178 \tR2: 0.767294\n",
      "Epoch: 7286 \tTraining Loss: 0.187464 \tR2: 0.767294\n",
      "Epoch: 7287 \tTraining Loss: 0.191849 \tR2: 0.767294\n",
      "Epoch: 7288 \tTraining Loss: 0.198744 \tR2: 0.767294\n",
      "Epoch: 7289 \tTraining Loss: 0.177005 \tR2: 0.767294\n",
      "Epoch: 7290 \tTraining Loss: 0.190679 \tR2: 0.767294\n",
      "Epoch: 7291 \tTraining Loss: 0.200973 \tR2: 0.767294\n",
      "Epoch: 7292 \tTraining Loss: 0.175638 \tR2: 0.767294\n",
      "Epoch: 7293 \tTraining Loss: 0.241905 \tR2: 0.767294\n",
      "Epoch: 7294 \tTraining Loss: 0.185049 \tR2: 0.767294\n",
      "Epoch: 7295 \tTraining Loss: 0.191703 \tR2: 0.767294\n",
      "Epoch: 7296 \tTraining Loss: 0.195134 \tR2: 0.767294\n",
      "Epoch: 7297 \tTraining Loss: 0.174618 \tR2: 0.767294\n",
      "Epoch: 7298 \tTraining Loss: 0.174342 \tR2: 0.767294\n",
      "Epoch: 7299 \tTraining Loss: 0.185594 \tR2: 0.767294\n",
      "Epoch: 7300 \tTraining Loss: 0.185739 \tR2: 0.596939\n",
      "Epoch: 7301 \tTraining Loss: 0.208506 \tR2: 0.596939\n",
      "Epoch: 7302 \tTraining Loss: 0.179263 \tR2: 0.596939\n",
      "Epoch: 7303 \tTraining Loss: 0.190650 \tR2: 0.596939\n",
      "Epoch: 7304 \tTraining Loss: 0.195304 \tR2: 0.596939\n",
      "Epoch: 7305 \tTraining Loss: 0.187292 \tR2: 0.596939\n",
      "Epoch: 7306 \tTraining Loss: 0.182595 \tR2: 0.596939\n",
      "Epoch: 7307 \tTraining Loss: 0.186412 \tR2: 0.596939\n",
      "Epoch: 7308 \tTraining Loss: 0.189866 \tR2: 0.596939\n",
      "Epoch: 7309 \tTraining Loss: 0.165674 \tR2: 0.596939\n",
      "Epoch: 7310 \tTraining Loss: 0.171509 \tR2: 0.596939\n",
      "Epoch: 7311 \tTraining Loss: 0.194689 \tR2: 0.596939\n",
      "Epoch: 7312 \tTraining Loss: 0.199609 \tR2: 0.596939\n",
      "Epoch: 7313 \tTraining Loss: 0.187599 \tR2: 0.596939\n",
      "Epoch: 7314 \tTraining Loss: 0.204369 \tR2: 0.596939\n",
      "Epoch: 7315 \tTraining Loss: 0.201014 \tR2: 0.596939\n",
      "Epoch: 7316 \tTraining Loss: 0.202448 \tR2: 0.596939\n",
      "Epoch: 7317 \tTraining Loss: 0.203228 \tR2: 0.596939\n",
      "Epoch: 7318 \tTraining Loss: 0.174072 \tR2: 0.596939\n",
      "Epoch: 7319 \tTraining Loss: 0.203711 \tR2: 0.596939\n",
      "Epoch: 7320 \tTraining Loss: 0.173346 \tR2: 0.596939\n",
      "Epoch: 7321 \tTraining Loss: 0.186277 \tR2: 0.596939\n",
      "Epoch: 7322 \tTraining Loss: 0.168948 \tR2: 0.596939\n",
      "Epoch: 7323 \tTraining Loss: 0.188161 \tR2: 0.596939\n",
      "Epoch: 7324 \tTraining Loss: 0.177833 \tR2: 0.596939\n",
      "Epoch: 7325 \tTraining Loss: 0.178361 \tR2: 0.596939\n",
      "Epoch: 7326 \tTraining Loss: 0.195881 \tR2: 0.596939\n",
      "Epoch: 7327 \tTraining Loss: 0.183941 \tR2: 0.596939\n",
      "Epoch: 7328 \tTraining Loss: 0.175707 \tR2: 0.596939\n",
      "Epoch: 7329 \tTraining Loss: 0.190704 \tR2: 0.596939\n",
      "Epoch: 7330 \tTraining Loss: 0.180361 \tR2: 0.596939\n",
      "Epoch: 7331 \tTraining Loss: 0.185524 \tR2: 0.596939\n",
      "Epoch: 7332 \tTraining Loss: 0.193241 \tR2: 0.596939\n",
      "Epoch: 7333 \tTraining Loss: 0.191657 \tR2: 0.596939\n",
      "Epoch: 7334 \tTraining Loss: 0.194142 \tR2: 0.596939\n",
      "Epoch: 7335 \tTraining Loss: 0.199571 \tR2: 0.596939\n",
      "Epoch: 7336 \tTraining Loss: 0.182770 \tR2: 0.596939\n",
      "Epoch: 7337 \tTraining Loss: 0.195707 \tR2: 0.596939\n",
      "Epoch: 7338 \tTraining Loss: 0.193661 \tR2: 0.596939\n",
      "Epoch: 7339 \tTraining Loss: 0.180010 \tR2: 0.596939\n",
      "Epoch: 7340 \tTraining Loss: 0.190435 \tR2: 0.596939\n",
      "Epoch: 7341 \tTraining Loss: 0.235823 \tR2: 0.596939\n",
      "Epoch: 7342 \tTraining Loss: 0.190367 \tR2: 0.596939\n",
      "Epoch: 7343 \tTraining Loss: 0.191617 \tR2: 0.596939\n",
      "Epoch: 7344 \tTraining Loss: 0.188054 \tR2: 0.596939\n",
      "Epoch: 7345 \tTraining Loss: 0.199048 \tR2: 0.596939\n",
      "Epoch: 7346 \tTraining Loss: 0.188065 \tR2: 0.596939\n",
      "Epoch: 7347 \tTraining Loss: 0.188412 \tR2: 0.596939\n",
      "Epoch: 7348 \tTraining Loss: 0.193376 \tR2: 0.596939\n",
      "Epoch: 7349 \tTraining Loss: 0.186321 \tR2: 0.596939\n",
      "Epoch: 7350 \tTraining Loss: 0.186286 \tR2: 0.596939\n",
      "Epoch: 7351 \tTraining Loss: 0.183614 \tR2: 0.596939\n",
      "Epoch: 7352 \tTraining Loss: 0.183585 \tR2: 0.596939\n",
      "Epoch: 7353 \tTraining Loss: 0.181810 \tR2: 0.596939\n",
      "Epoch: 7354 \tTraining Loss: 0.188602 \tR2: 0.596939\n",
      "Epoch: 7355 \tTraining Loss: 0.191104 \tR2: 0.596939\n",
      "Epoch: 7356 \tTraining Loss: 0.180935 \tR2: 0.596939\n",
      "Epoch: 7357 \tTraining Loss: 0.175299 \tR2: 0.596939\n",
      "Epoch: 7358 \tTraining Loss: 0.175334 \tR2: 0.596939\n",
      "Epoch: 7359 \tTraining Loss: 0.186850 \tR2: 0.596939\n",
      "Epoch: 7360 \tTraining Loss: 0.175964 \tR2: 0.596939\n",
      "Epoch: 7361 \tTraining Loss: 0.202184 \tR2: 0.596939\n",
      "Epoch: 7362 \tTraining Loss: 0.179258 \tR2: 0.596939\n",
      "Epoch: 7363 \tTraining Loss: 0.200614 \tR2: 0.596939\n",
      "Epoch: 7364 \tTraining Loss: 0.188989 \tR2: 0.596939\n",
      "Epoch: 7365 \tTraining Loss: 0.198385 \tR2: 0.596939\n",
      "Epoch: 7366 \tTraining Loss: 0.183970 \tR2: 0.596939\n",
      "Epoch: 7367 \tTraining Loss: 0.192956 \tR2: 0.596939\n",
      "Epoch: 7368 \tTraining Loss: 0.168694 \tR2: 0.596939\n",
      "Epoch: 7369 \tTraining Loss: 0.174861 \tR2: 0.596939\n",
      "Epoch: 7370 \tTraining Loss: 0.172645 \tR2: 0.596939\n",
      "Epoch: 7371 \tTraining Loss: 0.182322 \tR2: 0.596939\n",
      "Epoch: 7372 \tTraining Loss: 0.172178 \tR2: 0.596939\n",
      "Epoch: 7373 \tTraining Loss: 0.183125 \tR2: 0.596939\n",
      "Epoch: 7374 \tTraining Loss: 0.180044 \tR2: 0.596939\n",
      "Epoch: 7375 \tTraining Loss: 0.176824 \tR2: 0.596939\n",
      "Epoch: 7376 \tTraining Loss: 0.184557 \tR2: 0.596939\n",
      "Epoch: 7377 \tTraining Loss: 0.163936 \tR2: 0.596939\n",
      "Epoch: 7378 \tTraining Loss: 0.187482 \tR2: 0.596939\n",
      "Epoch: 7379 \tTraining Loss: 0.181580 \tR2: 0.596939\n",
      "Epoch: 7380 \tTraining Loss: 0.199027 \tR2: 0.596939\n",
      "Epoch: 7381 \tTraining Loss: 0.189215 \tR2: 0.596939\n",
      "Epoch: 7382 \tTraining Loss: 0.184919 \tR2: 0.596939\n",
      "Epoch: 7383 \tTraining Loss: 0.192268 \tR2: 0.596939\n",
      "Epoch: 7384 \tTraining Loss: 0.186551 \tR2: 0.596939\n",
      "Epoch: 7385 \tTraining Loss: 0.191579 \tR2: 0.596939\n",
      "Epoch: 7386 \tTraining Loss: 0.191499 \tR2: 0.596939\n",
      "Epoch: 7387 \tTraining Loss: 0.196753 \tR2: 0.596939\n",
      "Epoch: 7388 \tTraining Loss: 0.186093 \tR2: 0.596939\n",
      "Epoch: 7389 \tTraining Loss: 0.184920 \tR2: 0.596939\n",
      "Epoch: 7390 \tTraining Loss: 0.183031 \tR2: 0.596939\n",
      "Epoch: 7391 \tTraining Loss: 0.181029 \tR2: 0.596939\n",
      "Epoch: 7392 \tTraining Loss: 0.204799 \tR2: 0.596939\n",
      "Epoch: 7393 \tTraining Loss: 0.222775 \tR2: 0.596939\n",
      "Epoch: 7394 \tTraining Loss: 0.208528 \tR2: 0.596939\n",
      "Epoch: 7395 \tTraining Loss: 0.174429 \tR2: 0.596939\n",
      "Epoch: 7396 \tTraining Loss: 0.183972 \tR2: 0.596939\n",
      "Epoch: 7397 \tTraining Loss: 0.180798 \tR2: 0.596939\n",
      "Epoch: 7398 \tTraining Loss: 0.180468 \tR2: 0.596939\n",
      "Epoch: 7399 \tTraining Loss: 0.190442 \tR2: 0.596939\n",
      "Epoch: 7400 \tTraining Loss: 0.186714 \tR2: 0.732053\n",
      "Epoch: 7401 \tTraining Loss: 0.183835 \tR2: 0.732053\n",
      "Epoch: 7402 \tTraining Loss: 0.186011 \tR2: 0.732053\n",
      "Epoch: 7403 \tTraining Loss: 0.188271 \tR2: 0.732053\n",
      "Epoch: 7404 \tTraining Loss: 0.182943 \tR2: 0.732053\n",
      "Epoch: 7405 \tTraining Loss: 0.182203 \tR2: 0.732053\n",
      "Epoch: 7406 \tTraining Loss: 0.202544 \tR2: 0.732053\n",
      "Epoch: 7407 \tTraining Loss: 0.197610 \tR2: 0.732053\n",
      "Epoch: 7408 \tTraining Loss: 0.212474 \tR2: 0.732053\n",
      "Epoch: 7409 \tTraining Loss: 0.196504 \tR2: 0.732053\n",
      "Epoch: 7410 \tTraining Loss: 0.183888 \tR2: 0.732053\n",
      "Epoch: 7411 \tTraining Loss: 0.195497 \tR2: 0.732053\n",
      "Epoch: 7412 \tTraining Loss: 0.196525 \tR2: 0.732053\n",
      "Epoch: 7413 \tTraining Loss: 0.184945 \tR2: 0.732053\n",
      "Epoch: 7414 \tTraining Loss: 0.180883 \tR2: 0.732053\n",
      "Epoch: 7415 \tTraining Loss: 0.186370 \tR2: 0.732053\n",
      "Epoch: 7416 \tTraining Loss: 0.188503 \tR2: 0.732053\n",
      "Epoch: 7417 \tTraining Loss: 0.197452 \tR2: 0.732053\n",
      "Epoch: 7418 \tTraining Loss: 0.179614 \tR2: 0.732053\n",
      "Epoch: 7419 \tTraining Loss: 0.188373 \tR2: 0.732053\n",
      "Epoch: 7420 \tTraining Loss: 0.174731 \tR2: 0.732053\n",
      "Epoch: 7421 \tTraining Loss: 0.181912 \tR2: 0.732053\n",
      "Epoch: 7422 \tTraining Loss: 0.186396 \tR2: 0.732053\n",
      "Epoch: 7423 \tTraining Loss: 0.180488 \tR2: 0.732053\n",
      "Epoch: 7424 \tTraining Loss: 0.189539 \tR2: 0.732053\n",
      "Epoch: 7425 \tTraining Loss: 0.175514 \tR2: 0.732053\n",
      "Epoch: 7426 \tTraining Loss: 0.187202 \tR2: 0.732053\n",
      "Epoch: 7427 \tTraining Loss: 0.177067 \tR2: 0.732053\n",
      "Epoch: 7428 \tTraining Loss: 0.177477 \tR2: 0.732053\n",
      "Epoch: 7429 \tTraining Loss: 0.184502 \tR2: 0.732053\n",
      "Epoch: 7430 \tTraining Loss: 0.198402 \tR2: 0.732053\n",
      "Epoch: 7431 \tTraining Loss: 0.170427 \tR2: 0.732053\n",
      "Epoch: 7432 \tTraining Loss: 0.183090 \tR2: 0.732053\n",
      "Epoch: 7433 \tTraining Loss: 0.192308 \tR2: 0.732053\n",
      "Epoch: 7434 \tTraining Loss: 0.190373 \tR2: 0.732053\n",
      "Epoch: 7435 \tTraining Loss: 0.187296 \tR2: 0.732053\n",
      "Epoch: 7436 \tTraining Loss: 0.170805 \tR2: 0.732053\n",
      "Epoch: 7437 \tTraining Loss: 0.203168 \tR2: 0.732053\n",
      "Epoch: 7438 \tTraining Loss: 0.181032 \tR2: 0.732053\n",
      "Epoch: 7439 \tTraining Loss: 0.198554 \tR2: 0.732053\n",
      "Epoch: 7440 \tTraining Loss: 0.167021 \tR2: 0.732053\n",
      "Epoch: 7441 \tTraining Loss: 0.190261 \tR2: 0.732053\n",
      "Epoch: 7442 \tTraining Loss: 0.191047 \tR2: 0.732053\n",
      "Epoch: 7443 \tTraining Loss: 0.210573 \tR2: 0.732053\n",
      "Epoch: 7444 \tTraining Loss: 0.183675 \tR2: 0.732053\n",
      "Epoch: 7445 \tTraining Loss: 0.186687 \tR2: 0.732053\n",
      "Epoch: 7446 \tTraining Loss: 0.198538 \tR2: 0.732053\n",
      "Epoch: 7447 \tTraining Loss: 0.194111 \tR2: 0.732053\n",
      "Epoch: 7448 \tTraining Loss: 0.168961 \tR2: 0.732053\n",
      "Epoch: 7449 \tTraining Loss: 0.202963 \tR2: 0.732053\n",
      "Epoch: 7450 \tTraining Loss: 0.197405 \tR2: 0.732053\n",
      "Epoch: 7451 \tTraining Loss: 0.206882 \tR2: 0.732053\n",
      "Epoch: 7452 \tTraining Loss: 0.183971 \tR2: 0.732053\n",
      "Epoch: 7453 \tTraining Loss: 0.178581 \tR2: 0.732053\n",
      "Epoch: 7454 \tTraining Loss: 0.187455 \tR2: 0.732053\n",
      "Epoch: 7455 \tTraining Loss: 0.193091 \tR2: 0.732053\n",
      "Epoch: 7456 \tTraining Loss: 0.180749 \tR2: 0.732053\n",
      "Epoch: 7457 \tTraining Loss: 0.193911 \tR2: 0.732053\n",
      "Epoch: 7458 \tTraining Loss: 0.178464 \tR2: 0.732053\n",
      "Epoch: 7459 \tTraining Loss: 0.180129 \tR2: 0.732053\n",
      "Epoch: 7460 \tTraining Loss: 0.184383 \tR2: 0.732053\n",
      "Epoch: 7461 \tTraining Loss: 0.182085 \tR2: 0.732053\n",
      "Epoch: 7462 \tTraining Loss: 0.196311 \tR2: 0.732053\n",
      "Epoch: 7463 \tTraining Loss: 0.190581 \tR2: 0.732053\n",
      "Epoch: 7464 \tTraining Loss: 0.185635 \tR2: 0.732053\n",
      "Epoch: 7465 \tTraining Loss: 0.173214 \tR2: 0.732053\n",
      "Epoch: 7466 \tTraining Loss: 0.174455 \tR2: 0.732053\n",
      "Epoch: 7467 \tTraining Loss: 0.173948 \tR2: 0.732053\n",
      "Epoch: 7468 \tTraining Loss: 0.185027 \tR2: 0.732053\n",
      "Epoch: 7469 \tTraining Loss: 0.203734 \tR2: 0.732053\n",
      "Epoch: 7470 \tTraining Loss: 0.200980 \tR2: 0.732053\n",
      "Epoch: 7471 \tTraining Loss: 0.199057 \tR2: 0.732053\n",
      "Epoch: 7472 \tTraining Loss: 0.196703 \tR2: 0.732053\n",
      "Epoch: 7473 \tTraining Loss: 0.179971 \tR2: 0.732053\n",
      "Epoch: 7474 \tTraining Loss: 0.183680 \tR2: 0.732053\n",
      "Epoch: 7475 \tTraining Loss: 0.182268 \tR2: 0.732053\n",
      "Epoch: 7476 \tTraining Loss: 0.178800 \tR2: 0.732053\n",
      "Epoch: 7477 \tTraining Loss: 0.187576 \tR2: 0.732053\n",
      "Epoch: 7478 \tTraining Loss: 0.181092 \tR2: 0.732053\n",
      "Epoch: 7479 \tTraining Loss: 0.177419 \tR2: 0.732053\n",
      "Epoch: 7480 \tTraining Loss: 0.179454 \tR2: 0.732053\n",
      "Epoch: 7481 \tTraining Loss: 0.161849 \tR2: 0.732053\n",
      "Epoch: 7482 \tTraining Loss: 0.195461 \tR2: 0.732053\n",
      "Epoch: 7483 \tTraining Loss: 0.165474 \tR2: 0.732053\n",
      "Epoch: 7484 \tTraining Loss: 0.177840 \tR2: 0.732053\n",
      "Epoch: 7485 \tTraining Loss: 0.182581 \tR2: 0.732053\n",
      "Epoch: 7486 \tTraining Loss: 0.183472 \tR2: 0.732053\n",
      "Epoch: 7487 \tTraining Loss: 0.179894 \tR2: 0.732053\n",
      "Epoch: 7488 \tTraining Loss: 0.175197 \tR2: 0.732053\n",
      "Epoch: 7489 \tTraining Loss: 0.193330 \tR2: 0.732053\n",
      "Epoch: 7490 \tTraining Loss: 0.195519 \tR2: 0.732053\n",
      "Epoch: 7491 \tTraining Loss: 0.176428 \tR2: 0.732053\n",
      "Epoch: 7492 \tTraining Loss: 0.170040 \tR2: 0.732053\n",
      "Epoch: 7493 \tTraining Loss: 0.183923 \tR2: 0.732053\n",
      "Epoch: 7494 \tTraining Loss: 0.215638 \tR2: 0.732053\n",
      "Epoch: 7495 \tTraining Loss: 0.168000 \tR2: 0.732053\n",
      "Epoch: 7496 \tTraining Loss: 0.179127 \tR2: 0.732053\n",
      "Epoch: 7497 \tTraining Loss: 0.184785 \tR2: 0.732053\n",
      "Epoch: 7498 \tTraining Loss: 0.177706 \tR2: 0.732053\n",
      "Epoch: 7499 \tTraining Loss: 0.184930 \tR2: 0.732053\n",
      "Epoch: 7500 \tTraining Loss: 0.190225 \tR2: 0.888173\n",
      "Epoch: 7501 \tTraining Loss: 0.174395 \tR2: 0.888173\n",
      "Epoch: 7502 \tTraining Loss: 0.190756 \tR2: 0.888173\n",
      "Epoch: 7503 \tTraining Loss: 0.180167 \tR2: 0.888173\n",
      "Epoch: 7504 \tTraining Loss: 0.185448 \tR2: 0.888173\n",
      "Epoch: 7505 \tTraining Loss: 0.184881 \tR2: 0.888173\n",
      "Epoch: 7506 \tTraining Loss: 0.176218 \tR2: 0.888173\n",
      "Epoch: 7507 \tTraining Loss: 0.178152 \tR2: 0.888173\n",
      "Epoch: 7508 \tTraining Loss: 0.197925 \tR2: 0.888173\n",
      "Epoch: 7509 \tTraining Loss: 0.185697 \tR2: 0.888173\n",
      "Epoch: 7510 \tTraining Loss: 0.179471 \tR2: 0.888173\n",
      "Epoch: 7511 \tTraining Loss: 0.176355 \tR2: 0.888173\n",
      "Epoch: 7512 \tTraining Loss: 0.180260 \tR2: 0.888173\n",
      "Epoch: 7513 \tTraining Loss: 0.189985 \tR2: 0.888173\n",
      "Epoch: 7514 \tTraining Loss: 0.174499 \tR2: 0.888173\n",
      "Epoch: 7515 \tTraining Loss: 0.198582 \tR2: 0.888173\n",
      "Epoch: 7516 \tTraining Loss: 0.182720 \tR2: 0.888173\n",
      "Epoch: 7517 \tTraining Loss: 0.199306 \tR2: 0.888173\n",
      "Epoch: 7518 \tTraining Loss: 0.184607 \tR2: 0.888173\n",
      "Epoch: 7519 \tTraining Loss: 0.186492 \tR2: 0.888173\n",
      "Epoch: 7520 \tTraining Loss: 0.204309 \tR2: 0.888173\n",
      "Epoch: 7521 \tTraining Loss: 0.169760 \tR2: 0.888173\n",
      "Epoch: 7522 \tTraining Loss: 0.199130 \tR2: 0.888173\n",
      "Epoch: 7523 \tTraining Loss: 0.199194 \tR2: 0.888173\n",
      "Epoch: 7524 \tTraining Loss: 0.180515 \tR2: 0.888173\n",
      "Epoch: 7525 \tTraining Loss: 0.166049 \tR2: 0.888173\n",
      "Epoch: 7526 \tTraining Loss: 0.203392 \tR2: 0.888173\n",
      "Epoch: 7527 \tTraining Loss: 0.192571 \tR2: 0.888173\n",
      "Epoch: 7528 \tTraining Loss: 0.188842 \tR2: 0.888173\n",
      "Epoch: 7529 \tTraining Loss: 0.200060 \tR2: 0.888173\n",
      "Epoch: 7530 \tTraining Loss: 0.177999 \tR2: 0.888173\n",
      "Epoch: 7531 \tTraining Loss: 0.178111 \tR2: 0.888173\n",
      "Epoch: 7532 \tTraining Loss: 0.176571 \tR2: 0.888173\n",
      "Epoch: 7533 \tTraining Loss: 0.197794 \tR2: 0.888173\n",
      "Epoch: 7534 \tTraining Loss: 0.176991 \tR2: 0.888173\n",
      "Epoch: 7535 \tTraining Loss: 0.173938 \tR2: 0.888173\n",
      "Epoch: 7536 \tTraining Loss: 0.190761 \tR2: 0.888173\n",
      "Epoch: 7537 \tTraining Loss: 0.168673 \tR2: 0.888173\n",
      "Epoch: 7538 \tTraining Loss: 0.165920 \tR2: 0.888173\n",
      "Epoch: 7539 \tTraining Loss: 0.172542 \tR2: 0.888173\n",
      "Epoch: 7540 \tTraining Loss: 0.192606 \tR2: 0.888173\n",
      "Epoch: 7541 \tTraining Loss: 0.180535 \tR2: 0.888173\n",
      "Epoch: 7542 \tTraining Loss: 0.215387 \tR2: 0.888173\n",
      "Epoch: 7543 \tTraining Loss: 0.199245 \tR2: 0.888173\n",
      "Epoch: 7544 \tTraining Loss: 0.167455 \tR2: 0.888173\n",
      "Epoch: 7545 \tTraining Loss: 0.189779 \tR2: 0.888173\n",
      "Epoch: 7546 \tTraining Loss: 0.184081 \tR2: 0.888173\n",
      "Epoch: 7547 \tTraining Loss: 0.191193 \tR2: 0.888173\n",
      "Epoch: 7548 \tTraining Loss: 0.177738 \tR2: 0.888173\n",
      "Epoch: 7549 \tTraining Loss: 0.185494 \tR2: 0.888173\n",
      "Epoch: 7550 \tTraining Loss: 0.181487 \tR2: 0.888173\n",
      "Epoch: 7551 \tTraining Loss: 0.181006 \tR2: 0.888173\n",
      "Epoch: 7552 \tTraining Loss: 0.187781 \tR2: 0.888173\n",
      "Epoch: 7553 \tTraining Loss: 0.189283 \tR2: 0.888173\n",
      "Epoch: 7554 \tTraining Loss: 0.180832 \tR2: 0.888173\n",
      "Epoch: 7555 \tTraining Loss: 0.183232 \tR2: 0.888173\n",
      "Epoch: 7556 \tTraining Loss: 0.174862 \tR2: 0.888173\n",
      "Epoch: 7557 \tTraining Loss: 0.198373 \tR2: 0.888173\n",
      "Epoch: 7558 \tTraining Loss: 0.190018 \tR2: 0.888173\n",
      "Epoch: 7559 \tTraining Loss: 0.200457 \tR2: 0.888173\n",
      "Epoch: 7560 \tTraining Loss: 0.178966 \tR2: 0.888173\n",
      "Epoch: 7561 \tTraining Loss: 0.179738 \tR2: 0.888173\n",
      "Epoch: 7562 \tTraining Loss: 0.234671 \tR2: 0.888173\n",
      "Epoch: 7563 \tTraining Loss: 0.172411 \tR2: 0.888173\n",
      "Epoch: 7564 \tTraining Loss: 0.190268 \tR2: 0.888173\n",
      "Epoch: 7565 \tTraining Loss: 0.178847 \tR2: 0.888173\n",
      "Epoch: 7566 \tTraining Loss: 0.191624 \tR2: 0.888173\n",
      "Epoch: 7567 \tTraining Loss: 0.192149 \tR2: 0.888173\n",
      "Epoch: 7568 \tTraining Loss: 0.178213 \tR2: 0.888173\n",
      "Epoch: 7569 \tTraining Loss: 0.179741 \tR2: 0.888173\n",
      "Epoch: 7570 \tTraining Loss: 0.187208 \tR2: 0.888173\n",
      "Epoch: 7571 \tTraining Loss: 0.173921 \tR2: 0.888173\n",
      "Epoch: 7572 \tTraining Loss: 0.197118 \tR2: 0.888173\n",
      "Epoch: 7573 \tTraining Loss: 0.220767 \tR2: 0.888173\n",
      "Epoch: 7574 \tTraining Loss: 0.175791 \tR2: 0.888173\n",
      "Epoch: 7575 \tTraining Loss: 0.174477 \tR2: 0.888173\n",
      "Epoch: 7576 \tTraining Loss: 0.208884 \tR2: 0.888173\n",
      "Epoch: 7577 \tTraining Loss: 0.182835 \tR2: 0.888173\n",
      "Epoch: 7578 \tTraining Loss: 0.188906 \tR2: 0.888173\n",
      "Epoch: 7579 \tTraining Loss: 0.184509 \tR2: 0.888173\n",
      "Epoch: 7580 \tTraining Loss: 0.215434 \tR2: 0.888173\n",
      "Epoch: 7581 \tTraining Loss: 0.194858 \tR2: 0.888173\n",
      "Epoch: 7582 \tTraining Loss: 0.176505 \tR2: 0.888173\n",
      "Epoch: 7583 \tTraining Loss: 0.199739 \tR2: 0.888173\n",
      "Epoch: 7584 \tTraining Loss: 0.188381 \tR2: 0.888173\n",
      "Epoch: 7585 \tTraining Loss: 0.175071 \tR2: 0.888173\n",
      "Epoch: 7586 \tTraining Loss: 0.175381 \tR2: 0.888173\n",
      "Epoch: 7587 \tTraining Loss: 0.177816 \tR2: 0.888173\n",
      "Epoch: 7588 \tTraining Loss: 0.171862 \tR2: 0.888173\n",
      "Epoch: 7589 \tTraining Loss: 0.187248 \tR2: 0.888173\n",
      "Epoch: 7590 \tTraining Loss: 0.185205 \tR2: 0.888173\n",
      "Epoch: 7591 \tTraining Loss: 0.184374 \tR2: 0.888173\n",
      "Epoch: 7592 \tTraining Loss: 0.186553 \tR2: 0.888173\n",
      "Epoch: 7593 \tTraining Loss: 0.166873 \tR2: 0.888173\n",
      "Epoch: 7594 \tTraining Loss: 0.176358 \tR2: 0.888173\n",
      "Epoch: 7595 \tTraining Loss: 0.186542 \tR2: 0.888173\n",
      "Epoch: 7596 \tTraining Loss: 0.169895 \tR2: 0.888173\n",
      "Epoch: 7597 \tTraining Loss: 0.189104 \tR2: 0.888173\n",
      "Epoch: 7598 \tTraining Loss: 0.185017 \tR2: 0.888173\n",
      "Epoch: 7599 \tTraining Loss: 0.192588 \tR2: 0.888173\n",
      "Epoch: 7600 \tTraining Loss: 0.186595 \tR2: 0.758876\n",
      "Epoch: 7601 \tTraining Loss: 0.184870 \tR2: 0.758876\n",
      "Epoch: 7602 \tTraining Loss: 0.185052 \tR2: 0.758876\n",
      "Epoch: 7603 \tTraining Loss: 0.171898 \tR2: 0.758876\n",
      "Epoch: 7604 \tTraining Loss: 0.190102 \tR2: 0.758876\n",
      "Epoch: 7605 \tTraining Loss: 0.185892 \tR2: 0.758876\n",
      "Epoch: 7606 \tTraining Loss: 0.161501 \tR2: 0.758876\n",
      "Epoch: 7607 \tTraining Loss: 0.190990 \tR2: 0.758876\n",
      "Epoch: 7608 \tTraining Loss: 0.189811 \tR2: 0.758876\n",
      "Epoch: 7609 \tTraining Loss: 0.188475 \tR2: 0.758876\n",
      "Epoch: 7610 \tTraining Loss: 0.172603 \tR2: 0.758876\n",
      "Epoch: 7611 \tTraining Loss: 0.207389 \tR2: 0.758876\n",
      "Epoch: 7612 \tTraining Loss: 0.186387 \tR2: 0.758876\n",
      "Epoch: 7613 \tTraining Loss: 0.185875 \tR2: 0.758876\n",
      "Epoch: 7614 \tTraining Loss: 0.194229 \tR2: 0.758876\n",
      "Epoch: 7615 \tTraining Loss: 0.196754 \tR2: 0.758876\n",
      "Epoch: 7616 \tTraining Loss: 0.170398 \tR2: 0.758876\n",
      "Epoch: 7617 \tTraining Loss: 0.189125 \tR2: 0.758876\n",
      "Epoch: 7618 \tTraining Loss: 0.176177 \tR2: 0.758876\n",
      "Epoch: 7619 \tTraining Loss: 0.166631 \tR2: 0.758876\n",
      "Epoch: 7620 \tTraining Loss: 0.184738 \tR2: 0.758876\n",
      "Epoch: 7621 \tTraining Loss: 0.180485 \tR2: 0.758876\n",
      "Epoch: 7622 \tTraining Loss: 0.190202 \tR2: 0.758876\n",
      "Epoch: 7623 \tTraining Loss: 0.180304 \tR2: 0.758876\n",
      "Epoch: 7624 \tTraining Loss: 0.185583 \tR2: 0.758876\n",
      "Epoch: 7625 \tTraining Loss: 0.177355 \tR2: 0.758876\n",
      "Epoch: 7626 \tTraining Loss: 0.180942 \tR2: 0.758876\n",
      "Epoch: 7627 \tTraining Loss: 0.177171 \tR2: 0.758876\n",
      "Epoch: 7628 \tTraining Loss: 0.183557 \tR2: 0.758876\n",
      "Epoch: 7629 \tTraining Loss: 0.173353 \tR2: 0.758876\n",
      "Epoch: 7630 \tTraining Loss: 0.181942 \tR2: 0.758876\n",
      "Epoch: 7631 \tTraining Loss: 0.187254 \tR2: 0.758876\n",
      "Epoch: 7632 \tTraining Loss: 0.181294 \tR2: 0.758876\n",
      "Epoch: 7633 \tTraining Loss: 0.168377 \tR2: 0.758876\n",
      "Epoch: 7634 \tTraining Loss: 0.181343 \tR2: 0.758876\n",
      "Epoch: 7635 \tTraining Loss: 0.208898 \tR2: 0.758876\n",
      "Epoch: 7636 \tTraining Loss: 0.178480 \tR2: 0.758876\n",
      "Epoch: 7637 \tTraining Loss: 0.191863 \tR2: 0.758876\n",
      "Epoch: 7638 \tTraining Loss: 0.178365 \tR2: 0.758876\n",
      "Epoch: 7639 \tTraining Loss: 0.216129 \tR2: 0.758876\n",
      "Epoch: 7640 \tTraining Loss: 0.178094 \tR2: 0.758876\n",
      "Epoch: 7641 \tTraining Loss: 0.200203 \tR2: 0.758876\n",
      "Epoch: 7642 \tTraining Loss: 0.180303 \tR2: 0.758876\n",
      "Epoch: 7643 \tTraining Loss: 0.178499 \tR2: 0.758876\n",
      "Epoch: 7644 \tTraining Loss: 0.177623 \tR2: 0.758876\n",
      "Epoch: 7645 \tTraining Loss: 0.195923 \tR2: 0.758876\n",
      "Epoch: 7646 \tTraining Loss: 0.182625 \tR2: 0.758876\n",
      "Epoch: 7647 \tTraining Loss: 0.173572 \tR2: 0.758876\n",
      "Epoch: 7648 \tTraining Loss: 0.168957 \tR2: 0.758876\n",
      "Epoch: 7649 \tTraining Loss: 0.191668 \tR2: 0.758876\n",
      "Epoch: 7650 \tTraining Loss: 0.174990 \tR2: 0.758876\n",
      "Epoch: 7651 \tTraining Loss: 0.180164 \tR2: 0.758876\n",
      "Epoch: 7652 \tTraining Loss: 0.164486 \tR2: 0.758876\n",
      "Epoch: 7653 \tTraining Loss: 0.185413 \tR2: 0.758876\n",
      "Epoch: 7654 \tTraining Loss: 0.190704 \tR2: 0.758876\n",
      "Epoch: 7655 \tTraining Loss: 0.195460 \tR2: 0.758876\n",
      "Epoch: 7656 \tTraining Loss: 0.187633 \tR2: 0.758876\n",
      "Epoch: 7657 \tTraining Loss: 0.174175 \tR2: 0.758876\n",
      "Epoch: 7658 \tTraining Loss: 0.196539 \tR2: 0.758876\n",
      "Epoch: 7659 \tTraining Loss: 0.160192 \tR2: 0.758876\n",
      "Epoch: 7660 \tTraining Loss: 0.212406 \tR2: 0.758876\n",
      "Epoch: 7661 \tTraining Loss: 0.191522 \tR2: 0.758876\n",
      "Epoch: 7662 \tTraining Loss: 0.206978 \tR2: 0.758876\n",
      "Epoch: 7663 \tTraining Loss: 0.189475 \tR2: 0.758876\n",
      "Epoch: 7664 \tTraining Loss: 0.185597 \tR2: 0.758876\n",
      "Epoch: 7665 \tTraining Loss: 0.183749 \tR2: 0.758876\n",
      "Epoch: 7666 \tTraining Loss: 0.191759 \tR2: 0.758876\n",
      "Epoch: 7667 \tTraining Loss: 0.195358 \tR2: 0.758876\n",
      "Epoch: 7668 \tTraining Loss: 0.179691 \tR2: 0.758876\n",
      "Epoch: 7669 \tTraining Loss: 0.196176 \tR2: 0.758876\n",
      "Epoch: 7670 \tTraining Loss: 0.182487 \tR2: 0.758876\n",
      "Epoch: 7671 \tTraining Loss: 0.200316 \tR2: 0.758876\n",
      "Epoch: 7672 \tTraining Loss: 0.191048 \tR2: 0.758876\n",
      "Epoch: 7673 \tTraining Loss: 0.183703 \tR2: 0.758876\n",
      "Epoch: 7674 \tTraining Loss: 0.187259 \tR2: 0.758876\n",
      "Epoch: 7675 \tTraining Loss: 0.178150 \tR2: 0.758876\n",
      "Epoch: 7676 \tTraining Loss: 0.180042 \tR2: 0.758876\n",
      "Epoch: 7677 \tTraining Loss: 0.213246 \tR2: 0.758876\n",
      "Epoch: 7678 \tTraining Loss: 0.176807 \tR2: 0.758876\n",
      "Epoch: 7679 \tTraining Loss: 0.193600 \tR2: 0.758876\n",
      "Epoch: 7680 \tTraining Loss: 0.177445 \tR2: 0.758876\n",
      "Epoch: 7681 \tTraining Loss: 0.185754 \tR2: 0.758876\n",
      "Epoch: 7682 \tTraining Loss: 0.204212 \tR2: 0.758876\n",
      "Epoch: 7683 \tTraining Loss: 0.185093 \tR2: 0.758876\n",
      "Epoch: 7684 \tTraining Loss: 0.174484 \tR2: 0.758876\n",
      "Epoch: 7685 \tTraining Loss: 0.193057 \tR2: 0.758876\n",
      "Epoch: 7686 \tTraining Loss: 0.190217 \tR2: 0.758876\n",
      "Epoch: 7687 \tTraining Loss: 0.192414 \tR2: 0.758876\n",
      "Epoch: 7688 \tTraining Loss: 0.179891 \tR2: 0.758876\n",
      "Epoch: 7689 \tTraining Loss: 0.184172 \tR2: 0.758876\n",
      "Epoch: 7690 \tTraining Loss: 0.174135 \tR2: 0.758876\n",
      "Epoch: 7691 \tTraining Loss: 0.182525 \tR2: 0.758876\n",
      "Epoch: 7692 \tTraining Loss: 0.171160 \tR2: 0.758876\n",
      "Epoch: 7693 \tTraining Loss: 0.180486 \tR2: 0.758876\n",
      "Epoch: 7694 \tTraining Loss: 0.179251 \tR2: 0.758876\n",
      "Epoch: 7695 \tTraining Loss: 0.187578 \tR2: 0.758876\n",
      "Epoch: 7696 \tTraining Loss: 0.179343 \tR2: 0.758876\n",
      "Epoch: 7697 \tTraining Loss: 0.223616 \tR2: 0.758876\n",
      "Epoch: 7698 \tTraining Loss: 0.190529 \tR2: 0.758876\n",
      "Epoch: 7699 \tTraining Loss: 0.197939 \tR2: 0.758876\n",
      "Epoch: 7700 \tTraining Loss: 0.198802 \tR2: 0.797020\n",
      "Epoch: 7701 \tTraining Loss: 0.183890 \tR2: 0.797020\n",
      "Epoch: 7702 \tTraining Loss: 0.173712 \tR2: 0.797020\n",
      "Epoch: 7703 \tTraining Loss: 0.179020 \tR2: 0.797020\n",
      "Epoch: 7704 \tTraining Loss: 0.180738 \tR2: 0.797020\n",
      "Epoch: 7705 \tTraining Loss: 0.196841 \tR2: 0.797020\n",
      "Epoch: 7706 \tTraining Loss: 0.183273 \tR2: 0.797020\n",
      "Epoch: 7707 \tTraining Loss: 0.197049 \tR2: 0.797020\n",
      "Epoch: 7708 \tTraining Loss: 0.192794 \tR2: 0.797020\n",
      "Epoch: 7709 \tTraining Loss: 0.169856 \tR2: 0.797020\n",
      "Epoch: 7710 \tTraining Loss: 0.167569 \tR2: 0.797020\n",
      "Epoch: 7711 \tTraining Loss: 0.177096 \tR2: 0.797020\n",
      "Epoch: 7712 \tTraining Loss: 0.198342 \tR2: 0.797020\n",
      "Epoch: 7713 \tTraining Loss: 0.176538 \tR2: 0.797020\n",
      "Epoch: 7714 \tTraining Loss: 0.195551 \tR2: 0.797020\n",
      "Epoch: 7715 \tTraining Loss: 0.175597 \tR2: 0.797020\n",
      "Epoch: 7716 \tTraining Loss: 0.206460 \tR2: 0.797020\n",
      "Epoch: 7717 \tTraining Loss: 0.173668 \tR2: 0.797020\n",
      "Epoch: 7718 \tTraining Loss: 0.188960 \tR2: 0.797020\n",
      "Epoch: 7719 \tTraining Loss: 0.185334 \tR2: 0.797020\n",
      "Epoch: 7720 \tTraining Loss: 0.183180 \tR2: 0.797020\n",
      "Epoch: 7721 \tTraining Loss: 0.179924 \tR2: 0.797020\n",
      "Epoch: 7722 \tTraining Loss: 0.219626 \tR2: 0.797020\n",
      "Epoch: 7723 \tTraining Loss: 0.200787 \tR2: 0.797020\n",
      "Epoch: 7724 \tTraining Loss: 0.190636 \tR2: 0.797020\n",
      "Epoch: 7725 \tTraining Loss: 0.164978 \tR2: 0.797020\n",
      "Epoch: 7726 \tTraining Loss: 0.182791 \tR2: 0.797020\n",
      "Epoch: 7727 \tTraining Loss: 0.183560 \tR2: 0.797020\n",
      "Epoch: 7728 \tTraining Loss: 0.195270 \tR2: 0.797020\n",
      "Epoch: 7729 \tTraining Loss: 0.186016 \tR2: 0.797020\n",
      "Epoch: 7730 \tTraining Loss: 0.182928 \tR2: 0.797020\n",
      "Epoch: 7731 \tTraining Loss: 0.198974 \tR2: 0.797020\n",
      "Epoch: 7732 \tTraining Loss: 0.183730 \tR2: 0.797020\n",
      "Epoch: 7733 \tTraining Loss: 0.188749 \tR2: 0.797020\n",
      "Epoch: 7734 \tTraining Loss: 0.177982 \tR2: 0.797020\n",
      "Epoch: 7735 \tTraining Loss: 0.173462 \tR2: 0.797020\n",
      "Epoch: 7736 \tTraining Loss: 0.202273 \tR2: 0.797020\n",
      "Epoch: 7737 \tTraining Loss: 0.195752 \tR2: 0.797020\n",
      "Epoch: 7738 \tTraining Loss: 0.180665 \tR2: 0.797020\n",
      "Epoch: 7739 \tTraining Loss: 0.172365 \tR2: 0.797020\n",
      "Epoch: 7740 \tTraining Loss: 0.186301 \tR2: 0.797020\n",
      "Epoch: 7741 \tTraining Loss: 0.186445 \tR2: 0.797020\n",
      "Epoch: 7742 \tTraining Loss: 0.198557 \tR2: 0.797020\n",
      "Epoch: 7743 \tTraining Loss: 0.172084 \tR2: 0.797020\n",
      "Epoch: 7744 \tTraining Loss: 0.197594 \tR2: 0.797020\n",
      "Epoch: 7745 \tTraining Loss: 0.178914 \tR2: 0.797020\n",
      "Epoch: 7746 \tTraining Loss: 0.194870 \tR2: 0.797020\n",
      "Epoch: 7747 \tTraining Loss: 0.180334 \tR2: 0.797020\n",
      "Epoch: 7748 \tTraining Loss: 0.179221 \tR2: 0.797020\n",
      "Epoch: 7749 \tTraining Loss: 0.184981 \tR2: 0.797020\n",
      "Epoch: 7750 \tTraining Loss: 0.183142 \tR2: 0.797020\n",
      "Epoch: 7751 \tTraining Loss: 0.204908 \tR2: 0.797020\n",
      "Epoch: 7752 \tTraining Loss: 0.175503 \tR2: 0.797020\n",
      "Epoch: 7753 \tTraining Loss: 0.183815 \tR2: 0.797020\n",
      "Epoch: 7754 \tTraining Loss: 0.175941 \tR2: 0.797020\n",
      "Epoch: 7755 \tTraining Loss: 0.180997 \tR2: 0.797020\n",
      "Epoch: 7756 \tTraining Loss: 0.177926 \tR2: 0.797020\n",
      "Epoch: 7757 \tTraining Loss: 0.179529 \tR2: 0.797020\n",
      "Epoch: 7758 \tTraining Loss: 0.180217 \tR2: 0.797020\n",
      "Epoch: 7759 \tTraining Loss: 0.175591 \tR2: 0.797020\n",
      "Epoch: 7760 \tTraining Loss: 0.165397 \tR2: 0.797020\n",
      "Epoch: 7761 \tTraining Loss: 0.196016 \tR2: 0.797020\n",
      "Epoch: 7762 \tTraining Loss: 0.176777 \tR2: 0.797020\n",
      "Epoch: 7763 \tTraining Loss: 0.184883 \tR2: 0.797020\n",
      "Epoch: 7764 \tTraining Loss: 0.165960 \tR2: 0.797020\n",
      "Epoch: 7765 \tTraining Loss: 0.165391 \tR2: 0.797020\n",
      "Epoch: 7766 \tTraining Loss: 0.172917 \tR2: 0.797020\n",
      "Epoch: 7767 \tTraining Loss: 0.182393 \tR2: 0.797020\n",
      "Epoch: 7768 \tTraining Loss: 0.173229 \tR2: 0.797020\n",
      "Epoch: 7769 \tTraining Loss: 0.165135 \tR2: 0.797020\n",
      "Epoch: 7770 \tTraining Loss: 0.218357 \tR2: 0.797020\n",
      "Epoch: 7771 \tTraining Loss: 0.202181 \tR2: 0.797020\n",
      "Epoch: 7772 \tTraining Loss: 0.176535 \tR2: 0.797020\n",
      "Epoch: 7773 \tTraining Loss: 0.313055 \tR2: 0.797020\n",
      "Epoch: 7774 \tTraining Loss: 0.258414 \tR2: 0.797020\n",
      "Epoch: 7775 \tTraining Loss: 0.200372 \tR2: 0.797020\n",
      "Epoch: 7776 \tTraining Loss: 0.174933 \tR2: 0.797020\n",
      "Epoch: 7777 \tTraining Loss: 0.190415 \tR2: 0.797020\n",
      "Epoch: 7778 \tTraining Loss: 0.181518 \tR2: 0.797020\n",
      "Epoch: 7779 \tTraining Loss: 0.172999 \tR2: 0.797020\n",
      "Epoch: 7780 \tTraining Loss: 0.172371 \tR2: 0.797020\n",
      "Epoch: 7781 \tTraining Loss: 0.174051 \tR2: 0.797020\n",
      "Epoch: 7782 \tTraining Loss: 0.177747 \tR2: 0.797020\n",
      "Epoch: 7783 \tTraining Loss: 0.198341 \tR2: 0.797020\n",
      "Epoch: 7784 \tTraining Loss: 0.191991 \tR2: 0.797020\n",
      "Epoch: 7785 \tTraining Loss: 0.178239 \tR2: 0.797020\n",
      "Epoch: 7786 \tTraining Loss: 0.203313 \tR2: 0.797020\n",
      "Epoch: 7787 \tTraining Loss: 0.202462 \tR2: 0.797020\n",
      "Epoch: 7788 \tTraining Loss: 0.207150 \tR2: 0.797020\n",
      "Epoch: 7789 \tTraining Loss: 0.190181 \tR2: 0.797020\n",
      "Epoch: 7790 \tTraining Loss: 0.169893 \tR2: 0.797020\n",
      "Epoch: 7791 \tTraining Loss: 0.191313 \tR2: 0.797020\n",
      "Epoch: 7792 \tTraining Loss: 0.174532 \tR2: 0.797020\n",
      "Epoch: 7793 \tTraining Loss: 0.178839 \tR2: 0.797020\n",
      "Epoch: 7794 \tTraining Loss: 0.183507 \tR2: 0.797020\n",
      "Epoch: 7795 \tTraining Loss: 0.162229 \tR2: 0.797020\n",
      "Epoch: 7796 \tTraining Loss: 0.182589 \tR2: 0.797020\n",
      "Epoch: 7797 \tTraining Loss: 0.168252 \tR2: 0.797020\n",
      "Epoch: 7798 \tTraining Loss: 0.166083 \tR2: 0.797020\n",
      "Epoch: 7799 \tTraining Loss: 0.183516 \tR2: 0.797020\n",
      "Epoch: 7800 \tTraining Loss: 0.173548 \tR2: 0.925166\n",
      "Epoch: 7801 \tTraining Loss: 0.177150 \tR2: 0.925166\n",
      "Epoch: 7802 \tTraining Loss: 0.176420 \tR2: 0.925166\n",
      "Epoch: 7803 \tTraining Loss: 0.198864 \tR2: 0.925166\n",
      "Epoch: 7804 \tTraining Loss: 0.188314 \tR2: 0.925166\n",
      "Epoch: 7805 \tTraining Loss: 0.171540 \tR2: 0.925166\n",
      "Epoch: 7806 \tTraining Loss: 0.182915 \tR2: 0.925166\n",
      "Epoch: 7807 \tTraining Loss: 0.206360 \tR2: 0.925166\n",
      "Epoch: 7808 \tTraining Loss: 0.184828 \tR2: 0.925166\n",
      "Epoch: 7809 \tTraining Loss: 0.185626 \tR2: 0.925166\n",
      "Epoch: 7810 \tTraining Loss: 0.190510 \tR2: 0.925166\n",
      "Epoch: 7811 \tTraining Loss: 0.201260 \tR2: 0.925166\n",
      "Epoch: 7812 \tTraining Loss: 0.205329 \tR2: 0.925166\n",
      "Epoch: 7813 \tTraining Loss: 0.190492 \tR2: 0.925166\n",
      "Epoch: 7814 \tTraining Loss: 0.183685 \tR2: 0.925166\n",
      "Epoch: 7815 \tTraining Loss: 0.188584 \tR2: 0.925166\n",
      "Epoch: 7816 \tTraining Loss: 0.176568 \tR2: 0.925166\n",
      "Epoch: 7817 \tTraining Loss: 0.186083 \tR2: 0.925166\n",
      "Epoch: 7818 \tTraining Loss: 0.179145 \tR2: 0.925166\n",
      "Epoch: 7819 \tTraining Loss: 0.170010 \tR2: 0.925166\n",
      "Epoch: 7820 \tTraining Loss: 0.188115 \tR2: 0.925166\n",
      "Epoch: 7821 \tTraining Loss: 0.185768 \tR2: 0.925166\n",
      "Epoch: 7822 \tTraining Loss: 0.170732 \tR2: 0.925166\n",
      "Epoch: 7823 \tTraining Loss: 0.181497 \tR2: 0.925166\n",
      "Epoch: 7824 \tTraining Loss: 0.163728 \tR2: 0.925166\n",
      "Epoch: 7825 \tTraining Loss: 0.176000 \tR2: 0.925166\n",
      "Epoch: 7826 \tTraining Loss: 0.175732 \tR2: 0.925166\n",
      "Epoch: 7827 \tTraining Loss: 0.190613 \tR2: 0.925166\n",
      "Epoch: 7828 \tTraining Loss: 0.188391 \tR2: 0.925166\n",
      "Epoch: 7829 \tTraining Loss: 0.176775 \tR2: 0.925166\n",
      "Epoch: 7830 \tTraining Loss: 0.191631 \tR2: 0.925166\n",
      "Epoch: 7831 \tTraining Loss: 0.187623 \tR2: 0.925166\n",
      "Epoch: 7832 \tTraining Loss: 0.182441 \tR2: 0.925166\n",
      "Epoch: 7833 \tTraining Loss: 0.166171 \tR2: 0.925166\n",
      "Epoch: 7834 \tTraining Loss: 0.203683 \tR2: 0.925166\n",
      "Epoch: 7835 \tTraining Loss: 0.185130 \tR2: 0.925166\n",
      "Epoch: 7836 \tTraining Loss: 0.191758 \tR2: 0.925166\n",
      "Epoch: 7837 \tTraining Loss: 0.195514 \tR2: 0.925166\n",
      "Epoch: 7838 \tTraining Loss: 0.194074 \tR2: 0.925166\n",
      "Epoch: 7839 \tTraining Loss: 0.170532 \tR2: 0.925166\n",
      "Epoch: 7840 \tTraining Loss: 0.176047 \tR2: 0.925166\n",
      "Epoch: 7841 \tTraining Loss: 0.165497 \tR2: 0.925166\n",
      "Epoch: 7842 \tTraining Loss: 0.182267 \tR2: 0.925166\n",
      "Epoch: 7843 \tTraining Loss: 0.182889 \tR2: 0.925166\n",
      "Epoch: 7844 \tTraining Loss: 0.177102 \tR2: 0.925166\n",
      "Epoch: 7845 \tTraining Loss: 0.165143 \tR2: 0.925166\n",
      "Epoch: 7846 \tTraining Loss: 0.167103 \tR2: 0.925166\n",
      "Epoch: 7847 \tTraining Loss: 0.168485 \tR2: 0.925166\n",
      "Epoch: 7848 \tTraining Loss: 0.193300 \tR2: 0.925166\n",
      "Epoch: 7849 \tTraining Loss: 0.171848 \tR2: 0.925166\n",
      "Epoch: 7850 \tTraining Loss: 0.175538 \tR2: 0.925166\n",
      "Epoch: 7851 \tTraining Loss: 0.186128 \tR2: 0.925166\n",
      "Epoch: 7852 \tTraining Loss: 0.166727 \tR2: 0.925166\n",
      "Epoch: 7853 \tTraining Loss: 0.186088 \tR2: 0.925166\n",
      "Epoch: 7854 \tTraining Loss: 0.183527 \tR2: 0.925166\n",
      "Epoch: 7855 \tTraining Loss: 0.174524 \tR2: 0.925166\n",
      "Epoch: 7856 \tTraining Loss: 0.176297 \tR2: 0.925166\n",
      "Epoch: 7857 \tTraining Loss: 0.187832 \tR2: 0.925166\n",
      "Epoch: 7858 \tTraining Loss: 0.169961 \tR2: 0.925166\n",
      "Epoch: 7859 \tTraining Loss: 0.177495 \tR2: 0.925166\n",
      "Epoch: 7860 \tTraining Loss: 0.184062 \tR2: 0.925166\n",
      "Epoch: 7861 \tTraining Loss: 0.192768 \tR2: 0.925166\n",
      "Epoch: 7862 \tTraining Loss: 0.195366 \tR2: 0.925166\n",
      "Epoch: 7863 \tTraining Loss: 0.181925 \tR2: 0.925166\n",
      "Epoch: 7864 \tTraining Loss: 0.179889 \tR2: 0.925166\n",
      "Epoch: 7865 \tTraining Loss: 0.178653 \tR2: 0.925166\n",
      "Epoch: 7866 \tTraining Loss: 0.179462 \tR2: 0.925166\n",
      "Epoch: 7867 \tTraining Loss: 0.173829 \tR2: 0.925166\n",
      "Epoch: 7868 \tTraining Loss: 0.160207 \tR2: 0.925166\n",
      "Epoch: 7869 \tTraining Loss: 0.164639 \tR2: 0.925166\n",
      "Epoch: 7870 \tTraining Loss: 0.192456 \tR2: 0.925166\n",
      "Epoch: 7871 \tTraining Loss: 0.174389 \tR2: 0.925166\n",
      "Epoch: 7872 \tTraining Loss: 0.209885 \tR2: 0.925166\n",
      "Epoch: 7873 \tTraining Loss: 0.185438 \tR2: 0.925166\n",
      "Epoch: 7874 \tTraining Loss: 0.169912 \tR2: 0.925166\n",
      "Epoch: 7875 \tTraining Loss: 0.174931 \tR2: 0.925166\n",
      "Epoch: 7876 \tTraining Loss: 0.185634 \tR2: 0.925166\n",
      "Epoch: 7877 \tTraining Loss: 0.183909 \tR2: 0.925166\n",
      "Epoch: 7878 \tTraining Loss: 0.171556 \tR2: 0.925166\n",
      "Epoch: 7879 \tTraining Loss: 0.176784 \tR2: 0.925166\n",
      "Epoch: 7880 \tTraining Loss: 0.175278 \tR2: 0.925166\n",
      "Epoch: 7881 \tTraining Loss: 0.177735 \tR2: 0.925166\n",
      "Epoch: 7882 \tTraining Loss: 0.187205 \tR2: 0.925166\n",
      "Epoch: 7883 \tTraining Loss: 0.196382 \tR2: 0.925166\n",
      "Epoch: 7884 \tTraining Loss: 0.191525 \tR2: 0.925166\n",
      "Epoch: 7885 \tTraining Loss: 0.190126 \tR2: 0.925166\n",
      "Epoch: 7886 \tTraining Loss: 0.172300 \tR2: 0.925166\n",
      "Epoch: 7887 \tTraining Loss: 0.180005 \tR2: 0.925166\n",
      "Epoch: 7888 \tTraining Loss: 0.181043 \tR2: 0.925166\n",
      "Epoch: 7889 \tTraining Loss: 0.180553 \tR2: 0.925166\n",
      "Epoch: 7890 \tTraining Loss: 0.205117 \tR2: 0.925166\n",
      "Epoch: 7891 \tTraining Loss: 0.184659 \tR2: 0.925166\n",
      "Epoch: 7892 \tTraining Loss: 0.181802 \tR2: 0.925166\n",
      "Epoch: 7893 \tTraining Loss: 0.189165 \tR2: 0.925166\n",
      "Epoch: 7894 \tTraining Loss: 0.176307 \tR2: 0.925166\n",
      "Epoch: 7895 \tTraining Loss: 0.181711 \tR2: 0.925166\n",
      "Epoch: 7896 \tTraining Loss: 0.184911 \tR2: 0.925166\n",
      "Epoch: 7897 \tTraining Loss: 0.181579 \tR2: 0.925166\n",
      "Epoch: 7898 \tTraining Loss: 0.180293 \tR2: 0.925166\n",
      "Epoch: 7899 \tTraining Loss: 0.165693 \tR2: 0.925166\n",
      "Epoch: 7900 \tTraining Loss: 0.188829 \tR2: 0.808237\n",
      "Epoch: 7901 \tTraining Loss: 0.195576 \tR2: 0.808237\n",
      "Epoch: 7902 \tTraining Loss: 0.182924 \tR2: 0.808237\n",
      "Epoch: 7903 \tTraining Loss: 0.166823 \tR2: 0.808237\n",
      "Epoch: 7904 \tTraining Loss: 0.182114 \tR2: 0.808237\n",
      "Epoch: 7905 \tTraining Loss: 0.183892 \tR2: 0.808237\n",
      "Epoch: 7906 \tTraining Loss: 0.180424 \tR2: 0.808237\n",
      "Epoch: 7907 \tTraining Loss: 0.193750 \tR2: 0.808237\n",
      "Epoch: 7908 \tTraining Loss: 0.166988 \tR2: 0.808237\n",
      "Epoch: 7909 \tTraining Loss: 0.173955 \tR2: 0.808237\n",
      "Epoch: 7910 \tTraining Loss: 0.169159 \tR2: 0.808237\n",
      "Epoch: 7911 \tTraining Loss: 0.195406 \tR2: 0.808237\n",
      "Epoch: 7912 \tTraining Loss: 0.181341 \tR2: 0.808237\n",
      "Epoch: 7913 \tTraining Loss: 0.164101 \tR2: 0.808237\n",
      "Epoch: 7914 \tTraining Loss: 0.198344 \tR2: 0.808237\n",
      "Epoch: 7915 \tTraining Loss: 0.162779 \tR2: 0.808237\n",
      "Epoch: 7916 \tTraining Loss: 0.189395 \tR2: 0.808237\n",
      "Epoch: 7917 \tTraining Loss: 0.182433 \tR2: 0.808237\n",
      "Epoch: 7918 \tTraining Loss: 0.187138 \tR2: 0.808237\n",
      "Epoch: 7919 \tTraining Loss: 0.177788 \tR2: 0.808237\n",
      "Epoch: 7920 \tTraining Loss: 0.176086 \tR2: 0.808237\n",
      "Epoch: 7921 \tTraining Loss: 0.224580 \tR2: 0.808237\n",
      "Epoch: 7922 \tTraining Loss: 0.198117 \tR2: 0.808237\n",
      "Epoch: 7923 \tTraining Loss: 0.179460 \tR2: 0.808237\n",
      "Epoch: 7924 \tTraining Loss: 0.190189 \tR2: 0.808237\n",
      "Epoch: 7925 \tTraining Loss: 0.190748 \tR2: 0.808237\n",
      "Epoch: 7926 \tTraining Loss: 0.186726 \tR2: 0.808237\n",
      "Epoch: 7927 \tTraining Loss: 0.185168 \tR2: 0.808237\n",
      "Epoch: 7928 \tTraining Loss: 0.172004 \tR2: 0.808237\n",
      "Epoch: 7929 \tTraining Loss: 0.179659 \tR2: 0.808237\n",
      "Epoch: 7930 \tTraining Loss: 0.172020 \tR2: 0.808237\n",
      "Epoch: 7931 \tTraining Loss: 0.184441 \tR2: 0.808237\n",
      "Epoch: 7932 \tTraining Loss: 0.179844 \tR2: 0.808237\n",
      "Epoch: 7933 \tTraining Loss: 0.191197 \tR2: 0.808237\n",
      "Epoch: 7934 \tTraining Loss: 0.161932 \tR2: 0.808237\n",
      "Epoch: 7935 \tTraining Loss: 0.187946 \tR2: 0.808237\n",
      "Epoch: 7936 \tTraining Loss: 0.185548 \tR2: 0.808237\n",
      "Epoch: 7937 \tTraining Loss: 0.172035 \tR2: 0.808237\n",
      "Epoch: 7938 \tTraining Loss: 0.175712 \tR2: 0.808237\n",
      "Epoch: 7939 \tTraining Loss: 0.166577 \tR2: 0.808237\n",
      "Epoch: 7940 \tTraining Loss: 0.186575 \tR2: 0.808237\n",
      "Epoch: 7941 \tTraining Loss: 0.203586 \tR2: 0.808237\n",
      "Epoch: 7942 \tTraining Loss: 0.178621 \tR2: 0.808237\n",
      "Epoch: 7943 \tTraining Loss: 0.180570 \tR2: 0.808237\n",
      "Epoch: 7944 \tTraining Loss: 0.177755 \tR2: 0.808237\n",
      "Epoch: 7945 \tTraining Loss: 0.181171 \tR2: 0.808237\n",
      "Epoch: 7946 \tTraining Loss: 0.182380 \tR2: 0.808237\n",
      "Epoch: 7947 \tTraining Loss: 0.180746 \tR2: 0.808237\n",
      "Epoch: 7948 \tTraining Loss: 0.194094 \tR2: 0.808237\n",
      "Epoch: 7949 \tTraining Loss: 0.190361 \tR2: 0.808237\n",
      "Epoch: 7950 \tTraining Loss: 0.184070 \tR2: 0.808237\n",
      "Epoch: 7951 \tTraining Loss: 0.179802 \tR2: 0.808237\n",
      "Epoch: 7952 \tTraining Loss: 0.184961 \tR2: 0.808237\n",
      "Epoch: 7953 \tTraining Loss: 0.186664 \tR2: 0.808237\n",
      "Epoch: 7954 \tTraining Loss: 0.179715 \tR2: 0.808237\n",
      "Epoch: 7955 \tTraining Loss: 0.182142 \tR2: 0.808237\n",
      "Epoch: 7956 \tTraining Loss: 0.182266 \tR2: 0.808237\n",
      "Epoch: 7957 \tTraining Loss: 0.168432 \tR2: 0.808237\n",
      "Epoch: 7958 \tTraining Loss: 0.192731 \tR2: 0.808237\n",
      "Epoch: 7959 \tTraining Loss: 0.198962 \tR2: 0.808237\n",
      "Epoch: 7960 \tTraining Loss: 0.201113 \tR2: 0.808237\n",
      "Epoch: 7961 \tTraining Loss: 0.200976 \tR2: 0.808237\n",
      "Epoch: 7962 \tTraining Loss: 0.178208 \tR2: 0.808237\n",
      "Epoch: 7963 \tTraining Loss: 0.191551 \tR2: 0.808237\n",
      "Epoch: 7964 \tTraining Loss: 0.205651 \tR2: 0.808237\n",
      "Epoch: 7965 \tTraining Loss: 0.171681 \tR2: 0.808237\n",
      "Epoch: 7966 \tTraining Loss: 0.179199 \tR2: 0.808237\n",
      "Epoch: 7967 \tTraining Loss: 0.182112 \tR2: 0.808237\n",
      "Epoch: 7968 \tTraining Loss: 0.161925 \tR2: 0.808237\n",
      "Epoch: 7969 \tTraining Loss: 0.181122 \tR2: 0.808237\n",
      "Epoch: 7970 \tTraining Loss: 0.169864 \tR2: 0.808237\n",
      "Epoch: 7971 \tTraining Loss: 0.183580 \tR2: 0.808237\n",
      "Epoch: 7972 \tTraining Loss: 0.174876 \tR2: 0.808237\n",
      "Epoch: 7973 \tTraining Loss: 0.178552 \tR2: 0.808237\n",
      "Epoch: 7974 \tTraining Loss: 0.173604 \tR2: 0.808237\n",
      "Epoch: 7975 \tTraining Loss: 0.176610 \tR2: 0.808237\n",
      "Epoch: 7976 \tTraining Loss: 0.177956 \tR2: 0.808237\n",
      "Epoch: 7977 \tTraining Loss: 0.204220 \tR2: 0.808237\n",
      "Epoch: 7978 \tTraining Loss: 0.182384 \tR2: 0.808237\n",
      "Epoch: 7979 \tTraining Loss: 0.178793 \tR2: 0.808237\n",
      "Epoch: 7980 \tTraining Loss: 0.195506 \tR2: 0.808237\n",
      "Epoch: 7981 \tTraining Loss: 0.179091 \tR2: 0.808237\n",
      "Epoch: 7982 \tTraining Loss: 0.188458 \tR2: 0.808237\n",
      "Epoch: 7983 \tTraining Loss: 0.178841 \tR2: 0.808237\n",
      "Epoch: 7984 \tTraining Loss: 0.168675 \tR2: 0.808237\n",
      "Epoch: 7985 \tTraining Loss: 0.199247 \tR2: 0.808237\n",
      "Epoch: 7986 \tTraining Loss: 0.188929 \tR2: 0.808237\n",
      "Epoch: 7987 \tTraining Loss: 0.187221 \tR2: 0.808237\n",
      "Epoch: 7988 \tTraining Loss: 0.171175 \tR2: 0.808237\n",
      "Epoch: 7989 \tTraining Loss: 0.185839 \tR2: 0.808237\n",
      "Epoch: 7990 \tTraining Loss: 0.179814 \tR2: 0.808237\n",
      "Epoch: 7991 \tTraining Loss: 0.182137 \tR2: 0.808237\n",
      "Epoch: 7992 \tTraining Loss: 0.177000 \tR2: 0.808237\n",
      "Epoch: 7993 \tTraining Loss: 0.185609 \tR2: 0.808237\n",
      "Epoch: 7994 \tTraining Loss: 0.197332 \tR2: 0.808237\n",
      "Epoch: 7995 \tTraining Loss: 0.193113 \tR2: 0.808237\n",
      "Epoch: 7996 \tTraining Loss: 0.175472 \tR2: 0.808237\n",
      "Epoch: 7997 \tTraining Loss: 0.196121 \tR2: 0.808237\n",
      "Epoch: 7998 \tTraining Loss: 0.183533 \tR2: 0.808237\n",
      "Epoch: 7999 \tTraining Loss: 0.190183 \tR2: 0.808237\n",
      "Epoch: 8000 \tTraining Loss: 0.185605 \tR2: 0.715451\n",
      "Epoch: 8001 \tTraining Loss: 0.173447 \tR2: 0.715451\n",
      "Epoch: 8002 \tTraining Loss: 0.190215 \tR2: 0.715451\n",
      "Epoch: 8003 \tTraining Loss: 0.197829 \tR2: 0.715451\n",
      "Epoch: 8004 \tTraining Loss: 0.186548 \tR2: 0.715451\n",
      "Epoch: 8005 \tTraining Loss: 0.202801 \tR2: 0.715451\n",
      "Epoch: 8006 \tTraining Loss: 0.194926 \tR2: 0.715451\n",
      "Epoch: 8007 \tTraining Loss: 0.194260 \tR2: 0.715451\n",
      "Epoch: 8008 \tTraining Loss: 0.178000 \tR2: 0.715451\n",
      "Epoch: 8009 \tTraining Loss: 0.190366 \tR2: 0.715451\n",
      "Epoch: 8010 \tTraining Loss: 0.185607 \tR2: 0.715451\n",
      "Epoch: 8011 \tTraining Loss: 0.188845 \tR2: 0.715451\n",
      "Epoch: 8012 \tTraining Loss: 0.177927 \tR2: 0.715451\n",
      "Epoch: 8013 \tTraining Loss: 0.189700 \tR2: 0.715451\n",
      "Epoch: 8014 \tTraining Loss: 0.175325 \tR2: 0.715451\n",
      "Epoch: 8015 \tTraining Loss: 0.189280 \tR2: 0.715451\n",
      "Epoch: 8016 \tTraining Loss: 0.186217 \tR2: 0.715451\n",
      "Epoch: 8017 \tTraining Loss: 0.188317 \tR2: 0.715451\n",
      "Epoch: 8018 \tTraining Loss: 0.180330 \tR2: 0.715451\n",
      "Epoch: 8019 \tTraining Loss: 0.187168 \tR2: 0.715451\n",
      "Epoch: 8020 \tTraining Loss: 0.158208 \tR2: 0.715451\n",
      "Epoch: 8021 \tTraining Loss: 0.177913 \tR2: 0.715451\n",
      "Epoch: 8022 \tTraining Loss: 0.172257 \tR2: 0.715451\n",
      "Epoch: 8023 \tTraining Loss: 0.183505 \tR2: 0.715451\n",
      "Epoch: 8024 \tTraining Loss: 0.181605 \tR2: 0.715451\n",
      "Epoch: 8025 \tTraining Loss: 0.181062 \tR2: 0.715451\n",
      "Epoch: 8026 \tTraining Loss: 0.182491 \tR2: 0.715451\n",
      "Epoch: 8027 \tTraining Loss: 0.170424 \tR2: 0.715451\n",
      "Epoch: 8028 \tTraining Loss: 0.167460 \tR2: 0.715451\n",
      "Epoch: 8029 \tTraining Loss: 0.176271 \tR2: 0.715451\n",
      "Epoch: 8030 \tTraining Loss: 0.179120 \tR2: 0.715451\n",
      "Epoch: 8031 \tTraining Loss: 0.175598 \tR2: 0.715451\n",
      "Epoch: 8032 \tTraining Loss: 0.174345 \tR2: 0.715451\n",
      "Epoch: 8033 \tTraining Loss: 0.182743 \tR2: 0.715451\n",
      "Epoch: 8034 \tTraining Loss: 0.188513 \tR2: 0.715451\n",
      "Epoch: 8035 \tTraining Loss: 0.184606 \tR2: 0.715451\n",
      "Epoch: 8036 \tTraining Loss: 0.185912 \tR2: 0.715451\n",
      "Epoch: 8037 \tTraining Loss: 0.193837 \tR2: 0.715451\n",
      "Epoch: 8038 \tTraining Loss: 0.187070 \tR2: 0.715451\n",
      "Epoch: 8039 \tTraining Loss: 0.193890 \tR2: 0.715451\n",
      "Epoch: 8040 \tTraining Loss: 0.225420 \tR2: 0.715451\n",
      "Epoch: 8041 \tTraining Loss: 0.185657 \tR2: 0.715451\n",
      "Epoch: 8042 \tTraining Loss: 0.191239 \tR2: 0.715451\n",
      "Epoch: 8043 \tTraining Loss: 0.185191 \tR2: 0.715451\n",
      "Epoch: 8044 \tTraining Loss: 0.195237 \tR2: 0.715451\n",
      "Epoch: 8045 \tTraining Loss: 0.175812 \tR2: 0.715451\n",
      "Epoch: 8046 \tTraining Loss: 0.174876 \tR2: 0.715451\n",
      "Epoch: 8047 \tTraining Loss: 0.193729 \tR2: 0.715451\n",
      "Epoch: 8048 \tTraining Loss: 0.189204 \tR2: 0.715451\n",
      "Epoch: 8049 \tTraining Loss: 0.197827 \tR2: 0.715451\n",
      "Epoch: 8050 \tTraining Loss: 0.190451 \tR2: 0.715451\n",
      "Epoch: 8051 \tTraining Loss: 0.181106 \tR2: 0.715451\n",
      "Epoch: 8052 \tTraining Loss: 0.174773 \tR2: 0.715451\n",
      "Epoch: 8053 \tTraining Loss: 0.184710 \tR2: 0.715451\n",
      "Epoch: 8054 \tTraining Loss: 0.179990 \tR2: 0.715451\n",
      "Epoch: 8055 \tTraining Loss: 0.176910 \tR2: 0.715451\n",
      "Epoch: 8056 \tTraining Loss: 0.166461 \tR2: 0.715451\n",
      "Epoch: 8057 \tTraining Loss: 0.176676 \tR2: 0.715451\n",
      "Epoch: 8058 \tTraining Loss: 0.183486 \tR2: 0.715451\n",
      "Epoch: 8059 \tTraining Loss: 0.177625 \tR2: 0.715451\n",
      "Epoch: 8060 \tTraining Loss: 0.206610 \tR2: 0.715451\n",
      "Epoch: 8061 \tTraining Loss: 0.175284 \tR2: 0.715451\n",
      "Epoch: 8062 \tTraining Loss: 0.192975 \tR2: 0.715451\n",
      "Epoch: 8063 \tTraining Loss: 0.201440 \tR2: 0.715451\n",
      "Epoch: 8064 \tTraining Loss: 0.191841 \tR2: 0.715451\n",
      "Epoch: 8065 \tTraining Loss: 0.191891 \tR2: 0.715451\n",
      "Epoch: 8066 \tTraining Loss: 0.184736 \tR2: 0.715451\n",
      "Epoch: 8067 \tTraining Loss: 0.175126 \tR2: 0.715451\n",
      "Epoch: 8068 \tTraining Loss: 0.197317 \tR2: 0.715451\n",
      "Epoch: 8069 \tTraining Loss: 0.189992 \tR2: 0.715451\n",
      "Epoch: 8070 \tTraining Loss: 0.197570 \tR2: 0.715451\n",
      "Epoch: 8071 \tTraining Loss: 0.185153 \tR2: 0.715451\n",
      "Epoch: 8072 \tTraining Loss: 0.193480 \tR2: 0.715451\n",
      "Epoch: 8073 \tTraining Loss: 0.198306 \tR2: 0.715451\n",
      "Epoch: 8074 \tTraining Loss: 0.189958 \tR2: 0.715451\n",
      "Epoch: 8075 \tTraining Loss: 0.183994 \tR2: 0.715451\n",
      "Epoch: 8076 \tTraining Loss: 0.188088 \tR2: 0.715451\n",
      "Epoch: 8077 \tTraining Loss: 0.188150 \tR2: 0.715451\n",
      "Epoch: 8078 \tTraining Loss: 0.182790 \tR2: 0.715451\n",
      "Epoch: 8079 \tTraining Loss: 0.189483 \tR2: 0.715451\n",
      "Epoch: 8080 \tTraining Loss: 0.193484 \tR2: 0.715451\n",
      "Epoch: 8081 \tTraining Loss: 0.185467 \tR2: 0.715451\n",
      "Epoch: 8082 \tTraining Loss: 0.185506 \tR2: 0.715451\n",
      "Epoch: 8083 \tTraining Loss: 0.167133 \tR2: 0.715451\n",
      "Epoch: 8084 \tTraining Loss: 0.184235 \tR2: 0.715451\n",
      "Epoch: 8085 \tTraining Loss: 0.183496 \tR2: 0.715451\n",
      "Epoch: 8086 \tTraining Loss: 0.189232 \tR2: 0.715451\n",
      "Epoch: 8087 \tTraining Loss: 0.191592 \tR2: 0.715451\n",
      "Epoch: 8088 \tTraining Loss: 0.170471 \tR2: 0.715451\n",
      "Epoch: 8089 \tTraining Loss: 0.185227 \tR2: 0.715451\n",
      "Epoch: 8090 \tTraining Loss: 0.165363 \tR2: 0.715451\n",
      "Epoch: 8091 \tTraining Loss: 0.220208 \tR2: 0.715451\n",
      "Epoch: 8092 \tTraining Loss: 0.206797 \tR2: 0.715451\n",
      "Epoch: 8093 \tTraining Loss: 0.186603 \tR2: 0.715451\n",
      "Epoch: 8094 \tTraining Loss: 0.180650 \tR2: 0.715451\n",
      "Epoch: 8095 \tTraining Loss: 0.181366 \tR2: 0.715451\n",
      "Epoch: 8096 \tTraining Loss: 0.179013 \tR2: 0.715451\n",
      "Epoch: 8097 \tTraining Loss: 0.185055 \tR2: 0.715451\n",
      "Epoch: 8098 \tTraining Loss: 0.172137 \tR2: 0.715451\n",
      "Epoch: 8099 \tTraining Loss: 0.188434 \tR2: 0.715451\n",
      "Epoch: 8100 \tTraining Loss: 0.186873 \tR2: 0.808957\n",
      "Epoch: 8101 \tTraining Loss: 0.179472 \tR2: 0.808957\n",
      "Epoch: 8102 \tTraining Loss: 0.191705 \tR2: 0.808957\n",
      "Epoch: 8103 \tTraining Loss: 0.176628 \tR2: 0.808957\n",
      "Epoch: 8104 \tTraining Loss: 0.182623 \tR2: 0.808957\n",
      "Epoch: 8105 \tTraining Loss: 0.192275 \tR2: 0.808957\n",
      "Epoch: 8106 \tTraining Loss: 0.213201 \tR2: 0.808957\n",
      "Epoch: 8107 \tTraining Loss: 0.192823 \tR2: 0.808957\n",
      "Epoch: 8108 \tTraining Loss: 0.183674 \tR2: 0.808957\n",
      "Epoch: 8109 \tTraining Loss: 0.170206 \tR2: 0.808957\n",
      "Epoch: 8110 \tTraining Loss: 0.170020 \tR2: 0.808957\n",
      "Epoch: 8111 \tTraining Loss: 0.193917 \tR2: 0.808957\n",
      "Epoch: 8112 \tTraining Loss: 0.189301 \tR2: 0.808957\n",
      "Epoch: 8113 \tTraining Loss: 0.184310 \tR2: 0.808957\n",
      "Epoch: 8114 \tTraining Loss: 0.180905 \tR2: 0.808957\n",
      "Epoch: 8115 \tTraining Loss: 0.192996 \tR2: 0.808957\n",
      "Epoch: 8116 \tTraining Loss: 0.166200 \tR2: 0.808957\n",
      "Epoch: 8117 \tTraining Loss: 0.187020 \tR2: 0.808957\n",
      "Epoch: 8118 \tTraining Loss: 0.186943 \tR2: 0.808957\n",
      "Epoch: 8119 \tTraining Loss: 0.187040 \tR2: 0.808957\n",
      "Epoch: 8120 \tTraining Loss: 0.178907 \tR2: 0.808957\n",
      "Epoch: 8121 \tTraining Loss: 0.186294 \tR2: 0.808957\n",
      "Epoch: 8122 \tTraining Loss: 0.172119 \tR2: 0.808957\n",
      "Epoch: 8123 \tTraining Loss: 0.193809 \tR2: 0.808957\n",
      "Epoch: 8124 \tTraining Loss: 0.179859 \tR2: 0.808957\n",
      "Epoch: 8125 \tTraining Loss: 0.155501 \tR2: 0.808957\n",
      "Epoch: 8126 \tTraining Loss: 0.176007 \tR2: 0.808957\n",
      "Epoch: 8127 \tTraining Loss: 0.176232 \tR2: 0.808957\n",
      "Epoch: 8128 \tTraining Loss: 0.187977 \tR2: 0.808957\n",
      "Epoch: 8129 \tTraining Loss: 0.177985 \tR2: 0.808957\n",
      "Epoch: 8130 \tTraining Loss: 0.176600 \tR2: 0.808957\n",
      "Epoch: 8131 \tTraining Loss: 0.171534 \tR2: 0.808957\n",
      "Epoch: 8132 \tTraining Loss: 0.176723 \tR2: 0.808957\n",
      "Epoch: 8133 \tTraining Loss: 0.191389 \tR2: 0.808957\n",
      "Epoch: 8134 \tTraining Loss: 0.183005 \tR2: 0.808957\n",
      "Epoch: 8135 \tTraining Loss: 0.171443 \tR2: 0.808957\n",
      "Epoch: 8136 \tTraining Loss: 0.202356 \tR2: 0.808957\n",
      "Epoch: 8137 \tTraining Loss: 0.183202 \tR2: 0.808957\n",
      "Epoch: 8138 \tTraining Loss: 0.190695 \tR2: 0.808957\n",
      "Epoch: 8139 \tTraining Loss: 0.189075 \tR2: 0.808957\n",
      "Epoch: 8140 \tTraining Loss: 0.176840 \tR2: 0.808957\n",
      "Epoch: 8141 \tTraining Loss: 0.172197 \tR2: 0.808957\n",
      "Epoch: 8142 \tTraining Loss: 0.167498 \tR2: 0.808957\n",
      "Epoch: 8143 \tTraining Loss: 0.168582 \tR2: 0.808957\n",
      "Epoch: 8144 \tTraining Loss: 0.188284 \tR2: 0.808957\n",
      "Epoch: 8145 \tTraining Loss: 0.198225 \tR2: 0.808957\n",
      "Epoch: 8146 \tTraining Loss: 0.178380 \tR2: 0.808957\n",
      "Epoch: 8147 \tTraining Loss: 0.193558 \tR2: 0.808957\n",
      "Epoch: 8148 \tTraining Loss: 0.194749 \tR2: 0.808957\n",
      "Epoch: 8149 \tTraining Loss: 0.179646 \tR2: 0.808957\n",
      "Epoch: 8150 \tTraining Loss: 0.192701 \tR2: 0.808957\n",
      "Epoch: 8151 \tTraining Loss: 0.183490 \tR2: 0.808957\n",
      "Epoch: 8152 \tTraining Loss: 0.177688 \tR2: 0.808957\n",
      "Epoch: 8153 \tTraining Loss: 0.180070 \tR2: 0.808957\n",
      "Epoch: 8154 \tTraining Loss: 0.198848 \tR2: 0.808957\n",
      "Epoch: 8155 \tTraining Loss: 0.182900 \tR2: 0.808957\n",
      "Epoch: 8156 \tTraining Loss: 0.188315 \tR2: 0.808957\n",
      "Epoch: 8157 \tTraining Loss: 0.197333 \tR2: 0.808957\n",
      "Epoch: 8158 \tTraining Loss: 0.200333 \tR2: 0.808957\n",
      "Epoch: 8159 \tTraining Loss: 0.186339 \tR2: 0.808957\n",
      "Epoch: 8160 \tTraining Loss: 0.180092 \tR2: 0.808957\n",
      "Epoch: 8161 \tTraining Loss: 0.182556 \tR2: 0.808957\n",
      "Epoch: 8162 \tTraining Loss: 0.187461 \tR2: 0.808957\n",
      "Epoch: 8163 \tTraining Loss: 0.232360 \tR2: 0.808957\n",
      "Epoch: 8164 \tTraining Loss: 0.238867 \tR2: 0.808957\n",
      "Epoch: 8165 \tTraining Loss: 0.197165 \tR2: 0.808957\n",
      "Epoch: 8166 \tTraining Loss: 0.179188 \tR2: 0.808957\n",
      "Epoch: 8167 \tTraining Loss: 0.201429 \tR2: 0.808957\n",
      "Epoch: 8168 \tTraining Loss: 0.171921 \tR2: 0.808957\n",
      "Epoch: 8169 \tTraining Loss: 0.180687 \tR2: 0.808957\n",
      "Epoch: 8170 \tTraining Loss: 0.177196 \tR2: 0.808957\n",
      "Epoch: 8171 \tTraining Loss: 0.177191 \tR2: 0.808957\n",
      "Epoch: 8172 \tTraining Loss: 0.168571 \tR2: 0.808957\n",
      "Epoch: 8173 \tTraining Loss: 0.187116 \tR2: 0.808957\n",
      "Epoch: 8174 \tTraining Loss: 0.178408 \tR2: 0.808957\n",
      "Epoch: 8175 \tTraining Loss: 0.175760 \tR2: 0.808957\n",
      "Epoch: 8176 \tTraining Loss: 0.172805 \tR2: 0.808957\n",
      "Epoch: 8177 \tTraining Loss: 0.190800 \tR2: 0.808957\n",
      "Epoch: 8178 \tTraining Loss: 0.177554 \tR2: 0.808957\n",
      "Epoch: 8179 \tTraining Loss: 0.172168 \tR2: 0.808957\n",
      "Epoch: 8180 \tTraining Loss: 0.189921 \tR2: 0.808957\n",
      "Epoch: 8181 \tTraining Loss: 0.187468 \tR2: 0.808957\n",
      "Epoch: 8182 \tTraining Loss: 0.176974 \tR2: 0.808957\n",
      "Epoch: 8183 \tTraining Loss: 0.180222 \tR2: 0.808957\n",
      "Epoch: 8184 \tTraining Loss: 0.180224 \tR2: 0.808957\n",
      "Epoch: 8185 \tTraining Loss: 0.190019 \tR2: 0.808957\n",
      "Epoch: 8186 \tTraining Loss: 0.191529 \tR2: 0.808957\n",
      "Epoch: 8187 \tTraining Loss: 0.173515 \tR2: 0.808957\n",
      "Epoch: 8188 \tTraining Loss: 0.176056 \tR2: 0.808957\n",
      "Epoch: 8189 \tTraining Loss: 0.190793 \tR2: 0.808957\n",
      "Epoch: 8190 \tTraining Loss: 0.221415 \tR2: 0.808957\n",
      "Epoch: 8191 \tTraining Loss: 0.198206 \tR2: 0.808957\n",
      "Epoch: 8192 \tTraining Loss: 0.174159 \tR2: 0.808957\n",
      "Epoch: 8193 \tTraining Loss: 0.185463 \tR2: 0.808957\n",
      "Epoch: 8194 \tTraining Loss: 0.172237 \tR2: 0.808957\n",
      "Epoch: 8195 \tTraining Loss: 0.187389 \tR2: 0.808957\n",
      "Epoch: 8196 \tTraining Loss: 0.176645 \tR2: 0.808957\n",
      "Epoch: 8197 \tTraining Loss: 0.189763 \tR2: 0.808957\n",
      "Epoch: 8198 \tTraining Loss: 0.191932 \tR2: 0.808957\n",
      "Epoch: 8199 \tTraining Loss: 0.167173 \tR2: 0.808957\n",
      "Epoch: 8200 \tTraining Loss: 0.186661 \tR2: 0.649698\n",
      "Epoch: 8201 \tTraining Loss: 0.195228 \tR2: 0.649698\n",
      "Epoch: 8202 \tTraining Loss: 0.178916 \tR2: 0.649698\n",
      "Epoch: 8203 \tTraining Loss: 0.182560 \tR2: 0.649698\n",
      "Epoch: 8204 \tTraining Loss: 0.195337 \tR2: 0.649698\n",
      "Epoch: 8205 \tTraining Loss: 0.205023 \tR2: 0.649698\n",
      "Epoch: 8206 \tTraining Loss: 0.179347 \tR2: 0.649698\n",
      "Epoch: 8207 \tTraining Loss: 0.178181 \tR2: 0.649698\n",
      "Epoch: 8208 \tTraining Loss: 0.203153 \tR2: 0.649698\n",
      "Epoch: 8209 \tTraining Loss: 0.196063 \tR2: 0.649698\n",
      "Epoch: 8210 \tTraining Loss: 0.200507 \tR2: 0.649698\n",
      "Epoch: 8211 \tTraining Loss: 0.175001 \tR2: 0.649698\n",
      "Epoch: 8212 \tTraining Loss: 0.170971 \tR2: 0.649698\n",
      "Epoch: 8213 \tTraining Loss: 0.186166 \tR2: 0.649698\n",
      "Epoch: 8214 \tTraining Loss: 0.195746 \tR2: 0.649698\n",
      "Epoch: 8215 \tTraining Loss: 0.191264 \tR2: 0.649698\n",
      "Epoch: 8216 \tTraining Loss: 0.197307 \tR2: 0.649698\n",
      "Epoch: 8217 \tTraining Loss: 0.179949 \tR2: 0.649698\n",
      "Epoch: 8218 \tTraining Loss: 0.189944 \tR2: 0.649698\n",
      "Epoch: 8219 \tTraining Loss: 0.189041 \tR2: 0.649698\n",
      "Epoch: 8220 \tTraining Loss: 0.182603 \tR2: 0.649698\n",
      "Epoch: 8221 \tTraining Loss: 0.201127 \tR2: 0.649698\n",
      "Epoch: 8222 \tTraining Loss: 0.189540 \tR2: 0.649698\n",
      "Epoch: 8223 \tTraining Loss: 0.179491 \tR2: 0.649698\n",
      "Epoch: 8224 \tTraining Loss: 0.188506 \tR2: 0.649698\n",
      "Epoch: 8225 \tTraining Loss: 0.164183 \tR2: 0.649698\n",
      "Epoch: 8226 \tTraining Loss: 0.175639 \tR2: 0.649698\n",
      "Epoch: 8227 \tTraining Loss: 0.182954 \tR2: 0.649698\n",
      "Epoch: 8228 \tTraining Loss: 0.167581 \tR2: 0.649698\n",
      "Epoch: 8229 \tTraining Loss: 0.170850 \tR2: 0.649698\n",
      "Epoch: 8230 \tTraining Loss: 0.172858 \tR2: 0.649698\n",
      "Epoch: 8231 \tTraining Loss: 0.180110 \tR2: 0.649698\n",
      "Epoch: 8232 \tTraining Loss: 0.188020 \tR2: 0.649698\n",
      "Epoch: 8233 \tTraining Loss: 0.180291 \tR2: 0.649698\n",
      "Epoch: 8234 \tTraining Loss: 0.178502 \tR2: 0.649698\n",
      "Epoch: 8235 \tTraining Loss: 0.193669 \tR2: 0.649698\n",
      "Epoch: 8236 \tTraining Loss: 0.207249 \tR2: 0.649698\n",
      "Epoch: 8237 \tTraining Loss: 0.190877 \tR2: 0.649698\n",
      "Epoch: 8238 \tTraining Loss: 0.192580 \tR2: 0.649698\n",
      "Epoch: 8239 \tTraining Loss: 0.171089 \tR2: 0.649698\n",
      "Epoch: 8240 \tTraining Loss: 0.178575 \tR2: 0.649698\n",
      "Epoch: 8241 \tTraining Loss: 0.190728 \tR2: 0.649698\n",
      "Epoch: 8242 \tTraining Loss: 0.185654 \tR2: 0.649698\n",
      "Epoch: 8243 \tTraining Loss: 0.187482 \tR2: 0.649698\n",
      "Epoch: 8244 \tTraining Loss: 0.194440 \tR2: 0.649698\n",
      "Epoch: 8245 \tTraining Loss: 0.163321 \tR2: 0.649698\n",
      "Epoch: 8246 \tTraining Loss: 0.193539 \tR2: 0.649698\n",
      "Epoch: 8247 \tTraining Loss: 0.174786 \tR2: 0.649698\n",
      "Epoch: 8248 \tTraining Loss: 0.171478 \tR2: 0.649698\n",
      "Epoch: 8249 \tTraining Loss: 0.175327 \tR2: 0.649698\n",
      "Epoch: 8250 \tTraining Loss: 0.188035 \tR2: 0.649698\n",
      "Epoch: 8251 \tTraining Loss: 0.183797 \tR2: 0.649698\n",
      "Epoch: 8252 \tTraining Loss: 0.184592 \tR2: 0.649698\n",
      "Epoch: 8253 \tTraining Loss: 0.176193 \tR2: 0.649698\n",
      "Epoch: 8254 \tTraining Loss: 0.175315 \tR2: 0.649698\n",
      "Epoch: 8255 \tTraining Loss: 0.182499 \tR2: 0.649698\n",
      "Epoch: 8256 \tTraining Loss: 0.179818 \tR2: 0.649698\n",
      "Epoch: 8257 \tTraining Loss: 0.158482 \tR2: 0.649698\n",
      "Epoch: 8258 \tTraining Loss: 0.201363 \tR2: 0.649698\n",
      "Epoch: 8259 \tTraining Loss: 0.195076 \tR2: 0.649698\n",
      "Epoch: 8260 \tTraining Loss: 0.206513 \tR2: 0.649698\n",
      "Epoch: 8261 \tTraining Loss: 0.184503 \tR2: 0.649698\n",
      "Epoch: 8262 \tTraining Loss: 0.169984 \tR2: 0.649698\n",
      "Epoch: 8263 \tTraining Loss: 0.166390 \tR2: 0.649698\n",
      "Epoch: 8264 \tTraining Loss: 0.197355 \tR2: 0.649698\n",
      "Epoch: 8265 \tTraining Loss: 0.180690 \tR2: 0.649698\n",
      "Epoch: 8266 \tTraining Loss: 0.196698 \tR2: 0.649698\n",
      "Epoch: 8267 \tTraining Loss: 0.207259 \tR2: 0.649698\n",
      "Epoch: 8268 \tTraining Loss: 0.178588 \tR2: 0.649698\n",
      "Epoch: 8269 \tTraining Loss: 0.187555 \tR2: 0.649698\n",
      "Epoch: 8270 \tTraining Loss: 0.189581 \tR2: 0.649698\n",
      "Epoch: 8271 \tTraining Loss: 0.181415 \tR2: 0.649698\n",
      "Epoch: 8272 \tTraining Loss: 0.182165 \tR2: 0.649698\n",
      "Epoch: 8273 \tTraining Loss: 0.177491 \tR2: 0.649698\n",
      "Epoch: 8274 \tTraining Loss: 0.196040 \tR2: 0.649698\n",
      "Epoch: 8275 \tTraining Loss: 0.174746 \tR2: 0.649698\n",
      "Epoch: 8276 \tTraining Loss: 0.182995 \tR2: 0.649698\n",
      "Epoch: 8277 \tTraining Loss: 0.198509 \tR2: 0.649698\n",
      "Epoch: 8278 \tTraining Loss: 0.170309 \tR2: 0.649698\n",
      "Epoch: 8279 \tTraining Loss: 0.171008 \tR2: 0.649698\n",
      "Epoch: 8280 \tTraining Loss: 0.214833 \tR2: 0.649698\n",
      "Epoch: 8281 \tTraining Loss: 0.184397 \tR2: 0.649698\n",
      "Epoch: 8282 \tTraining Loss: 0.177369 \tR2: 0.649698\n",
      "Epoch: 8283 \tTraining Loss: 0.174989 \tR2: 0.649698\n",
      "Epoch: 8284 \tTraining Loss: 0.177322 \tR2: 0.649698\n",
      "Epoch: 8285 \tTraining Loss: 0.192215 \tR2: 0.649698\n",
      "Epoch: 8286 \tTraining Loss: 0.179814 \tR2: 0.649698\n",
      "Epoch: 8287 \tTraining Loss: 0.185605 \tR2: 0.649698\n",
      "Epoch: 8288 \tTraining Loss: 0.185399 \tR2: 0.649698\n",
      "Epoch: 8289 \tTraining Loss: 0.186440 \tR2: 0.649698\n",
      "Epoch: 8290 \tTraining Loss: 0.198782 \tR2: 0.649698\n",
      "Epoch: 8291 \tTraining Loss: 0.175552 \tR2: 0.649698\n",
      "Epoch: 8292 \tTraining Loss: 0.171556 \tR2: 0.649698\n",
      "Epoch: 8293 \tTraining Loss: 0.180429 \tR2: 0.649698\n",
      "Epoch: 8294 \tTraining Loss: 0.191898 \tR2: 0.649698\n",
      "Epoch: 8295 \tTraining Loss: 0.167292 \tR2: 0.649698\n",
      "Epoch: 8296 \tTraining Loss: 0.177647 \tR2: 0.649698\n",
      "Epoch: 8297 \tTraining Loss: 0.178204 \tR2: 0.649698\n",
      "Epoch: 8298 \tTraining Loss: 0.176417 \tR2: 0.649698\n",
      "Epoch: 8299 \tTraining Loss: 0.188135 \tR2: 0.649698\n",
      "Epoch: 8300 \tTraining Loss: 0.185916 \tR2: 0.785988\n",
      "Epoch: 8301 \tTraining Loss: 0.201716 \tR2: 0.785988\n",
      "Epoch: 8302 \tTraining Loss: 0.189794 \tR2: 0.785988\n",
      "Epoch: 8303 \tTraining Loss: 0.174636 \tR2: 0.785988\n",
      "Epoch: 8304 \tTraining Loss: 0.167912 \tR2: 0.785988\n",
      "Epoch: 8305 \tTraining Loss: 0.175310 \tR2: 0.785988\n",
      "Epoch: 8306 \tTraining Loss: 0.180308 \tR2: 0.785988\n",
      "Epoch: 8307 \tTraining Loss: 0.185366 \tR2: 0.785988\n",
      "Epoch: 8308 \tTraining Loss: 0.171609 \tR2: 0.785988\n",
      "Epoch: 8309 \tTraining Loss: 0.202863 \tR2: 0.785988\n",
      "Epoch: 8310 \tTraining Loss: 0.170603 \tR2: 0.785988\n",
      "Epoch: 8311 \tTraining Loss: 0.170992 \tR2: 0.785988\n",
      "Epoch: 8312 \tTraining Loss: 0.182421 \tR2: 0.785988\n",
      "Epoch: 8313 \tTraining Loss: 0.178157 \tR2: 0.785988\n",
      "Epoch: 8314 \tTraining Loss: 0.183220 \tR2: 0.785988\n",
      "Epoch: 8315 \tTraining Loss: 0.183506 \tR2: 0.785988\n",
      "Epoch: 8316 \tTraining Loss: 0.205052 \tR2: 0.785988\n",
      "Epoch: 8317 \tTraining Loss: 0.180572 \tR2: 0.785988\n",
      "Epoch: 8318 \tTraining Loss: 0.178919 \tR2: 0.785988\n",
      "Epoch: 8319 \tTraining Loss: 0.161520 \tR2: 0.785988\n",
      "Epoch: 8320 \tTraining Loss: 0.189493 \tR2: 0.785988\n",
      "Epoch: 8321 \tTraining Loss: 0.182865 \tR2: 0.785988\n",
      "Epoch: 8322 \tTraining Loss: 0.181040 \tR2: 0.785988\n",
      "Epoch: 8323 \tTraining Loss: 0.173807 \tR2: 0.785988\n",
      "Epoch: 8324 \tTraining Loss: 0.180282 \tR2: 0.785988\n",
      "Epoch: 8325 \tTraining Loss: 0.180834 \tR2: 0.785988\n",
      "Epoch: 8326 \tTraining Loss: 0.183939 \tR2: 0.785988\n",
      "Epoch: 8327 \tTraining Loss: 0.190752 \tR2: 0.785988\n",
      "Epoch: 8328 \tTraining Loss: 0.186678 \tR2: 0.785988\n",
      "Epoch: 8329 \tTraining Loss: 0.179682 \tR2: 0.785988\n",
      "Epoch: 8330 \tTraining Loss: 0.168492 \tR2: 0.785988\n",
      "Epoch: 8331 \tTraining Loss: 0.168209 \tR2: 0.785988\n",
      "Epoch: 8332 \tTraining Loss: 0.152030 \tR2: 0.785988\n",
      "Epoch: 8333 \tTraining Loss: 0.180555 \tR2: 0.785988\n",
      "Epoch: 8334 \tTraining Loss: 0.175497 \tR2: 0.785988\n",
      "Epoch: 8335 \tTraining Loss: 0.174810 \tR2: 0.785988\n",
      "Epoch: 8336 \tTraining Loss: 0.186845 \tR2: 0.785988\n",
      "Epoch: 8337 \tTraining Loss: 0.172704 \tR2: 0.785988\n",
      "Epoch: 8338 \tTraining Loss: 0.181213 \tR2: 0.785988\n",
      "Epoch: 8339 \tTraining Loss: 0.193467 \tR2: 0.785988\n",
      "Epoch: 8340 \tTraining Loss: 0.176007 \tR2: 0.785988\n",
      "Epoch: 8341 \tTraining Loss: 0.182390 \tR2: 0.785988\n",
      "Epoch: 8342 \tTraining Loss: 0.170382 \tR2: 0.785988\n",
      "Epoch: 8343 \tTraining Loss: 0.183343 \tR2: 0.785988\n",
      "Epoch: 8344 \tTraining Loss: 0.196636 \tR2: 0.785988\n",
      "Epoch: 8345 \tTraining Loss: 0.196676 \tR2: 0.785988\n",
      "Epoch: 8346 \tTraining Loss: 0.183150 \tR2: 0.785988\n",
      "Epoch: 8347 \tTraining Loss: 0.175532 \tR2: 0.785988\n",
      "Epoch: 8348 \tTraining Loss: 0.185050 \tR2: 0.785988\n",
      "Epoch: 8349 \tTraining Loss: 0.197111 \tR2: 0.785988\n",
      "Epoch: 8350 \tTraining Loss: 0.183723 \tR2: 0.785988\n",
      "Epoch: 8351 \tTraining Loss: 0.179403 \tR2: 0.785988\n",
      "Epoch: 8352 \tTraining Loss: 0.177185 \tR2: 0.785988\n",
      "Epoch: 8353 \tTraining Loss: 0.162319 \tR2: 0.785988\n",
      "Epoch: 8354 \tTraining Loss: 0.177615 \tR2: 0.785988\n",
      "Epoch: 8355 \tTraining Loss: 0.184301 \tR2: 0.785988\n",
      "Epoch: 8356 \tTraining Loss: 0.196759 \tR2: 0.785988\n",
      "Epoch: 8357 \tTraining Loss: 0.174825 \tR2: 0.785988\n",
      "Epoch: 8358 \tTraining Loss: 0.172918 \tR2: 0.785988\n",
      "Epoch: 8359 \tTraining Loss: 0.188930 \tR2: 0.785988\n",
      "Epoch: 8360 \tTraining Loss: 0.183444 \tR2: 0.785988\n",
      "Epoch: 8361 \tTraining Loss: 0.205910 \tR2: 0.785988\n",
      "Epoch: 8362 \tTraining Loss: 0.193201 \tR2: 0.785988\n",
      "Epoch: 8363 \tTraining Loss: 0.177692 \tR2: 0.785988\n",
      "Epoch: 8364 \tTraining Loss: 0.192137 \tR2: 0.785988\n",
      "Epoch: 8365 \tTraining Loss: 0.190981 \tR2: 0.785988\n",
      "Epoch: 8366 \tTraining Loss: 0.190625 \tR2: 0.785988\n",
      "Epoch: 8367 \tTraining Loss: 0.181479 \tR2: 0.785988\n",
      "Epoch: 8368 \tTraining Loss: 0.188597 \tR2: 0.785988\n",
      "Epoch: 8369 \tTraining Loss: 0.176113 \tR2: 0.785988\n",
      "Epoch: 8370 \tTraining Loss: 0.181222 \tR2: 0.785988\n",
      "Epoch: 8371 \tTraining Loss: 0.177892 \tR2: 0.785988\n",
      "Epoch: 8372 \tTraining Loss: 0.177312 \tR2: 0.785988\n",
      "Epoch: 8373 \tTraining Loss: 0.182456 \tR2: 0.785988\n",
      "Epoch: 8374 \tTraining Loss: 0.192066 \tR2: 0.785988\n",
      "Epoch: 8375 \tTraining Loss: 0.187212 \tR2: 0.785988\n",
      "Epoch: 8376 \tTraining Loss: 0.175189 \tR2: 0.785988\n",
      "Epoch: 8377 \tTraining Loss: 0.166337 \tR2: 0.785988\n",
      "Epoch: 8378 \tTraining Loss: 0.190986 \tR2: 0.785988\n",
      "Epoch: 8379 \tTraining Loss: 0.182600 \tR2: 0.785988\n",
      "Epoch: 8380 \tTraining Loss: 0.179884 \tR2: 0.785988\n",
      "Epoch: 8381 \tTraining Loss: 0.176940 \tR2: 0.785988\n",
      "Epoch: 8382 \tTraining Loss: 0.172845 \tR2: 0.785988\n",
      "Epoch: 8383 \tTraining Loss: 0.184232 \tR2: 0.785988\n",
      "Epoch: 8384 \tTraining Loss: 0.169339 \tR2: 0.785988\n",
      "Epoch: 8385 \tTraining Loss: 0.182599 \tR2: 0.785988\n",
      "Epoch: 8386 \tTraining Loss: 0.175108 \tR2: 0.785988\n",
      "Epoch: 8387 \tTraining Loss: 0.190487 \tR2: 0.785988\n",
      "Epoch: 8388 \tTraining Loss: 0.178286 \tR2: 0.785988\n",
      "Epoch: 8389 \tTraining Loss: 0.183555 \tR2: 0.785988\n",
      "Epoch: 8390 \tTraining Loss: 0.182014 \tR2: 0.785988\n",
      "Epoch: 8391 \tTraining Loss: 0.181739 \tR2: 0.785988\n",
      "Epoch: 8392 \tTraining Loss: 0.174923 \tR2: 0.785988\n",
      "Epoch: 8393 \tTraining Loss: 0.168708 \tR2: 0.785988\n",
      "Epoch: 8394 \tTraining Loss: 0.185361 \tR2: 0.785988\n",
      "Epoch: 8395 \tTraining Loss: 0.183943 \tR2: 0.785988\n",
      "Epoch: 8396 \tTraining Loss: 0.188823 \tR2: 0.785988\n",
      "Epoch: 8397 \tTraining Loss: 0.163966 \tR2: 0.785988\n",
      "Epoch: 8398 \tTraining Loss: 0.174280 \tR2: 0.785988\n",
      "Epoch: 8399 \tTraining Loss: 0.191878 \tR2: 0.785988\n",
      "Epoch: 8400 \tTraining Loss: 0.166514 \tR2: 0.859423\n",
      "Epoch: 8401 \tTraining Loss: 0.182448 \tR2: 0.859423\n",
      "Epoch: 8402 \tTraining Loss: 0.205788 \tR2: 0.859423\n",
      "Epoch: 8403 \tTraining Loss: 0.188420 \tR2: 0.859423\n",
      "Epoch: 8404 \tTraining Loss: 0.166961 \tR2: 0.859423\n",
      "Epoch: 8405 \tTraining Loss: 0.173962 \tR2: 0.859423\n",
      "Epoch: 8406 \tTraining Loss: 0.206284 \tR2: 0.859423\n",
      "Epoch: 8407 \tTraining Loss: 0.174407 \tR2: 0.859423\n",
      "Epoch: 8408 \tTraining Loss: 0.170367 \tR2: 0.859423\n",
      "Epoch: 8409 \tTraining Loss: 0.182786 \tR2: 0.859423\n",
      "Epoch: 8410 \tTraining Loss: 0.183700 \tR2: 0.859423\n",
      "Epoch: 8411 \tTraining Loss: 0.183439 \tR2: 0.859423\n",
      "Epoch: 8412 \tTraining Loss: 0.180752 \tR2: 0.859423\n",
      "Epoch: 8413 \tTraining Loss: 0.169755 \tR2: 0.859423\n",
      "Epoch: 8414 \tTraining Loss: 0.192368 \tR2: 0.859423\n",
      "Epoch: 8415 \tTraining Loss: 0.180946 \tR2: 0.859423\n",
      "Epoch: 8416 \tTraining Loss: 0.179175 \tR2: 0.859423\n",
      "Epoch: 8417 \tTraining Loss: 0.180856 \tR2: 0.859423\n",
      "Epoch: 8418 \tTraining Loss: 0.204773 \tR2: 0.859423\n",
      "Epoch: 8419 \tTraining Loss: 0.186039 \tR2: 0.859423\n",
      "Epoch: 8420 \tTraining Loss: 0.193761 \tR2: 0.859423\n",
      "Epoch: 8421 \tTraining Loss: 0.182122 \tR2: 0.859423\n",
      "Epoch: 8422 \tTraining Loss: 0.187075 \tR2: 0.859423\n",
      "Epoch: 8423 \tTraining Loss: 0.189603 \tR2: 0.859423\n",
      "Epoch: 8424 \tTraining Loss: 0.172472 \tR2: 0.859423\n",
      "Epoch: 8425 \tTraining Loss: 0.178895 \tR2: 0.859423\n",
      "Epoch: 8426 \tTraining Loss: 0.188755 \tR2: 0.859423\n",
      "Epoch: 8427 \tTraining Loss: 0.176921 \tR2: 0.859423\n",
      "Epoch: 8428 \tTraining Loss: 0.174877 \tR2: 0.859423\n",
      "Epoch: 8429 \tTraining Loss: 0.179838 \tR2: 0.859423\n",
      "Epoch: 8430 \tTraining Loss: 0.189023 \tR2: 0.859423\n",
      "Epoch: 8431 \tTraining Loss: 0.187007 \tR2: 0.859423\n",
      "Epoch: 8432 \tTraining Loss: 0.171937 \tR2: 0.859423\n",
      "Epoch: 8433 \tTraining Loss: 0.176605 \tR2: 0.859423\n",
      "Epoch: 8434 \tTraining Loss: 0.186651 \tR2: 0.859423\n",
      "Epoch: 8435 \tTraining Loss: 0.186351 \tR2: 0.859423\n",
      "Epoch: 8436 \tTraining Loss: 0.187888 \tR2: 0.859423\n",
      "Epoch: 8437 \tTraining Loss: 0.192582 \tR2: 0.859423\n",
      "Epoch: 8438 \tTraining Loss: 0.186716 \tR2: 0.859423\n",
      "Epoch: 8439 \tTraining Loss: 0.183414 \tR2: 0.859423\n",
      "Epoch: 8440 \tTraining Loss: 0.198724 \tR2: 0.859423\n",
      "Epoch: 8441 \tTraining Loss: 0.174544 \tR2: 0.859423\n",
      "Epoch: 8442 \tTraining Loss: 0.192296 \tR2: 0.859423\n",
      "Epoch: 8443 \tTraining Loss: 0.193116 \tR2: 0.859423\n",
      "Epoch: 8444 \tTraining Loss: 0.190910 \tR2: 0.859423\n",
      "Epoch: 8445 \tTraining Loss: 0.183361 \tR2: 0.859423\n",
      "Epoch: 8446 \tTraining Loss: 0.178424 \tR2: 0.859423\n",
      "Epoch: 8447 \tTraining Loss: 0.224121 \tR2: 0.859423\n",
      "Epoch: 8448 \tTraining Loss: 0.185551 \tR2: 0.859423\n",
      "Epoch: 8449 \tTraining Loss: 0.183993 \tR2: 0.859423\n",
      "Epoch: 8450 \tTraining Loss: 0.178044 \tR2: 0.859423\n",
      "Epoch: 8451 \tTraining Loss: 0.182362 \tR2: 0.859423\n",
      "Epoch: 8452 \tTraining Loss: 0.181919 \tR2: 0.859423\n",
      "Epoch: 8453 \tTraining Loss: 0.177721 \tR2: 0.859423\n",
      "Epoch: 8454 \tTraining Loss: 0.176073 \tR2: 0.859423\n",
      "Epoch: 8455 \tTraining Loss: 0.178822 \tR2: 0.859423\n",
      "Epoch: 8456 \tTraining Loss: 0.194567 \tR2: 0.859423\n",
      "Epoch: 8457 \tTraining Loss: 0.186555 \tR2: 0.859423\n",
      "Epoch: 8458 \tTraining Loss: 0.165920 \tR2: 0.859423\n",
      "Epoch: 8459 \tTraining Loss: 0.203753 \tR2: 0.859423\n",
      "Epoch: 8460 \tTraining Loss: 0.185246 \tR2: 0.859423\n",
      "Epoch: 8461 \tTraining Loss: 0.168591 \tR2: 0.859423\n",
      "Epoch: 8462 \tTraining Loss: 0.204589 \tR2: 0.859423\n",
      "Epoch: 8463 \tTraining Loss: 0.199504 \tR2: 0.859423\n",
      "Epoch: 8464 \tTraining Loss: 0.196501 \tR2: 0.859423\n",
      "Epoch: 8465 \tTraining Loss: 0.183926 \tR2: 0.859423\n",
      "Epoch: 8466 \tTraining Loss: 0.160173 \tR2: 0.859423\n",
      "Epoch: 8467 \tTraining Loss: 0.205181 \tR2: 0.859423\n",
      "Epoch: 8468 \tTraining Loss: 0.178906 \tR2: 0.859423\n",
      "Epoch: 8469 \tTraining Loss: 0.179921 \tR2: 0.859423\n",
      "Epoch: 8470 \tTraining Loss: 0.176539 \tR2: 0.859423\n",
      "Epoch: 8471 \tTraining Loss: 0.165848 \tR2: 0.859423\n",
      "Epoch: 8472 \tTraining Loss: 0.179724 \tR2: 0.859423\n",
      "Epoch: 8473 \tTraining Loss: 0.186283 \tR2: 0.859423\n",
      "Epoch: 8474 \tTraining Loss: 0.185275 \tR2: 0.859423\n",
      "Epoch: 8475 \tTraining Loss: 0.177419 \tR2: 0.859423\n",
      "Epoch: 8476 \tTraining Loss: 0.179248 \tR2: 0.859423\n",
      "Epoch: 8477 \tTraining Loss: 0.194660 \tR2: 0.859423\n",
      "Epoch: 8478 \tTraining Loss: 0.173654 \tR2: 0.859423\n",
      "Epoch: 8479 \tTraining Loss: 0.178493 \tR2: 0.859423\n",
      "Epoch: 8480 \tTraining Loss: 0.168477 \tR2: 0.859423\n",
      "Epoch: 8481 \tTraining Loss: 0.173635 \tR2: 0.859423\n",
      "Epoch: 8482 \tTraining Loss: 0.190826 \tR2: 0.859423\n",
      "Epoch: 8483 \tTraining Loss: 0.170012 \tR2: 0.859423\n",
      "Epoch: 8484 \tTraining Loss: 0.192595 \tR2: 0.859423\n",
      "Epoch: 8485 \tTraining Loss: 0.170511 \tR2: 0.859423\n",
      "Epoch: 8486 \tTraining Loss: 0.189090 \tR2: 0.859423\n",
      "Epoch: 8487 \tTraining Loss: 0.180627 \tR2: 0.859423\n",
      "Epoch: 8488 \tTraining Loss: 0.177524 \tR2: 0.859423\n",
      "Epoch: 8489 \tTraining Loss: 0.158315 \tR2: 0.859423\n",
      "Epoch: 8490 \tTraining Loss: 0.169165 \tR2: 0.859423\n",
      "Epoch: 8491 \tTraining Loss: 0.186506 \tR2: 0.859423\n",
      "Epoch: 8492 \tTraining Loss: 0.179314 \tR2: 0.859423\n",
      "Epoch: 8493 \tTraining Loss: 0.168798 \tR2: 0.859423\n",
      "Epoch: 8494 \tTraining Loss: 0.179312 \tR2: 0.859423\n",
      "Epoch: 8495 \tTraining Loss: 0.174327 \tR2: 0.859423\n",
      "Epoch: 8496 \tTraining Loss: 0.181796 \tR2: 0.859423\n",
      "Epoch: 8497 \tTraining Loss: 0.185854 \tR2: 0.859423\n",
      "Epoch: 8498 \tTraining Loss: 0.190982 \tR2: 0.859423\n",
      "Epoch: 8499 \tTraining Loss: 0.223488 \tR2: 0.859423\n",
      "Epoch: 8500 \tTraining Loss: 0.170504 \tR2: 0.807497\n",
      "Epoch: 8501 \tTraining Loss: 0.168748 \tR2: 0.807497\n",
      "Epoch: 8502 \tTraining Loss: 0.184872 \tR2: 0.807497\n",
      "Epoch: 8503 \tTraining Loss: 0.171877 \tR2: 0.807497\n",
      "Epoch: 8504 \tTraining Loss: 0.190835 \tR2: 0.807497\n",
      "Epoch: 8505 \tTraining Loss: 0.193613 \tR2: 0.807497\n",
      "Epoch: 8506 \tTraining Loss: 0.181190 \tR2: 0.807497\n",
      "Epoch: 8507 \tTraining Loss: 0.178600 \tR2: 0.807497\n",
      "Epoch: 8508 \tTraining Loss: 0.200799 \tR2: 0.807497\n",
      "Epoch: 8509 \tTraining Loss: 0.193806 \tR2: 0.807497\n",
      "Epoch: 8510 \tTraining Loss: 0.193071 \tR2: 0.807497\n",
      "Epoch: 8511 \tTraining Loss: 0.183127 \tR2: 0.807497\n",
      "Epoch: 8512 \tTraining Loss: 0.204814 \tR2: 0.807497\n",
      "Epoch: 8513 \tTraining Loss: 0.192366 \tR2: 0.807497\n",
      "Epoch: 8514 \tTraining Loss: 0.195956 \tR2: 0.807497\n",
      "Epoch: 8515 \tTraining Loss: 0.164143 \tR2: 0.807497\n",
      "Epoch: 8516 \tTraining Loss: 0.183836 \tR2: 0.807497\n",
      "Epoch: 8517 \tTraining Loss: 0.179196 \tR2: 0.807497\n",
      "Epoch: 8518 \tTraining Loss: 0.182973 \tR2: 0.807497\n",
      "Epoch: 8519 \tTraining Loss: 0.175813 \tR2: 0.807497\n",
      "Epoch: 8520 \tTraining Loss: 0.185788 \tR2: 0.807497\n",
      "Epoch: 8521 \tTraining Loss: 0.168115 \tR2: 0.807497\n",
      "Epoch: 8522 \tTraining Loss: 0.171339 \tR2: 0.807497\n",
      "Epoch: 8523 \tTraining Loss: 0.173048 \tR2: 0.807497\n",
      "Epoch: 8524 \tTraining Loss: 0.198685 \tR2: 0.807497\n",
      "Epoch: 8525 \tTraining Loss: 0.178447 \tR2: 0.807497\n",
      "Epoch: 8526 \tTraining Loss: 0.186980 \tR2: 0.807497\n",
      "Epoch: 8527 \tTraining Loss: 0.175714 \tR2: 0.807497\n",
      "Epoch: 8528 \tTraining Loss: 0.175976 \tR2: 0.807497\n",
      "Epoch: 8529 \tTraining Loss: 0.187600 \tR2: 0.807497\n",
      "Epoch: 8530 \tTraining Loss: 0.169894 \tR2: 0.807497\n",
      "Epoch: 8531 \tTraining Loss: 0.186466 \tR2: 0.807497\n",
      "Epoch: 8532 \tTraining Loss: 0.177925 \tR2: 0.807497\n",
      "Epoch: 8533 \tTraining Loss: 0.182766 \tR2: 0.807497\n",
      "Epoch: 8534 \tTraining Loss: 0.197489 \tR2: 0.807497\n",
      "Epoch: 8535 \tTraining Loss: 0.183389 \tR2: 0.807497\n",
      "Epoch: 8536 \tTraining Loss: 0.187403 \tR2: 0.807497\n",
      "Epoch: 8537 \tTraining Loss: 0.172245 \tR2: 0.807497\n",
      "Epoch: 8538 \tTraining Loss: 0.187489 \tR2: 0.807497\n",
      "Epoch: 8539 \tTraining Loss: 0.164162 \tR2: 0.807497\n",
      "Epoch: 8540 \tTraining Loss: 0.179930 \tR2: 0.807497\n",
      "Epoch: 8541 \tTraining Loss: 0.196487 \tR2: 0.807497\n",
      "Epoch: 8542 \tTraining Loss: 0.191701 \tR2: 0.807497\n",
      "Epoch: 8543 \tTraining Loss: 0.191124 \tR2: 0.807497\n",
      "Epoch: 8544 \tTraining Loss: 0.188654 \tR2: 0.807497\n",
      "Epoch: 8545 \tTraining Loss: 0.192676 \tR2: 0.807497\n",
      "Epoch: 8546 \tTraining Loss: 0.165696 \tR2: 0.807497\n",
      "Epoch: 8547 \tTraining Loss: 0.188491 \tR2: 0.807497\n",
      "Epoch: 8548 \tTraining Loss: 0.174716 \tR2: 0.807497\n",
      "Epoch: 8549 \tTraining Loss: 0.201342 \tR2: 0.807497\n",
      "Epoch: 8550 \tTraining Loss: 0.173540 \tR2: 0.807497\n",
      "Epoch: 8551 \tTraining Loss: 0.175188 \tR2: 0.807497\n",
      "Epoch: 8552 \tTraining Loss: 0.191919 \tR2: 0.807497\n",
      "Epoch: 8553 \tTraining Loss: 0.185902 \tR2: 0.807497\n",
      "Epoch: 8554 \tTraining Loss: 0.188748 \tR2: 0.807497\n",
      "Epoch: 8555 \tTraining Loss: 0.222979 \tR2: 0.807497\n",
      "Epoch: 8556 \tTraining Loss: 0.184597 \tR2: 0.807497\n",
      "Epoch: 8557 \tTraining Loss: 0.179681 \tR2: 0.807497\n",
      "Epoch: 8558 \tTraining Loss: 0.164457 \tR2: 0.807497\n",
      "Epoch: 8559 \tTraining Loss: 0.177211 \tR2: 0.807497\n",
      "Epoch: 8560 \tTraining Loss: 0.175572 \tR2: 0.807497\n",
      "Epoch: 8561 \tTraining Loss: 0.179084 \tR2: 0.807497\n",
      "Epoch: 8562 \tTraining Loss: 0.180253 \tR2: 0.807497\n",
      "Epoch: 8563 \tTraining Loss: 0.165418 \tR2: 0.807497\n",
      "Epoch: 8564 \tTraining Loss: 0.177961 \tR2: 0.807497\n",
      "Epoch: 8565 \tTraining Loss: 0.187586 \tR2: 0.807497\n",
      "Epoch: 8566 \tTraining Loss: 0.170475 \tR2: 0.807497\n",
      "Epoch: 8567 \tTraining Loss: 0.177016 \tR2: 0.807497\n",
      "Epoch: 8568 \tTraining Loss: 0.184011 \tR2: 0.807497\n",
      "Epoch: 8569 \tTraining Loss: 0.174580 \tR2: 0.807497\n",
      "Epoch: 8570 \tTraining Loss: 0.187901 \tR2: 0.807497\n",
      "Epoch: 8571 \tTraining Loss: 0.166911 \tR2: 0.807497\n",
      "Epoch: 8572 \tTraining Loss: 0.179229 \tR2: 0.807497\n",
      "Epoch: 8573 \tTraining Loss: 0.172809 \tR2: 0.807497\n",
      "Epoch: 8574 \tTraining Loss: 0.169016 \tR2: 0.807497\n",
      "Epoch: 8575 \tTraining Loss: 0.169088 \tR2: 0.807497\n",
      "Epoch: 8576 \tTraining Loss: 0.171191 \tR2: 0.807497\n",
      "Epoch: 8577 \tTraining Loss: 0.163319 \tR2: 0.807497\n",
      "Epoch: 8578 \tTraining Loss: 0.181824 \tR2: 0.807497\n",
      "Epoch: 8579 \tTraining Loss: 0.164531 \tR2: 0.807497\n",
      "Epoch: 8580 \tTraining Loss: 0.190889 \tR2: 0.807497\n",
      "Epoch: 8581 \tTraining Loss: 0.186657 \tR2: 0.807497\n",
      "Epoch: 8582 \tTraining Loss: 0.175252 \tR2: 0.807497\n",
      "Epoch: 8583 \tTraining Loss: 0.188310 \tR2: 0.807497\n",
      "Epoch: 8584 \tTraining Loss: 0.183047 \tR2: 0.807497\n",
      "Epoch: 8585 \tTraining Loss: 0.183886 \tR2: 0.807497\n",
      "Epoch: 8586 \tTraining Loss: 0.202316 \tR2: 0.807497\n",
      "Epoch: 8587 \tTraining Loss: 0.178810 \tR2: 0.807497\n",
      "Epoch: 8588 \tTraining Loss: 0.167369 \tR2: 0.807497\n",
      "Epoch: 8589 \tTraining Loss: 0.181064 \tR2: 0.807497\n",
      "Epoch: 8590 \tTraining Loss: 0.188641 \tR2: 0.807497\n",
      "Epoch: 8591 \tTraining Loss: 0.189392 \tR2: 0.807497\n",
      "Epoch: 8592 \tTraining Loss: 0.171165 \tR2: 0.807497\n",
      "Epoch: 8593 \tTraining Loss: 0.187565 \tR2: 0.807497\n",
      "Epoch: 8594 \tTraining Loss: 0.171971 \tR2: 0.807497\n",
      "Epoch: 8595 \tTraining Loss: 0.181502 \tR2: 0.807497\n",
      "Epoch: 8596 \tTraining Loss: 0.178551 \tR2: 0.807497\n",
      "Epoch: 8597 \tTraining Loss: 0.168401 \tR2: 0.807497\n",
      "Epoch: 8598 \tTraining Loss: 0.179220 \tR2: 0.807497\n",
      "Epoch: 8599 \tTraining Loss: 0.202244 \tR2: 0.807497\n",
      "Epoch: 8600 \tTraining Loss: 0.196805 \tR2: 0.768277\n",
      "Epoch: 8601 \tTraining Loss: 0.188038 \tR2: 0.768277\n",
      "Epoch: 8602 \tTraining Loss: 0.175185 \tR2: 0.768277\n",
      "Epoch: 8603 \tTraining Loss: 0.183154 \tR2: 0.768277\n",
      "Epoch: 8604 \tTraining Loss: 0.190483 \tR2: 0.768277\n",
      "Epoch: 8605 \tTraining Loss: 0.199588 \tR2: 0.768277\n",
      "Epoch: 8606 \tTraining Loss: 0.193201 \tR2: 0.768277\n",
      "Epoch: 8607 \tTraining Loss: 0.175295 \tR2: 0.768277\n",
      "Epoch: 8608 \tTraining Loss: 0.181934 \tR2: 0.768277\n",
      "Epoch: 8609 \tTraining Loss: 0.173718 \tR2: 0.768277\n",
      "Epoch: 8610 \tTraining Loss: 0.188990 \tR2: 0.768277\n",
      "Epoch: 8611 \tTraining Loss: 0.167240 \tR2: 0.768277\n",
      "Epoch: 8612 \tTraining Loss: 0.191773 \tR2: 0.768277\n",
      "Epoch: 8613 \tTraining Loss: 0.184464 \tR2: 0.768277\n",
      "Epoch: 8614 \tTraining Loss: 0.186365 \tR2: 0.768277\n",
      "Epoch: 8615 \tTraining Loss: 0.191090 \tR2: 0.768277\n",
      "Epoch: 8616 \tTraining Loss: 0.179517 \tR2: 0.768277\n",
      "Epoch: 8617 \tTraining Loss: 0.190799 \tR2: 0.768277\n",
      "Epoch: 8618 \tTraining Loss: 0.192241 \tR2: 0.768277\n",
      "Epoch: 8619 \tTraining Loss: 0.174180 \tR2: 0.768277\n",
      "Epoch: 8620 \tTraining Loss: 0.169705 \tR2: 0.768277\n",
      "Epoch: 8621 \tTraining Loss: 0.184457 \tR2: 0.768277\n",
      "Epoch: 8622 \tTraining Loss: 0.182554 \tR2: 0.768277\n",
      "Epoch: 8623 \tTraining Loss: 0.176309 \tR2: 0.768277\n",
      "Epoch: 8624 \tTraining Loss: 0.184428 \tR2: 0.768277\n",
      "Epoch: 8625 \tTraining Loss: 0.183084 \tR2: 0.768277\n",
      "Epoch: 8626 \tTraining Loss: 0.178488 \tR2: 0.768277\n",
      "Epoch: 8627 \tTraining Loss: 0.174744 \tR2: 0.768277\n",
      "Epoch: 8628 \tTraining Loss: 0.187057 \tR2: 0.768277\n",
      "Epoch: 8629 \tTraining Loss: 0.163240 \tR2: 0.768277\n",
      "Epoch: 8630 \tTraining Loss: 0.189711 \tR2: 0.768277\n",
      "Epoch: 8631 \tTraining Loss: 0.182935 \tR2: 0.768277\n",
      "Epoch: 8632 \tTraining Loss: 0.191247 \tR2: 0.768277\n",
      "Epoch: 8633 \tTraining Loss: 0.160770 \tR2: 0.768277\n",
      "Epoch: 8634 \tTraining Loss: 0.177959 \tR2: 0.768277\n",
      "Epoch: 8635 \tTraining Loss: 0.172624 \tR2: 0.768277\n",
      "Epoch: 8636 \tTraining Loss: 0.177988 \tR2: 0.768277\n",
      "Epoch: 8637 \tTraining Loss: 0.177926 \tR2: 0.768277\n",
      "Epoch: 8638 \tTraining Loss: 0.166812 \tR2: 0.768277\n",
      "Epoch: 8639 \tTraining Loss: 0.181018 \tR2: 0.768277\n",
      "Epoch: 8640 \tTraining Loss: 0.188425 \tR2: 0.768277\n",
      "Epoch: 8641 \tTraining Loss: 0.167055 \tR2: 0.768277\n",
      "Epoch: 8642 \tTraining Loss: 0.177823 \tR2: 0.768277\n",
      "Epoch: 8643 \tTraining Loss: 0.193532 \tR2: 0.768277\n",
      "Epoch: 8644 \tTraining Loss: 0.180730 \tR2: 0.768277\n",
      "Epoch: 8645 \tTraining Loss: 0.176477 \tR2: 0.768277\n",
      "Epoch: 8646 \tTraining Loss: 0.193575 \tR2: 0.768277\n",
      "Epoch: 8647 \tTraining Loss: 0.193300 \tR2: 0.768277\n",
      "Epoch: 8648 \tTraining Loss: 0.166149 \tR2: 0.768277\n",
      "Epoch: 8649 \tTraining Loss: 0.189169 \tR2: 0.768277\n",
      "Epoch: 8650 \tTraining Loss: 0.172475 \tR2: 0.768277\n",
      "Epoch: 8651 \tTraining Loss: 0.172513 \tR2: 0.768277\n",
      "Epoch: 8652 \tTraining Loss: 0.166564 \tR2: 0.768277\n",
      "Epoch: 8653 \tTraining Loss: 0.175645 \tR2: 0.768277\n",
      "Epoch: 8654 \tTraining Loss: 0.170272 \tR2: 0.768277\n",
      "Epoch: 8655 \tTraining Loss: 0.192318 \tR2: 0.768277\n",
      "Epoch: 8656 \tTraining Loss: 0.214474 \tR2: 0.768277\n",
      "Epoch: 8657 \tTraining Loss: 0.162318 \tR2: 0.768277\n",
      "Epoch: 8658 \tTraining Loss: 0.174580 \tR2: 0.768277\n",
      "Epoch: 8659 \tTraining Loss: 0.175942 \tR2: 0.768277\n",
      "Epoch: 8660 \tTraining Loss: 0.187654 \tR2: 0.768277\n",
      "Epoch: 8661 \tTraining Loss: 0.183294 \tR2: 0.768277\n",
      "Epoch: 8662 \tTraining Loss: 0.174044 \tR2: 0.768277\n",
      "Epoch: 8663 \tTraining Loss: 0.187219 \tR2: 0.768277\n",
      "Epoch: 8664 \tTraining Loss: 0.180885 \tR2: 0.768277\n",
      "Epoch: 8665 \tTraining Loss: 0.177802 \tR2: 0.768277\n",
      "Epoch: 8666 \tTraining Loss: 0.182599 \tR2: 0.768277\n",
      "Epoch: 8667 \tTraining Loss: 0.165935 \tR2: 0.768277\n",
      "Epoch: 8668 \tTraining Loss: 0.172815 \tR2: 0.768277\n",
      "Epoch: 8669 \tTraining Loss: 0.173224 \tR2: 0.768277\n",
      "Epoch: 8670 \tTraining Loss: 0.178997 \tR2: 0.768277\n",
      "Epoch: 8671 \tTraining Loss: 0.186433 \tR2: 0.768277\n",
      "Epoch: 8672 \tTraining Loss: 0.168728 \tR2: 0.768277\n",
      "Epoch: 8673 \tTraining Loss: 0.163338 \tR2: 0.768277\n",
      "Epoch: 8674 \tTraining Loss: 0.184693 \tR2: 0.768277\n",
      "Epoch: 8675 \tTraining Loss: 0.180402 \tR2: 0.768277\n",
      "Epoch: 8676 \tTraining Loss: 0.173650 \tR2: 0.768277\n",
      "Epoch: 8677 \tTraining Loss: 0.174195 \tR2: 0.768277\n",
      "Epoch: 8678 \tTraining Loss: 0.180475 \tR2: 0.768277\n",
      "Epoch: 8679 \tTraining Loss: 0.188039 \tR2: 0.768277\n",
      "Epoch: 8680 \tTraining Loss: 0.182543 \tR2: 0.768277\n",
      "Epoch: 8681 \tTraining Loss: 0.173341 \tR2: 0.768277\n",
      "Epoch: 8682 \tTraining Loss: 0.180182 \tR2: 0.768277\n",
      "Epoch: 8683 \tTraining Loss: 0.196322 \tR2: 0.768277\n",
      "Epoch: 8684 \tTraining Loss: 0.184414 \tR2: 0.768277\n",
      "Epoch: 8685 \tTraining Loss: 0.184558 \tR2: 0.768277\n",
      "Epoch: 8686 \tTraining Loss: 0.184722 \tR2: 0.768277\n",
      "Epoch: 8687 \tTraining Loss: 0.198231 \tR2: 0.768277\n",
      "Epoch: 8688 \tTraining Loss: 0.203085 \tR2: 0.768277\n",
      "Epoch: 8689 \tTraining Loss: 0.172241 \tR2: 0.768277\n",
      "Epoch: 8690 \tTraining Loss: 0.188626 \tR2: 0.768277\n",
      "Epoch: 8691 \tTraining Loss: 0.183558 \tR2: 0.768277\n",
      "Epoch: 8692 \tTraining Loss: 0.192704 \tR2: 0.768277\n",
      "Epoch: 8693 \tTraining Loss: 0.174945 \tR2: 0.768277\n",
      "Epoch: 8694 \tTraining Loss: 0.184757 \tR2: 0.768277\n",
      "Epoch: 8695 \tTraining Loss: 0.201803 \tR2: 0.768277\n",
      "Epoch: 8696 \tTraining Loss: 0.189442 \tR2: 0.768277\n",
      "Epoch: 8697 \tTraining Loss: 0.188441 \tR2: 0.768277\n",
      "Epoch: 8698 \tTraining Loss: 0.181017 \tR2: 0.768277\n",
      "Epoch: 8699 \tTraining Loss: 0.207524 \tR2: 0.768277\n",
      "Epoch: 8700 \tTraining Loss: 0.177970 \tR2: 0.839531\n",
      "Epoch: 8701 \tTraining Loss: 0.173168 \tR2: 0.839531\n",
      "Epoch: 8702 \tTraining Loss: 0.179550 \tR2: 0.839531\n",
      "Epoch: 8703 \tTraining Loss: 0.179564 \tR2: 0.839531\n",
      "Epoch: 8704 \tTraining Loss: 0.179016 \tR2: 0.839531\n",
      "Epoch: 8705 \tTraining Loss: 0.184419 \tR2: 0.839531\n",
      "Epoch: 8706 \tTraining Loss: 0.179447 \tR2: 0.839531\n",
      "Epoch: 8707 \tTraining Loss: 0.165983 \tR2: 0.839531\n",
      "Epoch: 8708 \tTraining Loss: 0.184554 \tR2: 0.839531\n",
      "Epoch: 8709 \tTraining Loss: 0.162258 \tR2: 0.839531\n",
      "Epoch: 8710 \tTraining Loss: 0.185502 \tR2: 0.839531\n",
      "Epoch: 8711 \tTraining Loss: 0.180846 \tR2: 0.839531\n",
      "Epoch: 8712 \tTraining Loss: 0.173096 \tR2: 0.839531\n",
      "Epoch: 8713 \tTraining Loss: 0.175750 \tR2: 0.839531\n",
      "Epoch: 8714 \tTraining Loss: 0.163254 \tR2: 0.839531\n",
      "Epoch: 8715 \tTraining Loss: 0.196966 \tR2: 0.839531\n",
      "Epoch: 8716 \tTraining Loss: 0.187422 \tR2: 0.839531\n",
      "Epoch: 8717 \tTraining Loss: 0.185869 \tR2: 0.839531\n",
      "Epoch: 8718 \tTraining Loss: 0.173768 \tR2: 0.839531\n",
      "Epoch: 8719 \tTraining Loss: 0.186358 \tR2: 0.839531\n",
      "Epoch: 8720 \tTraining Loss: 0.174175 \tR2: 0.839531\n",
      "Epoch: 8721 \tTraining Loss: 0.181775 \tR2: 0.839531\n",
      "Epoch: 8722 \tTraining Loss: 0.269940 \tR2: 0.839531\n",
      "Epoch: 8723 \tTraining Loss: 0.204599 \tR2: 0.839531\n",
      "Epoch: 8724 \tTraining Loss: 0.205073 \tR2: 0.839531\n",
      "Epoch: 8725 \tTraining Loss: 0.200424 \tR2: 0.839531\n",
      "Epoch: 8726 \tTraining Loss: 0.178771 \tR2: 0.839531\n",
      "Epoch: 8727 \tTraining Loss: 0.180797 \tR2: 0.839531\n",
      "Epoch: 8728 \tTraining Loss: 0.185283 \tR2: 0.839531\n",
      "Epoch: 8729 \tTraining Loss: 0.183499 \tR2: 0.839531\n",
      "Epoch: 8730 \tTraining Loss: 0.192725 \tR2: 0.839531\n",
      "Epoch: 8731 \tTraining Loss: 0.159487 \tR2: 0.839531\n",
      "Epoch: 8732 \tTraining Loss: 0.202015 \tR2: 0.839531\n",
      "Epoch: 8733 \tTraining Loss: 0.175807 \tR2: 0.839531\n",
      "Epoch: 8734 \tTraining Loss: 0.186265 \tR2: 0.839531\n",
      "Epoch: 8735 \tTraining Loss: 0.191494 \tR2: 0.839531\n",
      "Epoch: 8736 \tTraining Loss: 0.175677 \tR2: 0.839531\n",
      "Epoch: 8737 \tTraining Loss: 0.171686 \tR2: 0.839531\n",
      "Epoch: 8738 \tTraining Loss: 0.178846 \tR2: 0.839531\n",
      "Epoch: 8739 \tTraining Loss: 0.195040 \tR2: 0.839531\n",
      "Epoch: 8740 \tTraining Loss: 0.188059 \tR2: 0.839531\n",
      "Epoch: 8741 \tTraining Loss: 0.172230 \tR2: 0.839531\n",
      "Epoch: 8742 \tTraining Loss: 0.176626 \tR2: 0.839531\n",
      "Epoch: 8743 \tTraining Loss: 0.183632 \tR2: 0.839531\n",
      "Epoch: 8744 \tTraining Loss: 0.166818 \tR2: 0.839531\n",
      "Epoch: 8745 \tTraining Loss: 0.186897 \tR2: 0.839531\n",
      "Epoch: 8746 \tTraining Loss: 0.175366 \tR2: 0.839531\n",
      "Epoch: 8747 \tTraining Loss: 0.177898 \tR2: 0.839531\n",
      "Epoch: 8748 \tTraining Loss: 0.161839 \tR2: 0.839531\n",
      "Epoch: 8749 \tTraining Loss: 0.175911 \tR2: 0.839531\n",
      "Epoch: 8750 \tTraining Loss: 0.156162 \tR2: 0.839531\n",
      "Epoch: 8751 \tTraining Loss: 0.191673 \tR2: 0.839531\n",
      "Epoch: 8752 \tTraining Loss: 0.179812 \tR2: 0.839531\n",
      "Epoch: 8753 \tTraining Loss: 0.186372 \tR2: 0.839531\n",
      "Epoch: 8754 \tTraining Loss: 0.182406 \tR2: 0.839531\n",
      "Epoch: 8755 \tTraining Loss: 0.174583 \tR2: 0.839531\n",
      "Epoch: 8756 \tTraining Loss: 0.177715 \tR2: 0.839531\n",
      "Epoch: 8757 \tTraining Loss: 0.190438 \tR2: 0.839531\n",
      "Epoch: 8758 \tTraining Loss: 0.180708 \tR2: 0.839531\n",
      "Epoch: 8759 \tTraining Loss: 0.190967 \tR2: 0.839531\n",
      "Epoch: 8760 \tTraining Loss: 0.180609 \tR2: 0.839531\n",
      "Epoch: 8761 \tTraining Loss: 0.171131 \tR2: 0.839531\n",
      "Epoch: 8762 \tTraining Loss: 0.168858 \tR2: 0.839531\n",
      "Epoch: 8763 \tTraining Loss: 0.220015 \tR2: 0.839531\n",
      "Epoch: 8764 \tTraining Loss: 0.164611 \tR2: 0.839531\n",
      "Epoch: 8765 \tTraining Loss: 0.182176 \tR2: 0.839531\n",
      "Epoch: 8766 \tTraining Loss: 0.177270 \tR2: 0.839531\n",
      "Epoch: 8767 \tTraining Loss: 0.161994 \tR2: 0.839531\n",
      "Epoch: 8768 \tTraining Loss: 0.180094 \tR2: 0.839531\n",
      "Epoch: 8769 \tTraining Loss: 0.218646 \tR2: 0.839531\n",
      "Epoch: 8770 \tTraining Loss: 0.185531 \tR2: 0.839531\n",
      "Epoch: 8771 \tTraining Loss: 0.183498 \tR2: 0.839531\n",
      "Epoch: 8772 \tTraining Loss: 0.173904 \tR2: 0.839531\n",
      "Epoch: 8773 \tTraining Loss: 0.181867 \tR2: 0.839531\n",
      "Epoch: 8774 \tTraining Loss: 0.179160 \tR2: 0.839531\n",
      "Epoch: 8775 \tTraining Loss: 0.164584 \tR2: 0.839531\n",
      "Epoch: 8776 \tTraining Loss: 0.193843 \tR2: 0.839531\n",
      "Epoch: 8777 \tTraining Loss: 0.179509 \tR2: 0.839531\n",
      "Epoch: 8778 \tTraining Loss: 0.182196 \tR2: 0.839531\n",
      "Epoch: 8779 \tTraining Loss: 0.190852 \tR2: 0.839531\n",
      "Epoch: 8780 \tTraining Loss: 0.174972 \tR2: 0.839531\n",
      "Epoch: 8781 \tTraining Loss: 0.192598 \tR2: 0.839531\n",
      "Epoch: 8782 \tTraining Loss: 0.170710 \tR2: 0.839531\n",
      "Epoch: 8783 \tTraining Loss: 0.172553 \tR2: 0.839531\n",
      "Epoch: 8784 \tTraining Loss: 0.210695 \tR2: 0.839531\n",
      "Epoch: 8785 \tTraining Loss: 0.170184 \tR2: 0.839531\n",
      "Epoch: 8786 \tTraining Loss: 0.175568 \tR2: 0.839531\n",
      "Epoch: 8787 \tTraining Loss: 0.186407 \tR2: 0.839531\n",
      "Epoch: 8788 \tTraining Loss: 0.180740 \tR2: 0.839531\n",
      "Epoch: 8789 \tTraining Loss: 0.186299 \tR2: 0.839531\n",
      "Epoch: 8790 \tTraining Loss: 0.178639 \tR2: 0.839531\n",
      "Epoch: 8791 \tTraining Loss: 0.179662 \tR2: 0.839531\n",
      "Epoch: 8792 \tTraining Loss: 0.175749 \tR2: 0.839531\n",
      "Epoch: 8793 \tTraining Loss: 0.176283 \tR2: 0.839531\n",
      "Epoch: 8794 \tTraining Loss: 0.180810 \tR2: 0.839531\n",
      "Epoch: 8795 \tTraining Loss: 0.188019 \tR2: 0.839531\n",
      "Epoch: 8796 \tTraining Loss: 0.190696 \tR2: 0.839531\n",
      "Epoch: 8797 \tTraining Loss: 0.169237 \tR2: 0.839531\n",
      "Epoch: 8798 \tTraining Loss: 0.171365 \tR2: 0.839531\n",
      "Epoch: 8799 \tTraining Loss: 0.165562 \tR2: 0.839531\n",
      "Epoch: 8800 \tTraining Loss: 0.175038 \tR2: 0.851317\n",
      "Epoch: 8801 \tTraining Loss: 0.166412 \tR2: 0.851317\n",
      "Epoch: 8802 \tTraining Loss: 0.191088 \tR2: 0.851317\n",
      "Epoch: 8803 \tTraining Loss: 0.180145 \tR2: 0.851317\n",
      "Epoch: 8804 \tTraining Loss: 0.165221 \tR2: 0.851317\n",
      "Epoch: 8805 \tTraining Loss: 0.188231 \tR2: 0.851317\n",
      "Epoch: 8806 \tTraining Loss: 0.203542 \tR2: 0.851317\n",
      "Epoch: 8807 \tTraining Loss: 0.179916 \tR2: 0.851317\n",
      "Epoch: 8808 \tTraining Loss: 0.174571 \tR2: 0.851317\n",
      "Epoch: 8809 \tTraining Loss: 0.169700 \tR2: 0.851317\n",
      "Epoch: 8810 \tTraining Loss: 0.167166 \tR2: 0.851317\n",
      "Epoch: 8811 \tTraining Loss: 0.176959 \tR2: 0.851317\n",
      "Epoch: 8812 \tTraining Loss: 0.178125 \tR2: 0.851317\n",
      "Epoch: 8813 \tTraining Loss: 0.174511 \tR2: 0.851317\n",
      "Epoch: 8814 \tTraining Loss: 0.184262 \tR2: 0.851317\n",
      "Epoch: 8815 \tTraining Loss: 0.180968 \tR2: 0.851317\n",
      "Epoch: 8816 \tTraining Loss: 0.194777 \tR2: 0.851317\n",
      "Epoch: 8817 \tTraining Loss: 0.175783 \tR2: 0.851317\n",
      "Epoch: 8818 \tTraining Loss: 0.169294 \tR2: 0.851317\n",
      "Epoch: 8819 \tTraining Loss: 0.180934 \tR2: 0.851317\n",
      "Epoch: 8820 \tTraining Loss: 0.186522 \tR2: 0.851317\n",
      "Epoch: 8821 \tTraining Loss: 0.160953 \tR2: 0.851317\n",
      "Epoch: 8822 \tTraining Loss: 0.166638 \tR2: 0.851317\n",
      "Epoch: 8823 \tTraining Loss: 0.182829 \tR2: 0.851317\n",
      "Epoch: 8824 \tTraining Loss: 0.184634 \tR2: 0.851317\n",
      "Epoch: 8825 \tTraining Loss: 0.185293 \tR2: 0.851317\n",
      "Epoch: 8826 \tTraining Loss: 0.165218 \tR2: 0.851317\n",
      "Epoch: 8827 \tTraining Loss: 0.180188 \tR2: 0.851317\n",
      "Epoch: 8828 \tTraining Loss: 0.167638 \tR2: 0.851317\n",
      "Epoch: 8829 \tTraining Loss: 0.191526 \tR2: 0.851317\n",
      "Epoch: 8830 \tTraining Loss: 0.192750 \tR2: 0.851317\n",
      "Epoch: 8831 \tTraining Loss: 0.437513 \tR2: 0.851317\n",
      "Epoch: 8832 \tTraining Loss: 0.267434 \tR2: 0.851317\n",
      "Epoch: 8833 \tTraining Loss: 0.195256 \tR2: 0.851317\n",
      "Epoch: 8834 \tTraining Loss: 0.190276 \tR2: 0.851317\n",
      "Epoch: 8835 \tTraining Loss: 0.193406 \tR2: 0.851317\n",
      "Epoch: 8836 \tTraining Loss: 0.194306 \tR2: 0.851317\n",
      "Epoch: 8837 \tTraining Loss: 0.165788 \tR2: 0.851317\n",
      "Epoch: 8838 \tTraining Loss: 0.190434 \tR2: 0.851317\n",
      "Epoch: 8839 \tTraining Loss: 0.193184 \tR2: 0.851317\n",
      "Epoch: 8840 \tTraining Loss: 0.198346 \tR2: 0.851317\n",
      "Epoch: 8841 \tTraining Loss: 0.172014 \tR2: 0.851317\n",
      "Epoch: 8842 \tTraining Loss: 0.167727 \tR2: 0.851317\n",
      "Epoch: 8843 \tTraining Loss: 0.163300 \tR2: 0.851317\n",
      "Epoch: 8844 \tTraining Loss: 0.169893 \tR2: 0.851317\n",
      "Epoch: 8845 \tTraining Loss: 0.193753 \tR2: 0.851317\n",
      "Epoch: 8846 \tTraining Loss: 0.173106 \tR2: 0.851317\n",
      "Epoch: 8847 \tTraining Loss: 0.183419 \tR2: 0.851317\n",
      "Epoch: 8848 \tTraining Loss: 0.197631 \tR2: 0.851317\n",
      "Epoch: 8849 \tTraining Loss: 0.176152 \tR2: 0.851317\n",
      "Epoch: 8850 \tTraining Loss: 0.187309 \tR2: 0.851317\n",
      "Epoch: 8851 \tTraining Loss: 0.163343 \tR2: 0.851317\n",
      "Epoch: 8852 \tTraining Loss: 0.163517 \tR2: 0.851317\n",
      "Epoch: 8853 \tTraining Loss: 0.182840 \tR2: 0.851317\n",
      "Epoch: 8854 \tTraining Loss: 0.176242 \tR2: 0.851317\n",
      "Epoch: 8855 \tTraining Loss: 0.193543 \tR2: 0.851317\n",
      "Epoch: 8856 \tTraining Loss: 0.205908 \tR2: 0.851317\n",
      "Epoch: 8857 \tTraining Loss: 0.205623 \tR2: 0.851317\n",
      "Epoch: 8858 \tTraining Loss: 0.174598 \tR2: 0.851317\n",
      "Epoch: 8859 \tTraining Loss: 0.192485 \tR2: 0.851317\n",
      "Epoch: 8860 \tTraining Loss: 0.176737 \tR2: 0.851317\n",
      "Epoch: 8861 \tTraining Loss: 0.183922 \tR2: 0.851317\n",
      "Epoch: 8862 \tTraining Loss: 0.179152 \tR2: 0.851317\n",
      "Epoch: 8863 \tTraining Loss: 0.177920 \tR2: 0.851317\n",
      "Epoch: 8864 \tTraining Loss: 0.186712 \tR2: 0.851317\n",
      "Epoch: 8865 \tTraining Loss: 0.188184 \tR2: 0.851317\n",
      "Epoch: 8866 \tTraining Loss: 0.180023 \tR2: 0.851317\n",
      "Epoch: 8867 \tTraining Loss: 0.177056 \tR2: 0.851317\n",
      "Epoch: 8868 \tTraining Loss: 0.193779 \tR2: 0.851317\n",
      "Epoch: 8869 \tTraining Loss: 0.184668 \tR2: 0.851317\n",
      "Epoch: 8870 \tTraining Loss: 0.185737 \tR2: 0.851317\n",
      "Epoch: 8871 \tTraining Loss: 0.180954 \tR2: 0.851317\n",
      "Epoch: 8872 \tTraining Loss: 0.167829 \tR2: 0.851317\n",
      "Epoch: 8873 \tTraining Loss: 0.189164 \tR2: 0.851317\n",
      "Epoch: 8874 \tTraining Loss: 0.217379 \tR2: 0.851317\n",
      "Epoch: 8875 \tTraining Loss: 0.193056 \tR2: 0.851317\n",
      "Epoch: 8876 \tTraining Loss: 0.197684 \tR2: 0.851317\n",
      "Epoch: 8877 \tTraining Loss: 0.199528 \tR2: 0.851317\n",
      "Epoch: 8878 \tTraining Loss: 0.183206 \tR2: 0.851317\n",
      "Epoch: 8879 \tTraining Loss: 0.177228 \tR2: 0.851317\n",
      "Epoch: 8880 \tTraining Loss: 0.161415 \tR2: 0.851317\n",
      "Epoch: 8881 \tTraining Loss: 0.185731 \tR2: 0.851317\n",
      "Epoch: 8882 \tTraining Loss: 0.179208 \tR2: 0.851317\n",
      "Epoch: 8883 \tTraining Loss: 0.187198 \tR2: 0.851317\n",
      "Epoch: 8884 \tTraining Loss: 0.163592 \tR2: 0.851317\n",
      "Epoch: 8885 \tTraining Loss: 0.192863 \tR2: 0.851317\n",
      "Epoch: 8886 \tTraining Loss: 0.173102 \tR2: 0.851317\n",
      "Epoch: 8887 \tTraining Loss: 0.209129 \tR2: 0.851317\n",
      "Epoch: 8888 \tTraining Loss: 0.178346 \tR2: 0.851317\n",
      "Epoch: 8889 \tTraining Loss: 0.177732 \tR2: 0.851317\n",
      "Epoch: 8890 \tTraining Loss: 0.178981 \tR2: 0.851317\n",
      "Epoch: 8891 \tTraining Loss: 0.191295 \tR2: 0.851317\n",
      "Epoch: 8892 \tTraining Loss: 0.194981 \tR2: 0.851317\n",
      "Epoch: 8893 \tTraining Loss: 0.189252 \tR2: 0.851317\n",
      "Epoch: 8894 \tTraining Loss: 0.176164 \tR2: 0.851317\n",
      "Epoch: 8895 \tTraining Loss: 0.166112 \tR2: 0.851317\n",
      "Epoch: 8896 \tTraining Loss: 0.173619 \tR2: 0.851317\n",
      "Epoch: 8897 \tTraining Loss: 0.175928 \tR2: 0.851317\n",
      "Epoch: 8898 \tTraining Loss: 0.189942 \tR2: 0.851317\n",
      "Epoch: 8899 \tTraining Loss: 0.180580 \tR2: 0.851317\n",
      "Epoch: 8900 \tTraining Loss: 0.181201 \tR2: 0.842575\n",
      "Epoch: 8901 \tTraining Loss: 0.184113 \tR2: 0.842575\n",
      "Epoch: 8902 \tTraining Loss: 0.193775 \tR2: 0.842575\n",
      "Epoch: 8903 \tTraining Loss: 0.178783 \tR2: 0.842575\n",
      "Epoch: 8904 \tTraining Loss: 0.178121 \tR2: 0.842575\n",
      "Epoch: 8905 \tTraining Loss: 0.190857 \tR2: 0.842575\n",
      "Epoch: 8906 \tTraining Loss: 0.169567 \tR2: 0.842575\n",
      "Epoch: 8907 \tTraining Loss: 0.177573 \tR2: 0.842575\n",
      "Epoch: 8908 \tTraining Loss: 0.193566 \tR2: 0.842575\n",
      "Epoch: 8909 \tTraining Loss: 0.171317 \tR2: 0.842575\n",
      "Epoch: 8910 \tTraining Loss: 0.178868 \tR2: 0.842575\n",
      "Epoch: 8911 \tTraining Loss: 0.163440 \tR2: 0.842575\n",
      "Epoch: 8912 \tTraining Loss: 0.177736 \tR2: 0.842575\n",
      "Epoch: 8913 \tTraining Loss: 0.168980 \tR2: 0.842575\n",
      "Epoch: 8914 \tTraining Loss: 0.162877 \tR2: 0.842575\n",
      "Epoch: 8915 \tTraining Loss: 0.166594 \tR2: 0.842575\n",
      "Epoch: 8916 \tTraining Loss: 0.176068 \tR2: 0.842575\n",
      "Epoch: 8917 \tTraining Loss: 0.181759 \tR2: 0.842575\n",
      "Epoch: 8918 \tTraining Loss: 0.180637 \tR2: 0.842575\n",
      "Epoch: 8919 \tTraining Loss: 0.189488 \tR2: 0.842575\n",
      "Epoch: 8920 \tTraining Loss: 0.185792 \tR2: 0.842575\n",
      "Epoch: 8921 \tTraining Loss: 0.170663 \tR2: 0.842575\n",
      "Epoch: 8922 \tTraining Loss: 0.194180 \tR2: 0.842575\n",
      "Epoch: 8923 \tTraining Loss: 0.179998 \tR2: 0.842575\n",
      "Epoch: 8924 \tTraining Loss: 0.171227 \tR2: 0.842575\n",
      "Epoch: 8925 \tTraining Loss: 0.173584 \tR2: 0.842575\n",
      "Epoch: 8926 \tTraining Loss: 0.188345 \tR2: 0.842575\n",
      "Epoch: 8927 \tTraining Loss: 0.189082 \tR2: 0.842575\n",
      "Epoch: 8928 \tTraining Loss: 0.191230 \tR2: 0.842575\n",
      "Epoch: 8929 \tTraining Loss: 0.186094 \tR2: 0.842575\n",
      "Epoch: 8930 \tTraining Loss: 0.173613 \tR2: 0.842575\n",
      "Epoch: 8931 \tTraining Loss: 0.172586 \tR2: 0.842575\n",
      "Epoch: 8932 \tTraining Loss: 0.190606 \tR2: 0.842575\n",
      "Epoch: 8933 \tTraining Loss: 0.172555 \tR2: 0.842575\n",
      "Epoch: 8934 \tTraining Loss: 0.171993 \tR2: 0.842575\n",
      "Epoch: 8935 \tTraining Loss: 0.154607 \tR2: 0.842575\n",
      "Epoch: 8936 \tTraining Loss: 0.197681 \tR2: 0.842575\n",
      "Epoch: 8937 \tTraining Loss: 0.191512 \tR2: 0.842575\n",
      "Epoch: 8938 \tTraining Loss: 0.175835 \tR2: 0.842575\n",
      "Epoch: 8939 \tTraining Loss: 0.182162 \tR2: 0.842575\n",
      "Epoch: 8940 \tTraining Loss: 0.189436 \tR2: 0.842575\n",
      "Epoch: 8941 \tTraining Loss: 0.173671 \tR2: 0.842575\n",
      "Epoch: 8942 \tTraining Loss: 0.175978 \tR2: 0.842575\n",
      "Epoch: 8943 \tTraining Loss: 0.178533 \tR2: 0.842575\n",
      "Epoch: 8944 \tTraining Loss: 0.187844 \tR2: 0.842575\n",
      "Epoch: 8945 \tTraining Loss: 0.198638 \tR2: 0.842575\n",
      "Epoch: 8946 \tTraining Loss: 0.188163 \tR2: 0.842575\n",
      "Epoch: 8947 \tTraining Loss: 0.203220 \tR2: 0.842575\n",
      "Epoch: 8948 \tTraining Loss: 0.181087 \tR2: 0.842575\n",
      "Epoch: 8949 \tTraining Loss: 0.181653 \tR2: 0.842575\n",
      "Epoch: 8950 \tTraining Loss: 0.178312 \tR2: 0.842575\n",
      "Epoch: 8951 \tTraining Loss: 0.166213 \tR2: 0.842575\n",
      "Epoch: 8952 \tTraining Loss: 0.182013 \tR2: 0.842575\n",
      "Epoch: 8953 \tTraining Loss: 0.178642 \tR2: 0.842575\n",
      "Epoch: 8954 \tTraining Loss: 0.211543 \tR2: 0.842575\n",
      "Epoch: 8955 \tTraining Loss: 0.181594 \tR2: 0.842575\n",
      "Epoch: 8956 \tTraining Loss: 0.182856 \tR2: 0.842575\n",
      "Epoch: 8957 \tTraining Loss: 0.178267 \tR2: 0.842575\n",
      "Epoch: 8958 \tTraining Loss: 0.183877 \tR2: 0.842575\n",
      "Epoch: 8959 \tTraining Loss: 0.165306 \tR2: 0.842575\n",
      "Epoch: 8960 \tTraining Loss: 0.191642 \tR2: 0.842575\n",
      "Epoch: 8961 \tTraining Loss: 0.182758 \tR2: 0.842575\n",
      "Epoch: 8962 \tTraining Loss: 0.172102 \tR2: 0.842575\n",
      "Epoch: 8963 \tTraining Loss: 0.182188 \tR2: 0.842575\n",
      "Epoch: 8964 \tTraining Loss: 0.182346 \tR2: 0.842575\n",
      "Epoch: 8965 \tTraining Loss: 0.163626 \tR2: 0.842575\n",
      "Epoch: 8966 \tTraining Loss: 0.175618 \tR2: 0.842575\n",
      "Epoch: 8967 \tTraining Loss: 0.182257 \tR2: 0.842575\n",
      "Epoch: 8968 \tTraining Loss: 0.192947 \tR2: 0.842575\n",
      "Epoch: 8969 \tTraining Loss: 0.176952 \tR2: 0.842575\n",
      "Epoch: 8970 \tTraining Loss: 0.197066 \tR2: 0.842575\n",
      "Epoch: 8971 \tTraining Loss: 0.170164 \tR2: 0.842575\n",
      "Epoch: 8972 \tTraining Loss: 0.180686 \tR2: 0.842575\n",
      "Epoch: 8973 \tTraining Loss: 0.172508 \tR2: 0.842575\n",
      "Epoch: 8974 \tTraining Loss: 0.181447 \tR2: 0.842575\n",
      "Epoch: 8975 \tTraining Loss: 0.190591 \tR2: 0.842575\n",
      "Epoch: 8976 \tTraining Loss: 0.197863 \tR2: 0.842575\n",
      "Epoch: 8977 \tTraining Loss: 0.184392 \tR2: 0.842575\n",
      "Epoch: 8978 \tTraining Loss: 0.176503 \tR2: 0.842575\n",
      "Epoch: 8979 \tTraining Loss: 0.168681 \tR2: 0.842575\n",
      "Epoch: 8980 \tTraining Loss: 0.182450 \tR2: 0.842575\n",
      "Epoch: 8981 \tTraining Loss: 0.173197 \tR2: 0.842575\n",
      "Epoch: 8982 \tTraining Loss: 0.183989 \tR2: 0.842575\n",
      "Epoch: 8983 \tTraining Loss: 0.182218 \tR2: 0.842575\n",
      "Epoch: 8984 \tTraining Loss: 0.185682 \tR2: 0.842575\n",
      "Epoch: 8985 \tTraining Loss: 0.178713 \tR2: 0.842575\n",
      "Epoch: 8986 \tTraining Loss: 0.180946 \tR2: 0.842575\n",
      "Epoch: 8987 \tTraining Loss: 0.202485 \tR2: 0.842575\n",
      "Epoch: 8988 \tTraining Loss: 0.198784 \tR2: 0.842575\n",
      "Epoch: 8989 \tTraining Loss: 0.181171 \tR2: 0.842575\n",
      "Epoch: 8990 \tTraining Loss: 0.185443 \tR2: 0.842575\n",
      "Epoch: 8991 \tTraining Loss: 0.190920 \tR2: 0.842575\n",
      "Epoch: 8992 \tTraining Loss: 0.178810 \tR2: 0.842575\n",
      "Epoch: 8993 \tTraining Loss: 0.167568 \tR2: 0.842575\n",
      "Epoch: 8994 \tTraining Loss: 0.162618 \tR2: 0.842575\n",
      "Epoch: 8995 \tTraining Loss: 0.180799 \tR2: 0.842575\n",
      "Epoch: 8996 \tTraining Loss: 0.176751 \tR2: 0.842575\n",
      "Epoch: 8997 \tTraining Loss: 0.211353 \tR2: 0.842575\n",
      "Epoch: 8998 \tTraining Loss: 0.248047 \tR2: 0.842575\n",
      "Epoch: 8999 \tTraining Loss: 0.191787 \tR2: 0.842575\n",
      "Epoch: 9000 \tTraining Loss: 0.171878 \tR2: 0.717824\n",
      "Epoch: 9001 \tTraining Loss: 0.175885 \tR2: 0.717824\n",
      "Epoch: 9002 \tTraining Loss: 0.173883 \tR2: 0.717824\n",
      "Epoch: 9003 \tTraining Loss: 0.175851 \tR2: 0.717824\n",
      "Epoch: 9004 \tTraining Loss: 0.186545 \tR2: 0.717824\n",
      "Epoch: 9005 \tTraining Loss: 0.156499 \tR2: 0.717824\n",
      "Epoch: 9006 \tTraining Loss: 0.174067 \tR2: 0.717824\n",
      "Epoch: 9007 \tTraining Loss: 0.170282 \tR2: 0.717824\n",
      "Epoch: 9008 \tTraining Loss: 0.181878 \tR2: 0.717824\n",
      "Epoch: 9009 \tTraining Loss: 0.177874 \tR2: 0.717824\n",
      "Epoch: 9010 \tTraining Loss: 0.188532 \tR2: 0.717824\n",
      "Epoch: 9011 \tTraining Loss: 0.178329 \tR2: 0.717824\n",
      "Epoch: 9012 \tTraining Loss: 0.187917 \tR2: 0.717824\n",
      "Epoch: 9013 \tTraining Loss: 0.188645 \tR2: 0.717824\n",
      "Epoch: 9014 \tTraining Loss: 0.178251 \tR2: 0.717824\n",
      "Epoch: 9015 \tTraining Loss: 0.176944 \tR2: 0.717824\n",
      "Epoch: 9016 \tTraining Loss: 0.192328 \tR2: 0.717824\n",
      "Epoch: 9017 \tTraining Loss: 0.176406 \tR2: 0.717824\n",
      "Epoch: 9018 \tTraining Loss: 0.171692 \tR2: 0.717824\n",
      "Epoch: 9019 \tTraining Loss: 0.186502 \tR2: 0.717824\n",
      "Epoch: 9020 \tTraining Loss: 0.175454 \tR2: 0.717824\n",
      "Epoch: 9021 \tTraining Loss: 0.189540 \tR2: 0.717824\n",
      "Epoch: 9022 \tTraining Loss: 0.186038 \tR2: 0.717824\n",
      "Epoch: 9023 \tTraining Loss: 0.192910 \tR2: 0.717824\n",
      "Epoch: 9024 \tTraining Loss: 0.175422 \tR2: 0.717824\n",
      "Epoch: 9025 \tTraining Loss: 0.173879 \tR2: 0.717824\n",
      "Epoch: 9026 \tTraining Loss: 0.189064 \tR2: 0.717824\n",
      "Epoch: 9027 \tTraining Loss: 0.177644 \tR2: 0.717824\n",
      "Epoch: 9028 \tTraining Loss: 0.172055 \tR2: 0.717824\n",
      "Epoch: 9029 \tTraining Loss: 0.187267 \tR2: 0.717824\n",
      "Epoch: 9030 \tTraining Loss: 0.190418 \tR2: 0.717824\n",
      "Epoch: 9031 \tTraining Loss: 0.171256 \tR2: 0.717824\n",
      "Epoch: 9032 \tTraining Loss: 0.186576 \tR2: 0.717824\n",
      "Epoch: 9033 \tTraining Loss: 0.164002 \tR2: 0.717824\n",
      "Epoch: 9034 \tTraining Loss: 0.177653 \tR2: 0.717824\n",
      "Epoch: 9035 \tTraining Loss: 0.167150 \tR2: 0.717824\n",
      "Epoch: 9036 \tTraining Loss: 0.191460 \tR2: 0.717824\n",
      "Epoch: 9037 \tTraining Loss: 0.177712 \tR2: 0.717824\n",
      "Epoch: 9038 \tTraining Loss: 0.181314 \tR2: 0.717824\n",
      "Epoch: 9039 \tTraining Loss: 0.169915 \tR2: 0.717824\n",
      "Epoch: 9040 \tTraining Loss: 0.190081 \tR2: 0.717824\n",
      "Epoch: 9041 \tTraining Loss: 0.178690 \tR2: 0.717824\n",
      "Epoch: 9042 \tTraining Loss: 0.180999 \tR2: 0.717824\n",
      "Epoch: 9043 \tTraining Loss: 0.182435 \tR2: 0.717824\n",
      "Epoch: 9044 \tTraining Loss: 0.182167 \tR2: 0.717824\n",
      "Epoch: 9045 \tTraining Loss: 0.176618 \tR2: 0.717824\n",
      "Epoch: 9046 \tTraining Loss: 0.170260 \tR2: 0.717824\n",
      "Epoch: 9047 \tTraining Loss: 0.176023 \tR2: 0.717824\n",
      "Epoch: 9048 \tTraining Loss: 0.175109 \tR2: 0.717824\n",
      "Epoch: 9049 \tTraining Loss: 0.171591 \tR2: 0.717824\n",
      "Epoch: 9050 \tTraining Loss: 0.167309 \tR2: 0.717824\n",
      "Epoch: 9051 \tTraining Loss: 0.162682 \tR2: 0.717824\n",
      "Epoch: 9052 \tTraining Loss: 0.169564 \tR2: 0.717824\n",
      "Epoch: 9053 \tTraining Loss: 0.197694 \tR2: 0.717824\n",
      "Epoch: 9054 \tTraining Loss: 0.194565 \tR2: 0.717824\n",
      "Epoch: 9055 \tTraining Loss: 0.173186 \tR2: 0.717824\n",
      "Epoch: 9056 \tTraining Loss: 0.164469 \tR2: 0.717824\n",
      "Epoch: 9057 \tTraining Loss: 0.169522 \tR2: 0.717824\n",
      "Epoch: 9058 \tTraining Loss: 0.183192 \tR2: 0.717824\n",
      "Epoch: 9059 \tTraining Loss: 0.184114 \tR2: 0.717824\n",
      "Epoch: 9060 \tTraining Loss: 0.174314 \tR2: 0.717824\n",
      "Epoch: 9061 \tTraining Loss: 0.172466 \tR2: 0.717824\n",
      "Epoch: 9062 \tTraining Loss: 0.179927 \tR2: 0.717824\n",
      "Epoch: 9063 \tTraining Loss: 0.198777 \tR2: 0.717824\n",
      "Epoch: 9064 \tTraining Loss: 0.180912 \tR2: 0.717824\n",
      "Epoch: 9065 \tTraining Loss: 0.204257 \tR2: 0.717824\n",
      "Epoch: 9066 \tTraining Loss: 0.161054 \tR2: 0.717824\n",
      "Epoch: 9067 \tTraining Loss: 0.162142 \tR2: 0.717824\n",
      "Epoch: 9068 \tTraining Loss: 0.191172 \tR2: 0.717824\n",
      "Epoch: 9069 \tTraining Loss: 0.185926 \tR2: 0.717824\n",
      "Epoch: 9070 \tTraining Loss: 0.164417 \tR2: 0.717824\n",
      "Epoch: 9071 \tTraining Loss: 0.171290 \tR2: 0.717824\n",
      "Epoch: 9072 \tTraining Loss: 0.180996 \tR2: 0.717824\n",
      "Epoch: 9073 \tTraining Loss: 0.180612 \tR2: 0.717824\n",
      "Epoch: 9074 \tTraining Loss: 0.186645 \tR2: 0.717824\n",
      "Epoch: 9075 \tTraining Loss: 0.171497 \tR2: 0.717824\n",
      "Epoch: 9076 \tTraining Loss: 0.173280 \tR2: 0.717824\n",
      "Epoch: 9077 \tTraining Loss: 0.177898 \tR2: 0.717824\n",
      "Epoch: 9078 \tTraining Loss: 0.188507 \tR2: 0.717824\n",
      "Epoch: 9079 \tTraining Loss: 0.188757 \tR2: 0.717824\n",
      "Epoch: 9080 \tTraining Loss: 0.191805 \tR2: 0.717824\n",
      "Epoch: 9081 \tTraining Loss: 0.169278 \tR2: 0.717824\n",
      "Epoch: 9082 \tTraining Loss: 0.195373 \tR2: 0.717824\n",
      "Epoch: 9083 \tTraining Loss: 0.188892 \tR2: 0.717824\n",
      "Epoch: 9084 \tTraining Loss: 0.188448 \tR2: 0.717824\n",
      "Epoch: 9085 \tTraining Loss: 0.190611 \tR2: 0.717824\n",
      "Epoch: 9086 \tTraining Loss: 0.183922 \tR2: 0.717824\n",
      "Epoch: 9087 \tTraining Loss: 0.183683 \tR2: 0.717824\n",
      "Epoch: 9088 \tTraining Loss: 0.186782 \tR2: 0.717824\n",
      "Epoch: 9089 \tTraining Loss: 0.178248 \tR2: 0.717824\n",
      "Epoch: 9090 \tTraining Loss: 0.177339 \tR2: 0.717824\n",
      "Epoch: 9091 \tTraining Loss: 0.190987 \tR2: 0.717824\n",
      "Epoch: 9092 \tTraining Loss: 0.193960 \tR2: 0.717824\n",
      "Epoch: 9093 \tTraining Loss: 0.169419 \tR2: 0.717824\n",
      "Epoch: 9094 \tTraining Loss: 0.168500 \tR2: 0.717824\n",
      "Epoch: 9095 \tTraining Loss: 0.188391 \tR2: 0.717824\n",
      "Epoch: 9096 \tTraining Loss: 0.164843 \tR2: 0.717824\n",
      "Epoch: 9097 \tTraining Loss: 0.163969 \tR2: 0.717824\n",
      "Epoch: 9098 \tTraining Loss: 0.177746 \tR2: 0.717824\n",
      "Epoch: 9099 \tTraining Loss: 0.161920 \tR2: 0.717824\n",
      "Epoch: 9100 \tTraining Loss: 0.171298 \tR2: 0.709065\n",
      "Epoch: 9101 \tTraining Loss: 0.172031 \tR2: 0.709065\n",
      "Epoch: 9102 \tTraining Loss: 0.165981 \tR2: 0.709065\n",
      "Epoch: 9103 \tTraining Loss: 0.187549 \tR2: 0.709065\n",
      "Epoch: 9104 \tTraining Loss: 0.197202 \tR2: 0.709065\n",
      "Epoch: 9105 \tTraining Loss: 0.174037 \tR2: 0.709065\n",
      "Epoch: 9106 \tTraining Loss: 0.175568 \tR2: 0.709065\n",
      "Epoch: 9107 \tTraining Loss: 0.193254 \tR2: 0.709065\n",
      "Epoch: 9108 \tTraining Loss: 0.174447 \tR2: 0.709065\n",
      "Epoch: 9109 \tTraining Loss: 0.200790 \tR2: 0.709065\n",
      "Epoch: 9110 \tTraining Loss: 0.186498 \tR2: 0.709065\n",
      "Epoch: 9111 \tTraining Loss: 0.173542 \tR2: 0.709065\n",
      "Epoch: 9112 \tTraining Loss: 0.183859 \tR2: 0.709065\n",
      "Epoch: 9113 \tTraining Loss: 0.178647 \tR2: 0.709065\n",
      "Epoch: 9114 \tTraining Loss: 0.158488 \tR2: 0.709065\n",
      "Epoch: 9115 \tTraining Loss: 0.180057 \tR2: 0.709065\n",
      "Epoch: 9116 \tTraining Loss: 0.168722 \tR2: 0.709065\n",
      "Epoch: 9117 \tTraining Loss: 0.174211 \tR2: 0.709065\n",
      "Epoch: 9118 \tTraining Loss: 0.174784 \tR2: 0.709065\n",
      "Epoch: 9119 \tTraining Loss: 0.185493 \tR2: 0.709065\n",
      "Epoch: 9120 \tTraining Loss: 0.172628 \tR2: 0.709065\n",
      "Epoch: 9121 \tTraining Loss: 0.174676 \tR2: 0.709065\n",
      "Epoch: 9122 \tTraining Loss: 0.169934 \tR2: 0.709065\n",
      "Epoch: 9123 \tTraining Loss: 0.173068 \tR2: 0.709065\n",
      "Epoch: 9124 \tTraining Loss: 0.199741 \tR2: 0.709065\n",
      "Epoch: 9125 \tTraining Loss: 0.183041 \tR2: 0.709065\n",
      "Epoch: 9126 \tTraining Loss: 0.184825 \tR2: 0.709065\n",
      "Epoch: 9127 \tTraining Loss: 0.159778 \tR2: 0.709065\n",
      "Epoch: 9128 \tTraining Loss: 0.192249 \tR2: 0.709065\n",
      "Epoch: 9129 \tTraining Loss: 0.184774 \tR2: 0.709065\n",
      "Epoch: 9130 \tTraining Loss: 0.177308 \tR2: 0.709065\n",
      "Epoch: 9131 \tTraining Loss: 0.188469 \tR2: 0.709065\n",
      "Epoch: 9132 \tTraining Loss: 0.176852 \tR2: 0.709065\n",
      "Epoch: 9133 \tTraining Loss: 0.183025 \tR2: 0.709065\n",
      "Epoch: 9134 \tTraining Loss: 0.173998 \tR2: 0.709065\n",
      "Epoch: 9135 \tTraining Loss: 0.180049 \tR2: 0.709065\n",
      "Epoch: 9136 \tTraining Loss: 0.199015 \tR2: 0.709065\n",
      "Epoch: 9137 \tTraining Loss: 0.180970 \tR2: 0.709065\n",
      "Epoch: 9138 \tTraining Loss: 0.173812 \tR2: 0.709065\n",
      "Epoch: 9139 \tTraining Loss: 0.207367 \tR2: 0.709065\n",
      "Epoch: 9140 \tTraining Loss: 0.165832 \tR2: 0.709065\n",
      "Epoch: 9141 \tTraining Loss: 0.169464 \tR2: 0.709065\n",
      "Epoch: 9142 \tTraining Loss: 0.170444 \tR2: 0.709065\n",
      "Epoch: 9143 \tTraining Loss: 0.192548 \tR2: 0.709065\n",
      "Epoch: 9144 \tTraining Loss: 0.188940 \tR2: 0.709065\n",
      "Epoch: 9145 \tTraining Loss: 0.189971 \tR2: 0.709065\n",
      "Epoch: 9146 \tTraining Loss: 0.168952 \tR2: 0.709065\n",
      "Epoch: 9147 \tTraining Loss: 0.170962 \tR2: 0.709065\n",
      "Epoch: 9148 \tTraining Loss: 0.192649 \tR2: 0.709065\n",
      "Epoch: 9149 \tTraining Loss: 0.186858 \tR2: 0.709065\n",
      "Epoch: 9150 \tTraining Loss: 0.180240 \tR2: 0.709065\n",
      "Epoch: 9151 \tTraining Loss: 0.175936 \tR2: 0.709065\n",
      "Epoch: 9152 \tTraining Loss: 0.166492 \tR2: 0.709065\n",
      "Epoch: 9153 \tTraining Loss: 0.178622 \tR2: 0.709065\n",
      "Epoch: 9154 \tTraining Loss: 0.177759 \tR2: 0.709065\n",
      "Epoch: 9155 \tTraining Loss: 0.163005 \tR2: 0.709065\n",
      "Epoch: 9156 \tTraining Loss: 0.186559 \tR2: 0.709065\n",
      "Epoch: 9157 \tTraining Loss: 0.179535 \tR2: 0.709065\n",
      "Epoch: 9158 \tTraining Loss: 0.201832 \tR2: 0.709065\n",
      "Epoch: 9159 \tTraining Loss: 0.178072 \tR2: 0.709065\n",
      "Epoch: 9160 \tTraining Loss: 0.191923 \tR2: 0.709065\n",
      "Epoch: 9161 \tTraining Loss: 0.185603 \tR2: 0.709065\n",
      "Epoch: 9162 \tTraining Loss: 0.175546 \tR2: 0.709065\n",
      "Epoch: 9163 \tTraining Loss: 0.181311 \tR2: 0.709065\n",
      "Epoch: 9164 \tTraining Loss: 0.183972 \tR2: 0.709065\n",
      "Epoch: 9165 \tTraining Loss: 0.188023 \tR2: 0.709065\n",
      "Epoch: 9166 \tTraining Loss: 0.157333 \tR2: 0.709065\n",
      "Epoch: 9167 \tTraining Loss: 0.176401 \tR2: 0.709065\n",
      "Epoch: 9168 \tTraining Loss: 0.183231 \tR2: 0.709065\n",
      "Epoch: 9169 \tTraining Loss: 0.165992 \tR2: 0.709065\n",
      "Epoch: 9170 \tTraining Loss: 0.171115 \tR2: 0.709065\n",
      "Epoch: 9171 \tTraining Loss: 0.188276 \tR2: 0.709065\n",
      "Epoch: 9172 \tTraining Loss: 0.178057 \tR2: 0.709065\n",
      "Epoch: 9173 \tTraining Loss: 0.185026 \tR2: 0.709065\n",
      "Epoch: 9174 \tTraining Loss: 0.170785 \tR2: 0.709065\n",
      "Epoch: 9175 \tTraining Loss: 0.180738 \tR2: 0.709065\n",
      "Epoch: 9176 \tTraining Loss: 0.591369 \tR2: 0.709065\n",
      "Epoch: 9177 \tTraining Loss: 0.388765 \tR2: 0.709065\n",
      "Epoch: 9178 \tTraining Loss: 0.329531 \tR2: 0.709065\n",
      "Epoch: 9179 \tTraining Loss: 0.296410 \tR2: 0.709065\n",
      "Epoch: 9180 \tTraining Loss: 0.253016 \tR2: 0.709065\n",
      "Epoch: 9181 \tTraining Loss: 0.208436 \tR2: 0.709065\n",
      "Epoch: 9182 \tTraining Loss: 0.191505 \tR2: 0.709065\n",
      "Epoch: 9183 \tTraining Loss: 0.189903 \tR2: 0.709065\n",
      "Epoch: 9184 \tTraining Loss: 0.180849 \tR2: 0.709065\n",
      "Epoch: 9185 \tTraining Loss: 0.184722 \tR2: 0.709065\n",
      "Epoch: 9186 \tTraining Loss: 0.174372 \tR2: 0.709065\n",
      "Epoch: 9187 \tTraining Loss: 0.188889 \tR2: 0.709065\n",
      "Epoch: 9188 \tTraining Loss: 0.187724 \tR2: 0.709065\n",
      "Epoch: 9189 \tTraining Loss: 0.182357 \tR2: 0.709065\n",
      "Epoch: 9190 \tTraining Loss: 0.168255 \tR2: 0.709065\n",
      "Epoch: 9191 \tTraining Loss: 0.190064 \tR2: 0.709065\n",
      "Epoch: 9192 \tTraining Loss: 0.168279 \tR2: 0.709065\n",
      "Epoch: 9193 \tTraining Loss: 0.186301 \tR2: 0.709065\n",
      "Epoch: 9194 \tTraining Loss: 0.189513 \tR2: 0.709065\n",
      "Epoch: 9195 \tTraining Loss: 0.192385 \tR2: 0.709065\n",
      "Epoch: 9196 \tTraining Loss: 0.173979 \tR2: 0.709065\n",
      "Epoch: 9197 \tTraining Loss: 0.170041 \tR2: 0.709065\n",
      "Epoch: 9198 \tTraining Loss: 0.184508 \tR2: 0.709065\n",
      "Epoch: 9199 \tTraining Loss: 0.168401 \tR2: 0.709065\n",
      "Epoch: 9200 \tTraining Loss: 0.179905 \tR2: 0.824821\n",
      "Epoch: 9201 \tTraining Loss: 0.177391 \tR2: 0.824821\n",
      "Epoch: 9202 \tTraining Loss: 0.191415 \tR2: 0.824821\n",
      "Epoch: 9203 \tTraining Loss: 0.184124 \tR2: 0.824821\n",
      "Epoch: 9204 \tTraining Loss: 0.209980 \tR2: 0.824821\n",
      "Epoch: 9205 \tTraining Loss: 0.204654 \tR2: 0.824821\n",
      "Epoch: 9206 \tTraining Loss: 0.195044 \tR2: 0.824821\n",
      "Epoch: 9207 \tTraining Loss: 0.178561 \tR2: 0.824821\n",
      "Epoch: 9208 \tTraining Loss: 0.186932 \tR2: 0.824821\n",
      "Epoch: 9209 \tTraining Loss: 0.178940 \tR2: 0.824821\n",
      "Epoch: 9210 \tTraining Loss: 0.176427 \tR2: 0.824821\n",
      "Epoch: 9211 \tTraining Loss: 0.194552 \tR2: 0.824821\n",
      "Epoch: 9212 \tTraining Loss: 0.186729 \tR2: 0.824821\n",
      "Epoch: 9213 \tTraining Loss: 0.170130 \tR2: 0.824821\n",
      "Epoch: 9214 \tTraining Loss: 0.186590 \tR2: 0.824821\n",
      "Epoch: 9215 \tTraining Loss: 0.177858 \tR2: 0.824821\n",
      "Epoch: 9216 \tTraining Loss: 0.190523 \tR2: 0.824821\n",
      "Epoch: 9217 \tTraining Loss: 0.179913 \tR2: 0.824821\n",
      "Epoch: 9218 \tTraining Loss: 0.168733 \tR2: 0.824821\n",
      "Epoch: 9219 \tTraining Loss: 0.195138 \tR2: 0.824821\n",
      "Epoch: 9220 \tTraining Loss: 0.177035 \tR2: 0.824821\n",
      "Epoch: 9221 \tTraining Loss: 0.179565 \tR2: 0.824821\n",
      "Epoch: 9222 \tTraining Loss: 0.183730 \tR2: 0.824821\n",
      "Epoch: 9223 \tTraining Loss: 0.179065 \tR2: 0.824821\n",
      "Epoch: 9224 \tTraining Loss: 0.173859 \tR2: 0.824821\n",
      "Epoch: 9225 \tTraining Loss: 0.180930 \tR2: 0.824821\n",
      "Epoch: 9226 \tTraining Loss: 0.177542 \tR2: 0.824821\n",
      "Epoch: 9227 \tTraining Loss: 0.181370 \tR2: 0.824821\n",
      "Epoch: 9228 \tTraining Loss: 0.188417 \tR2: 0.824821\n",
      "Epoch: 9229 \tTraining Loss: 0.179843 \tR2: 0.824821\n",
      "Epoch: 9230 \tTraining Loss: 0.183980 \tR2: 0.824821\n",
      "Epoch: 9231 \tTraining Loss: 0.176205 \tR2: 0.824821\n",
      "Epoch: 9232 \tTraining Loss: 0.199757 \tR2: 0.824821\n",
      "Epoch: 9233 \tTraining Loss: 0.181454 \tR2: 0.824821\n",
      "Epoch: 9234 \tTraining Loss: 0.185538 \tR2: 0.824821\n",
      "Epoch: 9235 \tTraining Loss: 0.201918 \tR2: 0.824821\n",
      "Epoch: 9236 \tTraining Loss: 0.183073 \tR2: 0.824821\n",
      "Epoch: 9237 \tTraining Loss: 0.168620 \tR2: 0.824821\n",
      "Epoch: 9238 \tTraining Loss: 0.186681 \tR2: 0.824821\n",
      "Epoch: 9239 \tTraining Loss: 0.175468 \tR2: 0.824821\n",
      "Epoch: 9240 \tTraining Loss: 0.170440 \tR2: 0.824821\n",
      "Epoch: 9241 \tTraining Loss: 0.170087 \tR2: 0.824821\n",
      "Epoch: 9242 \tTraining Loss: 0.180357 \tR2: 0.824821\n",
      "Epoch: 9243 \tTraining Loss: 0.158268 \tR2: 0.824821\n",
      "Epoch: 9244 \tTraining Loss: 0.185068 \tR2: 0.824821\n",
      "Epoch: 9245 \tTraining Loss: 0.170504 \tR2: 0.824821\n",
      "Epoch: 9246 \tTraining Loss: 0.171709 \tR2: 0.824821\n",
      "Epoch: 9247 \tTraining Loss: 0.165523 \tR2: 0.824821\n",
      "Epoch: 9248 \tTraining Loss: 0.175651 \tR2: 0.824821\n",
      "Epoch: 9249 \tTraining Loss: 0.184932 \tR2: 0.824821\n",
      "Epoch: 9250 \tTraining Loss: 0.190823 \tR2: 0.824821\n",
      "Epoch: 9251 \tTraining Loss: 0.176594 \tR2: 0.824821\n",
      "Epoch: 9252 \tTraining Loss: 0.162801 \tR2: 0.824821\n",
      "Epoch: 9253 \tTraining Loss: 0.157175 \tR2: 0.824821\n",
      "Epoch: 9254 \tTraining Loss: 0.177122 \tR2: 0.824821\n",
      "Epoch: 9255 \tTraining Loss: 0.182879 \tR2: 0.824821\n",
      "Epoch: 9256 \tTraining Loss: 0.178485 \tR2: 0.824821\n",
      "Epoch: 9257 \tTraining Loss: 0.189599 \tR2: 0.824821\n",
      "Epoch: 9258 \tTraining Loss: 0.192541 \tR2: 0.824821\n",
      "Epoch: 9259 \tTraining Loss: 0.190072 \tR2: 0.824821\n",
      "Epoch: 9260 \tTraining Loss: 0.179750 \tR2: 0.824821\n",
      "Epoch: 9261 \tTraining Loss: 0.169691 \tR2: 0.824821\n",
      "Epoch: 9262 \tTraining Loss: 0.182052 \tR2: 0.824821\n",
      "Epoch: 9263 \tTraining Loss: 0.187964 \tR2: 0.824821\n",
      "Epoch: 9264 \tTraining Loss: 0.179899 \tR2: 0.824821\n",
      "Epoch: 9265 \tTraining Loss: 0.175907 \tR2: 0.824821\n",
      "Epoch: 9266 \tTraining Loss: 0.156170 \tR2: 0.824821\n",
      "Epoch: 9267 \tTraining Loss: 0.187022 \tR2: 0.824821\n",
      "Epoch: 9268 \tTraining Loss: 0.182140 \tR2: 0.824821\n",
      "Epoch: 9269 \tTraining Loss: 0.175308 \tR2: 0.824821\n",
      "Epoch: 9270 \tTraining Loss: 0.172816 \tR2: 0.824821\n",
      "Epoch: 9271 \tTraining Loss: 0.167136 \tR2: 0.824821\n",
      "Epoch: 9272 \tTraining Loss: 0.178558 \tR2: 0.824821\n",
      "Epoch: 9273 \tTraining Loss: 0.212120 \tR2: 0.824821\n",
      "Epoch: 9274 \tTraining Loss: 0.188725 \tR2: 0.824821\n",
      "Epoch: 9275 \tTraining Loss: 0.207698 \tR2: 0.824821\n",
      "Epoch: 9276 \tTraining Loss: 0.197466 \tR2: 0.824821\n",
      "Epoch: 9277 \tTraining Loss: 0.175254 \tR2: 0.824821\n",
      "Epoch: 9278 \tTraining Loss: 0.169065 \tR2: 0.824821\n",
      "Epoch: 9279 \tTraining Loss: 0.203606 \tR2: 0.824821\n",
      "Epoch: 9280 \tTraining Loss: 0.174946 \tR2: 0.824821\n",
      "Epoch: 9281 \tTraining Loss: 0.182251 \tR2: 0.824821\n",
      "Epoch: 9282 \tTraining Loss: 0.179331 \tR2: 0.824821\n",
      "Epoch: 9283 \tTraining Loss: 0.161875 \tR2: 0.824821\n",
      "Epoch: 9284 \tTraining Loss: 0.175097 \tR2: 0.824821\n",
      "Epoch: 9285 \tTraining Loss: 0.174799 \tR2: 0.824821\n",
      "Epoch: 9286 \tTraining Loss: 0.174251 \tR2: 0.824821\n",
      "Epoch: 9287 \tTraining Loss: 0.201941 \tR2: 0.824821\n",
      "Epoch: 9288 \tTraining Loss: 0.184518 \tR2: 0.824821\n",
      "Epoch: 9289 \tTraining Loss: 0.174955 \tR2: 0.824821\n",
      "Epoch: 9290 \tTraining Loss: 0.187906 \tR2: 0.824821\n",
      "Epoch: 9291 \tTraining Loss: 0.194697 \tR2: 0.824821\n",
      "Epoch: 9292 \tTraining Loss: 0.184555 \tR2: 0.824821\n",
      "Epoch: 9293 \tTraining Loss: 0.180418 \tR2: 0.824821\n",
      "Epoch: 9294 \tTraining Loss: 0.175484 \tR2: 0.824821\n",
      "Epoch: 9295 \tTraining Loss: 0.174979 \tR2: 0.824821\n",
      "Epoch: 9296 \tTraining Loss: 0.199856 \tR2: 0.824821\n",
      "Epoch: 9297 \tTraining Loss: 0.177037 \tR2: 0.824821\n",
      "Epoch: 9298 \tTraining Loss: 0.175172 \tR2: 0.824821\n",
      "Epoch: 9299 \tTraining Loss: 0.178405 \tR2: 0.824821\n",
      "Epoch: 9300 \tTraining Loss: 0.186079 \tR2: 0.702728\n",
      "Epoch: 9301 \tTraining Loss: 0.165593 \tR2: 0.702728\n",
      "Epoch: 9302 \tTraining Loss: 0.192232 \tR2: 0.702728\n",
      "Epoch: 9303 \tTraining Loss: 0.182027 \tR2: 0.702728\n",
      "Epoch: 9304 \tTraining Loss: 0.170615 \tR2: 0.702728\n",
      "Epoch: 9305 \tTraining Loss: 0.171909 \tR2: 0.702728\n",
      "Epoch: 9306 \tTraining Loss: 0.164772 \tR2: 0.702728\n",
      "Epoch: 9307 \tTraining Loss: 0.187063 \tR2: 0.702728\n",
      "Epoch: 9308 \tTraining Loss: 0.183843 \tR2: 0.702728\n",
      "Epoch: 9309 \tTraining Loss: 0.185590 \tR2: 0.702728\n",
      "Epoch: 9310 \tTraining Loss: 0.171260 \tR2: 0.702728\n",
      "Epoch: 9311 \tTraining Loss: 0.157332 \tR2: 0.702728\n",
      "Epoch: 9312 \tTraining Loss: 0.170670 \tR2: 0.702728\n",
      "Epoch: 9313 \tTraining Loss: 0.192752 \tR2: 0.702728\n",
      "Epoch: 9314 \tTraining Loss: 0.174507 \tR2: 0.702728\n",
      "Epoch: 9315 \tTraining Loss: 0.176646 \tR2: 0.702728\n",
      "Epoch: 9316 \tTraining Loss: 0.179221 \tR2: 0.702728\n",
      "Epoch: 9317 \tTraining Loss: 0.170519 \tR2: 0.702728\n",
      "Epoch: 9318 \tTraining Loss: 0.166468 \tR2: 0.702728\n",
      "Epoch: 9319 \tTraining Loss: 0.173530 \tR2: 0.702728\n",
      "Epoch: 9320 \tTraining Loss: 0.182163 \tR2: 0.702728\n",
      "Epoch: 9321 \tTraining Loss: 0.208938 \tR2: 0.702728\n",
      "Epoch: 9322 \tTraining Loss: 0.193926 \tR2: 0.702728\n",
      "Epoch: 9323 \tTraining Loss: 0.180218 \tR2: 0.702728\n",
      "Epoch: 9324 \tTraining Loss: 0.174668 \tR2: 0.702728\n",
      "Epoch: 9325 \tTraining Loss: 0.175511 \tR2: 0.702728\n",
      "Epoch: 9326 \tTraining Loss: 0.175418 \tR2: 0.702728\n",
      "Epoch: 9327 \tTraining Loss: 0.168911 \tR2: 0.702728\n",
      "Epoch: 9328 \tTraining Loss: 0.162771 \tR2: 0.702728\n",
      "Epoch: 9329 \tTraining Loss: 0.188097 \tR2: 0.702728\n",
      "Epoch: 9330 \tTraining Loss: 0.168914 \tR2: 0.702728\n",
      "Epoch: 9331 \tTraining Loss: 0.179911 \tR2: 0.702728\n",
      "Epoch: 9332 \tTraining Loss: 0.176977 \tR2: 0.702728\n",
      "Epoch: 9333 \tTraining Loss: 0.185202 \tR2: 0.702728\n",
      "Epoch: 9334 \tTraining Loss: 0.167738 \tR2: 0.702728\n",
      "Epoch: 9335 \tTraining Loss: 0.183343 \tR2: 0.702728\n",
      "Epoch: 9336 \tTraining Loss: 0.192725 \tR2: 0.702728\n",
      "Epoch: 9337 \tTraining Loss: 0.176196 \tR2: 0.702728\n",
      "Epoch: 9338 \tTraining Loss: 0.171349 \tR2: 0.702728\n",
      "Epoch: 9339 \tTraining Loss: 0.169742 \tR2: 0.702728\n",
      "Epoch: 9340 \tTraining Loss: 0.169736 \tR2: 0.702728\n",
      "Epoch: 9341 \tTraining Loss: 0.186667 \tR2: 0.702728\n",
      "Epoch: 9342 \tTraining Loss: 0.178062 \tR2: 0.702728\n",
      "Epoch: 9343 \tTraining Loss: 0.181005 \tR2: 0.702728\n",
      "Epoch: 9344 \tTraining Loss: 0.178766 \tR2: 0.702728\n",
      "Epoch: 9345 \tTraining Loss: 0.162440 \tR2: 0.702728\n",
      "Epoch: 9346 \tTraining Loss: 0.205976 \tR2: 0.702728\n",
      "Epoch: 9347 \tTraining Loss: 0.190045 \tR2: 0.702728\n",
      "Epoch: 9348 \tTraining Loss: 0.172221 \tR2: 0.702728\n",
      "Epoch: 9349 \tTraining Loss: 0.173588 \tR2: 0.702728\n",
      "Epoch: 9350 \tTraining Loss: 0.185661 \tR2: 0.702728\n",
      "Epoch: 9351 \tTraining Loss: 0.188268 \tR2: 0.702728\n",
      "Epoch: 9352 \tTraining Loss: 0.185066 \tR2: 0.702728\n",
      "Epoch: 9353 \tTraining Loss: 0.175495 \tR2: 0.702728\n",
      "Epoch: 9354 \tTraining Loss: 0.176431 \tR2: 0.702728\n",
      "Epoch: 9355 \tTraining Loss: 0.173366 \tR2: 0.702728\n",
      "Epoch: 9356 \tTraining Loss: 0.176188 \tR2: 0.702728\n",
      "Epoch: 9357 \tTraining Loss: 0.176304 \tR2: 0.702728\n",
      "Epoch: 9358 \tTraining Loss: 0.178372 \tR2: 0.702728\n",
      "Epoch: 9359 \tTraining Loss: 0.171129 \tR2: 0.702728\n",
      "Epoch: 9360 \tTraining Loss: 0.173994 \tR2: 0.702728\n",
      "Epoch: 9361 \tTraining Loss: 0.179104 \tR2: 0.702728\n",
      "Epoch: 9362 \tTraining Loss: 0.176550 \tR2: 0.702728\n",
      "Epoch: 9363 \tTraining Loss: 0.164090 \tR2: 0.702728\n",
      "Epoch: 9364 \tTraining Loss: 0.175863 \tR2: 0.702728\n",
      "Epoch: 9365 \tTraining Loss: 0.174492 \tR2: 0.702728\n",
      "Epoch: 9366 \tTraining Loss: 0.163125 \tR2: 0.702728\n",
      "Epoch: 9367 \tTraining Loss: 0.183563 \tR2: 0.702728\n",
      "Epoch: 9368 \tTraining Loss: 0.170251 \tR2: 0.702728\n",
      "Epoch: 9369 \tTraining Loss: 0.171071 \tR2: 0.702728\n",
      "Epoch: 9370 \tTraining Loss: 0.178779 \tR2: 0.702728\n",
      "Epoch: 9371 \tTraining Loss: 0.159555 \tR2: 0.702728\n",
      "Epoch: 9372 \tTraining Loss: 0.173466 \tR2: 0.702728\n",
      "Epoch: 9373 \tTraining Loss: 0.185885 \tR2: 0.702728\n",
      "Epoch: 9374 \tTraining Loss: 0.183476 \tR2: 0.702728\n",
      "Epoch: 9375 \tTraining Loss: 0.200199 \tR2: 0.702728\n",
      "Epoch: 9376 \tTraining Loss: 0.165878 \tR2: 0.702728\n",
      "Epoch: 9377 \tTraining Loss: 0.208356 \tR2: 0.702728\n",
      "Epoch: 9378 \tTraining Loss: 0.153587 \tR2: 0.702728\n",
      "Epoch: 9379 \tTraining Loss: 0.186510 \tR2: 0.702728\n",
      "Epoch: 9380 \tTraining Loss: 0.164059 \tR2: 0.702728\n",
      "Epoch: 9381 \tTraining Loss: 0.183812 \tR2: 0.702728\n",
      "Epoch: 9382 \tTraining Loss: 0.182927 \tR2: 0.702728\n",
      "Epoch: 9383 \tTraining Loss: 0.186516 \tR2: 0.702728\n",
      "Epoch: 9384 \tTraining Loss: 0.191871 \tR2: 0.702728\n",
      "Epoch: 9385 \tTraining Loss: 0.175375 \tR2: 0.702728\n",
      "Epoch: 9386 \tTraining Loss: 0.176685 \tR2: 0.702728\n",
      "Epoch: 9387 \tTraining Loss: 0.179873 \tR2: 0.702728\n",
      "Epoch: 9388 \tTraining Loss: 0.165010 \tR2: 0.702728\n",
      "Epoch: 9389 \tTraining Loss: 0.170908 \tR2: 0.702728\n",
      "Epoch: 9390 \tTraining Loss: 0.180225 \tR2: 0.702728\n",
      "Epoch: 9391 \tTraining Loss: 0.206446 \tR2: 0.702728\n",
      "Epoch: 9392 \tTraining Loss: 0.175996 \tR2: 0.702728\n",
      "Epoch: 9393 \tTraining Loss: 0.177893 \tR2: 0.702728\n",
      "Epoch: 9394 \tTraining Loss: 0.163907 \tR2: 0.702728\n",
      "Epoch: 9395 \tTraining Loss: 0.167558 \tR2: 0.702728\n",
      "Epoch: 9396 \tTraining Loss: 0.176628 \tR2: 0.702728\n",
      "Epoch: 9397 \tTraining Loss: 0.199386 \tR2: 0.702728\n",
      "Epoch: 9398 \tTraining Loss: 0.189155 \tR2: 0.702728\n",
      "Epoch: 9399 \tTraining Loss: 0.183917 \tR2: 0.702728\n",
      "Epoch: 9400 \tTraining Loss: 0.165780 \tR2: 0.820074\n",
      "Epoch: 9401 \tTraining Loss: 0.175392 \tR2: 0.820074\n",
      "Epoch: 9402 \tTraining Loss: 0.179619 \tR2: 0.820074\n",
      "Epoch: 9403 \tTraining Loss: 0.165764 \tR2: 0.820074\n",
      "Epoch: 9404 \tTraining Loss: 0.189695 \tR2: 0.820074\n",
      "Epoch: 9405 \tTraining Loss: 0.178097 \tR2: 0.820074\n",
      "Epoch: 9406 \tTraining Loss: 0.177422 \tR2: 0.820074\n",
      "Epoch: 9407 \tTraining Loss: 0.161085 \tR2: 0.820074\n",
      "Epoch: 9408 \tTraining Loss: 0.170975 \tR2: 0.820074\n",
      "Epoch: 9409 \tTraining Loss: 0.175767 \tR2: 0.820074\n",
      "Epoch: 9410 \tTraining Loss: 0.183967 \tR2: 0.820074\n",
      "Epoch: 9411 \tTraining Loss: 0.183495 \tR2: 0.820074\n",
      "Epoch: 9412 \tTraining Loss: 0.168527 \tR2: 0.820074\n",
      "Epoch: 9413 \tTraining Loss: 0.184229 \tR2: 0.820074\n",
      "Epoch: 9414 \tTraining Loss: 0.180418 \tR2: 0.820074\n",
      "Epoch: 9415 \tTraining Loss: 0.185444 \tR2: 0.820074\n",
      "Epoch: 9416 \tTraining Loss: 0.179898 \tR2: 0.820074\n",
      "Epoch: 9417 \tTraining Loss: 0.178855 \tR2: 0.820074\n",
      "Epoch: 9418 \tTraining Loss: 0.194884 \tR2: 0.820074\n",
      "Epoch: 9419 \tTraining Loss: 0.179269 \tR2: 0.820074\n",
      "Epoch: 9420 \tTraining Loss: 0.172638 \tR2: 0.820074\n",
      "Epoch: 9421 \tTraining Loss: 0.176954 \tR2: 0.820074\n",
      "Epoch: 9422 \tTraining Loss: 0.163781 \tR2: 0.820074\n",
      "Epoch: 9423 \tTraining Loss: 0.209393 \tR2: 0.820074\n",
      "Epoch: 9424 \tTraining Loss: 0.168634 \tR2: 0.820074\n",
      "Epoch: 9425 \tTraining Loss: 0.189706 \tR2: 0.820074\n",
      "Epoch: 9426 \tTraining Loss: 0.180713 \tR2: 0.820074\n",
      "Epoch: 9427 \tTraining Loss: 0.164759 \tR2: 0.820074\n",
      "Epoch: 9428 \tTraining Loss: 0.171834 \tR2: 0.820074\n",
      "Epoch: 9429 \tTraining Loss: 0.179763 \tR2: 0.820074\n",
      "Epoch: 9430 \tTraining Loss: 0.189517 \tR2: 0.820074\n",
      "Epoch: 9431 \tTraining Loss: 0.177962 \tR2: 0.820074\n",
      "Epoch: 9432 \tTraining Loss: 0.180715 \tR2: 0.820074\n",
      "Epoch: 9433 \tTraining Loss: 0.175415 \tR2: 0.820074\n",
      "Epoch: 9434 \tTraining Loss: 0.176512 \tR2: 0.820074\n",
      "Epoch: 9435 \tTraining Loss: 0.169327 \tR2: 0.820074\n",
      "Epoch: 9436 \tTraining Loss: 0.179926 \tR2: 0.820074\n",
      "Epoch: 9437 \tTraining Loss: 0.184191 \tR2: 0.820074\n",
      "Epoch: 9438 \tTraining Loss: 0.180623 \tR2: 0.820074\n",
      "Epoch: 9439 \tTraining Loss: 0.183559 \tR2: 0.820074\n",
      "Epoch: 9440 \tTraining Loss: 0.201834 \tR2: 0.820074\n",
      "Epoch: 9441 \tTraining Loss: 0.177763 \tR2: 0.820074\n",
      "Epoch: 9442 \tTraining Loss: 0.223642 \tR2: 0.820074\n",
      "Epoch: 9443 \tTraining Loss: 0.184293 \tR2: 0.820074\n",
      "Epoch: 9444 \tTraining Loss: 0.194094 \tR2: 0.820074\n",
      "Epoch: 9445 \tTraining Loss: 0.192507 \tR2: 0.820074\n",
      "Epoch: 9446 \tTraining Loss: 0.181217 \tR2: 0.820074\n",
      "Epoch: 9447 \tTraining Loss: 0.180015 \tR2: 0.820074\n",
      "Epoch: 9448 \tTraining Loss: 0.176443 \tR2: 0.820074\n",
      "Epoch: 9449 \tTraining Loss: 0.239494 \tR2: 0.820074\n",
      "Epoch: 9450 \tTraining Loss: 0.198977 \tR2: 0.820074\n",
      "Epoch: 9451 \tTraining Loss: 0.186665 \tR2: 0.820074\n",
      "Epoch: 9452 \tTraining Loss: 0.177454 \tR2: 0.820074\n",
      "Epoch: 9453 \tTraining Loss: 0.174818 \tR2: 0.820074\n",
      "Epoch: 9454 \tTraining Loss: 0.180676 \tR2: 0.820074\n",
      "Epoch: 9455 \tTraining Loss: 0.183941 \tR2: 0.820074\n",
      "Epoch: 9456 \tTraining Loss: 0.176800 \tR2: 0.820074\n",
      "Epoch: 9457 \tTraining Loss: 0.195062 \tR2: 0.820074\n",
      "Epoch: 9458 \tTraining Loss: 0.190432 \tR2: 0.820074\n",
      "Epoch: 9459 \tTraining Loss: 0.190849 \tR2: 0.820074\n",
      "Epoch: 9460 \tTraining Loss: 0.169482 \tR2: 0.820074\n",
      "Epoch: 9461 \tTraining Loss: 0.169332 \tR2: 0.820074\n",
      "Epoch: 9462 \tTraining Loss: 0.189070 \tR2: 0.820074\n",
      "Epoch: 9463 \tTraining Loss: 0.211715 \tR2: 0.820074\n",
      "Epoch: 9464 \tTraining Loss: 0.165523 \tR2: 0.820074\n",
      "Epoch: 9465 \tTraining Loss: 0.184848 \tR2: 0.820074\n",
      "Epoch: 9466 \tTraining Loss: 0.192737 \tR2: 0.820074\n",
      "Epoch: 9467 \tTraining Loss: 0.168200 \tR2: 0.820074\n",
      "Epoch: 9468 \tTraining Loss: 0.169955 \tR2: 0.820074\n",
      "Epoch: 9469 \tTraining Loss: 0.187445 \tR2: 0.820074\n",
      "Epoch: 9470 \tTraining Loss: 0.168560 \tR2: 0.820074\n",
      "Epoch: 9471 \tTraining Loss: 0.170359 \tR2: 0.820074\n",
      "Epoch: 9472 \tTraining Loss: 0.179090 \tR2: 0.820074\n",
      "Epoch: 9473 \tTraining Loss: 0.163900 \tR2: 0.820074\n",
      "Epoch: 9474 \tTraining Loss: 0.169174 \tR2: 0.820074\n",
      "Epoch: 9475 \tTraining Loss: 0.179157 \tR2: 0.820074\n",
      "Epoch: 9476 \tTraining Loss: 0.188762 \tR2: 0.820074\n",
      "Epoch: 9477 \tTraining Loss: 0.185429 \tR2: 0.820074\n",
      "Epoch: 9478 \tTraining Loss: 0.210994 \tR2: 0.820074\n",
      "Epoch: 9479 \tTraining Loss: 0.174328 \tR2: 0.820074\n",
      "Epoch: 9480 \tTraining Loss: 0.169400 \tR2: 0.820074\n",
      "Epoch: 9481 \tTraining Loss: 0.196396 \tR2: 0.820074\n",
      "Epoch: 9482 \tTraining Loss: 0.184150 \tR2: 0.820074\n",
      "Epoch: 9483 \tTraining Loss: 0.170134 \tR2: 0.820074\n",
      "Epoch: 9484 \tTraining Loss: 0.186291 \tR2: 0.820074\n",
      "Epoch: 9485 \tTraining Loss: 0.186296 \tR2: 0.820074\n",
      "Epoch: 9486 \tTraining Loss: 0.173041 \tR2: 0.820074\n",
      "Epoch: 9487 \tTraining Loss: 0.175318 \tR2: 0.820074\n",
      "Epoch: 9488 \tTraining Loss: 0.184281 \tR2: 0.820074\n",
      "Epoch: 9489 \tTraining Loss: 0.176564 \tR2: 0.820074\n",
      "Epoch: 9490 \tTraining Loss: 0.181014 \tR2: 0.820074\n",
      "Epoch: 9491 \tTraining Loss: 0.168817 \tR2: 0.820074\n",
      "Epoch: 9492 \tTraining Loss: 0.174750 \tR2: 0.820074\n",
      "Epoch: 9493 \tTraining Loss: 0.169727 \tR2: 0.820074\n",
      "Epoch: 9494 \tTraining Loss: 0.198983 \tR2: 0.820074\n",
      "Epoch: 9495 \tTraining Loss: 0.171917 \tR2: 0.820074\n",
      "Epoch: 9496 \tTraining Loss: 0.163051 \tR2: 0.820074\n",
      "Epoch: 9497 \tTraining Loss: 0.187924 \tR2: 0.820074\n",
      "Epoch: 9498 \tTraining Loss: 0.200457 \tR2: 0.820074\n",
      "Epoch: 9499 \tTraining Loss: 0.181476 \tR2: 0.820074\n",
      "Epoch: 9500 \tTraining Loss: 0.180204 \tR2: 0.852810\n",
      "Epoch: 9501 \tTraining Loss: 0.164004 \tR2: 0.852810\n",
      "Epoch: 9502 \tTraining Loss: 0.182772 \tR2: 0.852810\n",
      "Epoch: 9503 \tTraining Loss: 0.171182 \tR2: 0.852810\n",
      "Epoch: 9504 \tTraining Loss: 0.186218 \tR2: 0.852810\n",
      "Epoch: 9505 \tTraining Loss: 0.178102 \tR2: 0.852810\n",
      "Epoch: 9506 \tTraining Loss: 0.191480 \tR2: 0.852810\n",
      "Epoch: 9507 \tTraining Loss: 0.191835 \tR2: 0.852810\n",
      "Epoch: 9508 \tTraining Loss: 0.175077 \tR2: 0.852810\n",
      "Epoch: 9509 \tTraining Loss: 0.198872 \tR2: 0.852810\n",
      "Epoch: 9510 \tTraining Loss: 0.176874 \tR2: 0.852810\n",
      "Epoch: 9511 \tTraining Loss: 0.173062 \tR2: 0.852810\n",
      "Epoch: 9512 \tTraining Loss: 0.173155 \tR2: 0.852810\n",
      "Epoch: 9513 \tTraining Loss: 0.183112 \tR2: 0.852810\n",
      "Epoch: 9514 \tTraining Loss: 0.202817 \tR2: 0.852810\n",
      "Epoch: 9515 \tTraining Loss: 0.191034 \tR2: 0.852810\n",
      "Epoch: 9516 \tTraining Loss: 0.171694 \tR2: 0.852810\n",
      "Epoch: 9517 \tTraining Loss: 0.162261 \tR2: 0.852810\n",
      "Epoch: 9518 \tTraining Loss: 0.177448 \tR2: 0.852810\n",
      "Epoch: 9519 \tTraining Loss: 0.184259 \tR2: 0.852810\n",
      "Epoch: 9520 \tTraining Loss: 0.163641 \tR2: 0.852810\n",
      "Epoch: 9521 \tTraining Loss: 0.174224 \tR2: 0.852810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\DataWhalePytorch\\thorough-pytorch-main\\notebook\\第四章 PyTorch基础实战\\图像分类\\FashionMNIST时装分类实战\\GNNWR.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m200000\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32md:\\DataWhalePytorch\\thorough-pytorch-main\\notebook\\第四章 PyTorch基础实战\\图像分类\\FashionMNIST时装分类实战\\GNNWR.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mview(\u001b[39m50\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmul(coef)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m out(output)\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\DataWhalePytorch\\thorough-pytorch-main\\notebook\\第四章 PyTorch基础实战\\图像分类\\FashionMNIST时装分类实战\\GNNWR.ipynb Cell 10\u001b[0m in \u001b[0;36mGNNWR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataWhalePytorch/thorough-pytorch-main/notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/GNNWR.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200000+1):\n",
    "    train(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "861f9c34f7302a1aedb62edfc1533c524ce2793735e6b405602ea89eb9cb2484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
