{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.images = df.iloc[:,5:].values\n",
    "        self.coef = df.iloc[:,1:5].values\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        coef = self.coef[idx]\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        coef = torch.tensor(coef, dtype=torch.float)\n",
    "\n",
    "        return image, coef, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D://CO2_data4.csv\", encoding=\"utf-8\")\n",
    "dataset.shape[0]\n",
    "train_li = random.sample([i for i in range(0, dataset.shape[0])], int(0.8 * dataset.shape[0]))\n",
    "train_li.sort()\n",
    "\n",
    "j = 0\n",
    "test_li = []\n",
    "\n",
    "\n",
    "for i in range(0, dataset.shape[0], 1):\n",
    "    if i != train_li[j]:\n",
    "        test_li.append(i)\n",
    "    else:\n",
    "        j = j + 1\n",
    "\n",
    "train_set = dataset.iloc[train_li, :]\n",
    "test_set = dataset.iloc[test_li, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(P, C):\n",
    "    A = (P**2).sum(axis=1, keepdims=True)\n",
    " \n",
    "    B = (C**2).sum(axis=1, keepdims=True).T\n",
    " \n",
    "    return np.sqrt(A + B - 2* np.dot(P, C.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>fCO2</th>\n",
       "      <th>Chl</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Salt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-111.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.835711</td>\n",
       "      <td>0.874641</td>\n",
       "      <td>0.896934</td>\n",
       "      <td>3.617013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-115.75</td>\n",
       "      <td>28.25</td>\n",
       "      <td>0.341512</td>\n",
       "      <td>1.191270</td>\n",
       "      <td>0.169563</td>\n",
       "      <td>2.930019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>125.75</td>\n",
       "      <td>35.75</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>2.126348</td>\n",
       "      <td>0.537075</td>\n",
       "      <td>2.406186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-116.75</td>\n",
       "      <td>30.75</td>\n",
       "      <td>0.490971</td>\n",
       "      <td>0.905491</td>\n",
       "      <td>0.116287</td>\n",
       "      <td>2.378990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>132.75</td>\n",
       "      <td>36.25</td>\n",
       "      <td>-0.569101</td>\n",
       "      <td>1.014305</td>\n",
       "      <td>0.715598</td>\n",
       "      <td>2.265907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>-174.75</td>\n",
       "      <td>62.75</td>\n",
       "      <td>-3.048693</td>\n",
       "      <td>1.109700</td>\n",
       "      <td>-1.436857</td>\n",
       "      <td>-0.088224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6572</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>-175.25</td>\n",
       "      <td>62.75</td>\n",
       "      <td>-2.529471</td>\n",
       "      <td>1.098075</td>\n",
       "      <td>-1.431604</td>\n",
       "      <td>-0.107611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6577</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>1.036706</td>\n",
       "      <td>0.767869</td>\n",
       "      <td>1.497568</td>\n",
       "      <td>-0.141768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6583</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>-176.25</td>\n",
       "      <td>61.25</td>\n",
       "      <td>-3.318982</td>\n",
       "      <td>1.238195</td>\n",
       "      <td>-1.310038</td>\n",
       "      <td>-0.158595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6584</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>-145.25</td>\n",
       "      <td>58.75</td>\n",
       "      <td>-1.005445</td>\n",
       "      <td>1.098799</td>\n",
       "      <td>-0.640617</td>\n",
       "      <td>-0.162298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1321 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     lon    lat      fCO2       Chl      Temp      Salt\n",
       "2     1998/7/16 -111.25  23.25  1.835711  0.874641  0.896934  3.617013\n",
       "11    1998/7/16 -115.75  28.25  0.341512  1.191270  0.169563  2.930019\n",
       "22    1998/7/16  125.75  35.75  0.091208  2.126348  0.537075  2.406186\n",
       "23    1998/7/16 -116.75  30.75  0.490971  0.905491  0.116287  2.378990\n",
       "26    1998/7/16  132.75  36.25 -0.569101  1.014305  0.715598  2.265907\n",
       "...         ...     ...    ...       ...       ...       ...       ...\n",
       "6570  2020/7/16 -174.75  62.75 -3.048693  1.109700 -1.436857 -0.088224\n",
       "6572  2020/7/16 -175.25  62.75 -2.529471  1.098075 -1.431604 -0.107611\n",
       "6577  2020/7/16  137.75  10.75  1.036706  0.767869  1.497568 -0.141768\n",
       "6583  2020/7/16 -176.25  61.25 -3.318982  1.238195 -1.310038 -0.158595\n",
       "6584  2020/7/16 -145.25  58.75 -1.005445  1.098799 -0.640617 -0.162298\n",
       "\n",
       "[1321 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(my_set):\n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    dataset = my_set.reset_index(drop=True)\n",
    "    ycor = dataset.lat\n",
    "    #ycor = dataset.lon\n",
    "    label = dataset.fCO2\n",
    "\n",
    "    temp_df['label'] = label\n",
    "\n",
    "    temp_df['beta'] = np.ones(dataset.shape[0])\n",
    "    temp_df['Chl'] = dataset.Chl\n",
    "    temp_df['Temp'] = dataset.Temp\n",
    "    temp_df['Salt'] = dataset.Salt\n",
    "\n",
    "    alist = dataset.lon\n",
    "    temp = []\n",
    "    for i in alist:\n",
    "        if i < 0:\n",
    "            i = i+360\n",
    "        temp.append(i)\n",
    "    xcor = temp\n",
    "\n",
    "    cor_df = pd.DataFrame()\n",
    "    cor_df['xcor'] = xcor\n",
    "    cor_df['ycor'] = ycor\n",
    "\n",
    "    a = [[110.0, 0.0], [290.0,0.0], [110.0, 70.0], [290.0, 70.0]]\n",
    "    b = np.array(a)\n",
    "\n",
    "    cor_li = cor_df.to_numpy()\n",
    "    dis_li = compute_distances(cor_li, b)\n",
    "    dis_df = pd.DataFrame(dis_li)\n",
    "    temp_df = temp_df.join(dis_df)\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "\n",
    "train_data = MYDataset(process_df(my_set=train_set))\n",
    "test_data = MYDataset(process_df(my_set=test_set))\n",
    "train_loader = DataLoader(train_data, batch_size=50, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=50, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>fCO2</th>\n",
       "      <th>Chl</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Salt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-110.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>1.429020</td>\n",
       "      <td>0.932204</td>\n",
       "      <td>1.035501</td>\n",
       "      <td>3.714037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-111.75</td>\n",
       "      <td>22.25</td>\n",
       "      <td>0.653314</td>\n",
       "      <td>0.865872</td>\n",
       "      <td>0.980157</td>\n",
       "      <td>3.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-111.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.835711</td>\n",
       "      <td>0.874641</td>\n",
       "      <td>0.896934</td>\n",
       "      <td>3.617013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-112.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.925815</td>\n",
       "      <td>0.861615</td>\n",
       "      <td>0.827733</td>\n",
       "      <td>3.544569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-113.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>1.074433</td>\n",
       "      <td>0.922670</td>\n",
       "      <td>0.545926</td>\n",
       "      <td>3.229704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.75</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.178217</td>\n",
       "      <td>0.770692</td>\n",
       "      <td>1.481354</td>\n",
       "      <td>-0.398306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.25</td>\n",
       "      <td>12.75</td>\n",
       "      <td>1.116893</td>\n",
       "      <td>0.768039</td>\n",
       "      <td>1.489461</td>\n",
       "      <td>-0.467678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.75</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.849520</td>\n",
       "      <td>0.791038</td>\n",
       "      <td>1.508860</td>\n",
       "      <td>-0.471190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.25</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.219482</td>\n",
       "      <td>0.770857</td>\n",
       "      <td>1.481271</td>\n",
       "      <td>-0.520788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>136.75</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.149187</td>\n",
       "      <td>0.770841</td>\n",
       "      <td>1.480279</td>\n",
       "      <td>-0.613381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5283 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     lon    lat      fCO2       Chl      Temp      Salt\n",
       "0     1998/7/16 -110.25  22.25  1.429020  0.932204  1.035501  3.714037\n",
       "1     1998/7/16 -111.75  22.25  0.653314  0.865872  0.980157  3.665900\n",
       "2     1998/7/16 -111.25  23.25  1.835711  0.874641  0.896934  3.617013\n",
       "3     1998/7/16 -112.25  23.25  1.925815  0.861615  0.827733  3.544569\n",
       "4     1998/7/16 -113.25  25.25  1.074433  0.922670  0.545926  3.229704\n",
       "...         ...     ...    ...       ...       ...       ...       ...\n",
       "5278  2020/7/16  137.75  13.25  1.178217  0.770692  1.481354 -0.398306\n",
       "5279  2020/7/16  137.25  12.75  1.116893  0.768039  1.489461 -0.467678\n",
       "5280  2020/7/16  137.75   8.75  0.849520  0.791038  1.508860 -0.471190\n",
       "5281  2020/7/16  137.25  14.25  1.219482  0.770857  1.481271 -0.520788\n",
       "5282  2020/7/16  136.75  14.25  1.149187  0.770841  1.480279 -0.613381\n",
       "\n",
       "[5283 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWR(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(GNNWR, self).__init__()\n",
    "        self.insize = insize\n",
    "        self.outsize = outsize\n",
    "\n",
    "        lastsize = self.insize\n",
    "        thissize = 0\n",
    "        self.fc = nn.Sequential()\n",
    "        i = 2\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(1), nn.Linear(4, 600))\n",
    "        # self.fc.add_module(\"batc\"+str(1), nn.BatchNorm1d(600))\n",
    "        # self.fc.add_module(\"acti\"+str(1), nn.PReLU(init=0.4))\n",
    "        # self.fc.add_module(\"drop\"+str(1), nn.Dropout(0.2))\n",
    "\n",
    "        lastsize = 600\n",
    "        while math.pow(2, int(math.log2(lastsize))) >= max(128, outsize + 1):\n",
    "            if i == 1:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize))))\n",
    "            else:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize)) - 1))\n",
    "            \n",
    "            self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, thissize))\n",
    "            self.fc.add_module(\"batc\"+str(i), nn.BatchNorm1d(thissize))\n",
    "            self.fc.add_module(\"acti\"+str(i), nn.PReLU(init=0.4))\n",
    "            \n",
    "            self.fc.add_module(\"drop\"+str(i), nn.Dropout(0.2))\n",
    "\n",
    "            lastsize = thissize\n",
    "            i = i + 1\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, outsize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = GNNWR(623, 4)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "\n",
    "r2 = 0\n",
    "weightlist = []\n",
    "for i in range(1,2):\n",
    "    temp = []\n",
    "    temp.append(-0.172075)\n",
    "    temp.append(-0.175203)\n",
    "    temp.append(0.294790)\n",
    "    temp.append(0.385374)\n",
    "    weightlist.append(temp)\n",
    "out = nn.Linear(4, 1, bias = False)\n",
    "out.weight = nn.Parameter(torch.tensor(weightlist), requires_grad=False)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    global r2\n",
    "    global out\n",
    "    for data, coef, label in train_loader:\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        label = label.view(data.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        output = output.mul(coef)\n",
    "        output = out(output)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        a = output.view(-1).detach().numpy()\n",
    "        b = label.view(-1).numpy()\n",
    "        if epoch % 100 == 0:\n",
    "            r2 = r2_score(a, b)\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "\n",
    "def val(epoch):\n",
    "    model.eval()\n",
    "    global out\n",
    "    global r2\n",
    "    val_loss = 0\n",
    "\n",
    "    label_li = np.array([])\n",
    "    out_li = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, coef, label in test_loader:\n",
    "            data = data.view(data.shape[0], -1)\n",
    "            label = label.view(data.shape[0], -1)\n",
    "\n",
    "            output = model(data)\n",
    "            output = output.mul(coef)\n",
    "            output = out(output)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            a = output.view(-1).detach().numpy()\n",
    "            b = label.view(-1).numpy()\n",
    "            out_li = np.append(out_li, a)\n",
    "            label_li = np.append(label_li, b)\n",
    "            \n",
    "\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "        val_loss = val_loss/len(test_loader.dataset)\n",
    "        label_li = np.array(label_li).reshape(-1)\n",
    "        out_li = np.array(out_li).reshape(-1)\n",
    "        if epoch % 100 == 0:\n",
    "            r2 = r2_score(out_li, label_li)\n",
    "        #print(out_li)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tR2: {:.6f}'.format(epoch, val_loss, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.634171\n",
      "Epoch: 1 \tTraining Loss: 0.497975 \tR2: 0.000000\n",
      "Epoch: 2 \tTraining Loss: 0.636066\n",
      "Epoch: 2 \tTraining Loss: 0.507799 \tR2: 0.000000\n",
      "Epoch: 3 \tTraining Loss: 0.636882\n",
      "Epoch: 3 \tTraining Loss: 0.488938 \tR2: 0.000000\n",
      "Epoch: 4 \tTraining Loss: 0.627441\n",
      "Epoch: 4 \tTraining Loss: 0.482778 \tR2: 0.000000\n",
      "Epoch: 5 \tTraining Loss: 0.625009\n",
      "Epoch: 5 \tTraining Loss: 0.491593 \tR2: 0.000000\n",
      "Epoch: 6 \tTraining Loss: 0.618379\n",
      "Epoch: 6 \tTraining Loss: 0.520957 \tR2: 0.000000\n",
      "Epoch: 7 \tTraining Loss: 0.635133\n",
      "Epoch: 7 \tTraining Loss: 0.498776 \tR2: 0.000000\n",
      "Epoch: 8 \tTraining Loss: 0.645775\n",
      "Epoch: 8 \tTraining Loss: 0.496582 \tR2: 0.000000\n",
      "Epoch: 9 \tTraining Loss: 0.619559\n",
      "Epoch: 9 \tTraining Loss: 0.480638 \tR2: 0.000000\n",
      "Epoch: 10 \tTraining Loss: 0.627675\n",
      "Epoch: 10 \tTraining Loss: 0.506069 \tR2: 0.000000\n",
      "Epoch: 11 \tTraining Loss: 0.630460\n",
      "Epoch: 11 \tTraining Loss: 0.515857 \tR2: 0.000000\n",
      "Epoch: 12 \tTraining Loss: 0.615655\n",
      "Epoch: 12 \tTraining Loss: 0.475490 \tR2: 0.000000\n",
      "Epoch: 13 \tTraining Loss: 0.621911\n",
      "Epoch: 13 \tTraining Loss: 0.483892 \tR2: 0.000000\n",
      "Epoch: 14 \tTraining Loss: 0.636325\n",
      "Epoch: 14 \tTraining Loss: 0.479090 \tR2: 0.000000\n",
      "Epoch: 15 \tTraining Loss: 0.623194\n",
      "Epoch: 15 \tTraining Loss: 0.499706 \tR2: 0.000000\n",
      "Epoch: 16 \tTraining Loss: 0.628197\n",
      "Epoch: 16 \tTraining Loss: 0.473182 \tR2: 0.000000\n",
      "Epoch: 17 \tTraining Loss: 0.627975\n",
      "Epoch: 17 \tTraining Loss: 0.497835 \tR2: 0.000000\n",
      "Epoch: 18 \tTraining Loss: 0.615361\n",
      "Epoch: 18 \tTraining Loss: 0.484447 \tR2: 0.000000\n",
      "Epoch: 19 \tTraining Loss: 0.625432\n",
      "Epoch: 19 \tTraining Loss: 0.479753 \tR2: 0.000000\n",
      "Epoch: 20 \tTraining Loss: 0.624674\n",
      "Epoch: 20 \tTraining Loss: 0.484004 \tR2: 0.000000\n",
      "Epoch: 21 \tTraining Loss: 0.632407\n",
      "Epoch: 21 \tTraining Loss: 0.529696 \tR2: 0.000000\n",
      "Epoch: 22 \tTraining Loss: 0.627800\n",
      "Epoch: 22 \tTraining Loss: 0.494080 \tR2: 0.000000\n",
      "Epoch: 23 \tTraining Loss: 0.623215\n",
      "Epoch: 23 \tTraining Loss: 0.470003 \tR2: 0.000000\n",
      "Epoch: 24 \tTraining Loss: 0.628717\n",
      "Epoch: 24 \tTraining Loss: 0.491332 \tR2: 0.000000\n",
      "Epoch: 25 \tTraining Loss: 0.634068\n",
      "Epoch: 25 \tTraining Loss: 0.493124 \tR2: 0.000000\n",
      "Epoch: 26 \tTraining Loss: 0.631722\n",
      "Epoch: 26 \tTraining Loss: 0.506045 \tR2: 0.000000\n",
      "Epoch: 27 \tTraining Loss: 0.625589\n",
      "Epoch: 27 \tTraining Loss: 0.473624 \tR2: 0.000000\n",
      "Epoch: 28 \tTraining Loss: 0.602723\n",
      "Epoch: 28 \tTraining Loss: 0.476315 \tR2: 0.000000\n",
      "Epoch: 29 \tTraining Loss: 0.627404\n",
      "Epoch: 29 \tTraining Loss: 0.510774 \tR2: 0.000000\n",
      "Epoch: 30 \tTraining Loss: 0.608675\n",
      "Epoch: 30 \tTraining Loss: 0.476316 \tR2: 0.000000\n",
      "Epoch: 31 \tTraining Loss: 0.621984\n",
      "Epoch: 31 \tTraining Loss: 0.478638 \tR2: 0.000000\n",
      "Epoch: 32 \tTraining Loss: 0.615915\n",
      "Epoch: 32 \tTraining Loss: 0.485783 \tR2: 0.000000\n",
      "Epoch: 33 \tTraining Loss: 0.623102\n",
      "Epoch: 33 \tTraining Loss: 0.483900 \tR2: 0.000000\n",
      "Epoch: 34 \tTraining Loss: 0.631744\n",
      "Epoch: 34 \tTraining Loss: 0.483648 \tR2: 0.000000\n",
      "Epoch: 35 \tTraining Loss: 0.612554\n",
      "Epoch: 35 \tTraining Loss: 0.481388 \tR2: 0.000000\n",
      "Epoch: 36 \tTraining Loss: 0.620606\n",
      "Epoch: 36 \tTraining Loss: 0.483147 \tR2: 0.000000\n",
      "Epoch: 37 \tTraining Loss: 0.627030\n",
      "Epoch: 37 \tTraining Loss: 0.483788 \tR2: 0.000000\n",
      "Epoch: 38 \tTraining Loss: 0.618332\n",
      "Epoch: 38 \tTraining Loss: 0.457263 \tR2: 0.000000\n",
      "Epoch: 39 \tTraining Loss: 0.621841\n",
      "Epoch: 39 \tTraining Loss: 0.479254 \tR2: 0.000000\n",
      "Epoch: 40 \tTraining Loss: 0.634254\n",
      "Epoch: 40 \tTraining Loss: 0.471040 \tR2: 0.000000\n",
      "Epoch: 41 \tTraining Loss: 0.619292\n",
      "Epoch: 41 \tTraining Loss: 0.481030 \tR2: 0.000000\n",
      "Epoch: 42 \tTraining Loss: 0.634299\n",
      "Epoch: 42 \tTraining Loss: 0.473120 \tR2: 0.000000\n",
      "Epoch: 43 \tTraining Loss: 0.629825\n",
      "Epoch: 43 \tTraining Loss: 0.513747 \tR2: 0.000000\n",
      "Epoch: 44 \tTraining Loss: 0.613940\n",
      "Epoch: 44 \tTraining Loss: 0.500582 \tR2: 0.000000\n",
      "Epoch: 45 \tTraining Loss: 0.630470\n",
      "Epoch: 45 \tTraining Loss: 0.491181 \tR2: 0.000000\n",
      "Epoch: 46 \tTraining Loss: 0.622512\n",
      "Epoch: 46 \tTraining Loss: 0.480717 \tR2: 0.000000\n",
      "Epoch: 47 \tTraining Loss: 0.630581\n",
      "Epoch: 47 \tTraining Loss: 0.480207 \tR2: 0.000000\n",
      "Epoch: 48 \tTraining Loss: 0.612895\n",
      "Epoch: 48 \tTraining Loss: 0.486415 \tR2: 0.000000\n",
      "Epoch: 49 \tTraining Loss: 0.618382\n",
      "Epoch: 49 \tTraining Loss: 0.541700 \tR2: 0.000000\n",
      "Epoch: 50 \tTraining Loss: 0.631730\n",
      "Epoch: 50 \tTraining Loss: 0.466688 \tR2: 0.000000\n",
      "Epoch: 51 \tTraining Loss: 0.614645\n",
      "Epoch: 51 \tTraining Loss: 0.468661 \tR2: 0.000000\n",
      "Epoch: 52 \tTraining Loss: 0.617345\n",
      "Epoch: 52 \tTraining Loss: 0.492802 \tR2: 0.000000\n",
      "Epoch: 53 \tTraining Loss: 0.631683\n",
      "Epoch: 53 \tTraining Loss: 0.472973 \tR2: 0.000000\n",
      "Epoch: 54 \tTraining Loss: 0.617832\n",
      "Epoch: 54 \tTraining Loss: 0.491985 \tR2: 0.000000\n",
      "Epoch: 55 \tTraining Loss: 0.611993\n",
      "Epoch: 55 \tTraining Loss: 0.486270 \tR2: 0.000000\n",
      "Epoch: 56 \tTraining Loss: 0.611120\n",
      "Epoch: 56 \tTraining Loss: 0.476065 \tR2: 0.000000\n",
      "Epoch: 57 \tTraining Loss: 0.621668\n",
      "Epoch: 57 \tTraining Loss: 0.475077 \tR2: 0.000000\n",
      "Epoch: 58 \tTraining Loss: 0.614289\n",
      "Epoch: 58 \tTraining Loss: 0.462834 \tR2: 0.000000\n",
      "Epoch: 59 \tTraining Loss: 0.610529\n",
      "Epoch: 59 \tTraining Loss: 0.502662 \tR2: 0.000000\n",
      "Epoch: 60 \tTraining Loss: 0.619584\n",
      "Epoch: 60 \tTraining Loss: 0.457491 \tR2: 0.000000\n",
      "Epoch: 61 \tTraining Loss: 0.617321\n",
      "Epoch: 61 \tTraining Loss: 0.461926 \tR2: 0.000000\n",
      "Epoch: 62 \tTraining Loss: 0.620861\n",
      "Epoch: 62 \tTraining Loss: 0.495191 \tR2: 0.000000\n",
      "Epoch: 63 \tTraining Loss: 0.616185\n",
      "Epoch: 63 \tTraining Loss: 0.499689 \tR2: 0.000000\n",
      "Epoch: 64 \tTraining Loss: 0.604808\n",
      "Epoch: 64 \tTraining Loss: 0.460811 \tR2: 0.000000\n",
      "Epoch: 65 \tTraining Loss: 0.627464\n",
      "Epoch: 65 \tTraining Loss: 0.475229 \tR2: 0.000000\n",
      "Epoch: 66 \tTraining Loss: 0.624099\n",
      "Epoch: 66 \tTraining Loss: 0.486600 \tR2: 0.000000\n",
      "Epoch: 67 \tTraining Loss: 0.613246\n",
      "Epoch: 67 \tTraining Loss: 0.457342 \tR2: 0.000000\n",
      "Epoch: 68 \tTraining Loss: 0.609539\n",
      "Epoch: 68 \tTraining Loss: 0.458362 \tR2: 0.000000\n",
      "Epoch: 69 \tTraining Loss: 0.627193\n",
      "Epoch: 69 \tTraining Loss: 0.490220 \tR2: 0.000000\n",
      "Epoch: 70 \tTraining Loss: 0.612771\n",
      "Epoch: 70 \tTraining Loss: 0.465802 \tR2: 0.000000\n",
      "Epoch: 71 \tTraining Loss: 0.619686\n",
      "Epoch: 71 \tTraining Loss: 0.558686 \tR2: 0.000000\n",
      "Epoch: 72 \tTraining Loss: 0.621456\n",
      "Epoch: 72 \tTraining Loss: 0.478882 \tR2: 0.000000\n",
      "Epoch: 73 \tTraining Loss: 0.612783\n",
      "Epoch: 73 \tTraining Loss: 0.485863 \tR2: 0.000000\n",
      "Epoch: 74 \tTraining Loss: 0.622621\n",
      "Epoch: 74 \tTraining Loss: 0.492060 \tR2: 0.000000\n",
      "Epoch: 75 \tTraining Loss: 0.606219\n",
      "Epoch: 75 \tTraining Loss: 0.487812 \tR2: 0.000000\n",
      "Epoch: 76 \tTraining Loss: 0.605773\n",
      "Epoch: 76 \tTraining Loss: 0.482743 \tR2: 0.000000\n",
      "Epoch: 77 \tTraining Loss: 0.616469\n",
      "Epoch: 77 \tTraining Loss: 0.471854 \tR2: 0.000000\n",
      "Epoch: 78 \tTraining Loss: 0.609346\n",
      "Epoch: 78 \tTraining Loss: 0.490051 \tR2: 0.000000\n",
      "Epoch: 79 \tTraining Loss: 0.612683\n",
      "Epoch: 79 \tTraining Loss: 0.481207 \tR2: 0.000000\n",
      "Epoch: 80 \tTraining Loss: 0.605228\n",
      "Epoch: 80 \tTraining Loss: 0.486364 \tR2: 0.000000\n",
      "Epoch: 81 \tTraining Loss: 0.612850\n",
      "Epoch: 81 \tTraining Loss: 0.489733 \tR2: 0.000000\n",
      "Epoch: 82 \tTraining Loss: 0.606412\n",
      "Epoch: 82 \tTraining Loss: 0.494297 \tR2: 0.000000\n",
      "Epoch: 83 \tTraining Loss: 0.604562\n",
      "Epoch: 83 \tTraining Loss: 0.485305 \tR2: 0.000000\n",
      "Epoch: 84 \tTraining Loss: 0.629600\n",
      "Epoch: 84 \tTraining Loss: 0.476169 \tR2: 0.000000\n",
      "Epoch: 85 \tTraining Loss: 0.616580\n",
      "Epoch: 85 \tTraining Loss: 0.480714 \tR2: 0.000000\n",
      "Epoch: 86 \tTraining Loss: 0.599725\n",
      "Epoch: 86 \tTraining Loss: 0.477903 \tR2: 0.000000\n",
      "Epoch: 87 \tTraining Loss: 0.620442\n",
      "Epoch: 87 \tTraining Loss: 0.488700 \tR2: 0.000000\n",
      "Epoch: 88 \tTraining Loss: 0.603146\n",
      "Epoch: 88 \tTraining Loss: 0.475138 \tR2: 0.000000\n",
      "Epoch: 89 \tTraining Loss: 0.615382\n",
      "Epoch: 89 \tTraining Loss: 0.488019 \tR2: 0.000000\n",
      "Epoch: 90 \tTraining Loss: 0.607490\n",
      "Epoch: 90 \tTraining Loss: 0.473128 \tR2: 0.000000\n",
      "Epoch: 91 \tTraining Loss: 0.625942\n",
      "Epoch: 91 \tTraining Loss: 0.501778 \tR2: 0.000000\n",
      "Epoch: 92 \tTraining Loss: 0.596185\n",
      "Epoch: 92 \tTraining Loss: 0.468971 \tR2: 0.000000\n",
      "Epoch: 93 \tTraining Loss: 0.616995\n",
      "Epoch: 93 \tTraining Loss: 0.483467 \tR2: 0.000000\n",
      "Epoch: 94 \tTraining Loss: 0.618066\n",
      "Epoch: 94 \tTraining Loss: 0.455287 \tR2: 0.000000\n",
      "Epoch: 95 \tTraining Loss: 0.617386\n",
      "Epoch: 95 \tTraining Loss: 0.463300 \tR2: 0.000000\n",
      "Epoch: 96 \tTraining Loss: 0.600969\n",
      "Epoch: 96 \tTraining Loss: 0.465129 \tR2: 0.000000\n",
      "Epoch: 97 \tTraining Loss: 0.601318\n",
      "Epoch: 97 \tTraining Loss: 0.462405 \tR2: 0.000000\n",
      "Epoch: 98 \tTraining Loss: 0.606537\n",
      "Epoch: 98 \tTraining Loss: 0.479883 \tR2: 0.000000\n",
      "Epoch: 99 \tTraining Loss: 0.617870\n",
      "Epoch: 99 \tTraining Loss: 0.474963 \tR2: 0.000000\n",
      "Epoch: 100 \tTraining Loss: 0.609188\n",
      "Epoch: 100 \tTraining Loss: 0.472599 \tR2: 0.443612\n",
      "Epoch: 101 \tTraining Loss: 0.608627\n",
      "Epoch: 101 \tTraining Loss: 0.464352 \tR2: 0.443612\n",
      "Epoch: 102 \tTraining Loss: 0.611892\n",
      "Epoch: 102 \tTraining Loss: 0.477129 \tR2: 0.443612\n",
      "Epoch: 103 \tTraining Loss: 0.623725\n",
      "Epoch: 103 \tTraining Loss: 0.466306 \tR2: 0.443612\n",
      "Epoch: 104 \tTraining Loss: 0.623115\n",
      "Epoch: 104 \tTraining Loss: 0.473887 \tR2: 0.443612\n",
      "Epoch: 105 \tTraining Loss: 0.599537\n",
      "Epoch: 105 \tTraining Loss: 0.509785 \tR2: 0.443612\n",
      "Epoch: 106 \tTraining Loss: 0.603457\n",
      "Epoch: 106 \tTraining Loss: 0.480005 \tR2: 0.443612\n",
      "Epoch: 107 \tTraining Loss: 0.624226\n",
      "Epoch: 107 \tTraining Loss: 0.456867 \tR2: 0.443612\n",
      "Epoch: 108 \tTraining Loss: 0.608271\n",
      "Epoch: 108 \tTraining Loss: 0.475594 \tR2: 0.443612\n",
      "Epoch: 109 \tTraining Loss: 0.581430\n",
      "Epoch: 109 \tTraining Loss: 0.466180 \tR2: 0.443612\n",
      "Epoch: 110 \tTraining Loss: 0.599024\n",
      "Epoch: 110 \tTraining Loss: 0.467702 \tR2: 0.443612\n",
      "Epoch: 111 \tTraining Loss: 0.592825\n",
      "Epoch: 111 \tTraining Loss: 0.452136 \tR2: 0.443612\n",
      "Epoch: 112 \tTraining Loss: 0.600933\n",
      "Epoch: 112 \tTraining Loss: 0.477658 \tR2: 0.443612\n",
      "Epoch: 113 \tTraining Loss: 0.616889\n",
      "Epoch: 113 \tTraining Loss: 0.502579 \tR2: 0.443612\n",
      "Epoch: 114 \tTraining Loss: 0.610353\n",
      "Epoch: 114 \tTraining Loss: 0.462353 \tR2: 0.443612\n",
      "Epoch: 115 \tTraining Loss: 0.610640\n",
      "Epoch: 115 \tTraining Loss: 0.473794 \tR2: 0.443612\n",
      "Epoch: 116 \tTraining Loss: 0.600891\n",
      "Epoch: 116 \tTraining Loss: 0.469086 \tR2: 0.443612\n",
      "Epoch: 117 \tTraining Loss: 0.609026\n",
      "Epoch: 117 \tTraining Loss: 0.443585 \tR2: 0.443612\n",
      "Epoch: 118 \tTraining Loss: 0.592581\n",
      "Epoch: 118 \tTraining Loss: 0.444065 \tR2: 0.443612\n",
      "Epoch: 119 \tTraining Loss: 0.589730\n",
      "Epoch: 119 \tTraining Loss: 0.479346 \tR2: 0.443612\n",
      "Epoch: 120 \tTraining Loss: 0.597577\n",
      "Epoch: 120 \tTraining Loss: 0.466512 \tR2: 0.443612\n",
      "Epoch: 121 \tTraining Loss: 0.596942\n",
      "Epoch: 121 \tTraining Loss: 0.457073 \tR2: 0.443612\n",
      "Epoch: 122 \tTraining Loss: 0.599899\n",
      "Epoch: 122 \tTraining Loss: 0.478763 \tR2: 0.443612\n",
      "Epoch: 123 \tTraining Loss: 0.607554\n",
      "Epoch: 123 \tTraining Loss: 0.477859 \tR2: 0.443612\n",
      "Epoch: 124 \tTraining Loss: 0.598612\n",
      "Epoch: 124 \tTraining Loss: 0.466694 \tR2: 0.443612\n",
      "Epoch: 125 \tTraining Loss: 0.615689\n",
      "Epoch: 125 \tTraining Loss: 0.456090 \tR2: 0.443612\n",
      "Epoch: 126 \tTraining Loss: 0.596134\n",
      "Epoch: 126 \tTraining Loss: 0.461454 \tR2: 0.443612\n",
      "Epoch: 127 \tTraining Loss: 0.606946\n",
      "Epoch: 127 \tTraining Loss: 0.466544 \tR2: 0.443612\n",
      "Epoch: 128 \tTraining Loss: 0.594734\n",
      "Epoch: 128 \tTraining Loss: 0.441053 \tR2: 0.443612\n",
      "Epoch: 129 \tTraining Loss: 0.607817\n",
      "Epoch: 129 \tTraining Loss: 0.464191 \tR2: 0.443612\n",
      "Epoch: 130 \tTraining Loss: 0.599370\n",
      "Epoch: 130 \tTraining Loss: 0.515168 \tR2: 0.443612\n",
      "Epoch: 131 \tTraining Loss: 0.608117\n",
      "Epoch: 131 \tTraining Loss: 0.560169 \tR2: 0.443612\n",
      "Epoch: 132 \tTraining Loss: 0.598768\n",
      "Epoch: 132 \tTraining Loss: 0.478152 \tR2: 0.443612\n",
      "Epoch: 133 \tTraining Loss: 0.602259\n",
      "Epoch: 133 \tTraining Loss: 0.488253 \tR2: 0.443612\n",
      "Epoch: 134 \tTraining Loss: 0.599148\n",
      "Epoch: 134 \tTraining Loss: 0.475330 \tR2: 0.443612\n",
      "Epoch: 135 \tTraining Loss: 0.611712\n",
      "Epoch: 135 \tTraining Loss: 0.451325 \tR2: 0.443612\n",
      "Epoch: 136 \tTraining Loss: 0.617435\n",
      "Epoch: 136 \tTraining Loss: 0.474855 \tR2: 0.443612\n",
      "Epoch: 137 \tTraining Loss: 0.589091\n",
      "Epoch: 137 \tTraining Loss: 0.454479 \tR2: 0.443612\n",
      "Epoch: 138 \tTraining Loss: 0.607664\n",
      "Epoch: 138 \tTraining Loss: 0.462667 \tR2: 0.443612\n",
      "Epoch: 139 \tTraining Loss: 0.607851\n",
      "Epoch: 139 \tTraining Loss: 0.503175 \tR2: 0.443612\n",
      "Epoch: 140 \tTraining Loss: 0.591622\n",
      "Epoch: 140 \tTraining Loss: 0.509519 \tR2: 0.443612\n",
      "Epoch: 141 \tTraining Loss: 0.596770\n",
      "Epoch: 141 \tTraining Loss: 0.454924 \tR2: 0.443612\n",
      "Epoch: 142 \tTraining Loss: 0.607495\n",
      "Epoch: 142 \tTraining Loss: 0.467579 \tR2: 0.443612\n",
      "Epoch: 143 \tTraining Loss: 0.591728\n",
      "Epoch: 143 \tTraining Loss: 0.487779 \tR2: 0.443612\n",
      "Epoch: 144 \tTraining Loss: 0.597292\n",
      "Epoch: 144 \tTraining Loss: 0.462787 \tR2: 0.443612\n",
      "Epoch: 145 \tTraining Loss: 0.592390\n",
      "Epoch: 145 \tTraining Loss: 0.498591 \tR2: 0.443612\n",
      "Epoch: 146 \tTraining Loss: 0.596929\n",
      "Epoch: 146 \tTraining Loss: 0.458357 \tR2: 0.443612\n",
      "Epoch: 147 \tTraining Loss: 0.597251\n",
      "Epoch: 147 \tTraining Loss: 0.471779 \tR2: 0.443612\n",
      "Epoch: 148 \tTraining Loss: 0.605270\n",
      "Epoch: 148 \tTraining Loss: 0.439525 \tR2: 0.443612\n",
      "Epoch: 149 \tTraining Loss: 0.601276\n",
      "Epoch: 149 \tTraining Loss: 0.460473 \tR2: 0.443612\n",
      "Epoch: 150 \tTraining Loss: 0.597419\n",
      "Epoch: 150 \tTraining Loss: 0.464607 \tR2: 0.443612\n",
      "Epoch: 151 \tTraining Loss: 0.599427\n",
      "Epoch: 151 \tTraining Loss: 0.461840 \tR2: 0.443612\n",
      "Epoch: 152 \tTraining Loss: 0.595038\n",
      "Epoch: 152 \tTraining Loss: 0.453906 \tR2: 0.443612\n",
      "Epoch: 153 \tTraining Loss: 0.588834\n",
      "Epoch: 153 \tTraining Loss: 0.463162 \tR2: 0.443612\n",
      "Epoch: 154 \tTraining Loss: 0.602315\n",
      "Epoch: 154 \tTraining Loss: 0.496599 \tR2: 0.443612\n",
      "Epoch: 155 \tTraining Loss: 0.603574\n",
      "Epoch: 155 \tTraining Loss: 0.452361 \tR2: 0.443612\n",
      "Epoch: 156 \tTraining Loss: 0.616804\n",
      "Epoch: 156 \tTraining Loss: 0.463948 \tR2: 0.443612\n",
      "Epoch: 157 \tTraining Loss: 0.617355\n",
      "Epoch: 157 \tTraining Loss: 0.463992 \tR2: 0.443612\n",
      "Epoch: 158 \tTraining Loss: 0.599009\n",
      "Epoch: 158 \tTraining Loss: 0.465519 \tR2: 0.443612\n",
      "Epoch: 159 \tTraining Loss: 0.597876\n",
      "Epoch: 159 \tTraining Loss: 0.465652 \tR2: 0.443612\n",
      "Epoch: 160 \tTraining Loss: 0.614462\n",
      "Epoch: 160 \tTraining Loss: 0.487994 \tR2: 0.443612\n",
      "Epoch: 161 \tTraining Loss: 0.612698\n",
      "Epoch: 161 \tTraining Loss: 0.485109 \tR2: 0.443612\n",
      "Epoch: 162 \tTraining Loss: 0.604746\n",
      "Epoch: 162 \tTraining Loss: 0.465886 \tR2: 0.443612\n",
      "Epoch: 163 \tTraining Loss: 0.589057\n",
      "Epoch: 163 \tTraining Loss: 0.464649 \tR2: 0.443612\n",
      "Epoch: 164 \tTraining Loss: 0.585292\n",
      "Epoch: 164 \tTraining Loss: 0.472557 \tR2: 0.443612\n",
      "Epoch: 165 \tTraining Loss: 0.618454\n",
      "Epoch: 165 \tTraining Loss: 0.480819 \tR2: 0.443612\n",
      "Epoch: 166 \tTraining Loss: 0.597767\n",
      "Epoch: 166 \tTraining Loss: 0.487741 \tR2: 0.443612\n",
      "Epoch: 167 \tTraining Loss: 0.593947\n",
      "Epoch: 167 \tTraining Loss: 0.490860 \tR2: 0.443612\n",
      "Epoch: 168 \tTraining Loss: 0.605445\n",
      "Epoch: 168 \tTraining Loss: 0.440447 \tR2: 0.443612\n",
      "Epoch: 169 \tTraining Loss: 0.597450\n",
      "Epoch: 169 \tTraining Loss: 0.456857 \tR2: 0.443612\n",
      "Epoch: 170 \tTraining Loss: 0.599254\n",
      "Epoch: 170 \tTraining Loss: 0.462161 \tR2: 0.443612\n",
      "Epoch: 171 \tTraining Loss: 0.591017\n",
      "Epoch: 171 \tTraining Loss: 0.445063 \tR2: 0.443612\n",
      "Epoch: 172 \tTraining Loss: 0.614035\n",
      "Epoch: 172 \tTraining Loss: 0.475412 \tR2: 0.443612\n",
      "Epoch: 173 \tTraining Loss: 0.607111\n",
      "Epoch: 173 \tTraining Loss: 0.436945 \tR2: 0.443612\n",
      "Epoch: 174 \tTraining Loss: 0.594513\n",
      "Epoch: 174 \tTraining Loss: 0.471200 \tR2: 0.443612\n",
      "Epoch: 175 \tTraining Loss: 0.598094\n",
      "Epoch: 175 \tTraining Loss: 0.454405 \tR2: 0.443612\n",
      "Epoch: 176 \tTraining Loss: 0.589361\n",
      "Epoch: 176 \tTraining Loss: 0.456474 \tR2: 0.443612\n",
      "Epoch: 177 \tTraining Loss: 0.590696\n",
      "Epoch: 177 \tTraining Loss: 0.472272 \tR2: 0.443612\n",
      "Epoch: 178 \tTraining Loss: 0.606940\n",
      "Epoch: 178 \tTraining Loss: 0.489226 \tR2: 0.443612\n",
      "Epoch: 179 \tTraining Loss: 0.574020\n",
      "Epoch: 179 \tTraining Loss: 0.459293 \tR2: 0.443612\n",
      "Epoch: 180 \tTraining Loss: 0.609474\n",
      "Epoch: 180 \tTraining Loss: 0.466558 \tR2: 0.443612\n",
      "Epoch: 181 \tTraining Loss: 0.603058\n",
      "Epoch: 181 \tTraining Loss: 0.500271 \tR2: 0.443612\n",
      "Epoch: 182 \tTraining Loss: 0.589475\n",
      "Epoch: 182 \tTraining Loss: 0.455670 \tR2: 0.443612\n",
      "Epoch: 183 \tTraining Loss: 0.597707\n",
      "Epoch: 183 \tTraining Loss: 0.443089 \tR2: 0.443612\n",
      "Epoch: 184 \tTraining Loss: 0.594956\n",
      "Epoch: 184 \tTraining Loss: 0.446366 \tR2: 0.443612\n",
      "Epoch: 185 \tTraining Loss: 0.591388\n",
      "Epoch: 185 \tTraining Loss: 0.490604 \tR2: 0.443612\n",
      "Epoch: 186 \tTraining Loss: 0.601537\n",
      "Epoch: 186 \tTraining Loss: 0.514841 \tR2: 0.443612\n",
      "Epoch: 187 \tTraining Loss: 0.615429\n",
      "Epoch: 187 \tTraining Loss: 0.515652 \tR2: 0.443612\n",
      "Epoch: 188 \tTraining Loss: 0.606139\n",
      "Epoch: 188 \tTraining Loss: 0.457228 \tR2: 0.443612\n",
      "Epoch: 189 \tTraining Loss: 0.592859\n",
      "Epoch: 189 \tTraining Loss: 0.453781 \tR2: 0.443612\n",
      "Epoch: 190 \tTraining Loss: 0.615608\n",
      "Epoch: 190 \tTraining Loss: 0.448726 \tR2: 0.443612\n",
      "Epoch: 191 \tTraining Loss: 0.607921\n",
      "Epoch: 191 \tTraining Loss: 0.469517 \tR2: 0.443612\n",
      "Epoch: 192 \tTraining Loss: 0.594987\n",
      "Epoch: 192 \tTraining Loss: 0.442578 \tR2: 0.443612\n",
      "Epoch: 193 \tTraining Loss: 0.610705\n",
      "Epoch: 193 \tTraining Loss: 0.465910 \tR2: 0.443612\n",
      "Epoch: 194 \tTraining Loss: 0.598014\n",
      "Epoch: 194 \tTraining Loss: 0.447409 \tR2: 0.443612\n",
      "Epoch: 195 \tTraining Loss: 0.604742\n",
      "Epoch: 195 \tTraining Loss: 0.503388 \tR2: 0.443612\n",
      "Epoch: 196 \tTraining Loss: 0.596246\n",
      "Epoch: 196 \tTraining Loss: 0.447008 \tR2: 0.443612\n",
      "Epoch: 197 \tTraining Loss: 0.591020\n",
      "Epoch: 197 \tTraining Loss: 0.481663 \tR2: 0.443612\n",
      "Epoch: 198 \tTraining Loss: 0.592545\n",
      "Epoch: 198 \tTraining Loss: 0.535083 \tR2: 0.443612\n",
      "Epoch: 199 \tTraining Loss: 0.622654\n",
      "Epoch: 199 \tTraining Loss: 0.442767 \tR2: 0.443612\n",
      "Epoch: 200 \tTraining Loss: 0.584823\n",
      "Epoch: 200 \tTraining Loss: 0.466941 \tR2: 0.456130\n",
      "Epoch: 201 \tTraining Loss: 0.618015\n",
      "Epoch: 201 \tTraining Loss: 0.449802 \tR2: 0.456130\n",
      "Epoch: 202 \tTraining Loss: 0.599251\n",
      "Epoch: 202 \tTraining Loss: 0.461095 \tR2: 0.456130\n",
      "Epoch: 203 \tTraining Loss: 0.575500\n",
      "Epoch: 203 \tTraining Loss: 0.457232 \tR2: 0.456130\n",
      "Epoch: 204 \tTraining Loss: 0.598198\n",
      "Epoch: 204 \tTraining Loss: 0.447988 \tR2: 0.456130\n",
      "Epoch: 205 \tTraining Loss: 0.587489\n",
      "Epoch: 205 \tTraining Loss: 0.488319 \tR2: 0.456130\n",
      "Epoch: 206 \tTraining Loss: 0.598569\n",
      "Epoch: 206 \tTraining Loss: 0.457669 \tR2: 0.456130\n",
      "Epoch: 207 \tTraining Loss: 0.580767\n",
      "Epoch: 207 \tTraining Loss: 0.492805 \tR2: 0.456130\n",
      "Epoch: 208 \tTraining Loss: 0.594106\n",
      "Epoch: 208 \tTraining Loss: 0.482225 \tR2: 0.456130\n",
      "Epoch: 209 \tTraining Loss: 0.601823\n",
      "Epoch: 209 \tTraining Loss: 0.476610 \tR2: 0.456130\n",
      "Epoch: 210 \tTraining Loss: 0.590488\n",
      "Epoch: 210 \tTraining Loss: 0.449581 \tR2: 0.456130\n",
      "Epoch: 211 \tTraining Loss: 0.604676\n",
      "Epoch: 211 \tTraining Loss: 0.429247 \tR2: 0.456130\n",
      "Epoch: 212 \tTraining Loss: 0.606723\n",
      "Epoch: 212 \tTraining Loss: 0.453105 \tR2: 0.456130\n",
      "Epoch: 213 \tTraining Loss: 0.589517\n",
      "Epoch: 213 \tTraining Loss: 0.436870 \tR2: 0.456130\n",
      "Epoch: 214 \tTraining Loss: 0.592651\n",
      "Epoch: 214 \tTraining Loss: 0.507625 \tR2: 0.456130\n",
      "Epoch: 215 \tTraining Loss: 0.597198\n",
      "Epoch: 215 \tTraining Loss: 0.461658 \tR2: 0.456130\n",
      "Epoch: 216 \tTraining Loss: 0.584194\n",
      "Epoch: 216 \tTraining Loss: 0.454985 \tR2: 0.456130\n",
      "Epoch: 217 \tTraining Loss: 0.586458\n",
      "Epoch: 217 \tTraining Loss: 0.475092 \tR2: 0.456130\n",
      "Epoch: 218 \tTraining Loss: 0.599596\n",
      "Epoch: 218 \tTraining Loss: 0.466229 \tR2: 0.456130\n",
      "Epoch: 219 \tTraining Loss: 0.608054\n",
      "Epoch: 219 \tTraining Loss: 0.452432 \tR2: 0.456130\n",
      "Epoch: 220 \tTraining Loss: 0.603170\n",
      "Epoch: 220 \tTraining Loss: 0.467704 \tR2: 0.456130\n",
      "Epoch: 221 \tTraining Loss: 0.596562\n",
      "Epoch: 221 \tTraining Loss: 0.459947 \tR2: 0.456130\n",
      "Epoch: 222 \tTraining Loss: 0.586036\n",
      "Epoch: 222 \tTraining Loss: 0.462309 \tR2: 0.456130\n",
      "Epoch: 223 \tTraining Loss: 0.597806\n",
      "Epoch: 223 \tTraining Loss: 0.440356 \tR2: 0.456130\n",
      "Epoch: 224 \tTraining Loss: 0.605607\n",
      "Epoch: 224 \tTraining Loss: 0.509244 \tR2: 0.456130\n",
      "Epoch: 225 \tTraining Loss: 0.598836\n",
      "Epoch: 225 \tTraining Loss: 0.434917 \tR2: 0.456130\n",
      "Epoch: 226 \tTraining Loss: 0.596829\n",
      "Epoch: 226 \tTraining Loss: 0.498341 \tR2: 0.456130\n",
      "Epoch: 227 \tTraining Loss: 0.596903\n",
      "Epoch: 227 \tTraining Loss: 0.440286 \tR2: 0.456130\n",
      "Epoch: 228 \tTraining Loss: 0.583883\n",
      "Epoch: 228 \tTraining Loss: 0.474935 \tR2: 0.456130\n",
      "Epoch: 229 \tTraining Loss: 0.604354\n",
      "Epoch: 229 \tTraining Loss: 0.452927 \tR2: 0.456130\n",
      "Epoch: 230 \tTraining Loss: 0.582635\n",
      "Epoch: 230 \tTraining Loss: 0.497040 \tR2: 0.456130\n",
      "Epoch: 231 \tTraining Loss: 0.592171\n",
      "Epoch: 231 \tTraining Loss: 0.457566 \tR2: 0.456130\n",
      "Epoch: 232 \tTraining Loss: 0.584400\n",
      "Epoch: 232 \tTraining Loss: 0.455091 \tR2: 0.456130\n",
      "Epoch: 233 \tTraining Loss: 0.581872\n",
      "Epoch: 233 \tTraining Loss: 0.475648 \tR2: 0.456130\n",
      "Epoch: 234 \tTraining Loss: 0.617811\n",
      "Epoch: 234 \tTraining Loss: 0.464848 \tR2: 0.456130\n",
      "Epoch: 235 \tTraining Loss: 0.594671\n",
      "Epoch: 235 \tTraining Loss: 0.439486 \tR2: 0.456130\n",
      "Epoch: 236 \tTraining Loss: 0.605689\n",
      "Epoch: 236 \tTraining Loss: 0.435510 \tR2: 0.456130\n",
      "Epoch: 237 \tTraining Loss: 0.582568\n",
      "Epoch: 237 \tTraining Loss: 0.505945 \tR2: 0.456130\n",
      "Epoch: 238 \tTraining Loss: 0.604063\n",
      "Epoch: 238 \tTraining Loss: 0.469209 \tR2: 0.456130\n",
      "Epoch: 239 \tTraining Loss: 0.596682\n",
      "Epoch: 239 \tTraining Loss: 0.471388 \tR2: 0.456130\n",
      "Epoch: 240 \tTraining Loss: 0.592399\n",
      "Epoch: 240 \tTraining Loss: 0.413503 \tR2: 0.456130\n",
      "Epoch: 241 \tTraining Loss: 0.596544\n",
      "Epoch: 241 \tTraining Loss: 0.451908 \tR2: 0.456130\n",
      "Epoch: 242 \tTraining Loss: 0.583078\n",
      "Epoch: 242 \tTraining Loss: 0.470203 \tR2: 0.456130\n",
      "Epoch: 243 \tTraining Loss: 0.576549\n",
      "Epoch: 243 \tTraining Loss: 0.446232 \tR2: 0.456130\n",
      "Epoch: 244 \tTraining Loss: 0.600120\n",
      "Epoch: 244 \tTraining Loss: 0.505327 \tR2: 0.456130\n",
      "Epoch: 245 \tTraining Loss: 0.607654\n",
      "Epoch: 245 \tTraining Loss: 0.467619 \tR2: 0.456130\n",
      "Epoch: 246 \tTraining Loss: 0.597550\n",
      "Epoch: 246 \tTraining Loss: 0.438470 \tR2: 0.456130\n",
      "Epoch: 247 \tTraining Loss: 0.588764\n",
      "Epoch: 247 \tTraining Loss: 0.445267 \tR2: 0.456130\n",
      "Epoch: 248 \tTraining Loss: 0.585378\n",
      "Epoch: 248 \tTraining Loss: 0.449966 \tR2: 0.456130\n",
      "Epoch: 249 \tTraining Loss: 0.593120\n",
      "Epoch: 249 \tTraining Loss: 0.472057 \tR2: 0.456130\n",
      "Epoch: 250 \tTraining Loss: 0.595726\n",
      "Epoch: 250 \tTraining Loss: 0.462770 \tR2: 0.456130\n",
      "Epoch: 251 \tTraining Loss: 0.593509\n",
      "Epoch: 251 \tTraining Loss: 0.466292 \tR2: 0.456130\n",
      "Epoch: 252 \tTraining Loss: 0.574890\n",
      "Epoch: 252 \tTraining Loss: 0.493463 \tR2: 0.456130\n",
      "Epoch: 253 \tTraining Loss: 0.588565\n",
      "Epoch: 253 \tTraining Loss: 0.480111 \tR2: 0.456130\n",
      "Epoch: 254 \tTraining Loss: 0.589318\n",
      "Epoch: 254 \tTraining Loss: 0.450195 \tR2: 0.456130\n",
      "Epoch: 255 \tTraining Loss: 0.600885\n",
      "Epoch: 255 \tTraining Loss: 0.438774 \tR2: 0.456130\n",
      "Epoch: 256 \tTraining Loss: 0.599088\n",
      "Epoch: 256 \tTraining Loss: 0.443558 \tR2: 0.456130\n",
      "Epoch: 257 \tTraining Loss: 0.590248\n",
      "Epoch: 257 \tTraining Loss: 0.464694 \tR2: 0.456130\n",
      "Epoch: 258 \tTraining Loss: 0.594083\n",
      "Epoch: 258 \tTraining Loss: 0.465324 \tR2: 0.456130\n",
      "Epoch: 259 \tTraining Loss: 0.595965\n",
      "Epoch: 259 \tTraining Loss: 0.449578 \tR2: 0.456130\n",
      "Epoch: 260 \tTraining Loss: 0.603341\n",
      "Epoch: 260 \tTraining Loss: 0.460615 \tR2: 0.456130\n",
      "Epoch: 261 \tTraining Loss: 0.594107\n",
      "Epoch: 261 \tTraining Loss: 0.467404 \tR2: 0.456130\n",
      "Epoch: 262 \tTraining Loss: 0.587245\n",
      "Epoch: 262 \tTraining Loss: 0.470990 \tR2: 0.456130\n",
      "Epoch: 263 \tTraining Loss: 0.596740\n",
      "Epoch: 263 \tTraining Loss: 0.453328 \tR2: 0.456130\n",
      "Epoch: 264 \tTraining Loss: 0.595130\n",
      "Epoch: 264 \tTraining Loss: 0.444797 \tR2: 0.456130\n",
      "Epoch: 265 \tTraining Loss: 0.565128\n",
      "Epoch: 265 \tTraining Loss: 0.537535 \tR2: 0.456130\n",
      "Epoch: 266 \tTraining Loss: 0.603361\n",
      "Epoch: 266 \tTraining Loss: 0.456401 \tR2: 0.456130\n",
      "Epoch: 267 \tTraining Loss: 0.600228\n",
      "Epoch: 267 \tTraining Loss: 0.443235 \tR2: 0.456130\n",
      "Epoch: 268 \tTraining Loss: 0.601996\n",
      "Epoch: 268 \tTraining Loss: 0.467574 \tR2: 0.456130\n",
      "Epoch: 269 \tTraining Loss: 0.598294\n",
      "Epoch: 269 \tTraining Loss: 0.449603 \tR2: 0.456130\n",
      "Epoch: 270 \tTraining Loss: 0.593308\n",
      "Epoch: 270 \tTraining Loss: 0.462858 \tR2: 0.456130\n",
      "Epoch: 271 \tTraining Loss: 0.598633\n",
      "Epoch: 271 \tTraining Loss: 0.496488 \tR2: 0.456130\n",
      "Epoch: 272 \tTraining Loss: 0.581659\n",
      "Epoch: 272 \tTraining Loss: 0.470123 \tR2: 0.456130\n",
      "Epoch: 273 \tTraining Loss: 0.586261\n",
      "Epoch: 273 \tTraining Loss: 0.444764 \tR2: 0.456130\n",
      "Epoch: 274 \tTraining Loss: 0.587759\n",
      "Epoch: 274 \tTraining Loss: 0.492853 \tR2: 0.456130\n",
      "Epoch: 275 \tTraining Loss: 0.589207\n",
      "Epoch: 275 \tTraining Loss: 0.456771 \tR2: 0.456130\n",
      "Epoch: 276 \tTraining Loss: 0.580401\n",
      "Epoch: 276 \tTraining Loss: 0.518684 \tR2: 0.456130\n",
      "Epoch: 277 \tTraining Loss: 0.593295\n",
      "Epoch: 277 \tTraining Loss: 0.447342 \tR2: 0.456130\n",
      "Epoch: 278 \tTraining Loss: 0.595428\n",
      "Epoch: 278 \tTraining Loss: 0.451848 \tR2: 0.456130\n",
      "Epoch: 279 \tTraining Loss: 0.592396\n",
      "Epoch: 279 \tTraining Loss: 0.448519 \tR2: 0.456130\n",
      "Epoch: 280 \tTraining Loss: 0.597579\n",
      "Epoch: 280 \tTraining Loss: 0.464592 \tR2: 0.456130\n",
      "Epoch: 281 \tTraining Loss: 0.591038\n",
      "Epoch: 281 \tTraining Loss: 0.438813 \tR2: 0.456130\n",
      "Epoch: 282 \tTraining Loss: 0.578192\n",
      "Epoch: 282 \tTraining Loss: 0.442552 \tR2: 0.456130\n",
      "Epoch: 283 \tTraining Loss: 0.597344\n",
      "Epoch: 283 \tTraining Loss: 0.494538 \tR2: 0.456130\n",
      "Epoch: 284 \tTraining Loss: 0.583930\n",
      "Epoch: 284 \tTraining Loss: 0.448407 \tR2: 0.456130\n",
      "Epoch: 285 \tTraining Loss: 0.571050\n",
      "Epoch: 285 \tTraining Loss: 0.494561 \tR2: 0.456130\n",
      "Epoch: 286 \tTraining Loss: 0.604113\n",
      "Epoch: 286 \tTraining Loss: 0.493192 \tR2: 0.456130\n",
      "Epoch: 287 \tTraining Loss: 0.590847\n",
      "Epoch: 287 \tTraining Loss: 0.520221 \tR2: 0.456130\n",
      "Epoch: 288 \tTraining Loss: 0.614990\n",
      "Epoch: 288 \tTraining Loss: 0.463734 \tR2: 0.456130\n",
      "Epoch: 289 \tTraining Loss: 0.599583\n",
      "Epoch: 289 \tTraining Loss: 0.446787 \tR2: 0.456130\n",
      "Epoch: 290 \tTraining Loss: 0.590224\n",
      "Epoch: 290 \tTraining Loss: 0.456622 \tR2: 0.456130\n",
      "Epoch: 291 \tTraining Loss: 0.597825\n",
      "Epoch: 291 \tTraining Loss: 0.465442 \tR2: 0.456130\n",
      "Epoch: 292 \tTraining Loss: 0.590059\n",
      "Epoch: 292 \tTraining Loss: 0.437086 \tR2: 0.456130\n",
      "Epoch: 293 \tTraining Loss: 0.582176\n",
      "Epoch: 293 \tTraining Loss: 0.464444 \tR2: 0.456130\n",
      "Epoch: 294 \tTraining Loss: 0.587716\n",
      "Epoch: 294 \tTraining Loss: 0.458677 \tR2: 0.456130\n",
      "Epoch: 295 \tTraining Loss: 0.579395\n",
      "Epoch: 295 \tTraining Loss: 0.480431 \tR2: 0.456130\n",
      "Epoch: 296 \tTraining Loss: 0.585089\n",
      "Epoch: 296 \tTraining Loss: 0.439793 \tR2: 0.456130\n",
      "Epoch: 297 \tTraining Loss: 0.594805\n",
      "Epoch: 297 \tTraining Loss: 0.440711 \tR2: 0.456130\n",
      "Epoch: 298 \tTraining Loss: 0.578086\n",
      "Epoch: 298 \tTraining Loss: 0.463641 \tR2: 0.456130\n",
      "Epoch: 299 \tTraining Loss: 0.581372\n",
      "Epoch: 299 \tTraining Loss: 0.452845 \tR2: 0.456130\n",
      "Epoch: 300 \tTraining Loss: 0.575650\n",
      "Epoch: 300 \tTraining Loss: 0.471084 \tR2: 0.484229\n",
      "Epoch: 301 \tTraining Loss: 0.594466\n",
      "Epoch: 301 \tTraining Loss: 0.499848 \tR2: 0.484229\n",
      "Epoch: 302 \tTraining Loss: 0.586343\n",
      "Epoch: 302 \tTraining Loss: 0.450564 \tR2: 0.484229\n",
      "Epoch: 303 \tTraining Loss: 0.575500\n",
      "Epoch: 303 \tTraining Loss: 0.479263 \tR2: 0.484229\n",
      "Epoch: 304 \tTraining Loss: 0.600646\n",
      "Epoch: 304 \tTraining Loss: 0.487876 \tR2: 0.484229\n",
      "Epoch: 305 \tTraining Loss: 0.599055\n",
      "Epoch: 305 \tTraining Loss: 0.477807 \tR2: 0.484229\n",
      "Epoch: 306 \tTraining Loss: 0.577933\n",
      "Epoch: 306 \tTraining Loss: 0.423971 \tR2: 0.484229\n",
      "Epoch: 307 \tTraining Loss: 0.575287\n",
      "Epoch: 307 \tTraining Loss: 0.523985 \tR2: 0.484229\n",
      "Epoch: 308 \tTraining Loss: 0.583107\n",
      "Epoch: 308 \tTraining Loss: 0.463993 \tR2: 0.484229\n",
      "Epoch: 309 \tTraining Loss: 0.583674\n",
      "Epoch: 309 \tTraining Loss: 0.472407 \tR2: 0.484229\n",
      "Epoch: 310 \tTraining Loss: 0.571878\n",
      "Epoch: 310 \tTraining Loss: 0.451059 \tR2: 0.484229\n",
      "Epoch: 311 \tTraining Loss: 0.586018\n",
      "Epoch: 311 \tTraining Loss: 0.442430 \tR2: 0.484229\n",
      "Epoch: 312 \tTraining Loss: 0.587003\n",
      "Epoch: 312 \tTraining Loss: 0.471044 \tR2: 0.484229\n",
      "Epoch: 313 \tTraining Loss: 0.592261\n",
      "Epoch: 313 \tTraining Loss: 0.472215 \tR2: 0.484229\n",
      "Epoch: 314 \tTraining Loss: 0.591134\n",
      "Epoch: 314 \tTraining Loss: 0.478242 \tR2: 0.484229\n",
      "Epoch: 315 \tTraining Loss: 0.592837\n",
      "Epoch: 315 \tTraining Loss: 0.447553 \tR2: 0.484229\n",
      "Epoch: 316 \tTraining Loss: 0.586538\n",
      "Epoch: 316 \tTraining Loss: 0.463614 \tR2: 0.484229\n",
      "Epoch: 317 \tTraining Loss: 0.589682\n",
      "Epoch: 317 \tTraining Loss: 0.442810 \tR2: 0.484229\n",
      "Epoch: 318 \tTraining Loss: 0.580470\n",
      "Epoch: 318 \tTraining Loss: 0.490335 \tR2: 0.484229\n",
      "Epoch: 319 \tTraining Loss: 0.582847\n",
      "Epoch: 319 \tTraining Loss: 0.474293 \tR2: 0.484229\n",
      "Epoch: 320 \tTraining Loss: 0.586162\n",
      "Epoch: 320 \tTraining Loss: 0.447682 \tR2: 0.484229\n",
      "Epoch: 321 \tTraining Loss: 0.589348\n",
      "Epoch: 321 \tTraining Loss: 0.437326 \tR2: 0.484229\n",
      "Epoch: 322 \tTraining Loss: 0.563629\n",
      "Epoch: 322 \tTraining Loss: 0.468928 \tR2: 0.484229\n",
      "Epoch: 323 \tTraining Loss: 0.600416\n",
      "Epoch: 323 \tTraining Loss: 0.442797 \tR2: 0.484229\n",
      "Epoch: 324 \tTraining Loss: 0.565203\n",
      "Epoch: 324 \tTraining Loss: 0.470026 \tR2: 0.484229\n",
      "Epoch: 325 \tTraining Loss: 0.583707\n",
      "Epoch: 325 \tTraining Loss: 0.481828 \tR2: 0.484229\n",
      "Epoch: 326 \tTraining Loss: 0.572809\n",
      "Epoch: 326 \tTraining Loss: 0.450502 \tR2: 0.484229\n",
      "Epoch: 327 \tTraining Loss: 0.571158\n",
      "Epoch: 327 \tTraining Loss: 0.467143 \tR2: 0.484229\n",
      "Epoch: 328 \tTraining Loss: 0.597701\n",
      "Epoch: 328 \tTraining Loss: 0.440652 \tR2: 0.484229\n",
      "Epoch: 329 \tTraining Loss: 0.592979\n",
      "Epoch: 329 \tTraining Loss: 0.480835 \tR2: 0.484229\n",
      "Epoch: 330 \tTraining Loss: 0.598058\n",
      "Epoch: 330 \tTraining Loss: 0.485731 \tR2: 0.484229\n",
      "Epoch: 331 \tTraining Loss: 0.575128\n",
      "Epoch: 331 \tTraining Loss: 0.445765 \tR2: 0.484229\n",
      "Epoch: 332 \tTraining Loss: 0.584347\n",
      "Epoch: 332 \tTraining Loss: 0.457555 \tR2: 0.484229\n",
      "Epoch: 333 \tTraining Loss: 0.580528\n",
      "Epoch: 333 \tTraining Loss: 0.448108 \tR2: 0.484229\n",
      "Epoch: 334 \tTraining Loss: 0.592107\n",
      "Epoch: 334 \tTraining Loss: 0.426912 \tR2: 0.484229\n",
      "Epoch: 335 \tTraining Loss: 0.573665\n",
      "Epoch: 335 \tTraining Loss: 0.457994 \tR2: 0.484229\n",
      "Epoch: 336 \tTraining Loss: 0.574878\n",
      "Epoch: 336 \tTraining Loss: 0.514613 \tR2: 0.484229\n",
      "Epoch: 337 \tTraining Loss: 0.593765\n",
      "Epoch: 337 \tTraining Loss: 0.439054 \tR2: 0.484229\n",
      "Epoch: 338 \tTraining Loss: 0.579058\n",
      "Epoch: 338 \tTraining Loss: 0.486204 \tR2: 0.484229\n",
      "Epoch: 339 \tTraining Loss: 0.587818\n",
      "Epoch: 339 \tTraining Loss: 0.487770 \tR2: 0.484229\n",
      "Epoch: 340 \tTraining Loss: 0.585713\n",
      "Epoch: 340 \tTraining Loss: 0.466555 \tR2: 0.484229\n",
      "Epoch: 341 \tTraining Loss: 0.596938\n",
      "Epoch: 341 \tTraining Loss: 0.438994 \tR2: 0.484229\n",
      "Epoch: 342 \tTraining Loss: 0.573447\n",
      "Epoch: 342 \tTraining Loss: 0.467058 \tR2: 0.484229\n",
      "Epoch: 343 \tTraining Loss: 0.579395\n",
      "Epoch: 343 \tTraining Loss: 0.471746 \tR2: 0.484229\n",
      "Epoch: 344 \tTraining Loss: 0.584959\n",
      "Epoch: 344 \tTraining Loss: 0.430824 \tR2: 0.484229\n",
      "Epoch: 345 \tTraining Loss: 0.579205\n",
      "Epoch: 345 \tTraining Loss: 0.493283 \tR2: 0.484229\n",
      "Epoch: 346 \tTraining Loss: 0.570537\n",
      "Epoch: 346 \tTraining Loss: 0.482257 \tR2: 0.484229\n",
      "Epoch: 347 \tTraining Loss: 0.585174\n",
      "Epoch: 347 \tTraining Loss: 0.471375 \tR2: 0.484229\n",
      "Epoch: 348 \tTraining Loss: 0.583292\n",
      "Epoch: 348 \tTraining Loss: 0.467269 \tR2: 0.484229\n",
      "Epoch: 349 \tTraining Loss: 0.576850\n",
      "Epoch: 349 \tTraining Loss: 0.472228 \tR2: 0.484229\n",
      "Epoch: 350 \tTraining Loss: 0.583536\n",
      "Epoch: 350 \tTraining Loss: 0.517909 \tR2: 0.484229\n",
      "Epoch: 351 \tTraining Loss: 0.585791\n",
      "Epoch: 351 \tTraining Loss: 0.509973 \tR2: 0.484229\n",
      "Epoch: 352 \tTraining Loss: 0.577101\n",
      "Epoch: 352 \tTraining Loss: 0.444719 \tR2: 0.484229\n",
      "Epoch: 353 \tTraining Loss: 0.579656\n",
      "Epoch: 353 \tTraining Loss: 0.438424 \tR2: 0.484229\n",
      "Epoch: 354 \tTraining Loss: 0.576254\n",
      "Epoch: 354 \tTraining Loss: 0.517037 \tR2: 0.484229\n",
      "Epoch: 355 \tTraining Loss: 0.591304\n",
      "Epoch: 355 \tTraining Loss: 0.447861 \tR2: 0.484229\n",
      "Epoch: 356 \tTraining Loss: 0.589767\n",
      "Epoch: 356 \tTraining Loss: 0.461616 \tR2: 0.484229\n",
      "Epoch: 357 \tTraining Loss: 0.584698\n",
      "Epoch: 357 \tTraining Loss: 0.437011 \tR2: 0.484229\n",
      "Epoch: 358 \tTraining Loss: 0.585482\n",
      "Epoch: 358 \tTraining Loss: 0.441609 \tR2: 0.484229\n",
      "Epoch: 359 \tTraining Loss: 0.578226\n",
      "Epoch: 359 \tTraining Loss: 0.478170 \tR2: 0.484229\n",
      "Epoch: 360 \tTraining Loss: 0.585936\n",
      "Epoch: 360 \tTraining Loss: 0.486392 \tR2: 0.484229\n",
      "Epoch: 361 \tTraining Loss: 0.580319\n",
      "Epoch: 361 \tTraining Loss: 0.435378 \tR2: 0.484229\n",
      "Epoch: 362 \tTraining Loss: 0.579089\n",
      "Epoch: 362 \tTraining Loss: 0.447176 \tR2: 0.484229\n",
      "Epoch: 363 \tTraining Loss: 0.581832\n",
      "Epoch: 363 \tTraining Loss: 0.458606 \tR2: 0.484229\n",
      "Epoch: 364 \tTraining Loss: 0.581311\n",
      "Epoch: 364 \tTraining Loss: 0.466130 \tR2: 0.484229\n",
      "Epoch: 365 \tTraining Loss: 0.575592\n",
      "Epoch: 365 \tTraining Loss: 0.450260 \tR2: 0.484229\n",
      "Epoch: 366 \tTraining Loss: 0.579552\n",
      "Epoch: 366 \tTraining Loss: 0.438805 \tR2: 0.484229\n",
      "Epoch: 367 \tTraining Loss: 0.585039\n",
      "Epoch: 367 \tTraining Loss: 0.463108 \tR2: 0.484229\n",
      "Epoch: 368 \tTraining Loss: 0.586987\n",
      "Epoch: 368 \tTraining Loss: 0.450589 \tR2: 0.484229\n",
      "Epoch: 369 \tTraining Loss: 0.590092\n",
      "Epoch: 369 \tTraining Loss: 0.597889 \tR2: 0.484229\n",
      "Epoch: 370 \tTraining Loss: 0.589203\n",
      "Epoch: 370 \tTraining Loss: 0.440960 \tR2: 0.484229\n",
      "Epoch: 371 \tTraining Loss: 0.577819\n",
      "Epoch: 371 \tTraining Loss: 0.471224 \tR2: 0.484229\n",
      "Epoch: 372 \tTraining Loss: 0.575626\n",
      "Epoch: 372 \tTraining Loss: 0.445883 \tR2: 0.484229\n",
      "Epoch: 373 \tTraining Loss: 0.571823\n",
      "Epoch: 373 \tTraining Loss: 0.435629 \tR2: 0.484229\n",
      "Epoch: 374 \tTraining Loss: 0.576343\n",
      "Epoch: 374 \tTraining Loss: 0.424715 \tR2: 0.484229\n",
      "Epoch: 375 \tTraining Loss: 0.586733\n",
      "Epoch: 375 \tTraining Loss: 0.462053 \tR2: 0.484229\n",
      "Epoch: 376 \tTraining Loss: 0.573460\n",
      "Epoch: 376 \tTraining Loss: 0.436223 \tR2: 0.484229\n",
      "Epoch: 377 \tTraining Loss: 0.597749\n",
      "Epoch: 377 \tTraining Loss: 0.469955 \tR2: 0.484229\n",
      "Epoch: 378 \tTraining Loss: 0.585180\n",
      "Epoch: 378 \tTraining Loss: 0.456045 \tR2: 0.484229\n",
      "Epoch: 379 \tTraining Loss: 0.606601\n",
      "Epoch: 379 \tTraining Loss: 0.450704 \tR2: 0.484229\n",
      "Epoch: 380 \tTraining Loss: 0.595274\n",
      "Epoch: 380 \tTraining Loss: 0.453441 \tR2: 0.484229\n",
      "Epoch: 381 \tTraining Loss: 0.573385\n",
      "Epoch: 381 \tTraining Loss: 0.470072 \tR2: 0.484229\n",
      "Epoch: 382 \tTraining Loss: 0.577958\n",
      "Epoch: 382 \tTraining Loss: 0.425188 \tR2: 0.484229\n",
      "Epoch: 383 \tTraining Loss: 0.592929\n",
      "Epoch: 383 \tTraining Loss: 0.452507 \tR2: 0.484229\n",
      "Epoch: 384 \tTraining Loss: 0.595695\n",
      "Epoch: 384 \tTraining Loss: 0.492597 \tR2: 0.484229\n",
      "Epoch: 385 \tTraining Loss: 0.573382\n",
      "Epoch: 385 \tTraining Loss: 0.480176 \tR2: 0.484229\n",
      "Epoch: 386 \tTraining Loss: 0.574900\n",
      "Epoch: 386 \tTraining Loss: 0.450976 \tR2: 0.484229\n",
      "Epoch: 387 \tTraining Loss: 0.594736\n",
      "Epoch: 387 \tTraining Loss: 0.440282 \tR2: 0.484229\n",
      "Epoch: 388 \tTraining Loss: 0.594915\n",
      "Epoch: 388 \tTraining Loss: 0.465173 \tR2: 0.484229\n",
      "Epoch: 389 \tTraining Loss: 0.571196\n",
      "Epoch: 389 \tTraining Loss: 0.452336 \tR2: 0.484229\n",
      "Epoch: 390 \tTraining Loss: 0.572594\n",
      "Epoch: 390 \tTraining Loss: 0.522283 \tR2: 0.484229\n",
      "Epoch: 391 \tTraining Loss: 0.597820\n",
      "Epoch: 391 \tTraining Loss: 0.502996 \tR2: 0.484229\n",
      "Epoch: 392 \tTraining Loss: 0.588527\n",
      "Epoch: 392 \tTraining Loss: 0.451024 \tR2: 0.484229\n",
      "Epoch: 393 \tTraining Loss: 0.579867\n",
      "Epoch: 393 \tTraining Loss: 0.434983 \tR2: 0.484229\n",
      "Epoch: 394 \tTraining Loss: 0.589706\n",
      "Epoch: 394 \tTraining Loss: 0.438855 \tR2: 0.484229\n",
      "Epoch: 395 \tTraining Loss: 0.576792\n",
      "Epoch: 395 \tTraining Loss: 0.442662 \tR2: 0.484229\n",
      "Epoch: 396 \tTraining Loss: 0.585040\n",
      "Epoch: 396 \tTraining Loss: 0.465526 \tR2: 0.484229\n",
      "Epoch: 397 \tTraining Loss: 0.577287\n",
      "Epoch: 397 \tTraining Loss: 0.489959 \tR2: 0.484229\n",
      "Epoch: 398 \tTraining Loss: 0.572782\n",
      "Epoch: 398 \tTraining Loss: 0.457440 \tR2: 0.484229\n",
      "Epoch: 399 \tTraining Loss: 0.589026\n",
      "Epoch: 399 \tTraining Loss: 0.426800 \tR2: 0.484229\n",
      "Epoch: 400 \tTraining Loss: 0.573089\n",
      "Epoch: 400 \tTraining Loss: 0.445292 \tR2: 0.539368\n",
      "Epoch: 401 \tTraining Loss: 0.593223\n",
      "Epoch: 401 \tTraining Loss: 0.439469 \tR2: 0.539368\n",
      "Epoch: 402 \tTraining Loss: 0.577620\n",
      "Epoch: 402 \tTraining Loss: 0.472418 \tR2: 0.539368\n",
      "Epoch: 403 \tTraining Loss: 0.576868\n",
      "Epoch: 403 \tTraining Loss: 0.447598 \tR2: 0.539368\n",
      "Epoch: 404 \tTraining Loss: 0.561935\n",
      "Epoch: 404 \tTraining Loss: 0.424182 \tR2: 0.539368\n",
      "Epoch: 405 \tTraining Loss: 0.586621\n",
      "Epoch: 405 \tTraining Loss: 0.458413 \tR2: 0.539368\n",
      "Epoch: 406 \tTraining Loss: 0.576959\n",
      "Epoch: 406 \tTraining Loss: 0.465091 \tR2: 0.539368\n",
      "Epoch: 407 \tTraining Loss: 0.595005\n",
      "Epoch: 407 \tTraining Loss: 0.436013 \tR2: 0.539368\n",
      "Epoch: 408 \tTraining Loss: 0.581690\n",
      "Epoch: 408 \tTraining Loss: 0.446083 \tR2: 0.539368\n",
      "Epoch: 409 \tTraining Loss: 0.564212\n",
      "Epoch: 409 \tTraining Loss: 0.460435 \tR2: 0.539368\n",
      "Epoch: 410 \tTraining Loss: 0.575398\n",
      "Epoch: 410 \tTraining Loss: 0.449557 \tR2: 0.539368\n",
      "Epoch: 411 \tTraining Loss: 0.575038\n",
      "Epoch: 411 \tTraining Loss: 0.410007 \tR2: 0.539368\n",
      "Epoch: 412 \tTraining Loss: 0.586759\n",
      "Epoch: 412 \tTraining Loss: 0.496866 \tR2: 0.539368\n",
      "Epoch: 413 \tTraining Loss: 0.583020\n",
      "Epoch: 413 \tTraining Loss: 0.458657 \tR2: 0.539368\n",
      "Epoch: 414 \tTraining Loss: 0.581310\n",
      "Epoch: 414 \tTraining Loss: 0.449152 \tR2: 0.539368\n",
      "Epoch: 415 \tTraining Loss: 0.588117\n",
      "Epoch: 415 \tTraining Loss: 0.452294 \tR2: 0.539368\n",
      "Epoch: 416 \tTraining Loss: 0.578477\n",
      "Epoch: 416 \tTraining Loss: 0.450291 \tR2: 0.539368\n",
      "Epoch: 417 \tTraining Loss: 0.579133\n",
      "Epoch: 417 \tTraining Loss: 0.453676 \tR2: 0.539368\n",
      "Epoch: 418 \tTraining Loss: 0.574191\n",
      "Epoch: 418 \tTraining Loss: 0.460632 \tR2: 0.539368\n",
      "Epoch: 419 \tTraining Loss: 0.587918\n",
      "Epoch: 419 \tTraining Loss: 0.473088 \tR2: 0.539368\n",
      "Epoch: 420 \tTraining Loss: 0.576164\n",
      "Epoch: 420 \tTraining Loss: 0.463758 \tR2: 0.539368\n",
      "Epoch: 421 \tTraining Loss: 0.595424\n",
      "Epoch: 421 \tTraining Loss: 0.442314 \tR2: 0.539368\n",
      "Epoch: 422 \tTraining Loss: 0.584674\n",
      "Epoch: 422 \tTraining Loss: 0.450399 \tR2: 0.539368\n",
      "Epoch: 423 \tTraining Loss: 0.577135\n",
      "Epoch: 423 \tTraining Loss: 0.437159 \tR2: 0.539368\n",
      "Epoch: 424 \tTraining Loss: 0.569694\n",
      "Epoch: 424 \tTraining Loss: 0.456668 \tR2: 0.539368\n",
      "Epoch: 425 \tTraining Loss: 0.585005\n",
      "Epoch: 425 \tTraining Loss: 0.438013 \tR2: 0.539368\n",
      "Epoch: 426 \tTraining Loss: 0.597711\n",
      "Epoch: 426 \tTraining Loss: 0.445516 \tR2: 0.539368\n",
      "Epoch: 427 \tTraining Loss: 0.561521\n",
      "Epoch: 427 \tTraining Loss: 0.476612 \tR2: 0.539368\n",
      "Epoch: 428 \tTraining Loss: 0.593640\n",
      "Epoch: 428 \tTraining Loss: 0.496848 \tR2: 0.539368\n",
      "Epoch: 429 \tTraining Loss: 0.582283\n",
      "Epoch: 429 \tTraining Loss: 0.472737 \tR2: 0.539368\n",
      "Epoch: 430 \tTraining Loss: 0.585253\n",
      "Epoch: 430 \tTraining Loss: 0.478113 \tR2: 0.539368\n",
      "Epoch: 431 \tTraining Loss: 0.586109\n",
      "Epoch: 431 \tTraining Loss: 0.466379 \tR2: 0.539368\n",
      "Epoch: 432 \tTraining Loss: 0.575744\n",
      "Epoch: 432 \tTraining Loss: 0.426575 \tR2: 0.539368\n",
      "Epoch: 433 \tTraining Loss: 0.576707\n",
      "Epoch: 433 \tTraining Loss: 0.426637 \tR2: 0.539368\n",
      "Epoch: 434 \tTraining Loss: 0.568547\n",
      "Epoch: 434 \tTraining Loss: 0.437319 \tR2: 0.539368\n",
      "Epoch: 435 \tTraining Loss: 0.574755\n",
      "Epoch: 435 \tTraining Loss: 0.449434 \tR2: 0.539368\n",
      "Epoch: 436 \tTraining Loss: 0.588197\n",
      "Epoch: 436 \tTraining Loss: 0.456420 \tR2: 0.539368\n",
      "Epoch: 437 \tTraining Loss: 0.580052\n",
      "Epoch: 437 \tTraining Loss: 0.442403 \tR2: 0.539368\n",
      "Epoch: 438 \tTraining Loss: 0.581882\n",
      "Epoch: 438 \tTraining Loss: 0.447596 \tR2: 0.539368\n",
      "Epoch: 439 \tTraining Loss: 0.582634\n",
      "Epoch: 439 \tTraining Loss: 0.472595 \tR2: 0.539368\n",
      "Epoch: 440 \tTraining Loss: 0.589700\n",
      "Epoch: 440 \tTraining Loss: 0.424623 \tR2: 0.539368\n",
      "Epoch: 441 \tTraining Loss: 0.585868\n",
      "Epoch: 441 \tTraining Loss: 0.491748 \tR2: 0.539368\n",
      "Epoch: 442 \tTraining Loss: 0.579127\n",
      "Epoch: 442 \tTraining Loss: 0.430099 \tR2: 0.539368\n",
      "Epoch: 443 \tTraining Loss: 0.580362\n",
      "Epoch: 443 \tTraining Loss: 0.427042 \tR2: 0.539368\n",
      "Epoch: 444 \tTraining Loss: 0.581074\n",
      "Epoch: 444 \tTraining Loss: 0.477385 \tR2: 0.539368\n",
      "Epoch: 445 \tTraining Loss: 0.578108\n",
      "Epoch: 445 \tTraining Loss: 0.453110 \tR2: 0.539368\n",
      "Epoch: 446 \tTraining Loss: 0.573069\n",
      "Epoch: 446 \tTraining Loss: 0.440066 \tR2: 0.539368\n",
      "Epoch: 447 \tTraining Loss: 0.577231\n",
      "Epoch: 447 \tTraining Loss: 0.435966 \tR2: 0.539368\n",
      "Epoch: 448 \tTraining Loss: 0.565892\n",
      "Epoch: 448 \tTraining Loss: 0.427820 \tR2: 0.539368\n",
      "Epoch: 449 \tTraining Loss: 0.600662\n",
      "Epoch: 449 \tTraining Loss: 0.429722 \tR2: 0.539368\n",
      "Epoch: 450 \tTraining Loss: 0.568186\n",
      "Epoch: 450 \tTraining Loss: 0.458639 \tR2: 0.539368\n",
      "Epoch: 451 \tTraining Loss: 0.576166\n",
      "Epoch: 451 \tTraining Loss: 0.456215 \tR2: 0.539368\n",
      "Epoch: 452 \tTraining Loss: 0.559020\n",
      "Epoch: 452 \tTraining Loss: 0.441454 \tR2: 0.539368\n",
      "Epoch: 453 \tTraining Loss: 0.572606\n",
      "Epoch: 453 \tTraining Loss: 0.475070 \tR2: 0.539368\n",
      "Epoch: 454 \tTraining Loss: 0.570244\n",
      "Epoch: 454 \tTraining Loss: 0.429539 \tR2: 0.539368\n",
      "Epoch: 455 \tTraining Loss: 0.578260\n",
      "Epoch: 455 \tTraining Loss: 0.447841 \tR2: 0.539368\n",
      "Epoch: 456 \tTraining Loss: 0.582041\n",
      "Epoch: 456 \tTraining Loss: 0.466978 \tR2: 0.539368\n",
      "Epoch: 457 \tTraining Loss: 0.596053\n",
      "Epoch: 457 \tTraining Loss: 0.461224 \tR2: 0.539368\n",
      "Epoch: 458 \tTraining Loss: 0.588424\n",
      "Epoch: 458 \tTraining Loss: 0.457567 \tR2: 0.539368\n",
      "Epoch: 459 \tTraining Loss: 0.569910\n",
      "Epoch: 459 \tTraining Loss: 0.457102 \tR2: 0.539368\n",
      "Epoch: 460 \tTraining Loss: 0.586147\n",
      "Epoch: 460 \tTraining Loss: 0.439847 \tR2: 0.539368\n",
      "Epoch: 461 \tTraining Loss: 0.575660\n",
      "Epoch: 461 \tTraining Loss: 0.451550 \tR2: 0.539368\n",
      "Epoch: 462 \tTraining Loss: 0.585480\n",
      "Epoch: 462 \tTraining Loss: 0.466078 \tR2: 0.539368\n",
      "Epoch: 463 \tTraining Loss: 0.587638\n",
      "Epoch: 463 \tTraining Loss: 0.430251 \tR2: 0.539368\n",
      "Epoch: 464 \tTraining Loss: 0.571947\n",
      "Epoch: 464 \tTraining Loss: 0.456234 \tR2: 0.539368\n",
      "Epoch: 465 \tTraining Loss: 0.579030\n",
      "Epoch: 465 \tTraining Loss: 0.512289 \tR2: 0.539368\n",
      "Epoch: 466 \tTraining Loss: 0.576996\n",
      "Epoch: 466 \tTraining Loss: 0.478056 \tR2: 0.539368\n",
      "Epoch: 467 \tTraining Loss: 0.569962\n",
      "Epoch: 467 \tTraining Loss: 0.454587 \tR2: 0.539368\n",
      "Epoch: 468 \tTraining Loss: 0.570595\n",
      "Epoch: 468 \tTraining Loss: 0.510449 \tR2: 0.539368\n",
      "Epoch: 469 \tTraining Loss: 0.594086\n",
      "Epoch: 469 \tTraining Loss: 0.441092 \tR2: 0.539368\n",
      "Epoch: 470 \tTraining Loss: 0.572681\n",
      "Epoch: 470 \tTraining Loss: 0.463781 \tR2: 0.539368\n",
      "Epoch: 471 \tTraining Loss: 0.574387\n",
      "Epoch: 471 \tTraining Loss: 0.460145 \tR2: 0.539368\n",
      "Epoch: 472 \tTraining Loss: 0.571606\n",
      "Epoch: 472 \tTraining Loss: 0.499484 \tR2: 0.539368\n",
      "Epoch: 473 \tTraining Loss: 0.587144\n",
      "Epoch: 473 \tTraining Loss: 0.434309 \tR2: 0.539368\n",
      "Epoch: 474 \tTraining Loss: 0.581326\n",
      "Epoch: 474 \tTraining Loss: 0.491557 \tR2: 0.539368\n",
      "Epoch: 475 \tTraining Loss: 0.578112\n",
      "Epoch: 475 \tTraining Loss: 0.438945 \tR2: 0.539368\n",
      "Epoch: 476 \tTraining Loss: 0.568587\n",
      "Epoch: 476 \tTraining Loss: 0.435950 \tR2: 0.539368\n",
      "Epoch: 477 \tTraining Loss: 0.572965\n",
      "Epoch: 477 \tTraining Loss: 0.484964 \tR2: 0.539368\n",
      "Epoch: 478 \tTraining Loss: 0.576959\n",
      "Epoch: 478 \tTraining Loss: 0.481242 \tR2: 0.539368\n",
      "Epoch: 479 \tTraining Loss: 0.566184\n",
      "Epoch: 479 \tTraining Loss: 0.468675 \tR2: 0.539368\n",
      "Epoch: 480 \tTraining Loss: 0.584980\n",
      "Epoch: 480 \tTraining Loss: 0.466447 \tR2: 0.539368\n",
      "Epoch: 481 \tTraining Loss: 0.571463\n",
      "Epoch: 481 \tTraining Loss: 0.427217 \tR2: 0.539368\n",
      "Epoch: 482 \tTraining Loss: 0.571957\n",
      "Epoch: 482 \tTraining Loss: 0.427625 \tR2: 0.539368\n",
      "Epoch: 483 \tTraining Loss: 0.585894\n",
      "Epoch: 483 \tTraining Loss: 0.450369 \tR2: 0.539368\n",
      "Epoch: 484 \tTraining Loss: 0.581365\n",
      "Epoch: 484 \tTraining Loss: 0.464288 \tR2: 0.539368\n",
      "Epoch: 485 \tTraining Loss: 0.577801\n",
      "Epoch: 485 \tTraining Loss: 0.440929 \tR2: 0.539368\n",
      "Epoch: 486 \tTraining Loss: 0.582864\n",
      "Epoch: 486 \tTraining Loss: 0.452937 \tR2: 0.539368\n",
      "Epoch: 487 \tTraining Loss: 0.584730\n",
      "Epoch: 487 \tTraining Loss: 0.436454 \tR2: 0.539368\n",
      "Epoch: 488 \tTraining Loss: 0.577855\n",
      "Epoch: 488 \tTraining Loss: 0.428370 \tR2: 0.539368\n",
      "Epoch: 489 \tTraining Loss: 0.574947\n",
      "Epoch: 489 \tTraining Loss: 0.441292 \tR2: 0.539368\n",
      "Epoch: 490 \tTraining Loss: 0.590692\n",
      "Epoch: 490 \tTraining Loss: 0.423263 \tR2: 0.539368\n",
      "Epoch: 491 \tTraining Loss: 0.565529\n",
      "Epoch: 491 \tTraining Loss: 0.465045 \tR2: 0.539368\n",
      "Epoch: 492 \tTraining Loss: 0.591341\n",
      "Epoch: 492 \tTraining Loss: 0.462288 \tR2: 0.539368\n",
      "Epoch: 493 \tTraining Loss: 0.579041\n",
      "Epoch: 493 \tTraining Loss: 0.447843 \tR2: 0.539368\n",
      "Epoch: 494 \tTraining Loss: 0.569796\n",
      "Epoch: 494 \tTraining Loss: 0.474072 \tR2: 0.539368\n",
      "Epoch: 495 \tTraining Loss: 0.565079\n",
      "Epoch: 495 \tTraining Loss: 0.444088 \tR2: 0.539368\n",
      "Epoch: 496 \tTraining Loss: 0.585700\n",
      "Epoch: 496 \tTraining Loss: 0.454157 \tR2: 0.539368\n",
      "Epoch: 497 \tTraining Loss: 0.578326\n",
      "Epoch: 497 \tTraining Loss: 0.483067 \tR2: 0.539368\n",
      "Epoch: 498 \tTraining Loss: 0.592473\n",
      "Epoch: 498 \tTraining Loss: 0.430882 \tR2: 0.539368\n",
      "Epoch: 499 \tTraining Loss: 0.576159\n",
      "Epoch: 499 \tTraining Loss: 0.534760 \tR2: 0.539368\n",
      "Epoch: 500 \tTraining Loss: 0.600075\n",
      "Epoch: 500 \tTraining Loss: 0.519031 \tR2: 0.421991\n",
      "Epoch: 501 \tTraining Loss: 0.579983\n",
      "Epoch: 501 \tTraining Loss: 0.483767 \tR2: 0.421991\n",
      "Epoch: 502 \tTraining Loss: 0.566764\n",
      "Epoch: 502 \tTraining Loss: 0.470512 \tR2: 0.421991\n",
      "Epoch: 503 \tTraining Loss: 0.570545\n",
      "Epoch: 503 \tTraining Loss: 0.443893 \tR2: 0.421991\n",
      "Epoch: 504 \tTraining Loss: 0.570275\n",
      "Epoch: 504 \tTraining Loss: 0.483175 \tR2: 0.421991\n",
      "Epoch: 505 \tTraining Loss: 0.579548\n",
      "Epoch: 505 \tTraining Loss: 0.458268 \tR2: 0.421991\n",
      "Epoch: 506 \tTraining Loss: 0.574878\n",
      "Epoch: 506 \tTraining Loss: 0.425026 \tR2: 0.421991\n",
      "Epoch: 507 \tTraining Loss: 0.571615\n",
      "Epoch: 507 \tTraining Loss: 0.458696 \tR2: 0.421991\n",
      "Epoch: 508 \tTraining Loss: 0.576999\n",
      "Epoch: 508 \tTraining Loss: 0.496349 \tR2: 0.421991\n",
      "Epoch: 509 \tTraining Loss: 0.557395\n",
      "Epoch: 509 \tTraining Loss: 0.469858 \tR2: 0.421991\n",
      "Epoch: 510 \tTraining Loss: 0.546634\n",
      "Epoch: 510 \tTraining Loss: 0.440641 \tR2: 0.421991\n",
      "Epoch: 511 \tTraining Loss: 0.569884\n",
      "Epoch: 511 \tTraining Loss: 0.442698 \tR2: 0.421991\n",
      "Epoch: 512 \tTraining Loss: 0.596259\n",
      "Epoch: 512 \tTraining Loss: 0.491980 \tR2: 0.421991\n",
      "Epoch: 513 \tTraining Loss: 0.596401\n",
      "Epoch: 513 \tTraining Loss: 0.456837 \tR2: 0.421991\n",
      "Epoch: 514 \tTraining Loss: 0.566139\n",
      "Epoch: 514 \tTraining Loss: 0.423827 \tR2: 0.421991\n",
      "Epoch: 515 \tTraining Loss: 0.578707\n",
      "Epoch: 515 \tTraining Loss: 0.514919 \tR2: 0.421991\n",
      "Epoch: 516 \tTraining Loss: 0.572304\n",
      "Epoch: 516 \tTraining Loss: 0.468541 \tR2: 0.421991\n",
      "Epoch: 517 \tTraining Loss: 0.559530\n",
      "Epoch: 517 \tTraining Loss: 0.427712 \tR2: 0.421991\n",
      "Epoch: 518 \tTraining Loss: 0.570717\n",
      "Epoch: 518 \tTraining Loss: 0.453759 \tR2: 0.421991\n",
      "Epoch: 519 \tTraining Loss: 0.576095\n",
      "Epoch: 519 \tTraining Loss: 0.440007 \tR2: 0.421991\n",
      "Epoch: 520 \tTraining Loss: 0.570139\n",
      "Epoch: 520 \tTraining Loss: 0.455599 \tR2: 0.421991\n",
      "Epoch: 521 \tTraining Loss: 0.573329\n",
      "Epoch: 521 \tTraining Loss: 0.424350 \tR2: 0.421991\n",
      "Epoch: 522 \tTraining Loss: 0.578557\n",
      "Epoch: 522 \tTraining Loss: 0.547662 \tR2: 0.421991\n",
      "Epoch: 523 \tTraining Loss: 0.572513\n",
      "Epoch: 523 \tTraining Loss: 0.459976 \tR2: 0.421991\n",
      "Epoch: 524 \tTraining Loss: 0.584838\n",
      "Epoch: 524 \tTraining Loss: 0.423754 \tR2: 0.421991\n",
      "Epoch: 525 \tTraining Loss: 0.587530\n",
      "Epoch: 525 \tTraining Loss: 0.452890 \tR2: 0.421991\n",
      "Epoch: 526 \tTraining Loss: 0.585701\n",
      "Epoch: 526 \tTraining Loss: 0.436938 \tR2: 0.421991\n",
      "Epoch: 527 \tTraining Loss: 0.574987\n",
      "Epoch: 527 \tTraining Loss: 0.412636 \tR2: 0.421991\n",
      "Epoch: 528 \tTraining Loss: 0.556126\n",
      "Epoch: 528 \tTraining Loss: 0.441669 \tR2: 0.421991\n",
      "Epoch: 529 \tTraining Loss: 0.571241\n",
      "Epoch: 529 \tTraining Loss: 0.438587 \tR2: 0.421991\n",
      "Epoch: 530 \tTraining Loss: 0.562581\n",
      "Epoch: 530 \tTraining Loss: 0.450248 \tR2: 0.421991\n",
      "Epoch: 531 \tTraining Loss: 0.580077\n",
      "Epoch: 531 \tTraining Loss: 0.423973 \tR2: 0.421991\n",
      "Epoch: 532 \tTraining Loss: 0.569096\n",
      "Epoch: 532 \tTraining Loss: 0.494204 \tR2: 0.421991\n",
      "Epoch: 533 \tTraining Loss: 0.573253\n",
      "Epoch: 533 \tTraining Loss: 0.439534 \tR2: 0.421991\n",
      "Epoch: 534 \tTraining Loss: 0.566523\n",
      "Epoch: 534 \tTraining Loss: 0.441097 \tR2: 0.421991\n",
      "Epoch: 535 \tTraining Loss: 0.575880\n",
      "Epoch: 535 \tTraining Loss: 0.470678 \tR2: 0.421991\n",
      "Epoch: 536 \tTraining Loss: 0.556868\n",
      "Epoch: 536 \tTraining Loss: 0.433078 \tR2: 0.421991\n",
      "Epoch: 537 \tTraining Loss: 0.580793\n",
      "Epoch: 537 \tTraining Loss: 0.445721 \tR2: 0.421991\n",
      "Epoch: 538 \tTraining Loss: 0.589793\n",
      "Epoch: 538 \tTraining Loss: 0.445803 \tR2: 0.421991\n",
      "Epoch: 539 \tTraining Loss: 0.565891\n",
      "Epoch: 539 \tTraining Loss: 0.447605 \tR2: 0.421991\n",
      "Epoch: 540 \tTraining Loss: 0.566199\n",
      "Epoch: 540 \tTraining Loss: 0.463118 \tR2: 0.421991\n",
      "Epoch: 541 \tTraining Loss: 0.562141\n",
      "Epoch: 541 \tTraining Loss: 0.449137 \tR2: 0.421991\n",
      "Epoch: 542 \tTraining Loss: 0.567301\n",
      "Epoch: 542 \tTraining Loss: 0.451876 \tR2: 0.421991\n",
      "Epoch: 543 \tTraining Loss: 0.590036\n",
      "Epoch: 543 \tTraining Loss: 0.412425 \tR2: 0.421991\n",
      "Epoch: 544 \tTraining Loss: 0.577780\n",
      "Epoch: 544 \tTraining Loss: 0.456916 \tR2: 0.421991\n",
      "Epoch: 545 \tTraining Loss: 0.578603\n",
      "Epoch: 545 \tTraining Loss: 0.430992 \tR2: 0.421991\n",
      "Epoch: 546 \tTraining Loss: 0.566088\n",
      "Epoch: 546 \tTraining Loss: 0.464425 \tR2: 0.421991\n",
      "Epoch: 547 \tTraining Loss: 0.588103\n",
      "Epoch: 547 \tTraining Loss: 0.450535 \tR2: 0.421991\n",
      "Epoch: 548 \tTraining Loss: 0.569135\n",
      "Epoch: 548 \tTraining Loss: 0.445999 \tR2: 0.421991\n",
      "Epoch: 549 \tTraining Loss: 0.571610\n",
      "Epoch: 549 \tTraining Loss: 0.489169 \tR2: 0.421991\n",
      "Epoch: 550 \tTraining Loss: 0.579202\n",
      "Epoch: 550 \tTraining Loss: 0.458600 \tR2: 0.421991\n",
      "Epoch: 551 \tTraining Loss: 0.569372\n",
      "Epoch: 551 \tTraining Loss: 0.459106 \tR2: 0.421991\n",
      "Epoch: 552 \tTraining Loss: 0.544250\n",
      "Epoch: 552 \tTraining Loss: 0.441207 \tR2: 0.421991\n",
      "Epoch: 553 \tTraining Loss: 0.593263\n",
      "Epoch: 553 \tTraining Loss: 0.466608 \tR2: 0.421991\n",
      "Epoch: 554 \tTraining Loss: 0.575621\n",
      "Epoch: 554 \tTraining Loss: 0.486789 \tR2: 0.421991\n",
      "Epoch: 555 \tTraining Loss: 0.577677\n",
      "Epoch: 555 \tTraining Loss: 0.443081 \tR2: 0.421991\n",
      "Epoch: 556 \tTraining Loss: 0.573580\n",
      "Epoch: 556 \tTraining Loss: 0.490131 \tR2: 0.421991\n",
      "Epoch: 557 \tTraining Loss: 0.566718\n",
      "Epoch: 557 \tTraining Loss: 0.459636 \tR2: 0.421991\n",
      "Epoch: 558 \tTraining Loss: 0.569045\n",
      "Epoch: 558 \tTraining Loss: 0.456817 \tR2: 0.421991\n",
      "Epoch: 559 \tTraining Loss: 0.566171\n",
      "Epoch: 559 \tTraining Loss: 0.454829 \tR2: 0.421991\n",
      "Epoch: 560 \tTraining Loss: 0.565109\n",
      "Epoch: 560 \tTraining Loss: 0.454985 \tR2: 0.421991\n",
      "Epoch: 561 \tTraining Loss: 0.576245\n",
      "Epoch: 561 \tTraining Loss: 0.422906 \tR2: 0.421991\n",
      "Epoch: 562 \tTraining Loss: 0.574608\n",
      "Epoch: 562 \tTraining Loss: 0.441342 \tR2: 0.421991\n",
      "Epoch: 563 \tTraining Loss: 0.564189\n",
      "Epoch: 563 \tTraining Loss: 0.456839 \tR2: 0.421991\n",
      "Epoch: 564 \tTraining Loss: 0.576828\n",
      "Epoch: 564 \tTraining Loss: 0.597148 \tR2: 0.421991\n",
      "Epoch: 565 \tTraining Loss: 0.588224\n",
      "Epoch: 565 \tTraining Loss: 0.446099 \tR2: 0.421991\n",
      "Epoch: 566 \tTraining Loss: 0.568745\n",
      "Epoch: 566 \tTraining Loss: 0.465253 \tR2: 0.421991\n",
      "Epoch: 567 \tTraining Loss: 0.565799\n",
      "Epoch: 567 \tTraining Loss: 0.467277 \tR2: 0.421991\n",
      "Epoch: 568 \tTraining Loss: 0.572339\n",
      "Epoch: 568 \tTraining Loss: 0.454799 \tR2: 0.421991\n",
      "Epoch: 569 \tTraining Loss: 0.574084\n",
      "Epoch: 569 \tTraining Loss: 0.468275 \tR2: 0.421991\n",
      "Epoch: 570 \tTraining Loss: 0.577169\n",
      "Epoch: 570 \tTraining Loss: 0.470729 \tR2: 0.421991\n",
      "Epoch: 571 \tTraining Loss: 0.558046\n",
      "Epoch: 571 \tTraining Loss: 0.456333 \tR2: 0.421991\n",
      "Epoch: 572 \tTraining Loss: 0.570986\n",
      "Epoch: 572 \tTraining Loss: 0.436681 \tR2: 0.421991\n",
      "Epoch: 573 \tTraining Loss: 0.573975\n",
      "Epoch: 573 \tTraining Loss: 0.448789 \tR2: 0.421991\n",
      "Epoch: 574 \tTraining Loss: 0.574333\n",
      "Epoch: 574 \tTraining Loss: 0.451212 \tR2: 0.421991\n",
      "Epoch: 575 \tTraining Loss: 0.557097\n",
      "Epoch: 575 \tTraining Loss: 0.413053 \tR2: 0.421991\n",
      "Epoch: 576 \tTraining Loss: 0.579067\n",
      "Epoch: 576 \tTraining Loss: 0.474805 \tR2: 0.421991\n",
      "Epoch: 577 \tTraining Loss: 0.557954\n",
      "Epoch: 577 \tTraining Loss: 0.533948 \tR2: 0.421991\n",
      "Epoch: 578 \tTraining Loss: 0.579871\n",
      "Epoch: 578 \tTraining Loss: 0.553493 \tR2: 0.421991\n",
      "Epoch: 579 \tTraining Loss: 0.568293\n",
      "Epoch: 579 \tTraining Loss: 0.424124 \tR2: 0.421991\n",
      "Epoch: 580 \tTraining Loss: 0.564884\n",
      "Epoch: 580 \tTraining Loss: 0.455461 \tR2: 0.421991\n",
      "Epoch: 581 \tTraining Loss: 0.567536\n",
      "Epoch: 581 \tTraining Loss: 0.452191 \tR2: 0.421991\n",
      "Epoch: 582 \tTraining Loss: 0.570137\n",
      "Epoch: 582 \tTraining Loss: 0.554588 \tR2: 0.421991\n",
      "Epoch: 583 \tTraining Loss: 0.569938\n",
      "Epoch: 583 \tTraining Loss: 0.470784 \tR2: 0.421991\n",
      "Epoch: 584 \tTraining Loss: 0.574262\n",
      "Epoch: 584 \tTraining Loss: 0.464417 \tR2: 0.421991\n",
      "Epoch: 585 \tTraining Loss: 0.580244\n",
      "Epoch: 585 \tTraining Loss: 0.524340 \tR2: 0.421991\n",
      "Epoch: 586 \tTraining Loss: 0.574644\n",
      "Epoch: 586 \tTraining Loss: 0.439790 \tR2: 0.421991\n",
      "Epoch: 587 \tTraining Loss: 0.571504\n",
      "Epoch: 587 \tTraining Loss: 0.447059 \tR2: 0.421991\n",
      "Epoch: 588 \tTraining Loss: 0.563349\n",
      "Epoch: 588 \tTraining Loss: 0.432198 \tR2: 0.421991\n",
      "Epoch: 589 \tTraining Loss: 0.584457\n",
      "Epoch: 589 \tTraining Loss: 0.416537 \tR2: 0.421991\n",
      "Epoch: 590 \tTraining Loss: 0.588327\n",
      "Epoch: 590 \tTraining Loss: 0.418232 \tR2: 0.421991\n",
      "Epoch: 591 \tTraining Loss: 0.575870\n",
      "Epoch: 591 \tTraining Loss: 0.475748 \tR2: 0.421991\n",
      "Epoch: 592 \tTraining Loss: 0.569731\n",
      "Epoch: 592 \tTraining Loss: 0.452275 \tR2: 0.421991\n",
      "Epoch: 593 \tTraining Loss: 0.570902\n",
      "Epoch: 593 \tTraining Loss: 0.423096 \tR2: 0.421991\n",
      "Epoch: 594 \tTraining Loss: 0.574272\n",
      "Epoch: 594 \tTraining Loss: 0.462045 \tR2: 0.421991\n",
      "Epoch: 595 \tTraining Loss: 0.574326\n",
      "Epoch: 595 \tTraining Loss: 0.433753 \tR2: 0.421991\n",
      "Epoch: 596 \tTraining Loss: 0.580649\n",
      "Epoch: 596 \tTraining Loss: 0.416057 \tR2: 0.421991\n",
      "Epoch: 597 \tTraining Loss: 0.564259\n",
      "Epoch: 597 \tTraining Loss: 0.425172 \tR2: 0.421991\n",
      "Epoch: 598 \tTraining Loss: 0.563521\n",
      "Epoch: 598 \tTraining Loss: 0.443794 \tR2: 0.421991\n",
      "Epoch: 599 \tTraining Loss: 0.547098\n",
      "Epoch: 599 \tTraining Loss: 0.421210 \tR2: 0.421991\n",
      "Epoch: 600 \tTraining Loss: 0.567457\n",
      "Epoch: 600 \tTraining Loss: 0.490843 \tR2: 0.463056\n",
      "Epoch: 601 \tTraining Loss: 0.574147\n",
      "Epoch: 601 \tTraining Loss: 0.441089 \tR2: 0.463056\n",
      "Epoch: 602 \tTraining Loss: 0.567957\n",
      "Epoch: 602 \tTraining Loss: 0.438107 \tR2: 0.463056\n",
      "Epoch: 603 \tTraining Loss: 0.570770\n",
      "Epoch: 603 \tTraining Loss: 0.467388 \tR2: 0.463056\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SRTP2022\\testing\\GNNWR.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1000\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     val(epoch)\n",
      "\u001b[1;32md:\\SRTP2022\\testing\\GNNWR.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m a \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m b \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\optim\\optimizer.py:112\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\autograd\\profiler.py:446\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[0;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1000+1):\n",
    "    train(epoch)\n",
    "    val(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1321"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "861f9c34f7302a1aedb62edfc1533c524ce2793735e6b405602ea89eb9cb2484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
