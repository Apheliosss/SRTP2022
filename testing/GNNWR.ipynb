{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.images = df.iloc[:,5:].values\n",
    "        self.coef = df.iloc[:,1:5].values\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        coef = self.coef[idx]\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        coef = torch.tensor(coef, dtype=torch.float)\n",
    "\n",
    "        return image, coef, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D://CO2_data4.csv\", encoding=\"utf-8\")\n",
    "dataset.shape[0]\n",
    "train_li = random.sample([i for i in range(0, dataset.shape[0])], int(0.8 * dataset.shape[0]))\n",
    "train_li.sort()\n",
    "\n",
    "j = 0\n",
    "test_li = []\n",
    "\n",
    "\n",
    "for i in range(0, dataset.shape[0], 1):\n",
    "    if i != train_li[j]:\n",
    "        test_li.append(i)\n",
    "    else:\n",
    "        j = j + 1\n",
    "\n",
    "train_set = dataset.iloc[train_li, :]\n",
    "test_set = dataset.iloc[test_li, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(P, C):\n",
    "    A = (P**2).sum(axis=1, keepdims=True)\n",
    " \n",
    "    B = (C**2).sum(axis=1, keepdims=True).T\n",
    " \n",
    "    return np.sqrt(A + B - 2* np.dot(P, C.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "dataset = train_set.reset_index(drop=True)\n",
    "ycor = dataset.lat\n",
    "#ycor = dataset.lon\n",
    "label = dataset.fCO2\n",
    "\n",
    "train_df['label'] = label\n",
    "\n",
    "train_df['beta'] = np.ones(5283)\n",
    "train_df['Chl'] = dataset.Chl\n",
    "train_df['Temp'] = dataset.Temp\n",
    "train_df['Salt'] = dataset.Salt\n",
    "\n",
    "alist = dataset.lon\n",
    "temp = []\n",
    "for i in alist:\n",
    "    if i < 0:\n",
    "        i = i+360\n",
    "    temp.append(i)\n",
    "xcor = temp\n",
    "\n",
    "cor_df = pd.DataFrame()\n",
    "cor_df['xcor'] = xcor\n",
    "cor_df['ycor'] = ycor\n",
    "\n",
    "a = [[110.0, 0.0], [290.0,0.0], [110.0, 70.0], [290.0, 70.0]]\n",
    "b = np.array(a)\n",
    "\n",
    "cor_li = cor_df.to_numpy()\n",
    "dis_li = compute_distances(cor_li, b)\n",
    "dis_df = pd.DataFrame(dis_li)\n",
    "train_df = train_df.join(dis_df)\n",
    "\n",
    "\n",
    "\n",
    "train_data = MYDataset(train_df)\n",
    "#test_data = MYDataset(test_df)\n",
    "train_loader = DataLoader(train_data, batch_size=50, shuffle=True, num_workers=0, drop_last=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>fCO2</th>\n",
       "      <th>Chl</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Salt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-110.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>1.429020</td>\n",
       "      <td>0.932204</td>\n",
       "      <td>1.035501</td>\n",
       "      <td>3.714037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-111.75</td>\n",
       "      <td>22.25</td>\n",
       "      <td>0.653314</td>\n",
       "      <td>0.865872</td>\n",
       "      <td>0.980157</td>\n",
       "      <td>3.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-111.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.835711</td>\n",
       "      <td>0.874641</td>\n",
       "      <td>0.896934</td>\n",
       "      <td>3.617013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-112.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.925815</td>\n",
       "      <td>0.861615</td>\n",
       "      <td>0.827733</td>\n",
       "      <td>3.544569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998/7/16</td>\n",
       "      <td>-113.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>1.074433</td>\n",
       "      <td>0.922670</td>\n",
       "      <td>0.545926</td>\n",
       "      <td>3.229704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.75</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.178217</td>\n",
       "      <td>0.770692</td>\n",
       "      <td>1.481354</td>\n",
       "      <td>-0.398306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.25</td>\n",
       "      <td>12.75</td>\n",
       "      <td>1.116893</td>\n",
       "      <td>0.768039</td>\n",
       "      <td>1.489461</td>\n",
       "      <td>-0.467678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.75</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.849520</td>\n",
       "      <td>0.791038</td>\n",
       "      <td>1.508860</td>\n",
       "      <td>-0.471190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>137.25</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.219482</td>\n",
       "      <td>0.770857</td>\n",
       "      <td>1.481271</td>\n",
       "      <td>-0.520788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>2020/7/16</td>\n",
       "      <td>136.75</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.149187</td>\n",
       "      <td>0.770841</td>\n",
       "      <td>1.480279</td>\n",
       "      <td>-0.613381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5283 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     lon    lat      fCO2       Chl      Temp      Salt\n",
       "0     1998/7/16 -110.25  22.25  1.429020  0.932204  1.035501  3.714037\n",
       "1     1998/7/16 -111.75  22.25  0.653314  0.865872  0.980157  3.665900\n",
       "2     1998/7/16 -111.25  23.25  1.835711  0.874641  0.896934  3.617013\n",
       "3     1998/7/16 -112.25  23.25  1.925815  0.861615  0.827733  3.544569\n",
       "4     1998/7/16 -113.25  25.25  1.074433  0.922670  0.545926  3.229704\n",
       "...         ...     ...    ...       ...       ...       ...       ...\n",
       "5278  2020/7/16  137.75  13.25  1.178217  0.770692  1.481354 -0.398306\n",
       "5279  2020/7/16  137.25  12.75  1.116893  0.768039  1.489461 -0.467678\n",
       "5280  2020/7/16  137.75   8.75  0.849520  0.791038  1.508860 -0.471190\n",
       "5281  2020/7/16  137.25  14.25  1.219482  0.770857  1.481271 -0.520788\n",
       "5282  2020/7/16  136.75  14.25  1.149187  0.770841  1.480279 -0.613381\n",
       "\n",
       "[5283 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class GNNWR(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(GNNWR, self).__init__()\n",
    "        self.insize = insize\n",
    "        self.outsize = outsize\n",
    "\n",
    "        lastsize = self.insize\n",
    "        thissize = 0\n",
    "        self.fc = nn.Sequential()\n",
    "        i = 2\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(1), nn.Linear(4, 600))\n",
    "        # self.fc.add_module(\"batc\"+str(1), nn.BatchNorm1d(600))\n",
    "        # self.fc.add_module(\"acti\"+str(1), nn.PReLU(init=0.4))\n",
    "        # self.fc.add_module(\"drop\"+str(1), nn.Dropout(0.2))\n",
    "\n",
    "        lastsize = 600\n",
    "        while math.pow(2, int(math.log2(lastsize))) >= max(128, outsize + 1):\n",
    "            if i == 1:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize))))\n",
    "            else:\n",
    "                thissize = int(math.pow(2, int(math.log2(lastsize)) - 1))\n",
    "            \n",
    "            self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, thissize))\n",
    "            self.fc.add_module(\"batc\"+str(i), nn.BatchNorm1d(thissize))\n",
    "            self.fc.add_module(\"acti\"+str(i), nn.PReLU(init=0.4))\n",
    "            \n",
    "            self.fc.add_module(\"drop\"+str(i), nn.Dropout(0.2))\n",
    "\n",
    "            lastsize = thissize\n",
    "            i = i + 1\n",
    "\n",
    "        self.fc.add_module(\"full\"+str(i), nn.Linear(lastsize, outsize))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = GNNWR(623, 4)\n",
    "criterion = nn.MSELoss(reduce=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 0\n",
    "weightlist = []\n",
    "for i in range(1,2):\n",
    "    temp = []\n",
    "    temp.append(-0.172075)\n",
    "    temp.append(-0.175203)\n",
    "    temp.append(0.294790)\n",
    "    temp.append(0.385374)\n",
    "    weightlist.append(temp)\n",
    "out = nn.Linear(4, 1, bias = False)\n",
    "out.weight = nn.Parameter(torch.tensor(weightlist), requires_grad=False)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    global r2\n",
    "    global out\n",
    "    for data, coef, label in train_loader:\n",
    "        data = data.view(50, -1)\n",
    "        label = label.view(50, -1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        output = output.mul(coef)\n",
    "        output = out(output)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        a = output.view(-1).detach().numpy()\n",
    "        b = label.view(-1).numpy()\n",
    "        if epoch % 100 == 0:\n",
    "            r2 = r2_score(a, b)\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tR2: {:.6f}'.format(epoch, train_loss, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.991468 \tR2: 0.000000\n",
      "Epoch: 2 \tTraining Loss: 0.881064 \tR2: 0.000000\n",
      "Epoch: 3 \tTraining Loss: 0.868819 \tR2: 0.000000\n",
      "Epoch: 4 \tTraining Loss: 0.846512 \tR2: 0.000000\n",
      "Epoch: 5 \tTraining Loss: 0.845521 \tR2: 0.000000\n",
      "Epoch: 6 \tTraining Loss: 0.834410 \tR2: 0.000000\n",
      "Epoch: 7 \tTraining Loss: 0.816901 \tR2: 0.000000\n",
      "Epoch: 8 \tTraining Loss: 0.814040 \tR2: 0.000000\n",
      "Epoch: 9 \tTraining Loss: 0.812420 \tR2: 0.000000\n",
      "Epoch: 10 \tTraining Loss: 0.828404 \tR2: 0.000000\n",
      "Epoch: 11 \tTraining Loss: 0.803942 \tR2: 0.000000\n",
      "Epoch: 12 \tTraining Loss: 0.796138 \tR2: 0.000000\n",
      "Epoch: 13 \tTraining Loss: 0.789802 \tR2: 0.000000\n",
      "Epoch: 14 \tTraining Loss: 0.789559 \tR2: 0.000000\n",
      "Epoch: 15 \tTraining Loss: 0.792491 \tR2: 0.000000\n",
      "Epoch: 16 \tTraining Loss: 0.784210 \tR2: 0.000000\n",
      "Epoch: 17 \tTraining Loss: 0.777894 \tR2: 0.000000\n",
      "Epoch: 18 \tTraining Loss: 0.773159 \tR2: 0.000000\n",
      "Epoch: 19 \tTraining Loss: 0.764474 \tR2: 0.000000\n",
      "Epoch: 20 \tTraining Loss: 0.763126 \tR2: 0.000000\n",
      "Epoch: 21 \tTraining Loss: 0.757104 \tR2: 0.000000\n",
      "Epoch: 22 \tTraining Loss: 0.765610 \tR2: 0.000000\n",
      "Epoch: 23 \tTraining Loss: 0.754548 \tR2: 0.000000\n",
      "Epoch: 24 \tTraining Loss: 0.755419 \tR2: 0.000000\n",
      "Epoch: 25 \tTraining Loss: 0.762537 \tR2: 0.000000\n",
      "Epoch: 26 \tTraining Loss: 0.742504 \tR2: 0.000000\n",
      "Epoch: 27 \tTraining Loss: 0.736484 \tR2: 0.000000\n",
      "Epoch: 28 \tTraining Loss: 0.731977 \tR2: 0.000000\n",
      "Epoch: 29 \tTraining Loss: 0.715684 \tR2: 0.000000\n",
      "Epoch: 30 \tTraining Loss: 0.749267 \tR2: 0.000000\n",
      "Epoch: 31 \tTraining Loss: 0.745482 \tR2: 0.000000\n",
      "Epoch: 32 \tTraining Loss: 0.737945 \tR2: 0.000000\n",
      "Epoch: 33 \tTraining Loss: 0.725630 \tR2: 0.000000\n",
      "Epoch: 34 \tTraining Loss: 0.718438 \tR2: 0.000000\n",
      "Epoch: 35 \tTraining Loss: 0.715014 \tR2: 0.000000\n",
      "Epoch: 36 \tTraining Loss: 0.730875 \tR2: 0.000000\n",
      "Epoch: 37 \tTraining Loss: 0.717860 \tR2: 0.000000\n",
      "Epoch: 38 \tTraining Loss: 0.708584 \tR2: 0.000000\n",
      "Epoch: 39 \tTraining Loss: 0.726150 \tR2: 0.000000\n",
      "Epoch: 40 \tTraining Loss: 0.727860 \tR2: 0.000000\n",
      "Epoch: 41 \tTraining Loss: 0.720409 \tR2: 0.000000\n",
      "Epoch: 42 \tTraining Loss: 0.721919 \tR2: 0.000000\n",
      "Epoch: 43 \tTraining Loss: 0.703985 \tR2: 0.000000\n",
      "Epoch: 44 \tTraining Loss: 0.706463 \tR2: 0.000000\n",
      "Epoch: 45 \tTraining Loss: 0.722276 \tR2: 0.000000\n",
      "Epoch: 46 \tTraining Loss: 0.727866 \tR2: 0.000000\n",
      "Epoch: 47 \tTraining Loss: 0.722390 \tR2: 0.000000\n",
      "Epoch: 48 \tTraining Loss: 0.717603 \tR2: 0.000000\n",
      "Epoch: 49 \tTraining Loss: 0.699726 \tR2: 0.000000\n",
      "Epoch: 50 \tTraining Loss: 0.710257 \tR2: 0.000000\n",
      "Epoch: 51 \tTraining Loss: 0.700526 \tR2: 0.000000\n",
      "Epoch: 52 \tTraining Loss: 0.708421 \tR2: 0.000000\n",
      "Epoch: 53 \tTraining Loss: 0.706077 \tR2: 0.000000\n",
      "Epoch: 54 \tTraining Loss: 0.708492 \tR2: 0.000000\n",
      "Epoch: 55 \tTraining Loss: 0.705272 \tR2: 0.000000\n",
      "Epoch: 56 \tTraining Loss: 0.696552 \tR2: 0.000000\n",
      "Epoch: 57 \tTraining Loss: 0.708406 \tR2: 0.000000\n",
      "Epoch: 58 \tTraining Loss: 0.695210 \tR2: 0.000000\n",
      "Epoch: 59 \tTraining Loss: 0.700950 \tR2: 0.000000\n",
      "Epoch: 60 \tTraining Loss: 0.709686 \tR2: 0.000000\n",
      "Epoch: 61 \tTraining Loss: 0.703456 \tR2: 0.000000\n",
      "Epoch: 62 \tTraining Loss: 0.698299 \tR2: 0.000000\n",
      "Epoch: 63 \tTraining Loss: 0.707405 \tR2: 0.000000\n",
      "Epoch: 64 \tTraining Loss: 0.713495 \tR2: 0.000000\n",
      "Epoch: 65 \tTraining Loss: 0.694526 \tR2: 0.000000\n",
      "Epoch: 66 \tTraining Loss: 0.694215 \tR2: 0.000000\n",
      "Epoch: 67 \tTraining Loss: 0.696748 \tR2: 0.000000\n",
      "Epoch: 68 \tTraining Loss: 0.711348 \tR2: 0.000000\n",
      "Epoch: 69 \tTraining Loss: 0.690556 \tR2: 0.000000\n",
      "Epoch: 70 \tTraining Loss: 0.709529 \tR2: 0.000000\n",
      "Epoch: 71 \tTraining Loss: 0.701744 \tR2: 0.000000\n",
      "Epoch: 72 \tTraining Loss: 0.679289 \tR2: 0.000000\n",
      "Epoch: 73 \tTraining Loss: 0.685462 \tR2: 0.000000\n",
      "Epoch: 74 \tTraining Loss: 0.693324 \tR2: 0.000000\n",
      "Epoch: 75 \tTraining Loss: 0.687450 \tR2: 0.000000\n",
      "Epoch: 76 \tTraining Loss: 0.682751 \tR2: 0.000000\n",
      "Epoch: 77 \tTraining Loss: 0.681072 \tR2: 0.000000\n",
      "Epoch: 78 \tTraining Loss: 0.701133 \tR2: 0.000000\n",
      "Epoch: 79 \tTraining Loss: 0.686495 \tR2: 0.000000\n",
      "Epoch: 80 \tTraining Loss: 0.686273 \tR2: 0.000000\n",
      "Epoch: 81 \tTraining Loss: 0.684529 \tR2: 0.000000\n",
      "Epoch: 82 \tTraining Loss: 0.689724 \tR2: 0.000000\n",
      "Epoch: 83 \tTraining Loss: 0.686416 \tR2: 0.000000\n",
      "Epoch: 84 \tTraining Loss: 0.675156 \tR2: 0.000000\n",
      "Epoch: 85 \tTraining Loss: 0.676777 \tR2: 0.000000\n",
      "Epoch: 86 \tTraining Loss: 0.693352 \tR2: 0.000000\n",
      "Epoch: 87 \tTraining Loss: 0.676451 \tR2: 0.000000\n",
      "Epoch: 88 \tTraining Loss: 0.675022 \tR2: 0.000000\n",
      "Epoch: 89 \tTraining Loss: 0.678562 \tR2: 0.000000\n",
      "Epoch: 90 \tTraining Loss: 0.676913 \tR2: 0.000000\n",
      "Epoch: 91 \tTraining Loss: 0.672017 \tR2: 0.000000\n",
      "Epoch: 92 \tTraining Loss: 0.695248 \tR2: 0.000000\n",
      "Epoch: 93 \tTraining Loss: 0.674692 \tR2: 0.000000\n",
      "Epoch: 94 \tTraining Loss: 0.669902 \tR2: 0.000000\n",
      "Epoch: 95 \tTraining Loss: 0.675081 \tR2: 0.000000\n",
      "Epoch: 96 \tTraining Loss: 0.672051 \tR2: 0.000000\n",
      "Epoch: 97 \tTraining Loss: 0.679060 \tR2: 0.000000\n",
      "Epoch: 98 \tTraining Loss: 0.681569 \tR2: 0.000000\n",
      "Epoch: 99 \tTraining Loss: 0.659945 \tR2: 0.000000\n",
      "Epoch: 100 \tTraining Loss: 0.667685 \tR2: 0.080640\n",
      "Epoch: 101 \tTraining Loss: 0.655953 \tR2: 0.080640\n",
      "Epoch: 102 \tTraining Loss: 0.667400 \tR2: 0.080640\n",
      "Epoch: 103 \tTraining Loss: 0.659111 \tR2: 0.080640\n",
      "Epoch: 104 \tTraining Loss: 0.657787 \tR2: 0.080640\n",
      "Epoch: 105 \tTraining Loss: 0.668248 \tR2: 0.080640\n",
      "Epoch: 106 \tTraining Loss: 0.662546 \tR2: 0.080640\n",
      "Epoch: 107 \tTraining Loss: 0.653469 \tR2: 0.080640\n",
      "Epoch: 108 \tTraining Loss: 0.653579 \tR2: 0.080640\n",
      "Epoch: 109 \tTraining Loss: 0.662444 \tR2: 0.080640\n",
      "Epoch: 110 \tTraining Loss: 0.656084 \tR2: 0.080640\n",
      "Epoch: 111 \tTraining Loss: 0.664732 \tR2: 0.080640\n",
      "Epoch: 112 \tTraining Loss: 0.662557 \tR2: 0.080640\n",
      "Epoch: 113 \tTraining Loss: 0.656546 \tR2: 0.080640\n",
      "Epoch: 114 \tTraining Loss: 0.653793 \tR2: 0.080640\n",
      "Epoch: 115 \tTraining Loss: 0.667003 \tR2: 0.080640\n",
      "Epoch: 116 \tTraining Loss: 0.660913 \tR2: 0.080640\n",
      "Epoch: 117 \tTraining Loss: 0.661085 \tR2: 0.080640\n",
      "Epoch: 118 \tTraining Loss: 0.652139 \tR2: 0.080640\n",
      "Epoch: 119 \tTraining Loss: 0.663684 \tR2: 0.080640\n",
      "Epoch: 120 \tTraining Loss: 0.662879 \tR2: 0.080640\n",
      "Epoch: 121 \tTraining Loss: 0.652411 \tR2: 0.080640\n",
      "Epoch: 122 \tTraining Loss: 0.650941 \tR2: 0.080640\n",
      "Epoch: 123 \tTraining Loss: 0.646593 \tR2: 0.080640\n",
      "Epoch: 124 \tTraining Loss: 0.658603 \tR2: 0.080640\n",
      "Epoch: 125 \tTraining Loss: 0.656348 \tR2: 0.080640\n",
      "Epoch: 126 \tTraining Loss: 0.652997 \tR2: 0.080640\n",
      "Epoch: 127 \tTraining Loss: 0.653938 \tR2: 0.080640\n",
      "Epoch: 128 \tTraining Loss: 0.645648 \tR2: 0.080640\n",
      "Epoch: 129 \tTraining Loss: 0.656678 \tR2: 0.080640\n",
      "Epoch: 130 \tTraining Loss: 0.642151 \tR2: 0.080640\n",
      "Epoch: 131 \tTraining Loss: 0.651752 \tR2: 0.080640\n",
      "Epoch: 132 \tTraining Loss: 0.643237 \tR2: 0.080640\n",
      "Epoch: 133 \tTraining Loss: 0.639414 \tR2: 0.080640\n",
      "Epoch: 134 \tTraining Loss: 0.661823 \tR2: 0.080640\n",
      "Epoch: 135 \tTraining Loss: 0.652175 \tR2: 0.080640\n",
      "Epoch: 136 \tTraining Loss: 0.645675 \tR2: 0.080640\n",
      "Epoch: 137 \tTraining Loss: 0.657406 \tR2: 0.080640\n",
      "Epoch: 138 \tTraining Loss: 0.637001 \tR2: 0.080640\n",
      "Epoch: 139 \tTraining Loss: 0.641429 \tR2: 0.080640\n",
      "Epoch: 140 \tTraining Loss: 0.634886 \tR2: 0.080640\n",
      "Epoch: 141 \tTraining Loss: 0.634392 \tR2: 0.080640\n",
      "Epoch: 142 \tTraining Loss: 0.647750 \tR2: 0.080640\n",
      "Epoch: 143 \tTraining Loss: 0.655504 \tR2: 0.080640\n",
      "Epoch: 144 \tTraining Loss: 0.644327 \tR2: 0.080640\n",
      "Epoch: 145 \tTraining Loss: 0.646488 \tR2: 0.080640\n",
      "Epoch: 146 \tTraining Loss: 0.640285 \tR2: 0.080640\n",
      "Epoch: 147 \tTraining Loss: 0.642613 \tR2: 0.080640\n",
      "Epoch: 148 \tTraining Loss: 0.635760 \tR2: 0.080640\n",
      "Epoch: 149 \tTraining Loss: 0.649074 \tR2: 0.080640\n",
      "Epoch: 150 \tTraining Loss: 0.668609 \tR2: 0.080640\n",
      "Epoch: 151 \tTraining Loss: 0.647477 \tR2: 0.080640\n",
      "Epoch: 152 \tTraining Loss: 0.646605 \tR2: 0.080640\n",
      "Epoch: 153 \tTraining Loss: 0.634469 \tR2: 0.080640\n",
      "Epoch: 154 \tTraining Loss: 0.651029 \tR2: 0.080640\n",
      "Epoch: 155 \tTraining Loss: 0.647292 \tR2: 0.080640\n",
      "Epoch: 156 \tTraining Loss: 0.635315 \tR2: 0.080640\n",
      "Epoch: 157 \tTraining Loss: 0.631761 \tR2: 0.080640\n",
      "Epoch: 158 \tTraining Loss: 0.640051 \tR2: 0.080640\n",
      "Epoch: 159 \tTraining Loss: 0.640732 \tR2: 0.080640\n",
      "Epoch: 160 \tTraining Loss: 0.641337 \tR2: 0.080640\n",
      "Epoch: 161 \tTraining Loss: 0.634797 \tR2: 0.080640\n",
      "Epoch: 162 \tTraining Loss: 0.627777 \tR2: 0.080640\n",
      "Epoch: 163 \tTraining Loss: 0.636188 \tR2: 0.080640\n",
      "Epoch: 164 \tTraining Loss: 0.638039 \tR2: 0.080640\n",
      "Epoch: 165 \tTraining Loss: 0.644586 \tR2: 0.080640\n",
      "Epoch: 166 \tTraining Loss: 0.633788 \tR2: 0.080640\n",
      "Epoch: 167 \tTraining Loss: 0.643125 \tR2: 0.080640\n",
      "Epoch: 168 \tTraining Loss: 0.633822 \tR2: 0.080640\n",
      "Epoch: 169 \tTraining Loss: 0.637049 \tR2: 0.080640\n",
      "Epoch: 170 \tTraining Loss: 0.628684 \tR2: 0.080640\n",
      "Epoch: 171 \tTraining Loss: 0.640344 \tR2: 0.080640\n",
      "Epoch: 172 \tTraining Loss: 0.653612 \tR2: 0.080640\n",
      "Epoch: 173 \tTraining Loss: 0.634212 \tR2: 0.080640\n",
      "Epoch: 174 \tTraining Loss: 0.631441 \tR2: 0.080640\n",
      "Epoch: 175 \tTraining Loss: 0.642226 \tR2: 0.080640\n",
      "Epoch: 176 \tTraining Loss: 0.633857 \tR2: 0.080640\n",
      "Epoch: 177 \tTraining Loss: 0.641631 \tR2: 0.080640\n",
      "Epoch: 178 \tTraining Loss: 0.635116 \tR2: 0.080640\n",
      "Epoch: 179 \tTraining Loss: 0.632709 \tR2: 0.080640\n",
      "Epoch: 180 \tTraining Loss: 0.636765 \tR2: 0.080640\n",
      "Epoch: 181 \tTraining Loss: 0.621910 \tR2: 0.080640\n",
      "Epoch: 182 \tTraining Loss: 0.620969 \tR2: 0.080640\n",
      "Epoch: 183 \tTraining Loss: 0.629054 \tR2: 0.080640\n",
      "Epoch: 184 \tTraining Loss: 0.621717 \tR2: 0.080640\n",
      "Epoch: 185 \tTraining Loss: 0.636578 \tR2: 0.080640\n",
      "Epoch: 186 \tTraining Loss: 0.619115 \tR2: 0.080640\n",
      "Epoch: 187 \tTraining Loss: 0.628930 \tR2: 0.080640\n",
      "Epoch: 188 \tTraining Loss: 0.636818 \tR2: 0.080640\n",
      "Epoch: 189 \tTraining Loss: 0.639403 \tR2: 0.080640\n",
      "Epoch: 190 \tTraining Loss: 0.621691 \tR2: 0.080640\n",
      "Epoch: 191 \tTraining Loss: 0.651587 \tR2: 0.080640\n",
      "Epoch: 192 \tTraining Loss: 0.638100 \tR2: 0.080640\n",
      "Epoch: 193 \tTraining Loss: 0.644292 \tR2: 0.080640\n",
      "Epoch: 194 \tTraining Loss: 0.635703 \tR2: 0.080640\n",
      "Epoch: 195 \tTraining Loss: 0.627592 \tR2: 0.080640\n",
      "Epoch: 196 \tTraining Loss: 0.618435 \tR2: 0.080640\n",
      "Epoch: 197 \tTraining Loss: 0.636598 \tR2: 0.080640\n",
      "Epoch: 198 \tTraining Loss: 0.608631 \tR2: 0.080640\n",
      "Epoch: 199 \tTraining Loss: 0.633863 \tR2: 0.080640\n",
      "Epoch: 200 \tTraining Loss: 0.611495 \tR2: 0.286171\n",
      "Epoch: 201 \tTraining Loss: 0.621344 \tR2: 0.286171\n",
      "Epoch: 202 \tTraining Loss: 0.612213 \tR2: 0.286171\n",
      "Epoch: 203 \tTraining Loss: 0.642477 \tR2: 0.286171\n",
      "Epoch: 204 \tTraining Loss: 0.622598 \tR2: 0.286171\n",
      "Epoch: 205 \tTraining Loss: 0.624480 \tR2: 0.286171\n",
      "Epoch: 206 \tTraining Loss: 0.632342 \tR2: 0.286171\n",
      "Epoch: 207 \tTraining Loss: 0.626105 \tR2: 0.286171\n",
      "Epoch: 208 \tTraining Loss: 0.616981 \tR2: 0.286171\n",
      "Epoch: 209 \tTraining Loss: 0.623264 \tR2: 0.286171\n",
      "Epoch: 210 \tTraining Loss: 0.618517 \tR2: 0.286171\n",
      "Epoch: 211 \tTraining Loss: 0.613412 \tR2: 0.286171\n",
      "Epoch: 212 \tTraining Loss: 0.619019 \tR2: 0.286171\n",
      "Epoch: 213 \tTraining Loss: 0.621409 \tR2: 0.286171\n",
      "Epoch: 214 \tTraining Loss: 0.620699 \tR2: 0.286171\n",
      "Epoch: 215 \tTraining Loss: 0.611496 \tR2: 0.286171\n",
      "Epoch: 216 \tTraining Loss: 0.620693 \tR2: 0.286171\n",
      "Epoch: 217 \tTraining Loss: 0.628921 \tR2: 0.286171\n",
      "Epoch: 218 \tTraining Loss: 0.586512 \tR2: 0.286171\n",
      "Epoch: 219 \tTraining Loss: 0.631457 \tR2: 0.286171\n",
      "Epoch: 220 \tTraining Loss: 0.610363 \tR2: 0.286171\n",
      "Epoch: 221 \tTraining Loss: 0.599659 \tR2: 0.286171\n",
      "Epoch: 222 \tTraining Loss: 0.611172 \tR2: 0.286171\n",
      "Epoch: 223 \tTraining Loss: 0.628123 \tR2: 0.286171\n",
      "Epoch: 224 \tTraining Loss: 0.621328 \tR2: 0.286171\n",
      "Epoch: 225 \tTraining Loss: 0.622818 \tR2: 0.286171\n",
      "Epoch: 226 \tTraining Loss: 0.623758 \tR2: 0.286171\n",
      "Epoch: 227 \tTraining Loss: 0.628933 \tR2: 0.286171\n",
      "Epoch: 228 \tTraining Loss: 0.622629 \tR2: 0.286171\n",
      "Epoch: 229 \tTraining Loss: 0.633210 \tR2: 0.286171\n",
      "Epoch: 230 \tTraining Loss: 0.634697 \tR2: 0.286171\n",
      "Epoch: 231 \tTraining Loss: 0.610271 \tR2: 0.286171\n",
      "Epoch: 232 \tTraining Loss: 0.605934 \tR2: 0.286171\n",
      "Epoch: 233 \tTraining Loss: 0.606725 \tR2: 0.286171\n",
      "Epoch: 234 \tTraining Loss: 0.614696 \tR2: 0.286171\n",
      "Epoch: 235 \tTraining Loss: 0.589602 \tR2: 0.286171\n",
      "Epoch: 236 \tTraining Loss: 0.626732 \tR2: 0.286171\n",
      "Epoch: 237 \tTraining Loss: 0.609289 \tR2: 0.286171\n",
      "Epoch: 238 \tTraining Loss: 0.603441 \tR2: 0.286171\n",
      "Epoch: 239 \tTraining Loss: 0.621417 \tR2: 0.286171\n",
      "Epoch: 240 \tTraining Loss: 0.623702 \tR2: 0.286171\n",
      "Epoch: 241 \tTraining Loss: 0.605265 \tR2: 0.286171\n",
      "Epoch: 242 \tTraining Loss: 0.608901 \tR2: 0.286171\n",
      "Epoch: 243 \tTraining Loss: 0.615590 \tR2: 0.286171\n",
      "Epoch: 244 \tTraining Loss: 0.616079 \tR2: 0.286171\n",
      "Epoch: 245 \tTraining Loss: 0.603672 \tR2: 0.286171\n",
      "Epoch: 246 \tTraining Loss: 0.606209 \tR2: 0.286171\n",
      "Epoch: 247 \tTraining Loss: 0.615787 \tR2: 0.286171\n",
      "Epoch: 248 \tTraining Loss: 0.617585 \tR2: 0.286171\n",
      "Epoch: 249 \tTraining Loss: 0.619648 \tR2: 0.286171\n",
      "Epoch: 250 \tTraining Loss: 0.613409 \tR2: 0.286171\n",
      "Epoch: 251 \tTraining Loss: 0.602219 \tR2: 0.286171\n",
      "Epoch: 252 \tTraining Loss: 0.594668 \tR2: 0.286171\n",
      "Epoch: 253 \tTraining Loss: 0.606679 \tR2: 0.286171\n",
      "Epoch: 254 \tTraining Loss: 0.603626 \tR2: 0.286171\n",
      "Epoch: 255 \tTraining Loss: 0.606717 \tR2: 0.286171\n",
      "Epoch: 256 \tTraining Loss: 0.605742 \tR2: 0.286171\n",
      "Epoch: 257 \tTraining Loss: 0.602773 \tR2: 0.286171\n",
      "Epoch: 258 \tTraining Loss: 0.605559 \tR2: 0.286171\n",
      "Epoch: 259 \tTraining Loss: 0.610726 \tR2: 0.286171\n",
      "Epoch: 260 \tTraining Loss: 0.613080 \tR2: 0.286171\n",
      "Epoch: 261 \tTraining Loss: 0.601022 \tR2: 0.286171\n",
      "Epoch: 262 \tTraining Loss: 0.602279 \tR2: 0.286171\n",
      "Epoch: 263 \tTraining Loss: 0.606437 \tR2: 0.286171\n",
      "Epoch: 264 \tTraining Loss: 0.599135 \tR2: 0.286171\n",
      "Epoch: 265 \tTraining Loss: 0.610617 \tR2: 0.286171\n",
      "Epoch: 266 \tTraining Loss: 0.603841 \tR2: 0.286171\n",
      "Epoch: 267 \tTraining Loss: 0.605746 \tR2: 0.286171\n",
      "Epoch: 268 \tTraining Loss: 0.600601 \tR2: 0.286171\n",
      "Epoch: 269 \tTraining Loss: 0.601436 \tR2: 0.286171\n",
      "Epoch: 270 \tTraining Loss: 0.604724 \tR2: 0.286171\n",
      "Epoch: 271 \tTraining Loss: 0.584578 \tR2: 0.286171\n",
      "Epoch: 272 \tTraining Loss: 0.604882 \tR2: 0.286171\n",
      "Epoch: 273 \tTraining Loss: 0.607350 \tR2: 0.286171\n",
      "Epoch: 274 \tTraining Loss: 0.606607 \tR2: 0.286171\n",
      "Epoch: 275 \tTraining Loss: 0.606895 \tR2: 0.286171\n",
      "Epoch: 276 \tTraining Loss: 0.611607 \tR2: 0.286171\n",
      "Epoch: 277 \tTraining Loss: 0.600122 \tR2: 0.286171\n",
      "Epoch: 278 \tTraining Loss: 0.603276 \tR2: 0.286171\n",
      "Epoch: 279 \tTraining Loss: 0.589373 \tR2: 0.286171\n",
      "Epoch: 280 \tTraining Loss: 0.610549 \tR2: 0.286171\n",
      "Epoch: 281 \tTraining Loss: 0.603439 \tR2: 0.286171\n",
      "Epoch: 282 \tTraining Loss: 0.599099 \tR2: 0.286171\n",
      "Epoch: 283 \tTraining Loss: 0.594510 \tR2: 0.286171\n",
      "Epoch: 284 \tTraining Loss: 0.591308 \tR2: 0.286171\n",
      "Epoch: 285 \tTraining Loss: 0.612612 \tR2: 0.286171\n",
      "Epoch: 286 \tTraining Loss: 0.591372 \tR2: 0.286171\n",
      "Epoch: 287 \tTraining Loss: 0.585505 \tR2: 0.286171\n",
      "Epoch: 288 \tTraining Loss: 0.605242 \tR2: 0.286171\n",
      "Epoch: 289 \tTraining Loss: 0.594560 \tR2: 0.286171\n",
      "Epoch: 290 \tTraining Loss: 0.597141 \tR2: 0.286171\n",
      "Epoch: 291 \tTraining Loss: 0.600371 \tR2: 0.286171\n",
      "Epoch: 292 \tTraining Loss: 0.597824 \tR2: 0.286171\n",
      "Epoch: 293 \tTraining Loss: 0.607540 \tR2: 0.286171\n",
      "Epoch: 294 \tTraining Loss: 0.601816 \tR2: 0.286171\n",
      "Epoch: 295 \tTraining Loss: 0.580373 \tR2: 0.286171\n",
      "Epoch: 296 \tTraining Loss: 0.609626 \tR2: 0.286171\n",
      "Epoch: 297 \tTraining Loss: 0.599306 \tR2: 0.286171\n",
      "Epoch: 298 \tTraining Loss: 0.586761 \tR2: 0.286171\n",
      "Epoch: 299 \tTraining Loss: 0.591998 \tR2: 0.286171\n",
      "Epoch: 300 \tTraining Loss: 0.579815 \tR2: -0.054816\n",
      "Epoch: 301 \tTraining Loss: 0.584257 \tR2: -0.054816\n",
      "Epoch: 302 \tTraining Loss: 0.592904 \tR2: -0.054816\n",
      "Epoch: 303 \tTraining Loss: 0.591464 \tR2: -0.054816\n",
      "Epoch: 304 \tTraining Loss: 0.600250 \tR2: -0.054816\n",
      "Epoch: 305 \tTraining Loss: 0.592621 \tR2: -0.054816\n",
      "Epoch: 306 \tTraining Loss: 0.602702 \tR2: -0.054816\n",
      "Epoch: 307 \tTraining Loss: 0.600579 \tR2: -0.054816\n",
      "Epoch: 308 \tTraining Loss: 0.596365 \tR2: -0.054816\n",
      "Epoch: 309 \tTraining Loss: 0.585952 \tR2: -0.054816\n",
      "Epoch: 310 \tTraining Loss: 0.591062 \tR2: -0.054816\n",
      "Epoch: 311 \tTraining Loss: 0.592664 \tR2: -0.054816\n",
      "Epoch: 312 \tTraining Loss: 0.583978 \tR2: -0.054816\n",
      "Epoch: 313 \tTraining Loss: 0.606377 \tR2: -0.054816\n",
      "Epoch: 314 \tTraining Loss: 0.591499 \tR2: -0.054816\n",
      "Epoch: 315 \tTraining Loss: 0.591605 \tR2: -0.054816\n",
      "Epoch: 316 \tTraining Loss: 0.606137 \tR2: -0.054816\n",
      "Epoch: 317 \tTraining Loss: 0.584099 \tR2: -0.054816\n",
      "Epoch: 318 \tTraining Loss: 0.597504 \tR2: -0.054816\n",
      "Epoch: 319 \tTraining Loss: 0.576833 \tR2: -0.054816\n",
      "Epoch: 320 \tTraining Loss: 0.599854 \tR2: -0.054816\n",
      "Epoch: 321 \tTraining Loss: 0.595744 \tR2: -0.054816\n",
      "Epoch: 322 \tTraining Loss: 0.606466 \tR2: -0.054816\n",
      "Epoch: 323 \tTraining Loss: 0.611473 \tR2: -0.054816\n",
      "Epoch: 324 \tTraining Loss: 0.579982 \tR2: -0.054816\n",
      "Epoch: 325 \tTraining Loss: 0.598812 \tR2: -0.054816\n",
      "Epoch: 326 \tTraining Loss: 0.586773 \tR2: -0.054816\n",
      "Epoch: 327 \tTraining Loss: 0.594671 \tR2: -0.054816\n",
      "Epoch: 328 \tTraining Loss: 0.588024 \tR2: -0.054816\n",
      "Epoch: 329 \tTraining Loss: 0.593206 \tR2: -0.054816\n",
      "Epoch: 330 \tTraining Loss: 0.592564 \tR2: -0.054816\n",
      "Epoch: 331 \tTraining Loss: 0.599432 \tR2: -0.054816\n",
      "Epoch: 332 \tTraining Loss: 0.590919 \tR2: -0.054816\n",
      "Epoch: 333 \tTraining Loss: 0.578676 \tR2: -0.054816\n",
      "Epoch: 334 \tTraining Loss: 0.588219 \tR2: -0.054816\n",
      "Epoch: 335 \tTraining Loss: 0.576794 \tR2: -0.054816\n",
      "Epoch: 336 \tTraining Loss: 0.608442 \tR2: -0.054816\n",
      "Epoch: 337 \tTraining Loss: 0.592307 \tR2: -0.054816\n",
      "Epoch: 338 \tTraining Loss: 0.586235 \tR2: -0.054816\n",
      "Epoch: 339 \tTraining Loss: 0.588578 \tR2: -0.054816\n",
      "Epoch: 340 \tTraining Loss: 0.582479 \tR2: -0.054816\n",
      "Epoch: 341 \tTraining Loss: 0.591458 \tR2: -0.054816\n",
      "Epoch: 342 \tTraining Loss: 0.569214 \tR2: -0.054816\n",
      "Epoch: 343 \tTraining Loss: 0.594271 \tR2: -0.054816\n",
      "Epoch: 344 \tTraining Loss: 0.594597 \tR2: -0.054816\n",
      "Epoch: 345 \tTraining Loss: 0.584896 \tR2: -0.054816\n",
      "Epoch: 346 \tTraining Loss: 0.604109 \tR2: -0.054816\n",
      "Epoch: 347 \tTraining Loss: 0.578934 \tR2: -0.054816\n",
      "Epoch: 348 \tTraining Loss: 0.595383 \tR2: -0.054816\n",
      "Epoch: 349 \tTraining Loss: 0.580869 \tR2: -0.054816\n",
      "Epoch: 350 \tTraining Loss: 0.577345 \tR2: -0.054816\n",
      "Epoch: 351 \tTraining Loss: 0.582114 \tR2: -0.054816\n",
      "Epoch: 352 \tTraining Loss: 0.579298 \tR2: -0.054816\n",
      "Epoch: 353 \tTraining Loss: 0.586939 \tR2: -0.054816\n",
      "Epoch: 354 \tTraining Loss: 0.572101 \tR2: -0.054816\n",
      "Epoch: 355 \tTraining Loss: 0.568923 \tR2: -0.054816\n",
      "Epoch: 356 \tTraining Loss: 0.583810 \tR2: -0.054816\n",
      "Epoch: 357 \tTraining Loss: 0.592685 \tR2: -0.054816\n",
      "Epoch: 358 \tTraining Loss: 0.580656 \tR2: -0.054816\n",
      "Epoch: 359 \tTraining Loss: 0.561030 \tR2: -0.054816\n",
      "Epoch: 360 \tTraining Loss: 0.557807 \tR2: -0.054816\n",
      "Epoch: 361 \tTraining Loss: 0.589597 \tR2: -0.054816\n",
      "Epoch: 362 \tTraining Loss: 0.574395 \tR2: -0.054816\n",
      "Epoch: 363 \tTraining Loss: 0.591516 \tR2: -0.054816\n",
      "Epoch: 364 \tTraining Loss: 0.587250 \tR2: -0.054816\n",
      "Epoch: 365 \tTraining Loss: 0.576809 \tR2: -0.054816\n",
      "Epoch: 366 \tTraining Loss: 0.588249 \tR2: -0.054816\n",
      "Epoch: 367 \tTraining Loss: 0.581892 \tR2: -0.054816\n",
      "Epoch: 368 \tTraining Loss: 0.577425 \tR2: -0.054816\n",
      "Epoch: 369 \tTraining Loss: 0.571997 \tR2: -0.054816\n",
      "Epoch: 370 \tTraining Loss: 0.582841 \tR2: -0.054816\n",
      "Epoch: 371 \tTraining Loss: 0.565513 \tR2: -0.054816\n",
      "Epoch: 372 \tTraining Loss: 0.609067 \tR2: -0.054816\n",
      "Epoch: 373 \tTraining Loss: 0.586003 \tR2: -0.054816\n",
      "Epoch: 374 \tTraining Loss: 0.572998 \tR2: -0.054816\n",
      "Epoch: 375 \tTraining Loss: 0.596748 \tR2: -0.054816\n",
      "Epoch: 376 \tTraining Loss: 0.578232 \tR2: -0.054816\n",
      "Epoch: 377 \tTraining Loss: 0.572707 \tR2: -0.054816\n",
      "Epoch: 378 \tTraining Loss: 0.584958 \tR2: -0.054816\n",
      "Epoch: 379 \tTraining Loss: 0.574812 \tR2: -0.054816\n",
      "Epoch: 380 \tTraining Loss: 0.577083 \tR2: -0.054816\n",
      "Epoch: 381 \tTraining Loss: 0.572575 \tR2: -0.054816\n",
      "Epoch: 382 \tTraining Loss: 0.585193 \tR2: -0.054816\n",
      "Epoch: 383 \tTraining Loss: 0.581455 \tR2: -0.054816\n",
      "Epoch: 384 \tTraining Loss: 0.575419 \tR2: -0.054816\n",
      "Epoch: 385 \tTraining Loss: 0.581839 \tR2: -0.054816\n",
      "Epoch: 386 \tTraining Loss: 0.573891 \tR2: -0.054816\n",
      "Epoch: 387 \tTraining Loss: 0.568199 \tR2: -0.054816\n",
      "Epoch: 388 \tTraining Loss: 0.586149 \tR2: -0.054816\n",
      "Epoch: 389 \tTraining Loss: 0.580805 \tR2: -0.054816\n",
      "Epoch: 390 \tTraining Loss: 0.577228 \tR2: -0.054816\n",
      "Epoch: 391 \tTraining Loss: 0.587478 \tR2: -0.054816\n",
      "Epoch: 392 \tTraining Loss: 0.565921 \tR2: -0.054816\n",
      "Epoch: 393 \tTraining Loss: 0.575897 \tR2: -0.054816\n",
      "Epoch: 394 \tTraining Loss: 0.560690 \tR2: -0.054816\n",
      "Epoch: 395 \tTraining Loss: 0.573841 \tR2: -0.054816\n",
      "Epoch: 396 \tTraining Loss: 0.583485 \tR2: -0.054816\n",
      "Epoch: 397 \tTraining Loss: 0.574874 \tR2: -0.054816\n",
      "Epoch: 398 \tTraining Loss: 0.571107 \tR2: -0.054816\n",
      "Epoch: 399 \tTraining Loss: 0.586966 \tR2: -0.054816\n",
      "Epoch: 400 \tTraining Loss: 0.582823 \tR2: 0.342374\n",
      "Epoch: 401 \tTraining Loss: 0.566029 \tR2: 0.342374\n",
      "Epoch: 402 \tTraining Loss: 0.579634 \tR2: 0.342374\n",
      "Epoch: 403 \tTraining Loss: 0.584048 \tR2: 0.342374\n",
      "Epoch: 404 \tTraining Loss: 0.571821 \tR2: 0.342374\n",
      "Epoch: 405 \tTraining Loss: 0.594983 \tR2: 0.342374\n",
      "Epoch: 406 \tTraining Loss: 0.564594 \tR2: 0.342374\n",
      "Epoch: 407 \tTraining Loss: 0.573909 \tR2: 0.342374\n",
      "Epoch: 408 \tTraining Loss: 0.571393 \tR2: 0.342374\n",
      "Epoch: 409 \tTraining Loss: 0.581379 \tR2: 0.342374\n",
      "Epoch: 410 \tTraining Loss: 0.571922 \tR2: 0.342374\n",
      "Epoch: 411 \tTraining Loss: 0.579440 \tR2: 0.342374\n",
      "Epoch: 412 \tTraining Loss: 0.576333 \tR2: 0.342374\n",
      "Epoch: 413 \tTraining Loss: 0.570984 \tR2: 0.342374\n",
      "Epoch: 414 \tTraining Loss: 0.563382 \tR2: 0.342374\n",
      "Epoch: 415 \tTraining Loss: 0.585236 \tR2: 0.342374\n",
      "Epoch: 416 \tTraining Loss: 0.563786 \tR2: 0.342374\n",
      "Epoch: 417 \tTraining Loss: 0.576708 \tR2: 0.342374\n",
      "Epoch: 418 \tTraining Loss: 0.575987 \tR2: 0.342374\n",
      "Epoch: 419 \tTraining Loss: 0.565377 \tR2: 0.342374\n",
      "Epoch: 420 \tTraining Loss: 0.561658 \tR2: 0.342374\n",
      "Epoch: 421 \tTraining Loss: 0.555456 \tR2: 0.342374\n",
      "Epoch: 422 \tTraining Loss: 0.562822 \tR2: 0.342374\n",
      "Epoch: 423 \tTraining Loss: 0.561968 \tR2: 0.342374\n",
      "Epoch: 424 \tTraining Loss: 0.569487 \tR2: 0.342374\n",
      "Epoch: 425 \tTraining Loss: 0.560769 \tR2: 0.342374\n",
      "Epoch: 426 \tTraining Loss: 0.586780 \tR2: 0.342374\n",
      "Epoch: 427 \tTraining Loss: 0.578532 \tR2: 0.342374\n",
      "Epoch: 428 \tTraining Loss: 0.562456 \tR2: 0.342374\n",
      "Epoch: 429 \tTraining Loss: 0.573906 \tR2: 0.342374\n",
      "Epoch: 430 \tTraining Loss: 0.581450 \tR2: 0.342374\n",
      "Epoch: 431 \tTraining Loss: 0.578695 \tR2: 0.342374\n",
      "Epoch: 432 \tTraining Loss: 0.564456 \tR2: 0.342374\n",
      "Epoch: 433 \tTraining Loss: 0.566252 \tR2: 0.342374\n",
      "Epoch: 434 \tTraining Loss: 0.576168 \tR2: 0.342374\n",
      "Epoch: 435 \tTraining Loss: 0.578532 \tR2: 0.342374\n",
      "Epoch: 436 \tTraining Loss: 0.580822 \tR2: 0.342374\n",
      "Epoch: 437 \tTraining Loss: 0.576738 \tR2: 0.342374\n",
      "Epoch: 438 \tTraining Loss: 0.566004 \tR2: 0.342374\n",
      "Epoch: 439 \tTraining Loss: 0.576215 \tR2: 0.342374\n",
      "Epoch: 440 \tTraining Loss: 0.567998 \tR2: 0.342374\n",
      "Epoch: 441 \tTraining Loss: 0.572782 \tR2: 0.342374\n",
      "Epoch: 442 \tTraining Loss: 0.565438 \tR2: 0.342374\n",
      "Epoch: 443 \tTraining Loss: 0.569038 \tR2: 0.342374\n",
      "Epoch: 444 \tTraining Loss: 0.559712 \tR2: 0.342374\n",
      "Epoch: 445 \tTraining Loss: 0.567753 \tR2: 0.342374\n",
      "Epoch: 446 \tTraining Loss: 0.566317 \tR2: 0.342374\n",
      "Epoch: 447 \tTraining Loss: 0.573260 \tR2: 0.342374\n",
      "Epoch: 448 \tTraining Loss: 0.578490 \tR2: 0.342374\n",
      "Epoch: 449 \tTraining Loss: 0.564114 \tR2: 0.342374\n",
      "Epoch: 450 \tTraining Loss: 0.582183 \tR2: 0.342374\n",
      "Epoch: 451 \tTraining Loss: 0.583038 \tR2: 0.342374\n",
      "Epoch: 452 \tTraining Loss: 0.577458 \tR2: 0.342374\n",
      "Epoch: 453 \tTraining Loss: 0.569948 \tR2: 0.342374\n",
      "Epoch: 454 \tTraining Loss: 0.844136 \tR2: 0.342374\n",
      "Epoch: 455 \tTraining Loss: 0.745359 \tR2: 0.342374\n",
      "Epoch: 456 \tTraining Loss: 0.642444 \tR2: 0.342374\n",
      "Epoch: 457 \tTraining Loss: 0.636629 \tR2: 0.342374\n",
      "Epoch: 458 \tTraining Loss: 0.585007 \tR2: 0.342374\n",
      "Epoch: 459 \tTraining Loss: 0.608511 \tR2: 0.342374\n",
      "Epoch: 460 \tTraining Loss: 0.587713 \tR2: 0.342374\n",
      "Epoch: 461 \tTraining Loss: 0.585494 \tR2: 0.342374\n",
      "Epoch: 462 \tTraining Loss: 0.599485 \tR2: 0.342374\n",
      "Epoch: 463 \tTraining Loss: 0.590047 \tR2: 0.342374\n",
      "Epoch: 464 \tTraining Loss: 0.604171 \tR2: 0.342374\n",
      "Epoch: 465 \tTraining Loss: 0.570261 \tR2: 0.342374\n",
      "Epoch: 466 \tTraining Loss: 0.585755 \tR2: 0.342374\n",
      "Epoch: 467 \tTraining Loss: 0.594836 \tR2: 0.342374\n",
      "Epoch: 468 \tTraining Loss: 0.588745 \tR2: 0.342374\n",
      "Epoch: 469 \tTraining Loss: 0.602703 \tR2: 0.342374\n",
      "Epoch: 470 \tTraining Loss: 0.590554 \tR2: 0.342374\n",
      "Epoch: 471 \tTraining Loss: 0.571329 \tR2: 0.342374\n",
      "Epoch: 472 \tTraining Loss: 0.596325 \tR2: 0.342374\n",
      "Epoch: 473 \tTraining Loss: 0.574152 \tR2: 0.342374\n",
      "Epoch: 474 \tTraining Loss: 0.587032 \tR2: 0.342374\n",
      "Epoch: 475 \tTraining Loss: 0.587349 \tR2: 0.342374\n",
      "Epoch: 476 \tTraining Loss: 0.574442 \tR2: 0.342374\n",
      "Epoch: 477 \tTraining Loss: 0.570138 \tR2: 0.342374\n",
      "Epoch: 478 \tTraining Loss: 0.585557 \tR2: 0.342374\n",
      "Epoch: 479 \tTraining Loss: 0.566444 \tR2: 0.342374\n",
      "Epoch: 480 \tTraining Loss: 0.578510 \tR2: 0.342374\n",
      "Epoch: 481 \tTraining Loss: 0.571751 \tR2: 0.342374\n",
      "Epoch: 482 \tTraining Loss: 0.585490 \tR2: 0.342374\n",
      "Epoch: 483 \tTraining Loss: 0.553747 \tR2: 0.342374\n",
      "Epoch: 484 \tTraining Loss: 0.571976 \tR2: 0.342374\n",
      "Epoch: 485 \tTraining Loss: 0.570576 \tR2: 0.342374\n",
      "Epoch: 486 \tTraining Loss: 0.585829 \tR2: 0.342374\n",
      "Epoch: 487 \tTraining Loss: 0.575266 \tR2: 0.342374\n",
      "Epoch: 488 \tTraining Loss: 0.567393 \tR2: 0.342374\n",
      "Epoch: 489 \tTraining Loss: 0.561300 \tR2: 0.342374\n",
      "Epoch: 490 \tTraining Loss: 0.549573 \tR2: 0.342374\n",
      "Epoch: 491 \tTraining Loss: 0.565898 \tR2: 0.342374\n",
      "Epoch: 492 \tTraining Loss: 0.581747 \tR2: 0.342374\n",
      "Epoch: 493 \tTraining Loss: 0.562472 \tR2: 0.342374\n",
      "Epoch: 494 \tTraining Loss: 0.568042 \tR2: 0.342374\n",
      "Epoch: 495 \tTraining Loss: 0.571298 \tR2: 0.342374\n",
      "Epoch: 496 \tTraining Loss: 0.570985 \tR2: 0.342374\n",
      "Epoch: 497 \tTraining Loss: 0.572710 \tR2: 0.342374\n",
      "Epoch: 498 \tTraining Loss: 0.567428 \tR2: 0.342374\n",
      "Epoch: 499 \tTraining Loss: 0.567528 \tR2: 0.342374\n",
      "Epoch: 500 \tTraining Loss: 0.556625 \tR2: -0.283845\n",
      "Epoch: 501 \tTraining Loss: 0.542531 \tR2: -0.283845\n",
      "Epoch: 502 \tTraining Loss: 0.589053 \tR2: -0.283845\n",
      "Epoch: 503 \tTraining Loss: 0.557867 \tR2: -0.283845\n",
      "Epoch: 504 \tTraining Loss: 0.578197 \tR2: -0.283845\n",
      "Epoch: 505 \tTraining Loss: 0.549570 \tR2: -0.283845\n",
      "Epoch: 506 \tTraining Loss: 0.582336 \tR2: -0.283845\n",
      "Epoch: 507 \tTraining Loss: 0.561064 \tR2: -0.283845\n",
      "Epoch: 508 \tTraining Loss: 0.581046 \tR2: -0.283845\n",
      "Epoch: 509 \tTraining Loss: 0.580040 \tR2: -0.283845\n",
      "Epoch: 510 \tTraining Loss: 0.565053 \tR2: -0.283845\n",
      "Epoch: 511 \tTraining Loss: 0.569787 \tR2: -0.283845\n",
      "Epoch: 512 \tTraining Loss: 0.570441 \tR2: -0.283845\n",
      "Epoch: 513 \tTraining Loss: 0.582633 \tR2: -0.283845\n",
      "Epoch: 514 \tTraining Loss: 0.576695 \tR2: -0.283845\n",
      "Epoch: 515 \tTraining Loss: 0.582621 \tR2: -0.283845\n",
      "Epoch: 516 \tTraining Loss: 0.576977 \tR2: -0.283845\n",
      "Epoch: 517 \tTraining Loss: 0.557908 \tR2: -0.283845\n",
      "Epoch: 518 \tTraining Loss: 0.571221 \tR2: -0.283845\n",
      "Epoch: 519 \tTraining Loss: 0.554410 \tR2: -0.283845\n",
      "Epoch: 520 \tTraining Loss: 0.578795 \tR2: -0.283845\n",
      "Epoch: 521 \tTraining Loss: 0.557926 \tR2: -0.283845\n",
      "Epoch: 522 \tTraining Loss: 0.564290 \tR2: -0.283845\n",
      "Epoch: 523 \tTraining Loss: 0.562831 \tR2: -0.283845\n",
      "Epoch: 524 \tTraining Loss: 0.553767 \tR2: -0.283845\n",
      "Epoch: 525 \tTraining Loss: 0.555262 \tR2: -0.283845\n",
      "Epoch: 526 \tTraining Loss: 0.579037 \tR2: -0.283845\n",
      "Epoch: 527 \tTraining Loss: 0.572112 \tR2: -0.283845\n",
      "Epoch: 528 \tTraining Loss: 0.557578 \tR2: -0.283845\n",
      "Epoch: 529 \tTraining Loss: 0.562415 \tR2: -0.283845\n",
      "Epoch: 530 \tTraining Loss: 0.575236 \tR2: -0.283845\n",
      "Epoch: 531 \tTraining Loss: 0.560488 \tR2: -0.283845\n",
      "Epoch: 532 \tTraining Loss: 0.567271 \tR2: -0.283845\n",
      "Epoch: 533 \tTraining Loss: 0.565755 \tR2: -0.283845\n",
      "Epoch: 534 \tTraining Loss: 0.567835 \tR2: -0.283845\n",
      "Epoch: 535 \tTraining Loss: 0.574359 \tR2: -0.283845\n",
      "Epoch: 536 \tTraining Loss: 0.564554 \tR2: -0.283845\n",
      "Epoch: 537 \tTraining Loss: 0.576598 \tR2: -0.283845\n",
      "Epoch: 538 \tTraining Loss: 0.584911 \tR2: -0.283845\n",
      "Epoch: 539 \tTraining Loss: 0.565869 \tR2: -0.283845\n",
      "Epoch: 540 \tTraining Loss: 0.568948 \tR2: -0.283845\n",
      "Epoch: 541 \tTraining Loss: 0.589939 \tR2: -0.283845\n",
      "Epoch: 542 \tTraining Loss: 0.591364 \tR2: -0.283845\n",
      "Epoch: 543 \tTraining Loss: 0.545784 \tR2: -0.283845\n",
      "Epoch: 544 \tTraining Loss: 0.566971 \tR2: -0.283845\n",
      "Epoch: 545 \tTraining Loss: 0.552260 \tR2: -0.283845\n",
      "Epoch: 546 \tTraining Loss: 0.568588 \tR2: -0.283845\n",
      "Epoch: 547 \tTraining Loss: 0.557887 \tR2: -0.283845\n",
      "Epoch: 548 \tTraining Loss: 0.562152 \tR2: -0.283845\n",
      "Epoch: 549 \tTraining Loss: 0.567626 \tR2: -0.283845\n",
      "Epoch: 550 \tTraining Loss: 0.574208 \tR2: -0.283845\n",
      "Epoch: 551 \tTraining Loss: 0.554626 \tR2: -0.283845\n",
      "Epoch: 552 \tTraining Loss: 0.574895 \tR2: -0.283845\n",
      "Epoch: 553 \tTraining Loss: 0.557264 \tR2: -0.283845\n",
      "Epoch: 554 \tTraining Loss: 0.565821 \tR2: -0.283845\n",
      "Epoch: 555 \tTraining Loss: 0.572568 \tR2: -0.283845\n",
      "Epoch: 556 \tTraining Loss: 0.560363 \tR2: -0.283845\n",
      "Epoch: 557 \tTraining Loss: 0.577983 \tR2: -0.283845\n",
      "Epoch: 558 \tTraining Loss: 0.565561 \tR2: -0.283845\n",
      "Epoch: 559 \tTraining Loss: 0.555586 \tR2: -0.283845\n",
      "Epoch: 560 \tTraining Loss: 0.566800 \tR2: -0.283845\n",
      "Epoch: 561 \tTraining Loss: 0.558666 \tR2: -0.283845\n",
      "Epoch: 562 \tTraining Loss: 0.561315 \tR2: -0.283845\n",
      "Epoch: 563 \tTraining Loss: 0.571949 \tR2: -0.283845\n",
      "Epoch: 564 \tTraining Loss: 0.565473 \tR2: -0.283845\n",
      "Epoch: 565 \tTraining Loss: 0.553279 \tR2: -0.283845\n",
      "Epoch: 566 \tTraining Loss: 0.569799 \tR2: -0.283845\n",
      "Epoch: 567 \tTraining Loss: 0.561592 \tR2: -0.283845\n",
      "Epoch: 568 \tTraining Loss: 0.564862 \tR2: -0.283845\n",
      "Epoch: 569 \tTraining Loss: 0.576055 \tR2: -0.283845\n",
      "Epoch: 570 \tTraining Loss: 0.573012 \tR2: -0.283845\n",
      "Epoch: 571 \tTraining Loss: 0.565058 \tR2: -0.283845\n",
      "Epoch: 572 \tTraining Loss: 0.558588 \tR2: -0.283845\n",
      "Epoch: 573 \tTraining Loss: 0.567939 \tR2: -0.283845\n",
      "Epoch: 574 \tTraining Loss: 0.559857 \tR2: -0.283845\n",
      "Epoch: 575 \tTraining Loss: 0.569301 \tR2: -0.283845\n",
      "Epoch: 576 \tTraining Loss: 0.561684 \tR2: -0.283845\n",
      "Epoch: 577 \tTraining Loss: 0.566823 \tR2: -0.283845\n",
      "Epoch: 578 \tTraining Loss: 0.549377 \tR2: -0.283845\n",
      "Epoch: 579 \tTraining Loss: 0.557661 \tR2: -0.283845\n",
      "Epoch: 580 \tTraining Loss: 0.568245 \tR2: -0.283845\n",
      "Epoch: 581 \tTraining Loss: 0.577180 \tR2: -0.283845\n",
      "Epoch: 582 \tTraining Loss: 0.558360 \tR2: -0.283845\n",
      "Epoch: 583 \tTraining Loss: 0.563972 \tR2: -0.283845\n",
      "Epoch: 584 \tTraining Loss: 0.566153 \tR2: -0.283845\n",
      "Epoch: 585 \tTraining Loss: 0.564884 \tR2: -0.283845\n",
      "Epoch: 586 \tTraining Loss: 0.563951 \tR2: -0.283845\n",
      "Epoch: 587 \tTraining Loss: 0.556428 \tR2: -0.283845\n",
      "Epoch: 588 \tTraining Loss: 0.566818 \tR2: -0.283845\n",
      "Epoch: 589 \tTraining Loss: 0.571869 \tR2: -0.283845\n",
      "Epoch: 590 \tTraining Loss: 0.568145 \tR2: -0.283845\n",
      "Epoch: 591 \tTraining Loss: 0.551310 \tR2: -0.283845\n",
      "Epoch: 592 \tTraining Loss: 0.555043 \tR2: -0.283845\n",
      "Epoch: 593 \tTraining Loss: 0.561525 \tR2: -0.283845\n",
      "Epoch: 594 \tTraining Loss: 0.577234 \tR2: -0.283845\n",
      "Epoch: 595 \tTraining Loss: 0.582032 \tR2: -0.283845\n",
      "Epoch: 596 \tTraining Loss: 0.565933 \tR2: -0.283845\n",
      "Epoch: 597 \tTraining Loss: 0.551223 \tR2: -0.283845\n",
      "Epoch: 598 \tTraining Loss: 0.564368 \tR2: -0.283845\n",
      "Epoch: 599 \tTraining Loss: 0.563632 \tR2: -0.283845\n",
      "Epoch: 600 \tTraining Loss: 0.550949 \tR2: -0.522416\n",
      "Epoch: 601 \tTraining Loss: 0.573365 \tR2: -0.522416\n",
      "Epoch: 602 \tTraining Loss: 0.558585 \tR2: -0.522416\n",
      "Epoch: 603 \tTraining Loss: 0.574658 \tR2: -0.522416\n",
      "Epoch: 604 \tTraining Loss: 0.554034 \tR2: -0.522416\n",
      "Epoch: 605 \tTraining Loss: 0.570665 \tR2: -0.522416\n",
      "Epoch: 606 \tTraining Loss: 0.565199 \tR2: -0.522416\n",
      "Epoch: 607 \tTraining Loss: 0.569301 \tR2: -0.522416\n",
      "Epoch: 608 \tTraining Loss: 0.547197 \tR2: -0.522416\n",
      "Epoch: 609 \tTraining Loss: 0.571067 \tR2: -0.522416\n",
      "Epoch: 610 \tTraining Loss: 0.548862 \tR2: -0.522416\n",
      "Epoch: 611 \tTraining Loss: 0.559902 \tR2: -0.522416\n",
      "Epoch: 612 \tTraining Loss: 0.540759 \tR2: -0.522416\n",
      "Epoch: 613 \tTraining Loss: 0.548984 \tR2: -0.522416\n",
      "Epoch: 614 \tTraining Loss: 0.563922 \tR2: -0.522416\n",
      "Epoch: 615 \tTraining Loss: 0.535347 \tR2: -0.522416\n",
      "Epoch: 616 \tTraining Loss: 0.558368 \tR2: -0.522416\n",
      "Epoch: 617 \tTraining Loss: 0.545724 \tR2: -0.522416\n",
      "Epoch: 618 \tTraining Loss: 0.562342 \tR2: -0.522416\n",
      "Epoch: 619 \tTraining Loss: 0.555132 \tR2: -0.522416\n",
      "Epoch: 620 \tTraining Loss: 0.577379 \tR2: -0.522416\n",
      "Epoch: 621 \tTraining Loss: 0.553308 \tR2: -0.522416\n",
      "Epoch: 622 \tTraining Loss: 0.544333 \tR2: -0.522416\n",
      "Epoch: 623 \tTraining Loss: 0.533143 \tR2: -0.522416\n",
      "Epoch: 624 \tTraining Loss: 0.567392 \tR2: -0.522416\n",
      "Epoch: 625 \tTraining Loss: 0.559322 \tR2: -0.522416\n",
      "Epoch: 626 \tTraining Loss: 0.550070 \tR2: -0.522416\n",
      "Epoch: 627 \tTraining Loss: 0.564746 \tR2: -0.522416\n",
      "Epoch: 628 \tTraining Loss: 0.572686 \tR2: -0.522416\n",
      "Epoch: 629 \tTraining Loss: 0.551336 \tR2: -0.522416\n",
      "Epoch: 630 \tTraining Loss: 0.555584 \tR2: -0.522416\n",
      "Epoch: 631 \tTraining Loss: 0.551088 \tR2: -0.522416\n",
      "Epoch: 632 \tTraining Loss: 0.551478 \tR2: -0.522416\n",
      "Epoch: 633 \tTraining Loss: 0.560584 \tR2: -0.522416\n",
      "Epoch: 634 \tTraining Loss: 0.565522 \tR2: -0.522416\n",
      "Epoch: 635 \tTraining Loss: 0.541404 \tR2: -0.522416\n",
      "Epoch: 636 \tTraining Loss: 0.548112 \tR2: -0.522416\n",
      "Epoch: 637 \tTraining Loss: 0.551005 \tR2: -0.522416\n",
      "Epoch: 638 \tTraining Loss: 0.553218 \tR2: -0.522416\n",
      "Epoch: 639 \tTraining Loss: 0.559672 \tR2: -0.522416\n",
      "Epoch: 640 \tTraining Loss: 0.561250 \tR2: -0.522416\n",
      "Epoch: 641 \tTraining Loss: 0.554889 \tR2: -0.522416\n",
      "Epoch: 642 \tTraining Loss: 0.545296 \tR2: -0.522416\n",
      "Epoch: 643 \tTraining Loss: 0.555047 \tR2: -0.522416\n",
      "Epoch: 644 \tTraining Loss: 0.546283 \tR2: -0.522416\n",
      "Epoch: 645 \tTraining Loss: 0.557278 \tR2: -0.522416\n",
      "Epoch: 646 \tTraining Loss: 0.548312 \tR2: -0.522416\n",
      "Epoch: 647 \tTraining Loss: 0.567518 \tR2: -0.522416\n",
      "Epoch: 648 \tTraining Loss: 0.536418 \tR2: -0.522416\n",
      "Epoch: 649 \tTraining Loss: 0.546148 \tR2: -0.522416\n",
      "Epoch: 650 \tTraining Loss: 0.551966 \tR2: -0.522416\n",
      "Epoch: 651 \tTraining Loss: 0.561084 \tR2: -0.522416\n",
      "Epoch: 652 \tTraining Loss: 0.560580 \tR2: -0.522416\n",
      "Epoch: 653 \tTraining Loss: 0.546393 \tR2: -0.522416\n",
      "Epoch: 654 \tTraining Loss: 0.560149 \tR2: -0.522416\n",
      "Epoch: 655 \tTraining Loss: 0.555107 \tR2: -0.522416\n",
      "Epoch: 656 \tTraining Loss: 0.560086 \tR2: -0.522416\n",
      "Epoch: 657 \tTraining Loss: 0.551722 \tR2: -0.522416\n",
      "Epoch: 658 \tTraining Loss: 0.563589 \tR2: -0.522416\n",
      "Epoch: 659 \tTraining Loss: 0.551063 \tR2: -0.522416\n",
      "Epoch: 660 \tTraining Loss: 0.553182 \tR2: -0.522416\n",
      "Epoch: 661 \tTraining Loss: 0.543768 \tR2: -0.522416\n",
      "Epoch: 662 \tTraining Loss: 0.545481 \tR2: -0.522416\n",
      "Epoch: 663 \tTraining Loss: 0.557137 \tR2: -0.522416\n",
      "Epoch: 664 \tTraining Loss: 0.559994 \tR2: -0.522416\n",
      "Epoch: 665 \tTraining Loss: 0.574776 \tR2: -0.522416\n",
      "Epoch: 666 \tTraining Loss: 0.565554 \tR2: -0.522416\n",
      "Epoch: 667 \tTraining Loss: 0.551466 \tR2: -0.522416\n",
      "Epoch: 668 \tTraining Loss: 0.534100 \tR2: -0.522416\n",
      "Epoch: 669 \tTraining Loss: 0.556364 \tR2: -0.522416\n",
      "Epoch: 670 \tTraining Loss: 0.567746 \tR2: -0.522416\n",
      "Epoch: 671 \tTraining Loss: 0.566155 \tR2: -0.522416\n",
      "Epoch: 672 \tTraining Loss: 0.547647 \tR2: -0.522416\n",
      "Epoch: 673 \tTraining Loss: 0.550842 \tR2: -0.522416\n",
      "Epoch: 674 \tTraining Loss: 0.567926 \tR2: -0.522416\n",
      "Epoch: 675 \tTraining Loss: 0.548695 \tR2: -0.522416\n",
      "Epoch: 676 \tTraining Loss: 0.563324 \tR2: -0.522416\n",
      "Epoch: 677 \tTraining Loss: 0.566724 \tR2: -0.522416\n",
      "Epoch: 678 \tTraining Loss: 0.553923 \tR2: -0.522416\n",
      "Epoch: 679 \tTraining Loss: 0.568606 \tR2: -0.522416\n",
      "Epoch: 680 \tTraining Loss: 0.547669 \tR2: -0.522416\n",
      "Epoch: 681 \tTraining Loss: 0.566701 \tR2: -0.522416\n",
      "Epoch: 682 \tTraining Loss: 0.551182 \tR2: -0.522416\n",
      "Epoch: 683 \tTraining Loss: 0.548654 \tR2: -0.522416\n",
      "Epoch: 684 \tTraining Loss: 0.564245 \tR2: -0.522416\n",
      "Epoch: 685 \tTraining Loss: 0.560979 \tR2: -0.522416\n",
      "Epoch: 686 \tTraining Loss: 0.551342 \tR2: -0.522416\n",
      "Epoch: 687 \tTraining Loss: 0.550185 \tR2: -0.522416\n",
      "Epoch: 688 \tTraining Loss: 0.542912 \tR2: -0.522416\n",
      "Epoch: 689 \tTraining Loss: 0.548873 \tR2: -0.522416\n",
      "Epoch: 690 \tTraining Loss: 0.548784 \tR2: -0.522416\n",
      "Epoch: 691 \tTraining Loss: 0.555780 \tR2: -0.522416\n",
      "Epoch: 692 \tTraining Loss: 0.555679 \tR2: -0.522416\n",
      "Epoch: 693 \tTraining Loss: 0.550958 \tR2: -0.522416\n",
      "Epoch: 694 \tTraining Loss: 0.561302 \tR2: -0.522416\n",
      "Epoch: 695 \tTraining Loss: 0.550719 \tR2: -0.522416\n",
      "Epoch: 696 \tTraining Loss: 0.559570 \tR2: -0.522416\n",
      "Epoch: 697 \tTraining Loss: 0.549628 \tR2: -0.522416\n",
      "Epoch: 698 \tTraining Loss: 0.548575 \tR2: -0.522416\n",
      "Epoch: 699 \tTraining Loss: 0.541757 \tR2: -0.522416\n",
      "Epoch: 700 \tTraining Loss: 0.553018 \tR2: 0.590613\n",
      "Epoch: 701 \tTraining Loss: 0.553880 \tR2: 0.590613\n",
      "Epoch: 702 \tTraining Loss: 0.547598 \tR2: 0.590613\n",
      "Epoch: 703 \tTraining Loss: 0.549278 \tR2: 0.590613\n",
      "Epoch: 704 \tTraining Loss: 0.553947 \tR2: 0.590613\n",
      "Epoch: 705 \tTraining Loss: 0.556567 \tR2: 0.590613\n",
      "Epoch: 706 \tTraining Loss: 0.562147 \tR2: 0.590613\n",
      "Epoch: 707 \tTraining Loss: 0.538388 \tR2: 0.590613\n",
      "Epoch: 708 \tTraining Loss: 0.563582 \tR2: 0.590613\n",
      "Epoch: 709 \tTraining Loss: 0.568871 \tR2: 0.590613\n",
      "Epoch: 710 \tTraining Loss: 0.551389 \tR2: 0.590613\n",
      "Epoch: 711 \tTraining Loss: 0.543180 \tR2: 0.590613\n",
      "Epoch: 712 \tTraining Loss: 0.563919 \tR2: 0.590613\n",
      "Epoch: 713 \tTraining Loss: 0.552167 \tR2: 0.590613\n",
      "Epoch: 714 \tTraining Loss: 0.557493 \tR2: 0.590613\n",
      "Epoch: 715 \tTraining Loss: 0.544988 \tR2: 0.590613\n",
      "Epoch: 716 \tTraining Loss: 0.561160 \tR2: 0.590613\n",
      "Epoch: 717 \tTraining Loss: 0.535656 \tR2: 0.590613\n",
      "Epoch: 718 \tTraining Loss: 0.565375 \tR2: 0.590613\n",
      "Epoch: 719 \tTraining Loss: 0.555564 \tR2: 0.590613\n",
      "Epoch: 720 \tTraining Loss: 0.548492 \tR2: 0.590613\n",
      "Epoch: 721 \tTraining Loss: 0.572362 \tR2: 0.590613\n",
      "Epoch: 722 \tTraining Loss: 0.543968 \tR2: 0.590613\n",
      "Epoch: 723 \tTraining Loss: 0.556743 \tR2: 0.590613\n",
      "Epoch: 724 \tTraining Loss: 0.549679 \tR2: 0.590613\n",
      "Epoch: 725 \tTraining Loss: 0.549097 \tR2: 0.590613\n",
      "Epoch: 726 \tTraining Loss: 0.562266 \tR2: 0.590613\n",
      "Epoch: 727 \tTraining Loss: 0.569141 \tR2: 0.590613\n",
      "Epoch: 728 \tTraining Loss: 0.553120 \tR2: 0.590613\n",
      "Epoch: 729 \tTraining Loss: 0.545376 \tR2: 0.590613\n",
      "Epoch: 730 \tTraining Loss: 0.549401 \tR2: 0.590613\n",
      "Epoch: 731 \tTraining Loss: 0.557089 \tR2: 0.590613\n",
      "Epoch: 732 \tTraining Loss: 0.553403 \tR2: 0.590613\n",
      "Epoch: 733 \tTraining Loss: 0.549502 \tR2: 0.590613\n",
      "Epoch: 734 \tTraining Loss: 0.532858 \tR2: 0.590613\n",
      "Epoch: 735 \tTraining Loss: 0.535128 \tR2: 0.590613\n",
      "Epoch: 736 \tTraining Loss: 0.546259 \tR2: 0.590613\n",
      "Epoch: 737 \tTraining Loss: 0.559225 \tR2: 0.590613\n",
      "Epoch: 738 \tTraining Loss: 0.564372 \tR2: 0.590613\n",
      "Epoch: 739 \tTraining Loss: 0.560059 \tR2: 0.590613\n",
      "Epoch: 740 \tTraining Loss: 0.551871 \tR2: 0.590613\n",
      "Epoch: 741 \tTraining Loss: 0.546873 \tR2: 0.590613\n",
      "Epoch: 742 \tTraining Loss: 0.553823 \tR2: 0.590613\n",
      "Epoch: 743 \tTraining Loss: 0.557931 \tR2: 0.590613\n",
      "Epoch: 744 \tTraining Loss: 0.544957 \tR2: 0.590613\n",
      "Epoch: 745 \tTraining Loss: 0.551583 \tR2: 0.590613\n",
      "Epoch: 746 \tTraining Loss: 0.554577 \tR2: 0.590613\n",
      "Epoch: 747 \tTraining Loss: 0.546789 \tR2: 0.590613\n",
      "Epoch: 748 \tTraining Loss: 0.537431 \tR2: 0.590613\n",
      "Epoch: 749 \tTraining Loss: 0.561978 \tR2: 0.590613\n",
      "Epoch: 750 \tTraining Loss: 0.547547 \tR2: 0.590613\n",
      "Epoch: 751 \tTraining Loss: 0.552810 \tR2: 0.590613\n",
      "Epoch: 752 \tTraining Loss: 0.561022 \tR2: 0.590613\n",
      "Epoch: 753 \tTraining Loss: 0.561394 \tR2: 0.590613\n",
      "Epoch: 754 \tTraining Loss: 0.554052 \tR2: 0.590613\n",
      "Epoch: 755 \tTraining Loss: 0.537900 \tR2: 0.590613\n",
      "Epoch: 756 \tTraining Loss: 0.547941 \tR2: 0.590613\n",
      "Epoch: 757 \tTraining Loss: 0.548451 \tR2: 0.590613\n",
      "Epoch: 758 \tTraining Loss: 0.546153 \tR2: 0.590613\n",
      "Epoch: 759 \tTraining Loss: 0.554698 \tR2: 0.590613\n",
      "Epoch: 760 \tTraining Loss: 0.551008 \tR2: 0.590613\n",
      "Epoch: 761 \tTraining Loss: 0.556830 \tR2: 0.590613\n",
      "Epoch: 762 \tTraining Loss: 0.556378 \tR2: 0.590613\n",
      "Epoch: 763 \tTraining Loss: 0.558708 \tR2: 0.590613\n",
      "Epoch: 764 \tTraining Loss: 0.531810 \tR2: 0.590613\n",
      "Epoch: 765 \tTraining Loss: 0.555809 \tR2: 0.590613\n",
      "Epoch: 766 \tTraining Loss: 0.550520 \tR2: 0.590613\n",
      "Epoch: 767 \tTraining Loss: 0.537030 \tR2: 0.590613\n",
      "Epoch: 768 \tTraining Loss: 0.571644 \tR2: 0.590613\n",
      "Epoch: 769 \tTraining Loss: 0.548790 \tR2: 0.590613\n",
      "Epoch: 770 \tTraining Loss: 0.550498 \tR2: 0.590613\n",
      "Epoch: 771 \tTraining Loss: 0.555705 \tR2: 0.590613\n",
      "Epoch: 772 \tTraining Loss: 0.569940 \tR2: 0.590613\n",
      "Epoch: 773 \tTraining Loss: 0.549072 \tR2: 0.590613\n",
      "Epoch: 774 \tTraining Loss: 0.561814 \tR2: 0.590613\n",
      "Epoch: 775 \tTraining Loss: 0.548235 \tR2: 0.590613\n",
      "Epoch: 776 \tTraining Loss: 0.564088 \tR2: 0.590613\n",
      "Epoch: 777 \tTraining Loss: 0.558860 \tR2: 0.590613\n",
      "Epoch: 778 \tTraining Loss: 0.543488 \tR2: 0.590613\n",
      "Epoch: 779 \tTraining Loss: 0.541902 \tR2: 0.590613\n",
      "Epoch: 780 \tTraining Loss: 0.655408 \tR2: 0.590613\n",
      "Epoch: 781 \tTraining Loss: 0.556460 \tR2: 0.590613\n",
      "Epoch: 782 \tTraining Loss: 0.560781 \tR2: 0.590613\n",
      "Epoch: 783 \tTraining Loss: 0.540824 \tR2: 0.590613\n",
      "Epoch: 784 \tTraining Loss: 0.555210 \tR2: 0.590613\n",
      "Epoch: 785 \tTraining Loss: 0.539435 \tR2: 0.590613\n",
      "Epoch: 786 \tTraining Loss: 0.554133 \tR2: 0.590613\n",
      "Epoch: 787 \tTraining Loss: 0.563272 \tR2: 0.590613\n",
      "Epoch: 788 \tTraining Loss: 0.547821 \tR2: 0.590613\n",
      "Epoch: 789 \tTraining Loss: 0.552875 \tR2: 0.590613\n",
      "Epoch: 790 \tTraining Loss: 0.543179 \tR2: 0.590613\n",
      "Epoch: 791 \tTraining Loss: 0.545909 \tR2: 0.590613\n",
      "Epoch: 792 \tTraining Loss: 0.552788 \tR2: 0.590613\n",
      "Epoch: 793 \tTraining Loss: 0.538627 \tR2: 0.590613\n",
      "Epoch: 794 \tTraining Loss: 0.544570 \tR2: 0.590613\n",
      "Epoch: 795 \tTraining Loss: 0.550145 \tR2: 0.590613\n",
      "Epoch: 796 \tTraining Loss: 0.541282 \tR2: 0.590613\n",
      "Epoch: 797 \tTraining Loss: 0.547104 \tR2: 0.590613\n",
      "Epoch: 798 \tTraining Loss: 0.550571 \tR2: 0.590613\n",
      "Epoch: 799 \tTraining Loss: 0.565770 \tR2: 0.590613\n",
      "Epoch: 800 \tTraining Loss: 0.543199 \tR2: 0.218108\n",
      "Epoch: 801 \tTraining Loss: 0.544562 \tR2: 0.218108\n",
      "Epoch: 802 \tTraining Loss: 0.546478 \tR2: 0.218108\n",
      "Epoch: 803 \tTraining Loss: 0.552621 \tR2: 0.218108\n",
      "Epoch: 804 \tTraining Loss: 0.548761 \tR2: 0.218108\n",
      "Epoch: 805 \tTraining Loss: 0.546178 \tR2: 0.218108\n",
      "Epoch: 806 \tTraining Loss: 0.533000 \tR2: 0.218108\n",
      "Epoch: 807 \tTraining Loss: 0.548995 \tR2: 0.218108\n",
      "Epoch: 808 \tTraining Loss: 0.552687 \tR2: 0.218108\n",
      "Epoch: 809 \tTraining Loss: 0.566952 \tR2: 0.218108\n",
      "Epoch: 810 \tTraining Loss: 0.540989 \tR2: 0.218108\n",
      "Epoch: 811 \tTraining Loss: 0.554705 \tR2: 0.218108\n",
      "Epoch: 812 \tTraining Loss: 0.554265 \tR2: 0.218108\n",
      "Epoch: 813 \tTraining Loss: 0.551308 \tR2: 0.218108\n",
      "Epoch: 814 \tTraining Loss: 0.552092 \tR2: 0.218108\n",
      "Epoch: 815 \tTraining Loss: 0.545479 \tR2: 0.218108\n",
      "Epoch: 816 \tTraining Loss: 0.540642 \tR2: 0.218108\n",
      "Epoch: 817 \tTraining Loss: 0.533043 \tR2: 0.218108\n",
      "Epoch: 818 \tTraining Loss: 0.551965 \tR2: 0.218108\n",
      "Epoch: 819 \tTraining Loss: 0.551744 \tR2: 0.218108\n",
      "Epoch: 820 \tTraining Loss: 0.542762 \tR2: 0.218108\n",
      "Epoch: 821 \tTraining Loss: 0.537306 \tR2: 0.218108\n",
      "Epoch: 822 \tTraining Loss: 0.548473 \tR2: 0.218108\n",
      "Epoch: 823 \tTraining Loss: 0.560515 \tR2: 0.218108\n",
      "Epoch: 824 \tTraining Loss: 0.539530 \tR2: 0.218108\n",
      "Epoch: 825 \tTraining Loss: 0.539183 \tR2: 0.218108\n",
      "Epoch: 826 \tTraining Loss: 0.541689 \tR2: 0.218108\n",
      "Epoch: 827 \tTraining Loss: 0.547093 \tR2: 0.218108\n",
      "Epoch: 828 \tTraining Loss: 0.553441 \tR2: 0.218108\n",
      "Epoch: 829 \tTraining Loss: 0.525303 \tR2: 0.218108\n",
      "Epoch: 830 \tTraining Loss: 0.543434 \tR2: 0.218108\n",
      "Epoch: 831 \tTraining Loss: 0.553526 \tR2: 0.218108\n",
      "Epoch: 832 \tTraining Loss: 0.527831 \tR2: 0.218108\n",
      "Epoch: 833 \tTraining Loss: 0.553554 \tR2: 0.218108\n",
      "Epoch: 834 \tTraining Loss: 0.544811 \tR2: 0.218108\n",
      "Epoch: 835 \tTraining Loss: 0.562349 \tR2: 0.218108\n",
      "Epoch: 836 \tTraining Loss: 0.540280 \tR2: 0.218108\n",
      "Epoch: 837 \tTraining Loss: 0.542944 \tR2: 0.218108\n",
      "Epoch: 838 \tTraining Loss: 0.561135 \tR2: 0.218108\n",
      "Epoch: 839 \tTraining Loss: 0.548006 \tR2: 0.218108\n",
      "Epoch: 840 \tTraining Loss: 0.552168 \tR2: 0.218108\n",
      "Epoch: 841 \tTraining Loss: 0.538544 \tR2: 0.218108\n",
      "Epoch: 842 \tTraining Loss: 0.542701 \tR2: 0.218108\n",
      "Epoch: 843 \tTraining Loss: 0.552206 \tR2: 0.218108\n",
      "Epoch: 844 \tTraining Loss: 0.554355 \tR2: 0.218108\n",
      "Epoch: 845 \tTraining Loss: 0.538904 \tR2: 0.218108\n",
      "Epoch: 846 \tTraining Loss: 0.548832 \tR2: 0.218108\n",
      "Epoch: 847 \tTraining Loss: 0.554334 \tR2: 0.218108\n",
      "Epoch: 848 \tTraining Loss: 0.545115 \tR2: 0.218108\n",
      "Epoch: 849 \tTraining Loss: 0.536101 \tR2: 0.218108\n",
      "Epoch: 850 \tTraining Loss: 0.549057 \tR2: 0.218108\n",
      "Epoch: 851 \tTraining Loss: 0.534359 \tR2: 0.218108\n",
      "Epoch: 852 \tTraining Loss: 0.570949 \tR2: 0.218108\n",
      "Epoch: 853 \tTraining Loss: 0.553255 \tR2: 0.218108\n",
      "Epoch: 854 \tTraining Loss: 0.549795 \tR2: 0.218108\n",
      "Epoch: 855 \tTraining Loss: 0.553344 \tR2: 0.218108\n",
      "Epoch: 856 \tTraining Loss: 0.549693 \tR2: 0.218108\n",
      "Epoch: 857 \tTraining Loss: 0.541949 \tR2: 0.218108\n",
      "Epoch: 858 \tTraining Loss: 0.523859 \tR2: 0.218108\n",
      "Epoch: 859 \tTraining Loss: 0.545150 \tR2: 0.218108\n",
      "Epoch: 860 \tTraining Loss: 0.547673 \tR2: 0.218108\n",
      "Epoch: 861 \tTraining Loss: 0.533135 \tR2: 0.218108\n",
      "Epoch: 862 \tTraining Loss: 0.556317 \tR2: 0.218108\n",
      "Epoch: 863 \tTraining Loss: 0.562389 \tR2: 0.218108\n",
      "Epoch: 864 \tTraining Loss: 0.520731 \tR2: 0.218108\n",
      "Epoch: 865 \tTraining Loss: 0.526348 \tR2: 0.218108\n",
      "Epoch: 866 \tTraining Loss: 0.548284 \tR2: 0.218108\n",
      "Epoch: 867 \tTraining Loss: 0.540145 \tR2: 0.218108\n",
      "Epoch: 868 \tTraining Loss: 0.535059 \tR2: 0.218108\n",
      "Epoch: 869 \tTraining Loss: 0.539527 \tR2: 0.218108\n",
      "Epoch: 870 \tTraining Loss: 0.554281 \tR2: 0.218108\n",
      "Epoch: 871 \tTraining Loss: 0.547876 \tR2: 0.218108\n",
      "Epoch: 872 \tTraining Loss: 0.549972 \tR2: 0.218108\n",
      "Epoch: 873 \tTraining Loss: 0.535969 \tR2: 0.218108\n",
      "Epoch: 874 \tTraining Loss: 0.545891 \tR2: 0.218108\n",
      "Epoch: 875 \tTraining Loss: 0.535310 \tR2: 0.218108\n",
      "Epoch: 876 \tTraining Loss: 0.551698 \tR2: 0.218108\n",
      "Epoch: 877 \tTraining Loss: 0.568596 \tR2: 0.218108\n",
      "Epoch: 878 \tTraining Loss: 0.554942 \tR2: 0.218108\n",
      "Epoch: 879 \tTraining Loss: 0.552190 \tR2: 0.218108\n",
      "Epoch: 880 \tTraining Loss: 0.531956 \tR2: 0.218108\n",
      "Epoch: 881 \tTraining Loss: 0.535626 \tR2: 0.218108\n",
      "Epoch: 882 \tTraining Loss: 0.539238 \tR2: 0.218108\n",
      "Epoch: 883 \tTraining Loss: 0.513607 \tR2: 0.218108\n",
      "Epoch: 884 \tTraining Loss: 0.545979 \tR2: 0.218108\n",
      "Epoch: 885 \tTraining Loss: 0.562300 \tR2: 0.218108\n",
      "Epoch: 886 \tTraining Loss: 0.554430 \tR2: 0.218108\n",
      "Epoch: 887 \tTraining Loss: 0.538841 \tR2: 0.218108\n",
      "Epoch: 888 \tTraining Loss: 0.550586 \tR2: 0.218108\n",
      "Epoch: 889 \tTraining Loss: 0.546368 \tR2: 0.218108\n",
      "Epoch: 890 \tTraining Loss: 0.537104 \tR2: 0.218108\n",
      "Epoch: 891 \tTraining Loss: 0.537605 \tR2: 0.218108\n",
      "Epoch: 892 \tTraining Loss: 0.553832 \tR2: 0.218108\n",
      "Epoch: 893 \tTraining Loss: 0.531628 \tR2: 0.218108\n",
      "Epoch: 894 \tTraining Loss: 0.544078 \tR2: 0.218108\n",
      "Epoch: 895 \tTraining Loss: 0.540021 \tR2: 0.218108\n",
      "Epoch: 896 \tTraining Loss: 0.566863 \tR2: 0.218108\n",
      "Epoch: 897 \tTraining Loss: 0.535832 \tR2: 0.218108\n",
      "Epoch: 898 \tTraining Loss: 0.553380 \tR2: 0.218108\n",
      "Epoch: 899 \tTraining Loss: 0.554480 \tR2: 0.218108\n",
      "Epoch: 900 \tTraining Loss: 0.528655 \tR2: 0.121395\n",
      "Epoch: 901 \tTraining Loss: 0.527291 \tR2: 0.121395\n",
      "Epoch: 902 \tTraining Loss: 0.538137 \tR2: 0.121395\n",
      "Epoch: 903 \tTraining Loss: 0.541349 \tR2: 0.121395\n",
      "Epoch: 904 \tTraining Loss: 0.563227 \tR2: 0.121395\n",
      "Epoch: 905 \tTraining Loss: 0.542059 \tR2: 0.121395\n",
      "Epoch: 906 \tTraining Loss: 0.551347 \tR2: 0.121395\n",
      "Epoch: 907 \tTraining Loss: 0.536189 \tR2: 0.121395\n",
      "Epoch: 908 \tTraining Loss: 0.523554 \tR2: 0.121395\n",
      "Epoch: 909 \tTraining Loss: 0.539661 \tR2: 0.121395\n",
      "Epoch: 910 \tTraining Loss: 0.550908 \tR2: 0.121395\n",
      "Epoch: 911 \tTraining Loss: 0.543613 \tR2: 0.121395\n",
      "Epoch: 912 \tTraining Loss: 0.550375 \tR2: 0.121395\n",
      "Epoch: 913 \tTraining Loss: 0.543590 \tR2: 0.121395\n",
      "Epoch: 914 \tTraining Loss: 0.532429 \tR2: 0.121395\n",
      "Epoch: 915 \tTraining Loss: 0.553191 \tR2: 0.121395\n",
      "Epoch: 916 \tTraining Loss: 0.550926 \tR2: 0.121395\n",
      "Epoch: 917 \tTraining Loss: 0.528489 \tR2: 0.121395\n",
      "Epoch: 918 \tTraining Loss: 0.558162 \tR2: 0.121395\n",
      "Epoch: 919 \tTraining Loss: 0.552911 \tR2: 0.121395\n",
      "Epoch: 920 \tTraining Loss: 0.556943 \tR2: 0.121395\n",
      "Epoch: 921 \tTraining Loss: 0.532916 \tR2: 0.121395\n",
      "Epoch: 922 \tTraining Loss: 0.539227 \tR2: 0.121395\n",
      "Epoch: 923 \tTraining Loss: 0.557353 \tR2: 0.121395\n",
      "Epoch: 924 \tTraining Loss: 0.531530 \tR2: 0.121395\n",
      "Epoch: 925 \tTraining Loss: 0.544729 \tR2: 0.121395\n",
      "Epoch: 926 \tTraining Loss: 0.539007 \tR2: 0.121395\n",
      "Epoch: 927 \tTraining Loss: 0.545522 \tR2: 0.121395\n",
      "Epoch: 928 \tTraining Loss: 0.525069 \tR2: 0.121395\n",
      "Epoch: 929 \tTraining Loss: 0.560266 \tR2: 0.121395\n",
      "Epoch: 930 \tTraining Loss: 0.535208 \tR2: 0.121395\n",
      "Epoch: 931 \tTraining Loss: 0.556593 \tR2: 0.121395\n",
      "Epoch: 932 \tTraining Loss: 0.532739 \tR2: 0.121395\n",
      "Epoch: 933 \tTraining Loss: 0.544444 \tR2: 0.121395\n",
      "Epoch: 934 \tTraining Loss: 0.541888 \tR2: 0.121395\n",
      "Epoch: 935 \tTraining Loss: 0.543934 \tR2: 0.121395\n",
      "Epoch: 936 \tTraining Loss: 0.534303 \tR2: 0.121395\n",
      "Epoch: 937 \tTraining Loss: 0.535405 \tR2: 0.121395\n",
      "Epoch: 938 \tTraining Loss: 0.535714 \tR2: 0.121395\n",
      "Epoch: 939 \tTraining Loss: 0.534475 \tR2: 0.121395\n",
      "Epoch: 940 \tTraining Loss: 0.544966 \tR2: 0.121395\n",
      "Epoch: 941 \tTraining Loss: 0.548314 \tR2: 0.121395\n",
      "Epoch: 942 \tTraining Loss: 0.542940 \tR2: 0.121395\n",
      "Epoch: 943 \tTraining Loss: 0.546817 \tR2: 0.121395\n",
      "Epoch: 944 \tTraining Loss: 0.540863 \tR2: 0.121395\n",
      "Epoch: 945 \tTraining Loss: 0.549859 \tR2: 0.121395\n",
      "Epoch: 946 \tTraining Loss: 0.541525 \tR2: 0.121395\n",
      "Epoch: 947 \tTraining Loss: 0.541410 \tR2: 0.121395\n",
      "Epoch: 948 \tTraining Loss: 0.545711 \tR2: 0.121395\n",
      "Epoch: 949 \tTraining Loss: 0.541278 \tR2: 0.121395\n",
      "Epoch: 950 \tTraining Loss: 0.536196 \tR2: 0.121395\n",
      "Epoch: 951 \tTraining Loss: 0.539987 \tR2: 0.121395\n",
      "Epoch: 952 \tTraining Loss: 0.549257 \tR2: 0.121395\n",
      "Epoch: 953 \tTraining Loss: 0.525368 \tR2: 0.121395\n",
      "Epoch: 954 \tTraining Loss: 0.547059 \tR2: 0.121395\n",
      "Epoch: 955 \tTraining Loss: 0.537850 \tR2: 0.121395\n",
      "Epoch: 956 \tTraining Loss: 0.553472 \tR2: 0.121395\n",
      "Epoch: 957 \tTraining Loss: 0.535145 \tR2: 0.121395\n",
      "Epoch: 958 \tTraining Loss: 0.548195 \tR2: 0.121395\n",
      "Epoch: 959 \tTraining Loss: 0.533986 \tR2: 0.121395\n",
      "Epoch: 960 \tTraining Loss: 0.523190 \tR2: 0.121395\n",
      "Epoch: 961 \tTraining Loss: 0.530868 \tR2: 0.121395\n",
      "Epoch: 962 \tTraining Loss: 0.529105 \tR2: 0.121395\n",
      "Epoch: 963 \tTraining Loss: 0.553813 \tR2: 0.121395\n",
      "Epoch: 964 \tTraining Loss: 0.549217 \tR2: 0.121395\n",
      "Epoch: 965 \tTraining Loss: 0.529850 \tR2: 0.121395\n",
      "Epoch: 966 \tTraining Loss: 0.541695 \tR2: 0.121395\n",
      "Epoch: 967 \tTraining Loss: 0.544575 \tR2: 0.121395\n",
      "Epoch: 968 \tTraining Loss: 0.550558 \tR2: 0.121395\n",
      "Epoch: 969 \tTraining Loss: 0.549481 \tR2: 0.121395\n",
      "Epoch: 970 \tTraining Loss: 0.543208 \tR2: 0.121395\n",
      "Epoch: 971 \tTraining Loss: 0.553194 \tR2: 0.121395\n",
      "Epoch: 972 \tTraining Loss: 0.546090 \tR2: 0.121395\n",
      "Epoch: 973 \tTraining Loss: 0.546197 \tR2: 0.121395\n",
      "Epoch: 974 \tTraining Loss: 0.542393 \tR2: 0.121395\n",
      "Epoch: 975 \tTraining Loss: 0.541203 \tR2: 0.121395\n",
      "Epoch: 976 \tTraining Loss: 0.541660 \tR2: 0.121395\n",
      "Epoch: 977 \tTraining Loss: 0.546280 \tR2: 0.121395\n",
      "Epoch: 978 \tTraining Loss: 0.541195 \tR2: 0.121395\n",
      "Epoch: 979 \tTraining Loss: 0.539600 \tR2: 0.121395\n",
      "Epoch: 980 \tTraining Loss: 0.533059 \tR2: 0.121395\n",
      "Epoch: 981 \tTraining Loss: 0.529650 \tR2: 0.121395\n",
      "Epoch: 982 \tTraining Loss: 0.544350 \tR2: 0.121395\n",
      "Epoch: 983 \tTraining Loss: 0.521138 \tR2: 0.121395\n",
      "Epoch: 984 \tTraining Loss: 0.536355 \tR2: 0.121395\n",
      "Epoch: 985 \tTraining Loss: 0.540444 \tR2: 0.121395\n",
      "Epoch: 986 \tTraining Loss: 0.551238 \tR2: 0.121395\n",
      "Epoch: 987 \tTraining Loss: 0.539279 \tR2: 0.121395\n",
      "Epoch: 988 \tTraining Loss: 0.549754 \tR2: 0.121395\n",
      "Epoch: 989 \tTraining Loss: 0.525137 \tR2: 0.121395\n",
      "Epoch: 990 \tTraining Loss: 0.547141 \tR2: 0.121395\n",
      "Epoch: 991 \tTraining Loss: 0.537147 \tR2: 0.121395\n",
      "Epoch: 992 \tTraining Loss: 0.546642 \tR2: 0.121395\n",
      "Epoch: 993 \tTraining Loss: 0.538325 \tR2: 0.121395\n",
      "Epoch: 994 \tTraining Loss: 0.525374 \tR2: 0.121395\n",
      "Epoch: 995 \tTraining Loss: 0.574522 \tR2: 0.121395\n",
      "Epoch: 996 \tTraining Loss: 0.548445 \tR2: 0.121395\n",
      "Epoch: 997 \tTraining Loss: 0.531882 \tR2: 0.121395\n",
      "Epoch: 998 \tTraining Loss: 0.527973 \tR2: 0.121395\n",
      "Epoch: 999 \tTraining Loss: 0.542154 \tR2: 0.121395\n",
      "Epoch: 1000 \tTraining Loss: 0.561288 \tR2: 0.196672\n",
      "Epoch: 1001 \tTraining Loss: 0.529820 \tR2: 0.196672\n",
      "Epoch: 1002 \tTraining Loss: 0.537947 \tR2: 0.196672\n",
      "Epoch: 1003 \tTraining Loss: 0.549060 \tR2: 0.196672\n",
      "Epoch: 1004 \tTraining Loss: 0.536860 \tR2: 0.196672\n",
      "Epoch: 1005 \tTraining Loss: 0.540346 \tR2: 0.196672\n",
      "Epoch: 1006 \tTraining Loss: 0.539701 \tR2: 0.196672\n",
      "Epoch: 1007 \tTraining Loss: 0.527227 \tR2: 0.196672\n",
      "Epoch: 1008 \tTraining Loss: 0.525082 \tR2: 0.196672\n",
      "Epoch: 1009 \tTraining Loss: 0.539349 \tR2: 0.196672\n",
      "Epoch: 1010 \tTraining Loss: 0.533197 \tR2: 0.196672\n",
      "Epoch: 1011 \tTraining Loss: 0.520566 \tR2: 0.196672\n",
      "Epoch: 1012 \tTraining Loss: 0.525637 \tR2: 0.196672\n",
      "Epoch: 1013 \tTraining Loss: 0.541256 \tR2: 0.196672\n",
      "Epoch: 1014 \tTraining Loss: 0.535073 \tR2: 0.196672\n",
      "Epoch: 1015 \tTraining Loss: 0.539308 \tR2: 0.196672\n",
      "Epoch: 1016 \tTraining Loss: 0.539543 \tR2: 0.196672\n",
      "Epoch: 1017 \tTraining Loss: 0.530204 \tR2: 0.196672\n",
      "Epoch: 1018 \tTraining Loss: 0.557349 \tR2: 0.196672\n",
      "Epoch: 1019 \tTraining Loss: 0.547697 \tR2: 0.196672\n",
      "Epoch: 1020 \tTraining Loss: 0.537816 \tR2: 0.196672\n",
      "Epoch: 1021 \tTraining Loss: 0.548595 \tR2: 0.196672\n",
      "Epoch: 1022 \tTraining Loss: 0.532238 \tR2: 0.196672\n",
      "Epoch: 1023 \tTraining Loss: 0.547904 \tR2: 0.196672\n",
      "Epoch: 1024 \tTraining Loss: 0.535234 \tR2: 0.196672\n",
      "Epoch: 1025 \tTraining Loss: 0.535932 \tR2: 0.196672\n",
      "Epoch: 1026 \tTraining Loss: 0.549805 \tR2: 0.196672\n",
      "Epoch: 1027 \tTraining Loss: 0.554459 \tR2: 0.196672\n",
      "Epoch: 1028 \tTraining Loss: 0.549401 \tR2: 0.196672\n",
      "Epoch: 1029 \tTraining Loss: 0.547465 \tR2: 0.196672\n",
      "Epoch: 1030 \tTraining Loss: 0.537795 \tR2: 0.196672\n",
      "Epoch: 1031 \tTraining Loss: 0.534786 \tR2: 0.196672\n",
      "Epoch: 1032 \tTraining Loss: 0.553337 \tR2: 0.196672\n",
      "Epoch: 1033 \tTraining Loss: 0.542498 \tR2: 0.196672\n",
      "Epoch: 1034 \tTraining Loss: 0.532819 \tR2: 0.196672\n",
      "Epoch: 1035 \tTraining Loss: 0.536027 \tR2: 0.196672\n",
      "Epoch: 1036 \tTraining Loss: 0.534074 \tR2: 0.196672\n",
      "Epoch: 1037 \tTraining Loss: 0.539525 \tR2: 0.196672\n",
      "Epoch: 1038 \tTraining Loss: 0.526635 \tR2: 0.196672\n",
      "Epoch: 1039 \tTraining Loss: 0.525335 \tR2: 0.196672\n",
      "Epoch: 1040 \tTraining Loss: 0.531462 \tR2: 0.196672\n",
      "Epoch: 1041 \tTraining Loss: 0.545275 \tR2: 0.196672\n",
      "Epoch: 1042 \tTraining Loss: 0.538588 \tR2: 0.196672\n",
      "Epoch: 1043 \tTraining Loss: 0.556423 \tR2: 0.196672\n",
      "Epoch: 1044 \tTraining Loss: 0.536496 \tR2: 0.196672\n",
      "Epoch: 1045 \tTraining Loss: 0.545325 \tR2: 0.196672\n",
      "Epoch: 1046 \tTraining Loss: 0.538493 \tR2: 0.196672\n",
      "Epoch: 1047 \tTraining Loss: 0.543838 \tR2: 0.196672\n",
      "Epoch: 1048 \tTraining Loss: 0.540420 \tR2: 0.196672\n",
      "Epoch: 1049 \tTraining Loss: 0.551666 \tR2: 0.196672\n",
      "Epoch: 1050 \tTraining Loss: 0.552039 \tR2: 0.196672\n",
      "Epoch: 1051 \tTraining Loss: 0.536330 \tR2: 0.196672\n",
      "Epoch: 1052 \tTraining Loss: 0.550318 \tR2: 0.196672\n",
      "Epoch: 1053 \tTraining Loss: 0.528020 \tR2: 0.196672\n",
      "Epoch: 1054 \tTraining Loss: 0.536298 \tR2: 0.196672\n",
      "Epoch: 1055 \tTraining Loss: 0.538847 \tR2: 0.196672\n",
      "Epoch: 1056 \tTraining Loss: 0.530955 \tR2: 0.196672\n",
      "Epoch: 1057 \tTraining Loss: 0.534195 \tR2: 0.196672\n",
      "Epoch: 1058 \tTraining Loss: 0.534668 \tR2: 0.196672\n",
      "Epoch: 1059 \tTraining Loss: 0.533651 \tR2: 0.196672\n",
      "Epoch: 1060 \tTraining Loss: 0.533109 \tR2: 0.196672\n",
      "Epoch: 1061 \tTraining Loss: 0.535569 \tR2: 0.196672\n",
      "Epoch: 1062 \tTraining Loss: 0.538846 \tR2: 0.196672\n",
      "Epoch: 1063 \tTraining Loss: 0.540577 \tR2: 0.196672\n",
      "Epoch: 1064 \tTraining Loss: 0.537578 \tR2: 0.196672\n",
      "Epoch: 1065 \tTraining Loss: 0.538889 \tR2: 0.196672\n",
      "Epoch: 1066 \tTraining Loss: 0.543301 \tR2: 0.196672\n",
      "Epoch: 1067 \tTraining Loss: 0.545160 \tR2: 0.196672\n",
      "Epoch: 1068 \tTraining Loss: 0.550496 \tR2: 0.196672\n",
      "Epoch: 1069 \tTraining Loss: 0.540045 \tR2: 0.196672\n",
      "Epoch: 1070 \tTraining Loss: 0.530466 \tR2: 0.196672\n",
      "Epoch: 1071 \tTraining Loss: 0.532576 \tR2: 0.196672\n",
      "Epoch: 1072 \tTraining Loss: 0.535274 \tR2: 0.196672\n",
      "Epoch: 1073 \tTraining Loss: 0.544287 \tR2: 0.196672\n",
      "Epoch: 1074 \tTraining Loss: 0.527599 \tR2: 0.196672\n",
      "Epoch: 1075 \tTraining Loss: 0.516536 \tR2: 0.196672\n",
      "Epoch: 1076 \tTraining Loss: 0.552265 \tR2: 0.196672\n",
      "Epoch: 1077 \tTraining Loss: 0.530621 \tR2: 0.196672\n",
      "Epoch: 1078 \tTraining Loss: 0.533846 \tR2: 0.196672\n",
      "Epoch: 1079 \tTraining Loss: 0.526999 \tR2: 0.196672\n",
      "Epoch: 1080 \tTraining Loss: 0.532396 \tR2: 0.196672\n",
      "Epoch: 1081 \tTraining Loss: 0.531911 \tR2: 0.196672\n",
      "Epoch: 1082 \tTraining Loss: 0.536498 \tR2: 0.196672\n",
      "Epoch: 1083 \tTraining Loss: 0.551729 \tR2: 0.196672\n",
      "Epoch: 1084 \tTraining Loss: 0.544190 \tR2: 0.196672\n",
      "Epoch: 1085 \tTraining Loss: 0.556713 \tR2: 0.196672\n",
      "Epoch: 1086 \tTraining Loss: 0.532883 \tR2: 0.196672\n",
      "Epoch: 1087 \tTraining Loss: 0.534672 \tR2: 0.196672\n",
      "Epoch: 1088 \tTraining Loss: 0.531102 \tR2: 0.196672\n",
      "Epoch: 1089 \tTraining Loss: 0.554151 \tR2: 0.196672\n",
      "Epoch: 1090 \tTraining Loss: 0.535606 \tR2: 0.196672\n",
      "Epoch: 1091 \tTraining Loss: 0.529420 \tR2: 0.196672\n",
      "Epoch: 1092 \tTraining Loss: 0.543546 \tR2: 0.196672\n",
      "Epoch: 1093 \tTraining Loss: 0.534078 \tR2: 0.196672\n",
      "Epoch: 1094 \tTraining Loss: 0.542909 \tR2: 0.196672\n",
      "Epoch: 1095 \tTraining Loss: 0.523582 \tR2: 0.196672\n",
      "Epoch: 1096 \tTraining Loss: 0.533505 \tR2: 0.196672\n",
      "Epoch: 1097 \tTraining Loss: 0.547403 \tR2: 0.196672\n",
      "Epoch: 1098 \tTraining Loss: 0.537372 \tR2: 0.196672\n",
      "Epoch: 1099 \tTraining Loss: 0.533793 \tR2: 0.196672\n",
      "Epoch: 1100 \tTraining Loss: 0.534732 \tR2: -0.086035\n",
      "Epoch: 1101 \tTraining Loss: 0.523615 \tR2: -0.086035\n",
      "Epoch: 1102 \tTraining Loss: 0.512274 \tR2: -0.086035\n",
      "Epoch: 1103 \tTraining Loss: 0.524718 \tR2: -0.086035\n",
      "Epoch: 1104 \tTraining Loss: 0.538200 \tR2: -0.086035\n",
      "Epoch: 1105 \tTraining Loss: 0.556041 \tR2: -0.086035\n",
      "Epoch: 1106 \tTraining Loss: 0.543550 \tR2: -0.086035\n",
      "Epoch: 1107 \tTraining Loss: 0.534977 \tR2: -0.086035\n",
      "Epoch: 1108 \tTraining Loss: 0.537987 \tR2: -0.086035\n",
      "Epoch: 1109 \tTraining Loss: 0.532640 \tR2: -0.086035\n",
      "Epoch: 1110 \tTraining Loss: 0.553218 \tR2: -0.086035\n",
      "Epoch: 1111 \tTraining Loss: 0.517068 \tR2: -0.086035\n",
      "Epoch: 1112 \tTraining Loss: 0.643313 \tR2: -0.086035\n",
      "Epoch: 1113 \tTraining Loss: 0.532074 \tR2: -0.086035\n",
      "Epoch: 1114 \tTraining Loss: 0.546524 \tR2: -0.086035\n",
      "Epoch: 1115 \tTraining Loss: 0.545816 \tR2: -0.086035\n",
      "Epoch: 1116 \tTraining Loss: 0.538513 \tR2: -0.086035\n",
      "Epoch: 1117 \tTraining Loss: 0.538364 \tR2: -0.086035\n",
      "Epoch: 1118 \tTraining Loss: 0.551977 \tR2: -0.086035\n",
      "Epoch: 1119 \tTraining Loss: 0.525238 \tR2: -0.086035\n",
      "Epoch: 1120 \tTraining Loss: 0.516526 \tR2: -0.086035\n",
      "Epoch: 1121 \tTraining Loss: 0.532934 \tR2: -0.086035\n",
      "Epoch: 1122 \tTraining Loss: 0.538548 \tR2: -0.086035\n",
      "Epoch: 1123 \tTraining Loss: 0.540518 \tR2: -0.086035\n",
      "Epoch: 1124 \tTraining Loss: 0.536115 \tR2: -0.086035\n",
      "Epoch: 1125 \tTraining Loss: 0.544037 \tR2: -0.086035\n",
      "Epoch: 1126 \tTraining Loss: 0.537733 \tR2: -0.086035\n",
      "Epoch: 1127 \tTraining Loss: 0.522568 \tR2: -0.086035\n",
      "Epoch: 1128 \tTraining Loss: 0.530127 \tR2: -0.086035\n",
      "Epoch: 1129 \tTraining Loss: 0.540590 \tR2: -0.086035\n",
      "Epoch: 1130 \tTraining Loss: 0.537069 \tR2: -0.086035\n",
      "Epoch: 1131 \tTraining Loss: 0.529032 \tR2: -0.086035\n",
      "Epoch: 1132 \tTraining Loss: 0.548547 \tR2: -0.086035\n",
      "Epoch: 1133 \tTraining Loss: 0.532536 \tR2: -0.086035\n",
      "Epoch: 1134 \tTraining Loss: 0.521140 \tR2: -0.086035\n",
      "Epoch: 1135 \tTraining Loss: 0.545109 \tR2: -0.086035\n",
      "Epoch: 1136 \tTraining Loss: 0.540286 \tR2: -0.086035\n",
      "Epoch: 1137 \tTraining Loss: 0.525448 \tR2: -0.086035\n",
      "Epoch: 1138 \tTraining Loss: 0.542317 \tR2: -0.086035\n",
      "Epoch: 1139 \tTraining Loss: 0.547079 \tR2: -0.086035\n",
      "Epoch: 1140 \tTraining Loss: 0.538342 \tR2: -0.086035\n",
      "Epoch: 1141 \tTraining Loss: 0.544425 \tR2: -0.086035\n",
      "Epoch: 1142 \tTraining Loss: 0.529787 \tR2: -0.086035\n",
      "Epoch: 1143 \tTraining Loss: 0.529127 \tR2: -0.086035\n",
      "Epoch: 1144 \tTraining Loss: 0.532605 \tR2: -0.086035\n",
      "Epoch: 1145 \tTraining Loss: 0.537313 \tR2: -0.086035\n",
      "Epoch: 1146 \tTraining Loss: 0.530346 \tR2: -0.086035\n",
      "Epoch: 1147 \tTraining Loss: 0.539561 \tR2: -0.086035\n",
      "Epoch: 1148 \tTraining Loss: 0.529371 \tR2: -0.086035\n",
      "Epoch: 1149 \tTraining Loss: 0.534327 \tR2: -0.086035\n",
      "Epoch: 1150 \tTraining Loss: 0.529923 \tR2: -0.086035\n",
      "Epoch: 1151 \tTraining Loss: 0.527627 \tR2: -0.086035\n",
      "Epoch: 1152 \tTraining Loss: 0.525241 \tR2: -0.086035\n",
      "Epoch: 1153 \tTraining Loss: 0.528189 \tR2: -0.086035\n",
      "Epoch: 1154 \tTraining Loss: 0.529573 \tR2: -0.086035\n",
      "Epoch: 1155 \tTraining Loss: 0.536693 \tR2: -0.086035\n",
      "Epoch: 1156 \tTraining Loss: 0.520786 \tR2: -0.086035\n",
      "Epoch: 1157 \tTraining Loss: 0.529826 \tR2: -0.086035\n",
      "Epoch: 1158 \tTraining Loss: 0.550466 \tR2: -0.086035\n",
      "Epoch: 1159 \tTraining Loss: 0.571554 \tR2: -0.086035\n",
      "Epoch: 1160 \tTraining Loss: 0.544835 \tR2: -0.086035\n",
      "Epoch: 1161 \tTraining Loss: 0.531828 \tR2: -0.086035\n",
      "Epoch: 1162 \tTraining Loss: 0.531760 \tR2: -0.086035\n",
      "Epoch: 1163 \tTraining Loss: 0.533572 \tR2: -0.086035\n",
      "Epoch: 1164 \tTraining Loss: 0.544355 \tR2: -0.086035\n",
      "Epoch: 1165 \tTraining Loss: 0.537119 \tR2: -0.086035\n",
      "Epoch: 1166 \tTraining Loss: 0.514259 \tR2: -0.086035\n",
      "Epoch: 1167 \tTraining Loss: 0.538006 \tR2: -0.086035\n",
      "Epoch: 1168 \tTraining Loss: 0.539721 \tR2: -0.086035\n",
      "Epoch: 1169 \tTraining Loss: 0.543437 \tR2: -0.086035\n",
      "Epoch: 1170 \tTraining Loss: 0.529234 \tR2: -0.086035\n",
      "Epoch: 1171 \tTraining Loss: 0.519681 \tR2: -0.086035\n",
      "Epoch: 1172 \tTraining Loss: 0.535683 \tR2: -0.086035\n",
      "Epoch: 1173 \tTraining Loss: 0.525323 \tR2: -0.086035\n",
      "Epoch: 1174 \tTraining Loss: 0.540000 \tR2: -0.086035\n",
      "Epoch: 1175 \tTraining Loss: 0.541979 \tR2: -0.086035\n",
      "Epoch: 1176 \tTraining Loss: 0.532870 \tR2: -0.086035\n",
      "Epoch: 1177 \tTraining Loss: 0.528073 \tR2: -0.086035\n",
      "Epoch: 1178 \tTraining Loss: 0.530686 \tR2: -0.086035\n",
      "Epoch: 1179 \tTraining Loss: 0.551278 \tR2: -0.086035\n",
      "Epoch: 1180 \tTraining Loss: 0.525141 \tR2: -0.086035\n",
      "Epoch: 1181 \tTraining Loss: 0.524348 \tR2: -0.086035\n",
      "Epoch: 1182 \tTraining Loss: 0.512800 \tR2: -0.086035\n",
      "Epoch: 1183 \tTraining Loss: 0.536957 \tR2: -0.086035\n",
      "Epoch: 1184 \tTraining Loss: 0.528834 \tR2: -0.086035\n",
      "Epoch: 1185 \tTraining Loss: 0.536677 \tR2: -0.086035\n",
      "Epoch: 1186 \tTraining Loss: 0.562430 \tR2: -0.086035\n",
      "Epoch: 1187 \tTraining Loss: 0.541998 \tR2: -0.086035\n",
      "Epoch: 1188 \tTraining Loss: 0.554930 \tR2: -0.086035\n",
      "Epoch: 1189 \tTraining Loss: 0.541296 \tR2: -0.086035\n",
      "Epoch: 1190 \tTraining Loss: 0.536132 \tR2: -0.086035\n",
      "Epoch: 1191 \tTraining Loss: 0.540539 \tR2: -0.086035\n",
      "Epoch: 1192 \tTraining Loss: 0.508321 \tR2: -0.086035\n",
      "Epoch: 1193 \tTraining Loss: 0.513127 \tR2: -0.086035\n",
      "Epoch: 1194 \tTraining Loss: 0.544212 \tR2: -0.086035\n",
      "Epoch: 1195 \tTraining Loss: 0.544140 \tR2: -0.086035\n",
      "Epoch: 1196 \tTraining Loss: 0.525885 \tR2: -0.086035\n",
      "Epoch: 1197 \tTraining Loss: 0.530288 \tR2: -0.086035\n",
      "Epoch: 1198 \tTraining Loss: 0.545722 \tR2: -0.086035\n",
      "Epoch: 1199 \tTraining Loss: 0.541000 \tR2: -0.086035\n",
      "Epoch: 1200 \tTraining Loss: 0.529415 \tR2: 0.357904\n",
      "Epoch: 1201 \tTraining Loss: 0.522112 \tR2: 0.357904\n",
      "Epoch: 1202 \tTraining Loss: 0.536430 \tR2: 0.357904\n",
      "Epoch: 1203 \tTraining Loss: 0.548483 \tR2: 0.357904\n",
      "Epoch: 1204 \tTraining Loss: 0.535185 \tR2: 0.357904\n",
      "Epoch: 1205 \tTraining Loss: 0.537983 \tR2: 0.357904\n",
      "Epoch: 1206 \tTraining Loss: 0.526502 \tR2: 0.357904\n",
      "Epoch: 1207 \tTraining Loss: 0.537623 \tR2: 0.357904\n",
      "Epoch: 1208 \tTraining Loss: 0.536016 \tR2: 0.357904\n",
      "Epoch: 1209 \tTraining Loss: 0.525421 \tR2: 0.357904\n",
      "Epoch: 1210 \tTraining Loss: 0.535650 \tR2: 0.357904\n",
      "Epoch: 1211 \tTraining Loss: 0.523655 \tR2: 0.357904\n",
      "Epoch: 1212 \tTraining Loss: 0.534291 \tR2: 0.357904\n",
      "Epoch: 1213 \tTraining Loss: 0.525606 \tR2: 0.357904\n",
      "Epoch: 1214 \tTraining Loss: 0.522094 \tR2: 0.357904\n",
      "Epoch: 1215 \tTraining Loss: 0.526298 \tR2: 0.357904\n",
      "Epoch: 1216 \tTraining Loss: 0.543641 \tR2: 0.357904\n",
      "Epoch: 1217 \tTraining Loss: 0.535964 \tR2: 0.357904\n",
      "Epoch: 1218 \tTraining Loss: 0.517713 \tR2: 0.357904\n",
      "Epoch: 1219 \tTraining Loss: 0.523242 \tR2: 0.357904\n",
      "Epoch: 1220 \tTraining Loss: 0.528078 \tR2: 0.357904\n",
      "Epoch: 1221 \tTraining Loss: 0.537247 \tR2: 0.357904\n",
      "Epoch: 1222 \tTraining Loss: 0.536188 \tR2: 0.357904\n",
      "Epoch: 1223 \tTraining Loss: 0.511555 \tR2: 0.357904\n",
      "Epoch: 1224 \tTraining Loss: 0.525254 \tR2: 0.357904\n",
      "Epoch: 1225 \tTraining Loss: 0.531035 \tR2: 0.357904\n",
      "Epoch: 1226 \tTraining Loss: 0.496181 \tR2: 0.357904\n",
      "Epoch: 1227 \tTraining Loss: 0.527706 \tR2: 0.357904\n",
      "Epoch: 1228 \tTraining Loss: 0.521523 \tR2: 0.357904\n",
      "Epoch: 1229 \tTraining Loss: 0.550656 \tR2: 0.357904\n",
      "Epoch: 1230 \tTraining Loss: 0.528773 \tR2: 0.357904\n",
      "Epoch: 1231 \tTraining Loss: 0.541969 \tR2: 0.357904\n",
      "Epoch: 1232 \tTraining Loss: 0.529448 \tR2: 0.357904\n",
      "Epoch: 1233 \tTraining Loss: 0.547869 \tR2: 0.357904\n",
      "Epoch: 1234 \tTraining Loss: 0.521950 \tR2: 0.357904\n",
      "Epoch: 1235 \tTraining Loss: 0.526724 \tR2: 0.357904\n",
      "Epoch: 1236 \tTraining Loss: 0.533580 \tR2: 0.357904\n",
      "Epoch: 1237 \tTraining Loss: 0.522723 \tR2: 0.357904\n",
      "Epoch: 1238 \tTraining Loss: 0.538146 \tR2: 0.357904\n",
      "Epoch: 1239 \tTraining Loss: 0.517277 \tR2: 0.357904\n",
      "Epoch: 1240 \tTraining Loss: 0.525853 \tR2: 0.357904\n",
      "Epoch: 1241 \tTraining Loss: 0.531926 \tR2: 0.357904\n",
      "Epoch: 1242 \tTraining Loss: 0.540476 \tR2: 0.357904\n",
      "Epoch: 1243 \tTraining Loss: 0.545999 \tR2: 0.357904\n",
      "Epoch: 1244 \tTraining Loss: 0.542724 \tR2: 0.357904\n",
      "Epoch: 1245 \tTraining Loss: 0.520694 \tR2: 0.357904\n",
      "Epoch: 1246 \tTraining Loss: 0.529519 \tR2: 0.357904\n",
      "Epoch: 1247 \tTraining Loss: 0.541022 \tR2: 0.357904\n",
      "Epoch: 1248 \tTraining Loss: 0.530727 \tR2: 0.357904\n",
      "Epoch: 1249 \tTraining Loss: 0.545355 \tR2: 0.357904\n",
      "Epoch: 1250 \tTraining Loss: 0.528302 \tR2: 0.357904\n",
      "Epoch: 1251 \tTraining Loss: 0.533520 \tR2: 0.357904\n",
      "Epoch: 1252 \tTraining Loss: 0.530316 \tR2: 0.357904\n",
      "Epoch: 1253 \tTraining Loss: 0.546627 \tR2: 0.357904\n",
      "Epoch: 1254 \tTraining Loss: 0.538778 \tR2: 0.357904\n",
      "Epoch: 1255 \tTraining Loss: 0.530645 \tR2: 0.357904\n",
      "Epoch: 1256 \tTraining Loss: 0.545358 \tR2: 0.357904\n",
      "Epoch: 1257 \tTraining Loss: 0.542872 \tR2: 0.357904\n",
      "Epoch: 1258 \tTraining Loss: 0.546988 \tR2: 0.357904\n",
      "Epoch: 1259 \tTraining Loss: 0.513074 \tR2: 0.357904\n",
      "Epoch: 1260 \tTraining Loss: 0.522544 \tR2: 0.357904\n",
      "Epoch: 1261 \tTraining Loss: 0.519542 \tR2: 0.357904\n",
      "Epoch: 1262 \tTraining Loss: 0.514465 \tR2: 0.357904\n",
      "Epoch: 1263 \tTraining Loss: 0.526848 \tR2: 0.357904\n",
      "Epoch: 1264 \tTraining Loss: 0.553455 \tR2: 0.357904\n",
      "Epoch: 1265 \tTraining Loss: 0.539403 \tR2: 0.357904\n",
      "Epoch: 1266 \tTraining Loss: 0.529968 \tR2: 0.357904\n",
      "Epoch: 1267 \tTraining Loss: 0.523350 \tR2: 0.357904\n",
      "Epoch: 1268 \tTraining Loss: 0.531623 \tR2: 0.357904\n",
      "Epoch: 1269 \tTraining Loss: 0.525797 \tR2: 0.357904\n",
      "Epoch: 1270 \tTraining Loss: 0.532799 \tR2: 0.357904\n",
      "Epoch: 1271 \tTraining Loss: 0.539785 \tR2: 0.357904\n",
      "Epoch: 1272 \tTraining Loss: 0.529616 \tR2: 0.357904\n",
      "Epoch: 1273 \tTraining Loss: 0.538788 \tR2: 0.357904\n",
      "Epoch: 1274 \tTraining Loss: 0.526307 \tR2: 0.357904\n",
      "Epoch: 1275 \tTraining Loss: 0.533737 \tR2: 0.357904\n",
      "Epoch: 1276 \tTraining Loss: 0.524821 \tR2: 0.357904\n",
      "Epoch: 1277 \tTraining Loss: 0.525811 \tR2: 0.357904\n",
      "Epoch: 1278 \tTraining Loss: 0.535553 \tR2: 0.357904\n",
      "Epoch: 1279 \tTraining Loss: 0.529611 \tR2: 0.357904\n",
      "Epoch: 1280 \tTraining Loss: 0.534761 \tR2: 0.357904\n",
      "Epoch: 1281 \tTraining Loss: 0.557032 \tR2: 0.357904\n",
      "Epoch: 1282 \tTraining Loss: 0.519662 \tR2: 0.357904\n",
      "Epoch: 1283 \tTraining Loss: 0.540178 \tR2: 0.357904\n",
      "Epoch: 1284 \tTraining Loss: 0.531718 \tR2: 0.357904\n",
      "Epoch: 1285 \tTraining Loss: 0.559829 \tR2: 0.357904\n",
      "Epoch: 1286 \tTraining Loss: 0.537421 \tR2: 0.357904\n",
      "Epoch: 1287 \tTraining Loss: 0.533576 \tR2: 0.357904\n",
      "Epoch: 1288 \tTraining Loss: 0.535107 \tR2: 0.357904\n",
      "Epoch: 1289 \tTraining Loss: 0.504870 \tR2: 0.357904\n",
      "Epoch: 1290 \tTraining Loss: 0.525652 \tR2: 0.357904\n",
      "Epoch: 1291 \tTraining Loss: 0.525528 \tR2: 0.357904\n",
      "Epoch: 1292 \tTraining Loss: 0.543007 \tR2: 0.357904\n",
      "Epoch: 1293 \tTraining Loss: 0.519836 \tR2: 0.357904\n",
      "Epoch: 1294 \tTraining Loss: 0.536471 \tR2: 0.357904\n",
      "Epoch: 1295 \tTraining Loss: 0.533340 \tR2: 0.357904\n",
      "Epoch: 1296 \tTraining Loss: 0.522925 \tR2: 0.357904\n",
      "Epoch: 1297 \tTraining Loss: 0.534998 \tR2: 0.357904\n",
      "Epoch: 1298 \tTraining Loss: 0.527398 \tR2: 0.357904\n",
      "Epoch: 1299 \tTraining Loss: 0.548448 \tR2: 0.357904\n",
      "Epoch: 1300 \tTraining Loss: 0.533043 \tR2: 0.218620\n",
      "Epoch: 1301 \tTraining Loss: 0.556024 \tR2: 0.218620\n",
      "Epoch: 1302 \tTraining Loss: 0.522285 \tR2: 0.218620\n",
      "Epoch: 1303 \tTraining Loss: 0.528233 \tR2: 0.218620\n",
      "Epoch: 1304 \tTraining Loss: 0.526863 \tR2: 0.218620\n",
      "Epoch: 1305 \tTraining Loss: 0.523635 \tR2: 0.218620\n",
      "Epoch: 1306 \tTraining Loss: 0.545865 \tR2: 0.218620\n",
      "Epoch: 1307 \tTraining Loss: 0.519947 \tR2: 0.218620\n",
      "Epoch: 1308 \tTraining Loss: 0.530051 \tR2: 0.218620\n",
      "Epoch: 1309 \tTraining Loss: 0.513704 \tR2: 0.218620\n",
      "Epoch: 1310 \tTraining Loss: 0.513795 \tR2: 0.218620\n",
      "Epoch: 1311 \tTraining Loss: 0.529166 \tR2: 0.218620\n",
      "Epoch: 1312 \tTraining Loss: 0.552263 \tR2: 0.218620\n",
      "Epoch: 1313 \tTraining Loss: 0.536268 \tR2: 0.218620\n",
      "Epoch: 1314 \tTraining Loss: 0.530730 \tR2: 0.218620\n",
      "Epoch: 1315 \tTraining Loss: 0.528866 \tR2: 0.218620\n",
      "Epoch: 1316 \tTraining Loss: 0.527770 \tR2: 0.218620\n",
      "Epoch: 1317 \tTraining Loss: 0.554252 \tR2: 0.218620\n",
      "Epoch: 1318 \tTraining Loss: 0.514504 \tR2: 0.218620\n",
      "Epoch: 1319 \tTraining Loss: 0.546604 \tR2: 0.218620\n",
      "Epoch: 1320 \tTraining Loss: 0.517298 \tR2: 0.218620\n",
      "Epoch: 1321 \tTraining Loss: 0.536145 \tR2: 0.218620\n",
      "Epoch: 1322 \tTraining Loss: 0.547185 \tR2: 0.218620\n",
      "Epoch: 1323 \tTraining Loss: 0.545343 \tR2: 0.218620\n",
      "Epoch: 1324 \tTraining Loss: 0.517772 \tR2: 0.218620\n",
      "Epoch: 1325 \tTraining Loss: 0.523880 \tR2: 0.218620\n",
      "Epoch: 1326 \tTraining Loss: 0.531367 \tR2: 0.218620\n",
      "Epoch: 1327 \tTraining Loss: 0.527792 \tR2: 0.218620\n",
      "Epoch: 1328 \tTraining Loss: 0.484234 \tR2: 0.218620\n",
      "Epoch: 1329 \tTraining Loss: 0.522857 \tR2: 0.218620\n",
      "Epoch: 1330 \tTraining Loss: 0.539407 \tR2: 0.218620\n",
      "Epoch: 1331 \tTraining Loss: 0.546190 \tR2: 0.218620\n",
      "Epoch: 1332 \tTraining Loss: 0.529902 \tR2: 0.218620\n",
      "Epoch: 1333 \tTraining Loss: 0.534653 \tR2: 0.218620\n",
      "Epoch: 1334 \tTraining Loss: 0.525413 \tR2: 0.218620\n",
      "Epoch: 1335 \tTraining Loss: 0.534515 \tR2: 0.218620\n",
      "Epoch: 1336 \tTraining Loss: 0.521642 \tR2: 0.218620\n",
      "Epoch: 1337 \tTraining Loss: 0.535652 \tR2: 0.218620\n",
      "Epoch: 1338 \tTraining Loss: 0.543939 \tR2: 0.218620\n",
      "Epoch: 1339 \tTraining Loss: 0.533717 \tR2: 0.218620\n",
      "Epoch: 1340 \tTraining Loss: 0.530918 \tR2: 0.218620\n",
      "Epoch: 1341 \tTraining Loss: 0.533906 \tR2: 0.218620\n",
      "Epoch: 1342 \tTraining Loss: 0.540930 \tR2: 0.218620\n",
      "Epoch: 1343 \tTraining Loss: 0.517634 \tR2: 0.218620\n",
      "Epoch: 1344 \tTraining Loss: 0.528164 \tR2: 0.218620\n",
      "Epoch: 1345 \tTraining Loss: 0.528216 \tR2: 0.218620\n",
      "Epoch: 1346 \tTraining Loss: 0.529078 \tR2: 0.218620\n",
      "Epoch: 1347 \tTraining Loss: 0.532207 \tR2: 0.218620\n",
      "Epoch: 1348 \tTraining Loss: 0.543641 \tR2: 0.218620\n",
      "Epoch: 1349 \tTraining Loss: 0.529360 \tR2: 0.218620\n",
      "Epoch: 1350 \tTraining Loss: 0.531087 \tR2: 0.218620\n",
      "Epoch: 1351 \tTraining Loss: 0.524983 \tR2: 0.218620\n",
      "Epoch: 1352 \tTraining Loss: 0.519374 \tR2: 0.218620\n",
      "Epoch: 1353 \tTraining Loss: 0.529268 \tR2: 0.218620\n",
      "Epoch: 1354 \tTraining Loss: 0.524294 \tR2: 0.218620\n",
      "Epoch: 1355 \tTraining Loss: 0.528636 \tR2: 0.218620\n",
      "Epoch: 1356 \tTraining Loss: 0.535666 \tR2: 0.218620\n",
      "Epoch: 1357 \tTraining Loss: 0.527767 \tR2: 0.218620\n",
      "Epoch: 1358 \tTraining Loss: 0.532729 \tR2: 0.218620\n",
      "Epoch: 1359 \tTraining Loss: 0.528743 \tR2: 0.218620\n",
      "Epoch: 1360 \tTraining Loss: 0.535672 \tR2: 0.218620\n",
      "Epoch: 1361 \tTraining Loss: 0.528355 \tR2: 0.218620\n",
      "Epoch: 1362 \tTraining Loss: 0.533037 \tR2: 0.218620\n",
      "Epoch: 1363 \tTraining Loss: 0.526863 \tR2: 0.218620\n",
      "Epoch: 1364 \tTraining Loss: 0.535111 \tR2: 0.218620\n",
      "Epoch: 1365 \tTraining Loss: 0.521386 \tR2: 0.218620\n",
      "Epoch: 1366 \tTraining Loss: 0.530102 \tR2: 0.218620\n",
      "Epoch: 1367 \tTraining Loss: 0.529787 \tR2: 0.218620\n",
      "Epoch: 1368 \tTraining Loss: 0.521692 \tR2: 0.218620\n",
      "Epoch: 1369 \tTraining Loss: 0.546818 \tR2: 0.218620\n",
      "Epoch: 1370 \tTraining Loss: 0.521123 \tR2: 0.218620\n",
      "Epoch: 1371 \tTraining Loss: 0.536910 \tR2: 0.218620\n",
      "Epoch: 1372 \tTraining Loss: 0.527555 \tR2: 0.218620\n",
      "Epoch: 1373 \tTraining Loss: 0.536103 \tR2: 0.218620\n",
      "Epoch: 1374 \tTraining Loss: 0.528202 \tR2: 0.218620\n",
      "Epoch: 1375 \tTraining Loss: 0.511597 \tR2: 0.218620\n",
      "Epoch: 1376 \tTraining Loss: 0.539989 \tR2: 0.218620\n",
      "Epoch: 1377 \tTraining Loss: 0.534619 \tR2: 0.218620\n",
      "Epoch: 1378 \tTraining Loss: 0.526543 \tR2: 0.218620\n",
      "Epoch: 1379 \tTraining Loss: 0.524399 \tR2: 0.218620\n",
      "Epoch: 1380 \tTraining Loss: 0.523418 \tR2: 0.218620\n",
      "Epoch: 1381 \tTraining Loss: 0.546332 \tR2: 0.218620\n",
      "Epoch: 1382 \tTraining Loss: 0.545901 \tR2: 0.218620\n",
      "Epoch: 1383 \tTraining Loss: 0.532957 \tR2: 0.218620\n",
      "Epoch: 1384 \tTraining Loss: 0.520629 \tR2: 0.218620\n",
      "Epoch: 1385 \tTraining Loss: 0.537389 \tR2: 0.218620\n",
      "Epoch: 1386 \tTraining Loss: 0.526387 \tR2: 0.218620\n",
      "Epoch: 1387 \tTraining Loss: 0.524606 \tR2: 0.218620\n",
      "Epoch: 1388 \tTraining Loss: 0.534477 \tR2: 0.218620\n",
      "Epoch: 1389 \tTraining Loss: 0.529154 \tR2: 0.218620\n",
      "Epoch: 1390 \tTraining Loss: 0.543699 \tR2: 0.218620\n",
      "Epoch: 1391 \tTraining Loss: 0.527724 \tR2: 0.218620\n",
      "Epoch: 1392 \tTraining Loss: 0.528855 \tR2: 0.218620\n",
      "Epoch: 1393 \tTraining Loss: 0.536179 \tR2: 0.218620\n",
      "Epoch: 1394 \tTraining Loss: 0.519372 \tR2: 0.218620\n",
      "Epoch: 1395 \tTraining Loss: 0.534988 \tR2: 0.218620\n",
      "Epoch: 1396 \tTraining Loss: 0.511761 \tR2: 0.218620\n",
      "Epoch: 1397 \tTraining Loss: 0.544922 \tR2: 0.218620\n",
      "Epoch: 1398 \tTraining Loss: 0.539199 \tR2: 0.218620\n",
      "Epoch: 1399 \tTraining Loss: 0.526590 \tR2: 0.218620\n",
      "Epoch: 1400 \tTraining Loss: 0.538288 \tR2: 0.416407\n",
      "Epoch: 1401 \tTraining Loss: 0.513196 \tR2: 0.416407\n",
      "Epoch: 1402 \tTraining Loss: 0.516781 \tR2: 0.416407\n",
      "Epoch: 1403 \tTraining Loss: 0.521744 \tR2: 0.416407\n",
      "Epoch: 1404 \tTraining Loss: 0.518373 \tR2: 0.416407\n",
      "Epoch: 1405 \tTraining Loss: 0.532536 \tR2: 0.416407\n",
      "Epoch: 1406 \tTraining Loss: 0.524263 \tR2: 0.416407\n",
      "Epoch: 1407 \tTraining Loss: 0.518691 \tR2: 0.416407\n",
      "Epoch: 1408 \tTraining Loss: 0.529891 \tR2: 0.416407\n",
      "Epoch: 1409 \tTraining Loss: 0.514912 \tR2: 0.416407\n",
      "Epoch: 1410 \tTraining Loss: 0.533732 \tR2: 0.416407\n",
      "Epoch: 1411 \tTraining Loss: 0.535739 \tR2: 0.416407\n",
      "Epoch: 1412 \tTraining Loss: 0.538324 \tR2: 0.416407\n",
      "Epoch: 1413 \tTraining Loss: 0.543241 \tR2: 0.416407\n",
      "Epoch: 1414 \tTraining Loss: 0.530102 \tR2: 0.416407\n",
      "Epoch: 1415 \tTraining Loss: 0.541328 \tR2: 0.416407\n",
      "Epoch: 1416 \tTraining Loss: 0.545392 \tR2: 0.416407\n",
      "Epoch: 1417 \tTraining Loss: 0.547326 \tR2: 0.416407\n",
      "Epoch: 1418 \tTraining Loss: 0.514126 \tR2: 0.416407\n",
      "Epoch: 1419 \tTraining Loss: 0.536081 \tR2: 0.416407\n",
      "Epoch: 1420 \tTraining Loss: 0.527405 \tR2: 0.416407\n",
      "Epoch: 1421 \tTraining Loss: 0.527573 \tR2: 0.416407\n",
      "Epoch: 1422 \tTraining Loss: 0.527811 \tR2: 0.416407\n",
      "Epoch: 1423 \tTraining Loss: 0.503856 \tR2: 0.416407\n",
      "Epoch: 1424 \tTraining Loss: 0.531596 \tR2: 0.416407\n",
      "Epoch: 1425 \tTraining Loss: 0.521409 \tR2: 0.416407\n",
      "Epoch: 1426 \tTraining Loss: 0.524996 \tR2: 0.416407\n",
      "Epoch: 1427 \tTraining Loss: 0.526478 \tR2: 0.416407\n",
      "Epoch: 1428 \tTraining Loss: 0.538327 \tR2: 0.416407\n",
      "Epoch: 1429 \tTraining Loss: 0.531693 \tR2: 0.416407\n",
      "Epoch: 1430 \tTraining Loss: 0.528288 \tR2: 0.416407\n",
      "Epoch: 1431 \tTraining Loss: 0.527606 \tR2: 0.416407\n",
      "Epoch: 1432 \tTraining Loss: 0.531383 \tR2: 0.416407\n",
      "Epoch: 1433 \tTraining Loss: 0.515765 \tR2: 0.416407\n",
      "Epoch: 1434 \tTraining Loss: 0.539941 \tR2: 0.416407\n",
      "Epoch: 1435 \tTraining Loss: 0.514988 \tR2: 0.416407\n",
      "Epoch: 1436 \tTraining Loss: 0.527377 \tR2: 0.416407\n",
      "Epoch: 1437 \tTraining Loss: 0.536374 \tR2: 0.416407\n",
      "Epoch: 1438 \tTraining Loss: 0.530480 \tR2: 0.416407\n",
      "Epoch: 1439 \tTraining Loss: 0.532494 \tR2: 0.416407\n",
      "Epoch: 1440 \tTraining Loss: 0.536689 \tR2: 0.416407\n",
      "Epoch: 1441 \tTraining Loss: 0.536149 \tR2: 0.416407\n",
      "Epoch: 1442 \tTraining Loss: 0.536159 \tR2: 0.416407\n",
      "Epoch: 1443 \tTraining Loss: 0.541393 \tR2: 0.416407\n",
      "Epoch: 1444 \tTraining Loss: 0.524110 \tR2: 0.416407\n",
      "Epoch: 1445 \tTraining Loss: 0.533715 \tR2: 0.416407\n",
      "Epoch: 1446 \tTraining Loss: 0.519009 \tR2: 0.416407\n",
      "Epoch: 1447 \tTraining Loss: 0.521441 \tR2: 0.416407\n",
      "Epoch: 1448 \tTraining Loss: 0.520926 \tR2: 0.416407\n",
      "Epoch: 1449 \tTraining Loss: 0.518013 \tR2: 0.416407\n",
      "Epoch: 1450 \tTraining Loss: 0.507166 \tR2: 0.416407\n",
      "Epoch: 1451 \tTraining Loss: 0.523072 \tR2: 0.416407\n",
      "Epoch: 1452 \tTraining Loss: 0.521412 \tR2: 0.416407\n",
      "Epoch: 1453 \tTraining Loss: 0.536372 \tR2: 0.416407\n",
      "Epoch: 1454 \tTraining Loss: 0.547716 \tR2: 0.416407\n",
      "Epoch: 1455 \tTraining Loss: 0.522997 \tR2: 0.416407\n",
      "Epoch: 1456 \tTraining Loss: 0.529791 \tR2: 0.416407\n",
      "Epoch: 1457 \tTraining Loss: 0.526890 \tR2: 0.416407\n",
      "Epoch: 1458 \tTraining Loss: 0.512551 \tR2: 0.416407\n",
      "Epoch: 1459 \tTraining Loss: 0.536109 \tR2: 0.416407\n",
      "Epoch: 1460 \tTraining Loss: 0.518720 \tR2: 0.416407\n",
      "Epoch: 1461 \tTraining Loss: 0.534817 \tR2: 0.416407\n",
      "Epoch: 1462 \tTraining Loss: 0.517210 \tR2: 0.416407\n",
      "Epoch: 1463 \tTraining Loss: 0.527188 \tR2: 0.416407\n",
      "Epoch: 1464 \tTraining Loss: 0.529675 \tR2: 0.416407\n",
      "Epoch: 1465 \tTraining Loss: 0.522339 \tR2: 0.416407\n",
      "Epoch: 1466 \tTraining Loss: 0.509489 \tR2: 0.416407\n",
      "Epoch: 1467 \tTraining Loss: 0.501224 \tR2: 0.416407\n",
      "Epoch: 1468 \tTraining Loss: 0.539190 \tR2: 0.416407\n",
      "Epoch: 1469 \tTraining Loss: 0.539530 \tR2: 0.416407\n",
      "Epoch: 1470 \tTraining Loss: 0.554827 \tR2: 0.416407\n",
      "Epoch: 1471 \tTraining Loss: 0.522223 \tR2: 0.416407\n",
      "Epoch: 1472 \tTraining Loss: 0.524607 \tR2: 0.416407\n",
      "Epoch: 1473 \tTraining Loss: 0.535347 \tR2: 0.416407\n",
      "Epoch: 1474 \tTraining Loss: 0.528996 \tR2: 0.416407\n",
      "Epoch: 1475 \tTraining Loss: 0.530381 \tR2: 0.416407\n",
      "Epoch: 1476 \tTraining Loss: 0.530016 \tR2: 0.416407\n",
      "Epoch: 1477 \tTraining Loss: 0.527185 \tR2: 0.416407\n",
      "Epoch: 1478 \tTraining Loss: 0.539877 \tR2: 0.416407\n",
      "Epoch: 1479 \tTraining Loss: 0.514628 \tR2: 0.416407\n",
      "Epoch: 1480 \tTraining Loss: 0.534203 \tR2: 0.416407\n",
      "Epoch: 1481 \tTraining Loss: 0.528213 \tR2: 0.416407\n",
      "Epoch: 1482 \tTraining Loss: 0.520108 \tR2: 0.416407\n",
      "Epoch: 1483 \tTraining Loss: 0.534474 \tR2: 0.416407\n",
      "Epoch: 1484 \tTraining Loss: 0.541473 \tR2: 0.416407\n",
      "Epoch: 1485 \tTraining Loss: 0.526932 \tR2: 0.416407\n",
      "Epoch: 1486 \tTraining Loss: 0.523526 \tR2: 0.416407\n",
      "Epoch: 1487 \tTraining Loss: 0.527465 \tR2: 0.416407\n",
      "Epoch: 1488 \tTraining Loss: 0.526300 \tR2: 0.416407\n",
      "Epoch: 1489 \tTraining Loss: 0.523132 \tR2: 0.416407\n",
      "Epoch: 1490 \tTraining Loss: 0.530759 \tR2: 0.416407\n",
      "Epoch: 1491 \tTraining Loss: 0.533306 \tR2: 0.416407\n",
      "Epoch: 1492 \tTraining Loss: 0.540819 \tR2: 0.416407\n",
      "Epoch: 1493 \tTraining Loss: 0.536625 \tR2: 0.416407\n",
      "Epoch: 1494 \tTraining Loss: 0.512218 \tR2: 0.416407\n",
      "Epoch: 1495 \tTraining Loss: 0.513552 \tR2: 0.416407\n",
      "Epoch: 1496 \tTraining Loss: 0.535161 \tR2: 0.416407\n",
      "Epoch: 1497 \tTraining Loss: 0.526919 \tR2: 0.416407\n",
      "Epoch: 1498 \tTraining Loss: 0.539748 \tR2: 0.416407\n",
      "Epoch: 1499 \tTraining Loss: 0.499815 \tR2: 0.416407\n",
      "Epoch: 1500 \tTraining Loss: 0.522629 \tR2: 0.597955\n",
      "Epoch: 1501 \tTraining Loss: 0.524219 \tR2: 0.597955\n",
      "Epoch: 1502 \tTraining Loss: 0.531039 \tR2: 0.597955\n",
      "Epoch: 1503 \tTraining Loss: 0.518537 \tR2: 0.597955\n",
      "Epoch: 1504 \tTraining Loss: 0.530529 \tR2: 0.597955\n",
      "Epoch: 1505 \tTraining Loss: 0.542420 \tR2: 0.597955\n",
      "Epoch: 1506 \tTraining Loss: 0.521760 \tR2: 0.597955\n",
      "Epoch: 1507 \tTraining Loss: 0.509567 \tR2: 0.597955\n",
      "Epoch: 1508 \tTraining Loss: 0.522043 \tR2: 0.597955\n",
      "Epoch: 1509 \tTraining Loss: 0.513422 \tR2: 0.597955\n",
      "Epoch: 1510 \tTraining Loss: 0.524262 \tR2: 0.597955\n",
      "Epoch: 1511 \tTraining Loss: 0.518731 \tR2: 0.597955\n",
      "Epoch: 1512 \tTraining Loss: 0.519097 \tR2: 0.597955\n",
      "Epoch: 1513 \tTraining Loss: 0.508902 \tR2: 0.597955\n",
      "Epoch: 1514 \tTraining Loss: 0.530663 \tR2: 0.597955\n",
      "Epoch: 1515 \tTraining Loss: 0.529359 \tR2: 0.597955\n",
      "Epoch: 1516 \tTraining Loss: 0.539275 \tR2: 0.597955\n",
      "Epoch: 1517 \tTraining Loss: 0.522769 \tR2: 0.597955\n",
      "Epoch: 1518 \tTraining Loss: 0.544677 \tR2: 0.597955\n",
      "Epoch: 1519 \tTraining Loss: 0.521309 \tR2: 0.597955\n",
      "Epoch: 1520 \tTraining Loss: 0.535015 \tR2: 0.597955\n",
      "Epoch: 1521 \tTraining Loss: 0.518785 \tR2: 0.597955\n",
      "Epoch: 1522 \tTraining Loss: 0.522403 \tR2: 0.597955\n",
      "Epoch: 1523 \tTraining Loss: 0.533574 \tR2: 0.597955\n",
      "Epoch: 1524 \tTraining Loss: 0.570658 \tR2: 0.597955\n",
      "Epoch: 1525 \tTraining Loss: 0.514199 \tR2: 0.597955\n",
      "Epoch: 1526 \tTraining Loss: 0.539245 \tR2: 0.597955\n",
      "Epoch: 1527 \tTraining Loss: 0.514652 \tR2: 0.597955\n",
      "Epoch: 1528 \tTraining Loss: 0.526186 \tR2: 0.597955\n",
      "Epoch: 1529 \tTraining Loss: 0.522216 \tR2: 0.597955\n",
      "Epoch: 1530 \tTraining Loss: 0.536481 \tR2: 0.597955\n",
      "Epoch: 1531 \tTraining Loss: 0.530281 \tR2: 0.597955\n",
      "Epoch: 1532 \tTraining Loss: 0.529534 \tR2: 0.597955\n",
      "Epoch: 1533 \tTraining Loss: 0.517285 \tR2: 0.597955\n",
      "Epoch: 1534 \tTraining Loss: 0.509271 \tR2: 0.597955\n",
      "Epoch: 1535 \tTraining Loss: 0.519378 \tR2: 0.597955\n",
      "Epoch: 1536 \tTraining Loss: 0.535756 \tR2: 0.597955\n",
      "Epoch: 1537 \tTraining Loss: 0.528962 \tR2: 0.597955\n",
      "Epoch: 1538 \tTraining Loss: 0.521549 \tR2: 0.597955\n",
      "Epoch: 1539 \tTraining Loss: 0.520776 \tR2: 0.597955\n",
      "Epoch: 1540 \tTraining Loss: 0.522132 \tR2: 0.597955\n",
      "Epoch: 1541 \tTraining Loss: 0.526196 \tR2: 0.597955\n",
      "Epoch: 1542 \tTraining Loss: 0.527520 \tR2: 0.597955\n",
      "Epoch: 1543 \tTraining Loss: 0.540820 \tR2: 0.597955\n",
      "Epoch: 1544 \tTraining Loss: 0.527511 \tR2: 0.597955\n",
      "Epoch: 1545 \tTraining Loss: 0.532641 \tR2: 0.597955\n",
      "Epoch: 1546 \tTraining Loss: 0.525221 \tR2: 0.597955\n",
      "Epoch: 1547 \tTraining Loss: 0.525918 \tR2: 0.597955\n",
      "Epoch: 1548 \tTraining Loss: 0.533247 \tR2: 0.597955\n",
      "Epoch: 1549 \tTraining Loss: 0.533749 \tR2: 0.597955\n",
      "Epoch: 1550 \tTraining Loss: 0.529268 \tR2: 0.597955\n",
      "Epoch: 1551 \tTraining Loss: 0.530885 \tR2: 0.597955\n",
      "Epoch: 1552 \tTraining Loss: 0.531316 \tR2: 0.597955\n",
      "Epoch: 1553 \tTraining Loss: 0.526757 \tR2: 0.597955\n",
      "Epoch: 1554 \tTraining Loss: 0.525453 \tR2: 0.597955\n",
      "Epoch: 1555 \tTraining Loss: 0.530069 \tR2: 0.597955\n",
      "Epoch: 1556 \tTraining Loss: 0.523045 \tR2: 0.597955\n",
      "Epoch: 1557 \tTraining Loss: 0.507049 \tR2: 0.597955\n",
      "Epoch: 1558 \tTraining Loss: 0.528623 \tR2: 0.597955\n",
      "Epoch: 1559 \tTraining Loss: 0.533095 \tR2: 0.597955\n",
      "Epoch: 1560 \tTraining Loss: 0.534452 \tR2: 0.597955\n",
      "Epoch: 1561 \tTraining Loss: 0.526010 \tR2: 0.597955\n",
      "Epoch: 1562 \tTraining Loss: 0.522534 \tR2: 0.597955\n",
      "Epoch: 1563 \tTraining Loss: 0.521607 \tR2: 0.597955\n",
      "Epoch: 1564 \tTraining Loss: 0.525359 \tR2: 0.597955\n",
      "Epoch: 1565 \tTraining Loss: 0.528906 \tR2: 0.597955\n",
      "Epoch: 1566 \tTraining Loss: 0.530621 \tR2: 0.597955\n",
      "Epoch: 1567 \tTraining Loss: 0.520705 \tR2: 0.597955\n",
      "Epoch: 1568 \tTraining Loss: 0.513271 \tR2: 0.597955\n",
      "Epoch: 1569 \tTraining Loss: 0.536971 \tR2: 0.597955\n",
      "Epoch: 1570 \tTraining Loss: 0.528400 \tR2: 0.597955\n",
      "Epoch: 1571 \tTraining Loss: 0.547641 \tR2: 0.597955\n",
      "Epoch: 1572 \tTraining Loss: 0.531522 \tR2: 0.597955\n",
      "Epoch: 1573 \tTraining Loss: 0.534969 \tR2: 0.597955\n",
      "Epoch: 1574 \tTraining Loss: 0.520331 \tR2: 0.597955\n",
      "Epoch: 1575 \tTraining Loss: 0.525881 \tR2: 0.597955\n",
      "Epoch: 1576 \tTraining Loss: 0.537963 \tR2: 0.597955\n",
      "Epoch: 1577 \tTraining Loss: 0.500806 \tR2: 0.597955\n",
      "Epoch: 1578 \tTraining Loss: 0.504629 \tR2: 0.597955\n",
      "Epoch: 1579 \tTraining Loss: 0.523445 \tR2: 0.597955\n",
      "Epoch: 1580 \tTraining Loss: 0.526996 \tR2: 0.597955\n",
      "Epoch: 1581 \tTraining Loss: 0.540406 \tR2: 0.597955\n",
      "Epoch: 1582 \tTraining Loss: 0.515441 \tR2: 0.597955\n",
      "Epoch: 1583 \tTraining Loss: 0.522901 \tR2: 0.597955\n",
      "Epoch: 1584 \tTraining Loss: 0.537520 \tR2: 0.597955\n",
      "Epoch: 1585 \tTraining Loss: 0.503697 \tR2: 0.597955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SRTP2022\\testing\\GNNWR.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m200000\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32md:\\SRTP2022\\testing\\GNNWR.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m out(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SRTP2022/testing/GNNWR.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m a \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\Miniconda\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200000+1):\n",
    "    train(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "861f9c34f7302a1aedb62edfc1533c524ce2793735e6b405602ea89eb9cb2484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
