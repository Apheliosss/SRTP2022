{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MMnlQkeZD6eZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import  DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import r2_score\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "class MYDataset(Dataset):\n",
        "    def __init__(self, df, varNum, gpu):\n",
        "        self.df = df\n",
        "        self.gpu = gpu\n",
        "        self.images = df.iloc[:,varNum+1:].values\n",
        "        self.coef = df.iloc[:,1:varNum+1].values\n",
        "        self.labels = df.iloc[:, 0].values\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        coef = self.coef[idx]\n",
        "        \n",
        "        image = torch.tensor(image, dtype=torch.float)\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "        coef = torch.tensor(coef, dtype=torch.float)\n",
        "\n",
        "        if self.gpu == True:\n",
        "            image, coef, label = image.cuda(), coef.cuda(), label.cuda()\n",
        "\n",
        "        return image, coef, label\n",
        "\n",
        "class SWNN(nn.Module):\n",
        "    def __init__(self, insize, outsize):\n",
        "        super(SWNN, self).__init__()\n",
        "        self.insize = insize\n",
        "        self.outsize = outsize\n",
        "        self.thissize = 0\n",
        "        self.lastsize = 512\n",
        "        i = 2\n",
        "\n",
        "        self.fc = nn.Sequential()\n",
        "        self.fc.add_module(\"full\"+str(1), nn.Linear(self.insize, 512))\n",
        "\n",
        "\n",
        "        while math.pow(2, int(math.log2(self.lastsize))) >= max(128, outsize + 1):\n",
        "            if i == 1:\n",
        "                self.thissize = int(math.pow(2, int(math.log2(self.lastsize))))\n",
        "            else:\n",
        "                self.thissize = int(math.pow(2, int(math.log2(self.lastsize)) - 1))\n",
        "            \n",
        "            self.fc.add_module(\"full\"+str(i), nn.Linear(self.lastsize, self.thissize))\n",
        "            self.fc.add_module(\"batc\"+str(i), nn.BatchNorm1d(self.thissize))\n",
        "            self.fc.add_module(\"acti\"+str(i), nn.PReLU(init=0.4))\n",
        "            self.fc.add_module(\"drop\"+str(i), nn.Dropout(0.2))\n",
        "\n",
        "            self.lastsize = self.thissize\n",
        "            i = i + 1\n",
        "\n",
        "        self.fc.add_module(\"full\"+str(i), nn.Linear(self.lastsize, self.outsize))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def compute_distances(P, C):\n",
        "    A = (P**2).sum(axis=1, keepdims=True)\n",
        "    B = (C**2).sum(axis=1, keepdims=True).T\n",
        " \n",
        "    return np.sqrt(A + B - 2* np.dot(P, C.T))\n",
        "\n",
        "def process_df(my_set, varName):\n",
        "    temp_df = pd.DataFrame()\n",
        "\n",
        "    dataset = my_set.reset_index(drop=True)\n",
        "\n",
        "    temp_df['label'] = dataset[varName[0]]\n",
        "    temp_df['beta'] = np.ones(dataset.shape[0])\n",
        "    temp_df[varName[1:4]] = dataset[varName[1:4]]\n",
        "\n",
        "    cor_df = pd.DataFrame()\n",
        "\n",
        "    dataset['lon'][dataset['lon'] < 0] = dataset['lon'][dataset['lon'] < 0].copy() + 360.0\n",
        "    cor_df['xcor'] = dataset['lon']\n",
        "    cor_df['ycor'] = dataset['lat']\n",
        "\n",
        "    sample_pt = np.array([[110.0, 0.0], [290.0,0.0], [110.0, 70.0], [290.0, 70.0]])\n",
        "\n",
        "    cor_li = cor_df.to_numpy()\n",
        "    dis_li = compute_distances(cor_li, sample_pt)\n",
        "    dis_df = pd.DataFrame(dis_li)\n",
        "    temp_df = temp_df.join(dis_df)\n",
        "    temp_df['year'] = dataset['year'].astype('float') - 2008\n",
        "    return temp_df\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    global r2\n",
        "    global out\n",
        "    for data, coef, label in train_loader:\n",
        "        data, coef, label = data.cuda(), coef.cuda(), label.cuda()\n",
        "        data, label = data.view(data.shape[0], -1), label.view(data.shape[0], -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        output = output.mul(coef)\n",
        "        output = out(output)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    print('\\r Epoch: {} \\tTraining Loss:   {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "def val(epoch):\n",
        "    model.eval()\n",
        "    global out\n",
        "    global r2\n",
        "    global best_loss\n",
        "    global last_min\n",
        "    val_loss = 0\n",
        "\n",
        "    label_li = np.array([])\n",
        "    out_li = np.array([])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, coef, label in test_loader:\n",
        "            data, coef, label = data.cuda(), coef.cuda(), label.cuda()\n",
        "            data,label = data.view(data.shape[0], -1), label.view(data.shape[0], -1)\n",
        "\n",
        "            output = model(data).mul(coef)\n",
        "            output = out(output)\n",
        "\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            a = output.view(-1).cpu().detach().numpy()\n",
        "            b = label.view(-1).cpu().numpy()\n",
        "            out_li = np.append(out_li, a)\n",
        "            label_li = np.append(label_li, b)\n",
        "            \n",
        "\n",
        "            val_loss += loss.item()*data.size(0)\n",
        "        val_loss = val_loss/len(test_loader.dataset)\n",
        "        r2 = r2_score(label_li, out_li)\n",
        "        if r2 > best_loss:\n",
        "            best_loss = r2\n",
        "            last_min = 0\n",
        "            torch.save(model, \"model.pkl\")\n",
        "        else:\n",
        "            last_min = last_min + 1\n",
        "\n",
        "        label_li = np.array(label_li).reshape(-1)\n",
        "        out_li = np.array(out_li).reshape(-1)\n",
        "        \n",
        "        print('\\r Epoch: {} \\tValidation Loss: {:.6f} \\tR2: {:.6f}'.format(epoch, val_loss, best_loss))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BhUTWXfPGpdu"
      },
      "outputs": [],
      "source": [
        "\n",
        "is_gpu = None\n",
        "if torch.cuda.is_available() == True:\n",
        "    is_gpu = True\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "else:\n",
        "    is_gpu = False\n",
        "\n",
        "\n",
        "varName = ['fCO2', 'Chl', 'Temp', 'Salt']\n",
        "varNum = len(varName)\n",
        "batch_size = 256\n",
        "\n",
        "dataset = pd.read_csv(\"CO2_data5.csv\", encoding=\"utf-8\")\n",
        "dataset = dataset.dropna()\n",
        "dataset = dataset[dataset.index % 4 == 0]\n",
        "\n",
        "df0 = dataset['date'].str.split(\"/\",expand = True)\n",
        "df0.columns = ['year', 'month', 'date']\n",
        "\n",
        "dataset['month'] = df0['month']\n",
        "dataset = dataset[dataset.month == '7']\n",
        "\n",
        "train_li = random.sample([i for i in range(0, dataset.shape[0])], int(0.8 * dataset.shape[0]))\n",
        "train_li.sort()\n",
        "\n",
        "test_li = list(set([i for i in range(0, dataset.shape[0])]) - set(train_li))\n",
        "test_li.sort()\n",
        "\n",
        "train_set = dataset.iloc[train_li, :]\n",
        "test_set  = dataset.iloc[test_li,  :]\n",
        "\n",
        "mean_li = []\n",
        "std_li = []\n",
        "\n",
        "for i in range(0, varNum, 1):\n",
        "    mean_li.append(train_set[varName[i]].mean())\n",
        "    std_li.append(train_set[varName[i]].std())\n",
        "\n",
        "train_set = train_set.copy()\n",
        "test_set = test_set.copy()\n",
        "\n",
        "for i in range(0, varNum, 1):\n",
        "    train_set.loc[:, varName[i]] = (train_set[varName[i]].copy() - mean_li[i] + 1.0) / std_li[i]\n",
        "    test_set.loc[:, varName[i]] = (test_set[varName[i]].copy() - mean_li[i] + 1.0) / std_li[i]\n",
        "\n",
        "train_data = MYDataset(process_df(my_set=train_set, varName=varName), varNum, gpu=is_gpu)\n",
        "test_data = MYDataset(process_df(my_set=test_set, varName=varName), varNum, gpu=is_gpu)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uUc-h12nGzYs",
        "outputId": "8132b819-adac-4123-f321-8c334a51e0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            " Epoch: 9846 \tTraining Loss:   0.285753\n",
            " Epoch: 9846 \tValidation Loss: 0.373815 \tR2: 0.587078\n",
            " Epoch: 9847 \tTraining Loss:   0.275197\n",
            " Epoch: 9847 \tValidation Loss: 0.340299 \tR2: 0.587078\n",
            " Epoch: 9848 \tTraining Loss:   0.284229\n",
            " Epoch: 9848 \tValidation Loss: 0.315391 \tR2: 0.587078\n",
            " Epoch: 9849 \tTraining Loss:   0.272013\n",
            " Epoch: 9849 \tValidation Loss: 0.341631 \tR2: 0.587078\n",
            " Epoch: 9850 \tTraining Loss:   0.291976\n",
            " Epoch: 9850 \tValidation Loss: 0.341471 \tR2: 0.587078\n",
            " Epoch: 9851 \tTraining Loss:   0.274721\n",
            " Epoch: 9851 \tValidation Loss: 0.303818 \tR2: 0.587078\n",
            " Epoch: 9852 \tTraining Loss:   0.273545\n",
            " Epoch: 9852 \tValidation Loss: 0.340535 \tR2: 0.587078\n",
            " Epoch: 9853 \tTraining Loss:   0.289428\n",
            " Epoch: 9853 \tValidation Loss: 0.311112 \tR2: 0.587078\n",
            " Epoch: 9854 \tTraining Loss:   0.280706\n",
            " Epoch: 9854 \tValidation Loss: 0.307151 \tR2: 0.587078\n",
            " Epoch: 9855 \tTraining Loss:   0.283658\n",
            " Epoch: 9855 \tValidation Loss: 0.324147 \tR2: 0.587078\n",
            " Epoch: 9856 \tTraining Loss:   0.269684\n",
            " Epoch: 9856 \tValidation Loss: 0.357694 \tR2: 0.587078\n",
            " Epoch: 9857 \tTraining Loss:   0.275878\n",
            " Epoch: 9857 \tValidation Loss: 0.318962 \tR2: 0.587078\n",
            " Epoch: 9858 \tTraining Loss:   0.277283\n",
            " Epoch: 9858 \tValidation Loss: 0.309983 \tR2: 0.587078\n",
            " Epoch: 9859 \tTraining Loss:   0.286665\n",
            " Epoch: 9859 \tValidation Loss: 0.318727 \tR2: 0.587078\n",
            " Epoch: 9860 \tTraining Loss:   0.272807\n",
            " Epoch: 9860 \tValidation Loss: 0.338037 \tR2: 0.587078\n",
            " Epoch: 9861 \tTraining Loss:   0.286375\n",
            " Epoch: 9861 \tValidation Loss: 0.338817 \tR2: 0.587078\n",
            " Epoch: 9862 \tTraining Loss:   0.303586\n",
            " Epoch: 9862 \tValidation Loss: 0.337751 \tR2: 0.587078\n",
            " Epoch: 9863 \tTraining Loss:   0.279937\n",
            " Epoch: 9863 \tValidation Loss: 0.350745 \tR2: 0.587078\n",
            " Epoch: 9864 \tTraining Loss:   0.286489\n",
            " Epoch: 9864 \tValidation Loss: 0.350303 \tR2: 0.587078\n",
            " Epoch: 9865 \tTraining Loss:   0.278986\n",
            " Epoch: 9865 \tValidation Loss: 0.342528 \tR2: 0.587078\n",
            " Epoch: 9866 \tTraining Loss:   0.277954\n",
            " Epoch: 9866 \tValidation Loss: 0.334892 \tR2: 0.587078\n",
            " Epoch: 9867 \tTraining Loss:   0.289527\n",
            " Epoch: 9867 \tValidation Loss: 0.408712 \tR2: 0.587078\n",
            " Epoch: 9868 \tTraining Loss:   0.297351\n",
            " Epoch: 9868 \tValidation Loss: 0.359002 \tR2: 0.587078\n",
            " Epoch: 9869 \tTraining Loss:   0.284254\n",
            " Epoch: 9869 \tValidation Loss: 0.336246 \tR2: 0.587078\n",
            " Epoch: 9870 \tTraining Loss:   0.280115\n",
            " Epoch: 9870 \tValidation Loss: 0.370854 \tR2: 0.587078\n",
            " Epoch: 9871 \tTraining Loss:   0.287375\n",
            " Epoch: 9871 \tValidation Loss: 0.323369 \tR2: 0.587078\n",
            " Epoch: 9872 \tTraining Loss:   0.279818\n",
            " Epoch: 9872 \tValidation Loss: 0.311187 \tR2: 0.587078\n",
            " Epoch: 9873 \tTraining Loss:   0.287852\n",
            " Epoch: 9873 \tValidation Loss: 0.397408 \tR2: 0.587078\n",
            " Epoch: 9874 \tTraining Loss:   0.297126\n",
            " Epoch: 9874 \tValidation Loss: 0.369385 \tR2: 0.587078\n",
            " Epoch: 9875 \tTraining Loss:   0.283948\n",
            " Epoch: 9875 \tValidation Loss: 0.319370 \tR2: 0.587078\n",
            " Epoch: 9876 \tTraining Loss:   0.277542\n",
            " Epoch: 9876 \tValidation Loss: 0.342050 \tR2: 0.587078\n",
            " Epoch: 9877 \tTraining Loss:   0.284059\n",
            " Epoch: 9877 \tValidation Loss: 0.325768 \tR2: 0.587078\n",
            " Epoch: 9878 \tTraining Loss:   0.271618\n",
            " Epoch: 9878 \tValidation Loss: 0.378134 \tR2: 0.587078\n",
            " Epoch: 9879 \tTraining Loss:   0.284627\n",
            " Epoch: 9879 \tValidation Loss: 0.315110 \tR2: 0.587078\n",
            " Epoch: 9880 \tTraining Loss:   0.289915\n",
            " Epoch: 9880 \tValidation Loss: 0.339653 \tR2: 0.587078\n",
            " Epoch: 9881 \tTraining Loss:   0.268018\n",
            " Epoch: 9881 \tValidation Loss: 0.332299 \tR2: 0.587078\n",
            " Epoch: 9882 \tTraining Loss:   0.275202\n",
            " Epoch: 9882 \tValidation Loss: 0.316145 \tR2: 0.587078\n",
            " Epoch: 9883 \tTraining Loss:   0.273757\n",
            " Epoch: 9883 \tValidation Loss: 0.306316 \tR2: 0.587078\n",
            " Epoch: 9884 \tTraining Loss:   0.272999\n",
            " Epoch: 9884 \tValidation Loss: 0.313510 \tR2: 0.587078\n",
            " Epoch: 9885 \tTraining Loss:   0.282326\n",
            " Epoch: 9885 \tValidation Loss: 0.389498 \tR2: 0.587078\n",
            " Epoch: 9886 \tTraining Loss:   0.274037\n",
            " Epoch: 9886 \tValidation Loss: 0.317219 \tR2: 0.587078\n",
            " Epoch: 9887 \tTraining Loss:   0.295475\n",
            " Epoch: 9887 \tValidation Loss: 0.321209 \tR2: 0.587078\n",
            " Epoch: 9888 \tTraining Loss:   0.278643\n",
            " Epoch: 9888 \tValidation Loss: 0.483010 \tR2: 0.587078\n",
            " Epoch: 9889 \tTraining Loss:   0.278365\n",
            " Epoch: 9889 \tValidation Loss: 0.378065 \tR2: 0.587078\n",
            " Epoch: 9890 \tTraining Loss:   0.279132\n",
            " Epoch: 9890 \tValidation Loss: 0.304940 \tR2: 0.587078\n",
            " Epoch: 9891 \tTraining Loss:   0.284333\n",
            " Epoch: 9891 \tValidation Loss: 0.303581 \tR2: 0.587078\n",
            " Epoch: 9892 \tTraining Loss:   0.281673\n",
            " Epoch: 9892 \tValidation Loss: 0.318251 \tR2: 0.587078\n",
            " Epoch: 9893 \tTraining Loss:   0.293380\n",
            " Epoch: 9893 \tValidation Loss: 0.291352 \tR2: 0.587078\n",
            " Epoch: 9894 \tTraining Loss:   0.289107\n",
            " Epoch: 9894 \tValidation Loss: 0.363763 \tR2: 0.587078\n",
            " Epoch: 9895 \tTraining Loss:   0.287015\n",
            " Epoch: 9895 \tValidation Loss: 0.326933 \tR2: 0.587078\n",
            " Epoch: 9896 \tTraining Loss:   0.286763\n",
            " Epoch: 9896 \tValidation Loss: 0.298796 \tR2: 0.587078\n",
            " Epoch: 9897 \tTraining Loss:   0.279271\n",
            " Epoch: 9897 \tValidation Loss: 0.338833 \tR2: 0.587078\n",
            " Epoch: 9898 \tTraining Loss:   0.285042\n",
            " Epoch: 9898 \tValidation Loss: 0.314024 \tR2: 0.587078\n",
            " Epoch: 9899 \tTraining Loss:   0.295274\n",
            " Epoch: 9899 \tValidation Loss: 0.289635 \tR2: 0.587078\n",
            " Epoch: 9900 \tTraining Loss:   0.281328\n",
            " Epoch: 9900 \tValidation Loss: 0.354202 \tR2: 0.352704\n",
            " Epoch: 9901 \tTraining Loss:   0.289404\n",
            " Epoch: 9901 \tValidation Loss: 0.408224 \tR2: 0.352704\n",
            " Epoch: 9902 \tTraining Loss:   0.278568\n",
            " Epoch: 9902 \tValidation Loss: 0.359483 \tR2: 0.352704\n",
            " Epoch: 9903 \tTraining Loss:   0.285945\n",
            " Epoch: 9903 \tValidation Loss: 0.325725 \tR2: 0.352704\n",
            " Epoch: 9904 \tTraining Loss:   0.288222\n",
            " Epoch: 9904 \tValidation Loss: 0.400596 \tR2: 0.352704\n",
            " Epoch: 9905 \tTraining Loss:   0.275857\n",
            " Epoch: 9905 \tValidation Loss: 0.315891 \tR2: 0.352704\n",
            " Epoch: 9906 \tTraining Loss:   0.275852\n",
            " Epoch: 9906 \tValidation Loss: 0.372554 \tR2: 0.352704\n",
            " Epoch: 9907 \tTraining Loss:   0.290594\n",
            " Epoch: 9907 \tValidation Loss: 0.308981 \tR2: 0.352704\n",
            " Epoch: 9908 \tTraining Loss:   0.289222\n",
            " Epoch: 9908 \tValidation Loss: 0.321640 \tR2: 0.352704\n",
            " Epoch: 9909 \tTraining Loss:   0.281185\n",
            " Epoch: 9909 \tValidation Loss: 0.285378 \tR2: 0.352704\n",
            " Epoch: 9910 \tTraining Loss:   0.281629\n",
            " Epoch: 9910 \tValidation Loss: 0.473822 \tR2: 0.352704\n",
            " Epoch: 9911 \tTraining Loss:   0.299140\n",
            " Epoch: 9911 \tValidation Loss: 0.328339 \tR2: 0.352704\n",
            " Epoch: 9912 \tTraining Loss:   0.283313\n",
            " Epoch: 9912 \tValidation Loss: 0.314252 \tR2: 0.352704\n",
            " Epoch: 9913 \tTraining Loss:   0.276266\n",
            " Epoch: 9913 \tValidation Loss: 0.319857 \tR2: 0.352704\n",
            " Epoch: 9914 \tTraining Loss:   0.276037\n",
            " Epoch: 9914 \tValidation Loss: 0.309790 \tR2: 0.352704\n",
            " Epoch: 9915 \tTraining Loss:   0.277683\n",
            " Epoch: 9915 \tValidation Loss: 0.331609 \tR2: 0.352704\n",
            " Epoch: 9916 \tTraining Loss:   0.288800\n",
            " Epoch: 9916 \tValidation Loss: 0.348465 \tR2: 0.352704\n",
            " Epoch: 9917 \tTraining Loss:   0.295834\n",
            " Epoch: 9917 \tValidation Loss: 0.346914 \tR2: 0.352704\n",
            " Epoch: 9918 \tTraining Loss:   0.282214\n",
            " Epoch: 9918 \tValidation Loss: 0.314511 \tR2: 0.352704\n",
            " Epoch: 9919 \tTraining Loss:   0.279882\n",
            " Epoch: 9919 \tValidation Loss: 0.370000 \tR2: 0.352704\n",
            " Epoch: 9920 \tTraining Loss:   0.282665\n",
            " Epoch: 9920 \tValidation Loss: 0.356562 \tR2: 0.352704\n",
            " Epoch: 9921 \tTraining Loss:   0.289462\n",
            " Epoch: 9921 \tValidation Loss: 0.360676 \tR2: 0.352704\n",
            " Epoch: 9922 \tTraining Loss:   0.285663\n",
            " Epoch: 9922 \tValidation Loss: 0.331787 \tR2: 0.352704\n",
            " Epoch: 9923 \tTraining Loss:   0.282975\n",
            " Epoch: 9923 \tValidation Loss: 0.326934 \tR2: 0.352704\n",
            " Epoch: 9924 \tTraining Loss:   0.274023\n",
            " Epoch: 9924 \tValidation Loss: 0.294938 \tR2: 0.352704\n",
            " Epoch: 9925 \tTraining Loss:   0.286112\n",
            " Epoch: 9925 \tValidation Loss: 0.300631 \tR2: 0.352704\n",
            " Epoch: 9926 \tTraining Loss:   0.271578\n",
            " Epoch: 9926 \tValidation Loss: 0.325028 \tR2: 0.352704\n",
            " Epoch: 9927 \tTraining Loss:   0.269442\n",
            " Epoch: 9927 \tValidation Loss: 0.298614 \tR2: 0.352704\n",
            " Epoch: 9928 \tTraining Loss:   0.269122\n",
            " Epoch: 9928 \tValidation Loss: 0.321134 \tR2: 0.352704\n",
            " Epoch: 9929 \tTraining Loss:   0.284079\n",
            " Epoch: 9929 \tValidation Loss: 0.322894 \tR2: 0.352704\n",
            " Epoch: 9930 \tTraining Loss:   0.298596\n",
            " Epoch: 9930 \tValidation Loss: 0.340570 \tR2: 0.352704\n",
            " Epoch: 9931 \tTraining Loss:   0.279508\n",
            " Epoch: 9931 \tValidation Loss: 0.313687 \tR2: 0.352704\n",
            " Epoch: 9932 \tTraining Loss:   0.282147\n",
            " Epoch: 9932 \tValidation Loss: 0.305329 \tR2: 0.352704\n",
            " Epoch: 9933 \tTraining Loss:   0.280060\n",
            " Epoch: 9933 \tValidation Loss: 0.321293 \tR2: 0.352704\n",
            " Epoch: 9934 \tTraining Loss:   0.268967\n",
            " Epoch: 9934 \tValidation Loss: 0.320179 \tR2: 0.352704\n",
            " Epoch: 9935 \tTraining Loss:   0.277012\n",
            " Epoch: 9935 \tValidation Loss: 0.306101 \tR2: 0.352704\n",
            " Epoch: 9936 \tTraining Loss:   0.279044\n",
            " Epoch: 9936 \tValidation Loss: 0.319635 \tR2: 0.352704\n",
            " Epoch: 9937 \tTraining Loss:   0.272805\n",
            " Epoch: 9937 \tValidation Loss: 0.320733 \tR2: 0.352704\n",
            " Epoch: 9938 \tTraining Loss:   0.269351\n",
            " Epoch: 9938 \tValidation Loss: 0.316507 \tR2: 0.352704\n",
            " Epoch: 9939 \tTraining Loss:   0.285581\n",
            " Epoch: 9939 \tValidation Loss: 0.329558 \tR2: 0.352704\n",
            " Epoch: 9940 \tTraining Loss:   0.276899\n",
            " Epoch: 9940 \tValidation Loss: 0.368316 \tR2: 0.352704\n",
            " Epoch: 9941 \tTraining Loss:   0.285737\n",
            " Epoch: 9941 \tValidation Loss: 0.343047 \tR2: 0.352704\n",
            " Epoch: 9942 \tTraining Loss:   0.278079\n",
            " Epoch: 9942 \tValidation Loss: 0.360764 \tR2: 0.352704\n",
            " Epoch: 9943 \tTraining Loss:   0.278022\n",
            " Epoch: 9943 \tValidation Loss: 0.299701 \tR2: 0.352704\n",
            " Epoch: 9944 \tTraining Loss:   0.275935\n",
            " Epoch: 9944 \tValidation Loss: 0.356339 \tR2: 0.352704\n",
            " Epoch: 9945 \tTraining Loss:   0.286023\n",
            " Epoch: 9945 \tValidation Loss: 0.322143 \tR2: 0.352704\n",
            " Epoch: 9946 \tTraining Loss:   0.275959\n",
            " Epoch: 9946 \tValidation Loss: 0.330180 \tR2: 0.352704\n",
            " Epoch: 9947 \tTraining Loss:   0.296272\n",
            " Epoch: 9947 \tValidation Loss: 0.324058 \tR2: 0.352704\n",
            " Epoch: 9948 \tTraining Loss:   0.281777\n",
            " Epoch: 9948 \tValidation Loss: 0.313163 \tR2: 0.352704\n",
            " Epoch: 9949 \tTraining Loss:   0.278017\n",
            " Epoch: 9949 \tValidation Loss: 0.307471 \tR2: 0.352704\n",
            " Epoch: 9950 \tTraining Loss:   0.280177\n",
            " Epoch: 9950 \tValidation Loss: 0.319592 \tR2: 0.352704\n",
            " Epoch: 9951 \tTraining Loss:   0.287265\n",
            " Epoch: 9951 \tValidation Loss: 0.332417 \tR2: 0.352704\n",
            " Epoch: 9952 \tTraining Loss:   0.285029\n",
            " Epoch: 9952 \tValidation Loss: 0.341221 \tR2: 0.352704\n",
            " Epoch: 9953 \tTraining Loss:   0.279885\n",
            " Epoch: 9953 \tValidation Loss: 0.328865 \tR2: 0.352704\n",
            " Epoch: 9954 \tTraining Loss:   0.278741\n",
            " Epoch: 9954 \tValidation Loss: 0.366450 \tR2: 0.352704\n",
            " Epoch: 9955 \tTraining Loss:   0.296384\n",
            " Epoch: 9955 \tValidation Loss: 0.303004 \tR2: 0.352704\n",
            " Epoch: 9956 \tTraining Loss:   0.292512\n",
            " Epoch: 9956 \tValidation Loss: 0.341652 \tR2: 0.352704\n",
            " Epoch: 9957 \tTraining Loss:   0.281690\n",
            " Epoch: 9957 \tValidation Loss: 0.317274 \tR2: 0.352704\n",
            " Epoch: 9958 \tTraining Loss:   0.273591\n",
            " Epoch: 9958 \tValidation Loss: 0.332622 \tR2: 0.352704\n",
            " Epoch: 9959 \tTraining Loss:   0.273690\n",
            " Epoch: 9959 \tValidation Loss: 0.334497 \tR2: 0.352704\n",
            " Epoch: 9960 \tTraining Loss:   0.276310\n",
            " Epoch: 9960 \tValidation Loss: 0.313974 \tR2: 0.352704\n",
            " Epoch: 9961 \tTraining Loss:   0.289002\n",
            " Epoch: 9961 \tValidation Loss: 0.330040 \tR2: 0.352704\n",
            " Epoch: 9962 \tTraining Loss:   0.285758\n",
            " Epoch: 9962 \tValidation Loss: 0.318459 \tR2: 0.352704\n",
            " Epoch: 9963 \tTraining Loss:   0.277602\n",
            " Epoch: 9963 \tValidation Loss: 0.331848 \tR2: 0.352704\n",
            " Epoch: 9964 \tTraining Loss:   0.285587\n",
            " Epoch: 9964 \tValidation Loss: 0.318741 \tR2: 0.352704\n",
            " Epoch: 9965 \tTraining Loss:   0.281113\n",
            " Epoch: 9965 \tValidation Loss: 0.323402 \tR2: 0.352704\n",
            " Epoch: 9966 \tTraining Loss:   0.286408\n",
            " Epoch: 9966 \tValidation Loss: 0.336738 \tR2: 0.352704\n",
            " Epoch: 9967 \tTraining Loss:   0.285550\n",
            " Epoch: 9967 \tValidation Loss: 0.325257 \tR2: 0.352704\n",
            " Epoch: 9968 \tTraining Loss:   0.282246\n",
            " Epoch: 9968 \tValidation Loss: 0.318744 \tR2: 0.352704\n",
            " Epoch: 9969 \tTraining Loss:   0.281826\n",
            " Epoch: 9969 \tValidation Loss: 0.307117 \tR2: 0.352704\n",
            " Epoch: 9970 \tTraining Loss:   0.288260\n",
            " Epoch: 9970 \tValidation Loss: 0.308421 \tR2: 0.352704\n",
            " Epoch: 9971 \tTraining Loss:   0.284462\n",
            " Epoch: 9971 \tValidation Loss: 0.318481 \tR2: 0.352704\n",
            " Epoch: 9972 \tTraining Loss:   0.291668\n",
            " Epoch: 9972 \tValidation Loss: 0.307634 \tR2: 0.352704\n",
            " Epoch: 9973 \tTraining Loss:   0.276949\n",
            " Epoch: 9973 \tValidation Loss: 0.335831 \tR2: 0.352704\n",
            " Epoch: 9974 \tTraining Loss:   0.289071\n",
            " Epoch: 9974 \tValidation Loss: 0.312658 \tR2: 0.352704\n",
            " Epoch: 9975 \tTraining Loss:   0.275460\n",
            " Epoch: 9975 \tValidation Loss: 0.301263 \tR2: 0.352704\n",
            " Epoch: 9976 \tTraining Loss:   0.291405\n",
            " Epoch: 9976 \tValidation Loss: 0.547739 \tR2: 0.352704\n",
            " Epoch: 9977 \tTraining Loss:   0.289725\n",
            " Epoch: 9977 \tValidation Loss: 0.404939 \tR2: 0.352704\n",
            " Epoch: 9978 \tTraining Loss:   0.276874\n",
            " Epoch: 9978 \tValidation Loss: 0.318797 \tR2: 0.352704\n",
            " Epoch: 9979 \tTraining Loss:   0.286152\n",
            " Epoch: 9979 \tValidation Loss: 0.317544 \tR2: 0.352704\n",
            " Epoch: 9980 \tTraining Loss:   0.271402\n",
            " Epoch: 9980 \tValidation Loss: 0.344685 \tR2: 0.352704\n",
            " Epoch: 9981 \tTraining Loss:   0.284422\n",
            " Epoch: 9981 \tValidation Loss: 0.319482 \tR2: 0.352704\n",
            " Epoch: 9982 \tTraining Loss:   0.277091\n",
            " Epoch: 9982 \tValidation Loss: 0.313573 \tR2: 0.352704\n",
            " Epoch: 9983 \tTraining Loss:   0.282057\n",
            " Epoch: 9983 \tValidation Loss: 0.331857 \tR2: 0.352704\n",
            " Epoch: 9984 \tTraining Loss:   0.287704\n",
            " Epoch: 9984 \tValidation Loss: 0.331107 \tR2: 0.352704\n",
            " Epoch: 9985 \tTraining Loss:   0.283118\n",
            " Epoch: 9985 \tValidation Loss: 0.521578 \tR2: 0.352704\n",
            " Epoch: 9986 \tTraining Loss:   0.288666\n",
            " Epoch: 9986 \tValidation Loss: 0.335197 \tR2: 0.352704\n",
            " Epoch: 9987 \tTraining Loss:   0.270153\n",
            " Epoch: 9987 \tValidation Loss: 0.329039 \tR2: 0.352704\n",
            " Epoch: 9988 \tTraining Loss:   0.282083\n",
            " Epoch: 9988 \tValidation Loss: 0.328194 \tR2: 0.352704\n",
            " Epoch: 9989 \tTraining Loss:   0.287411\n",
            " Epoch: 9989 \tValidation Loss: 0.341917 \tR2: 0.352704\n",
            " Epoch: 9990 \tTraining Loss:   0.274803\n",
            " Epoch: 9990 \tValidation Loss: 0.319433 \tR2: 0.352704\n",
            " Epoch: 9991 \tTraining Loss:   0.280971\n",
            " Epoch: 9991 \tValidation Loss: 0.333906 \tR2: 0.352704\n",
            " Epoch: 9992 \tTraining Loss:   0.279892\n",
            " Epoch: 9992 \tValidation Loss: 0.307921 \tR2: 0.352704\n",
            " Epoch: 9993 \tTraining Loss:   0.291281\n",
            " Epoch: 9993 \tValidation Loss: 0.422817 \tR2: 0.352704\n",
            " Epoch: 9994 \tTraining Loss:   0.267262\n",
            " Epoch: 9994 \tValidation Loss: 0.328429 \tR2: 0.352704\n",
            " Epoch: 9995 \tTraining Loss:   0.274760\n",
            " Epoch: 9995 \tValidation Loss: 0.324096 \tR2: 0.352704\n",
            " Epoch: 9996 \tTraining Loss:   0.276002\n",
            " Epoch: 9996 \tValidation Loss: 0.353493 \tR2: 0.352704\n",
            " Epoch: 9997 \tTraining Loss:   0.284535\n",
            " Epoch: 9997 \tValidation Loss: 0.331900 \tR2: 0.352704\n",
            " Epoch: 9998 \tTraining Loss:   0.275130\n",
            " Epoch: 9998 \tValidation Loss: 0.333188 \tR2: 0.352704\n",
            " Epoch: 9999 \tTraining Loss:   0.291745\n",
            " Epoch: 9999 \tValidation Loss: 0.365822 \tR2: 0.352704\n",
            " Epoch: 10000 \tTraining Loss:   0.277735\n",
            " Epoch: 10000 \tValidation Loss: 0.331700 \tR2: 0.495283\n",
            " Epoch: 10001 \tTraining Loss:   0.290217\n",
            " Epoch: 10001 \tValidation Loss: 0.327576 \tR2: 0.495283\n",
            " Epoch: 10002 \tTraining Loss:   0.274618\n",
            " Epoch: 10002 \tValidation Loss: 0.364193 \tR2: 0.495283\n",
            " Epoch: 10003 \tTraining Loss:   0.280141\n",
            " Epoch: 10003 \tValidation Loss: 0.341539 \tR2: 0.495283\n",
            " Epoch: 10004 \tTraining Loss:   0.275409\n",
            " Epoch: 10004 \tValidation Loss: 0.336808 \tR2: 0.495283\n",
            " Epoch: 10005 \tTraining Loss:   0.267117\n",
            " Epoch: 10005 \tValidation Loss: 0.304958 \tR2: 0.495283\n",
            " Epoch: 10006 \tTraining Loss:   0.290210\n",
            " Epoch: 10006 \tValidation Loss: 0.308070 \tR2: 0.495283\n",
            " Epoch: 10007 \tTraining Loss:   0.277119\n",
            " Epoch: 10007 \tValidation Loss: 0.305441 \tR2: 0.495283\n",
            " Epoch: 10008 \tTraining Loss:   0.283699\n",
            " Epoch: 10008 \tValidation Loss: 0.353363 \tR2: 0.495283\n",
            " Epoch: 10009 \tTraining Loss:   0.267778\n",
            " Epoch: 10009 \tValidation Loss: 0.310497 \tR2: 0.495283\n",
            " Epoch: 10010 \tTraining Loss:   0.270677\n",
            " Epoch: 10010 \tValidation Loss: 0.312484 \tR2: 0.495283\n",
            " Epoch: 10011 \tTraining Loss:   0.273820\n",
            " Epoch: 10011 \tValidation Loss: 0.327446 \tR2: 0.495283\n",
            " Epoch: 10012 \tTraining Loss:   0.296328\n",
            " Epoch: 10012 \tValidation Loss: 0.342707 \tR2: 0.495283\n",
            " Epoch: 10013 \tTraining Loss:   0.296349\n",
            " Epoch: 10013 \tValidation Loss: 0.330162 \tR2: 0.495283\n",
            " Epoch: 10014 \tTraining Loss:   0.272698\n",
            " Epoch: 10014 \tValidation Loss: 0.327442 \tR2: 0.495283\n",
            " Epoch: 10015 \tTraining Loss:   0.271846\n",
            " Epoch: 10015 \tValidation Loss: 0.294632 \tR2: 0.495283\n",
            " Epoch: 10016 \tTraining Loss:   0.277054\n",
            " Epoch: 10016 \tValidation Loss: 0.338757 \tR2: 0.495283\n",
            " Epoch: 10017 \tTraining Loss:   0.294200\n",
            " Epoch: 10017 \tValidation Loss: 0.337259 \tR2: 0.495283\n",
            " Epoch: 10018 \tTraining Loss:   0.305154\n",
            " Epoch: 10018 \tValidation Loss: 0.344374 \tR2: 0.495283\n",
            " Epoch: 10019 \tTraining Loss:   0.279869\n",
            " Epoch: 10019 \tValidation Loss: 0.327985 \tR2: 0.495283\n",
            " Epoch: 10020 \tTraining Loss:   0.276031\n",
            " Epoch: 10020 \tValidation Loss: 0.340813 \tR2: 0.495283\n",
            " Epoch: 10021 \tTraining Loss:   0.290648\n",
            " Epoch: 10021 \tValidation Loss: 0.355247 \tR2: 0.495283\n",
            " Epoch: 10022 \tTraining Loss:   0.271137\n",
            " Epoch: 10022 \tValidation Loss: 0.352260 \tR2: 0.495283\n",
            " Epoch: 10023 \tTraining Loss:   0.290324\n",
            " Epoch: 10023 \tValidation Loss: 0.321578 \tR2: 0.495283\n",
            " Epoch: 10024 \tTraining Loss:   0.283749\n",
            " Epoch: 10024 \tValidation Loss: 0.328183 \tR2: 0.495283\n",
            " Epoch: 10025 \tTraining Loss:   0.272505\n",
            " Epoch: 10025 \tValidation Loss: 0.341001 \tR2: 0.495283\n",
            " Epoch: 10026 \tTraining Loss:   0.278437\n",
            " Epoch: 10026 \tValidation Loss: 0.329926 \tR2: 0.495283\n",
            " Epoch: 10027 \tTraining Loss:   0.275961\n",
            " Epoch: 10027 \tValidation Loss: 0.358572 \tR2: 0.495283\n",
            " Epoch: 10028 \tTraining Loss:   0.293993\n",
            " Epoch: 10028 \tValidation Loss: 0.354770 \tR2: 0.495283\n",
            " Epoch: 10029 \tTraining Loss:   0.281524\n",
            " Epoch: 10029 \tValidation Loss: 0.325782 \tR2: 0.495283\n",
            " Epoch: 10030 \tTraining Loss:   0.272109\n",
            " Epoch: 10030 \tValidation Loss: 0.346105 \tR2: 0.495283\n",
            " Epoch: 10031 \tTraining Loss:   0.286028\n",
            " Epoch: 10031 \tValidation Loss: 0.330083 \tR2: 0.495283\n",
            " Epoch: 10032 \tTraining Loss:   0.275996\n",
            " Epoch: 10032 \tValidation Loss: 0.308844 \tR2: 0.495283\n",
            " Epoch: 10033 \tTraining Loss:   0.290584\n",
            " Epoch: 10033 \tValidation Loss: 0.337587 \tR2: 0.495283\n",
            " Epoch: 10034 \tTraining Loss:   0.280175\n",
            " Epoch: 10034 \tValidation Loss: 0.511460 \tR2: 0.495283\n",
            " Epoch: 10035 \tTraining Loss:   0.283871\n",
            " Epoch: 10035 \tValidation Loss: 0.313005 \tR2: 0.495283\n",
            " Epoch: 10036 \tTraining Loss:   0.287433\n",
            " Epoch: 10036 \tValidation Loss: 0.317619 \tR2: 0.495283\n",
            " Epoch: 10037 \tTraining Loss:   0.285465\n",
            " Epoch: 10037 \tValidation Loss: 0.349431 \tR2: 0.495283\n",
            " Epoch: 10038 \tTraining Loss:   0.295900\n",
            " Epoch: 10038 \tValidation Loss: 0.311708 \tR2: 0.495283\n",
            " Epoch: 10039 \tTraining Loss:   0.280168\n",
            " Epoch: 10039 \tValidation Loss: 0.345345 \tR2: 0.495283\n",
            " Epoch: 10040 \tTraining Loss:   0.274490\n",
            " Epoch: 10040 \tValidation Loss: 0.316306 \tR2: 0.495283\n",
            " Epoch: 10041 \tTraining Loss:   0.272642\n",
            " Epoch: 10041 \tValidation Loss: 0.365836 \tR2: 0.495283\n",
            " Epoch: 10042 \tTraining Loss:   0.271873\n",
            " Epoch: 10042 \tValidation Loss: 0.324853 \tR2: 0.495283\n",
            " Epoch: 10043 \tTraining Loss:   0.289888\n",
            " Epoch: 10043 \tValidation Loss: 0.340194 \tR2: 0.495283\n",
            " Epoch: 10044 \tTraining Loss:   0.277903\n",
            " Epoch: 10044 \tValidation Loss: 0.341960 \tR2: 0.495283\n",
            " Epoch: 10045 \tTraining Loss:   0.275035\n",
            " Epoch: 10045 \tValidation Loss: 0.327783 \tR2: 0.495283\n",
            " Epoch: 10046 \tTraining Loss:   0.278593\n",
            " Epoch: 10046 \tValidation Loss: 0.365710 \tR2: 0.495283\n",
            " Epoch: 10047 \tTraining Loss:   0.284668\n",
            " Epoch: 10047 \tValidation Loss: 0.366564 \tR2: 0.495283\n",
            " Epoch: 10048 \tTraining Loss:   0.285348\n",
            " Epoch: 10048 \tValidation Loss: 0.408137 \tR2: 0.495283\n",
            " Epoch: 10049 \tTraining Loss:   0.272316\n",
            " Epoch: 10049 \tValidation Loss: 0.311830 \tR2: 0.495283\n",
            " Epoch: 10050 \tTraining Loss:   0.289257\n",
            " Epoch: 10050 \tValidation Loss: 0.325311 \tR2: 0.495283\n",
            " Epoch: 10051 \tTraining Loss:   0.288988\n",
            " Epoch: 10051 \tValidation Loss: 0.324421 \tR2: 0.495283\n",
            " Epoch: 10052 \tTraining Loss:   0.279716\n",
            " Epoch: 10052 \tValidation Loss: 0.325939 \tR2: 0.495283\n",
            " Epoch: 10053 \tTraining Loss:   0.277675\n",
            " Epoch: 10053 \tValidation Loss: 0.736040 \tR2: 0.495283\n",
            " Epoch: 10054 \tTraining Loss:   0.278551\n",
            " Epoch: 10054 \tValidation Loss: 0.308256 \tR2: 0.495283\n",
            " Epoch: 10055 \tTraining Loss:   0.289650\n",
            " Epoch: 10055 \tValidation Loss: 0.338784 \tR2: 0.495283\n",
            " Epoch: 10056 \tTraining Loss:   0.291279\n",
            " Epoch: 10056 \tValidation Loss: 0.305496 \tR2: 0.495283\n",
            " Epoch: 10057 \tTraining Loss:   0.276970\n",
            " Epoch: 10057 \tValidation Loss: 0.308297 \tR2: 0.495283\n",
            " Epoch: 10058 \tTraining Loss:   0.295952\n",
            " Epoch: 10058 \tValidation Loss: 0.354621 \tR2: 0.495283\n",
            " Epoch: 10059 \tTraining Loss:   0.304053\n",
            " Epoch: 10059 \tValidation Loss: 0.328462 \tR2: 0.495283\n",
            " Epoch: 10060 \tTraining Loss:   0.279617\n",
            " Epoch: 10060 \tValidation Loss: 0.312799 \tR2: 0.495283\n",
            " Epoch: 10061 \tTraining Loss:   0.288883\n",
            " Epoch: 10061 \tValidation Loss: 0.343399 \tR2: 0.495283\n",
            " Epoch: 10062 \tTraining Loss:   0.285843\n",
            " Epoch: 10062 \tValidation Loss: 0.314937 \tR2: 0.495283\n",
            " Epoch: 10063 \tTraining Loss:   0.288080\n",
            " Epoch: 10063 \tValidation Loss: 0.318261 \tR2: 0.495283\n",
            " Epoch: 10064 \tTraining Loss:   0.268961\n",
            " Epoch: 10064 \tValidation Loss: 0.319786 \tR2: 0.495283\n",
            " Epoch: 10065 \tTraining Loss:   0.282535\n",
            " Epoch: 10065 \tValidation Loss: 0.309936 \tR2: 0.495283\n",
            " Epoch: 10066 \tTraining Loss:   0.267680\n",
            " Epoch: 10066 \tValidation Loss: 0.310916 \tR2: 0.495283\n",
            " Epoch: 10067 \tTraining Loss:   0.278346\n",
            " Epoch: 10067 \tValidation Loss: 0.333196 \tR2: 0.495283\n",
            " Epoch: 10068 \tTraining Loss:   0.274284\n",
            " Epoch: 10068 \tValidation Loss: 0.342753 \tR2: 0.495283\n",
            " Epoch: 10069 \tTraining Loss:   0.277100\n",
            " Epoch: 10069 \tValidation Loss: 0.323382 \tR2: 0.495283\n",
            " Epoch: 10070 \tTraining Loss:   0.278611\n",
            " Epoch: 10070 \tValidation Loss: 0.333345 \tR2: 0.495283\n",
            " Epoch: 10071 \tTraining Loss:   0.295746\n",
            " Epoch: 10071 \tValidation Loss: 0.407967 \tR2: 0.495283\n",
            " Epoch: 10072 \tTraining Loss:   0.270266\n",
            " Epoch: 10072 \tValidation Loss: 0.354826 \tR2: 0.495283\n",
            " Epoch: 10073 \tTraining Loss:   0.283585\n",
            " Epoch: 10073 \tValidation Loss: 0.333793 \tR2: 0.495283\n",
            " Epoch: 10074 \tTraining Loss:   0.275278\n",
            " Epoch: 10074 \tValidation Loss: 0.309506 \tR2: 0.495283\n",
            " Epoch: 10075 \tTraining Loss:   0.287903\n",
            " Epoch: 10075 \tValidation Loss: 0.311794 \tR2: 0.495283\n",
            " Epoch: 10076 \tTraining Loss:   0.282381\n",
            " Epoch: 10076 \tValidation Loss: 0.345648 \tR2: 0.495283\n",
            " Epoch: 10077 \tTraining Loss:   0.289924\n",
            " Epoch: 10077 \tValidation Loss: 0.298079 \tR2: 0.495283\n",
            " Epoch: 10078 \tTraining Loss:   0.286485\n",
            " Epoch: 10078 \tValidation Loss: 0.298591 \tR2: 0.495283\n",
            " Epoch: 10079 \tTraining Loss:   0.274754\n",
            " Epoch: 10079 \tValidation Loss: 0.303735 \tR2: 0.495283\n",
            " Epoch: 10080 \tTraining Loss:   0.280490\n",
            " Epoch: 10080 \tValidation Loss: 0.311798 \tR2: 0.495283\n",
            " Epoch: 10081 \tTraining Loss:   0.274694\n",
            " Epoch: 10081 \tValidation Loss: 0.309431 \tR2: 0.495283\n",
            " Epoch: 10082 \tTraining Loss:   0.288072\n",
            " Epoch: 10082 \tValidation Loss: 0.331221 \tR2: 0.495283\n",
            " Epoch: 10083 \tTraining Loss:   0.283482\n",
            " Epoch: 10083 \tValidation Loss: 0.345128 \tR2: 0.495283\n",
            " Epoch: 10084 \tTraining Loss:   0.279404\n",
            " Epoch: 10084 \tValidation Loss: 0.326657 \tR2: 0.495283\n",
            " Epoch: 10085 \tTraining Loss:   0.279031\n",
            " Epoch: 10085 \tValidation Loss: 0.346855 \tR2: 0.495283\n",
            " Epoch: 10086 \tTraining Loss:   0.284282\n",
            " Epoch: 10086 \tValidation Loss: 0.492616 \tR2: 0.495283\n",
            " Epoch: 10087 \tTraining Loss:   0.287915\n",
            " Epoch: 10087 \tValidation Loss: 0.321782 \tR2: 0.495283\n",
            " Epoch: 10088 \tTraining Loss:   0.313499\n",
            " Epoch: 10088 \tValidation Loss: 0.336606 \tR2: 0.495283\n",
            " Epoch: 10089 \tTraining Loss:   0.277983\n",
            " Epoch: 10089 \tValidation Loss: 0.427358 \tR2: 0.495283\n",
            " Epoch: 10090 \tTraining Loss:   0.277895\n",
            " Epoch: 10090 \tValidation Loss: 0.327069 \tR2: 0.495283\n",
            " Epoch: 10091 \tTraining Loss:   0.287365\n",
            " Epoch: 10091 \tValidation Loss: 0.417702 \tR2: 0.495283\n",
            " Epoch: 10092 \tTraining Loss:   0.288943\n",
            " Epoch: 10092 \tValidation Loss: 0.308617 \tR2: 0.495283\n",
            " Epoch: 10093 \tTraining Loss:   0.286089\n",
            " Epoch: 10093 \tValidation Loss: 0.417689 \tR2: 0.495283\n",
            " Epoch: 10094 \tTraining Loss:   0.287477\n",
            " Epoch: 10094 \tValidation Loss: 0.360484 \tR2: 0.495283\n",
            " Epoch: 10095 \tTraining Loss:   0.279789\n",
            " Epoch: 10095 \tValidation Loss: 0.342065 \tR2: 0.495283\n",
            " Epoch: 10096 \tTraining Loss:   0.283771\n",
            " Epoch: 10096 \tValidation Loss: 0.305254 \tR2: 0.495283\n",
            " Epoch: 10097 \tTraining Loss:   0.273200\n",
            " Epoch: 10097 \tValidation Loss: 0.340989 \tR2: 0.495283\n",
            " Epoch: 10098 \tTraining Loss:   0.276890\n",
            " Epoch: 10098 \tValidation Loss: 0.310421 \tR2: 0.495283\n",
            " Epoch: 10099 \tTraining Loss:   0.279781\n",
            " Epoch: 10099 \tValidation Loss: 0.321315 \tR2: 0.495283\n",
            " Epoch: 10100 \tTraining Loss:   0.288000\n",
            " Epoch: 10100 \tValidation Loss: 0.337887 \tR2: 0.236613\n",
            " Epoch: 10101 \tTraining Loss:   0.274961\n",
            " Epoch: 10101 \tValidation Loss: 0.315986 \tR2: 0.236613\n",
            " Epoch: 10102 \tTraining Loss:   0.285461\n",
            " Epoch: 10102 \tValidation Loss: 0.335321 \tR2: 0.236613\n",
            " Epoch: 10103 \tTraining Loss:   0.271505\n",
            " Epoch: 10103 \tValidation Loss: 0.313268 \tR2: 0.236613\n",
            " Epoch: 10104 \tTraining Loss:   0.278893\n",
            " Epoch: 10104 \tValidation Loss: 0.314142 \tR2: 0.236613\n",
            " Epoch: 10105 \tTraining Loss:   0.282282\n",
            " Epoch: 10105 \tValidation Loss: 0.339292 \tR2: 0.236613\n",
            " Epoch: 10106 \tTraining Loss:   0.282560\n",
            " Epoch: 10106 \tValidation Loss: 0.318681 \tR2: 0.236613\n",
            " Epoch: 10107 \tTraining Loss:   0.272964\n",
            " Epoch: 10107 \tValidation Loss: 0.342046 \tR2: 0.236613\n",
            " Epoch: 10108 \tTraining Loss:   0.274501\n",
            " Epoch: 10108 \tValidation Loss: 0.350677 \tR2: 0.236613\n",
            " Epoch: 10109 \tTraining Loss:   0.270171\n",
            " Epoch: 10109 \tValidation Loss: 0.328835 \tR2: 0.236613\n",
            " Epoch: 10110 \tTraining Loss:   0.286477\n",
            " Epoch: 10110 \tValidation Loss: 0.353484 \tR2: 0.236613\n",
            " Epoch: 10111 \tTraining Loss:   0.292863\n",
            " Epoch: 10111 \tValidation Loss: 0.351017 \tR2: 0.236613\n",
            " Epoch: 10112 \tTraining Loss:   0.281033\n",
            " Epoch: 10112 \tValidation Loss: 0.325138 \tR2: 0.236613\n",
            " Epoch: 10113 \tTraining Loss:   0.277014\n",
            " Epoch: 10113 \tValidation Loss: 0.330577 \tR2: 0.236613\n",
            " Epoch: 10114 \tTraining Loss:   0.303148\n",
            " Epoch: 10114 \tValidation Loss: 0.305643 \tR2: 0.236613\n",
            " Epoch: 10115 \tTraining Loss:   0.288949\n",
            " Epoch: 10115 \tValidation Loss: 0.341783 \tR2: 0.236613\n",
            " Epoch: 10116 \tTraining Loss:   0.276089\n",
            " Epoch: 10116 \tValidation Loss: 0.391654 \tR2: 0.236613\n",
            " Epoch: 10117 \tTraining Loss:   0.278214\n",
            " Epoch: 10117 \tValidation Loss: 0.299427 \tR2: 0.236613\n",
            " Epoch: 10118 \tTraining Loss:   0.280261\n",
            " Epoch: 10118 \tValidation Loss: 0.340720 \tR2: 0.236613\n",
            " Epoch: 10119 \tTraining Loss:   0.271193\n",
            " Epoch: 10119 \tValidation Loss: 0.325294 \tR2: 0.236613\n",
            " Epoch: 10120 \tTraining Loss:   0.289112\n",
            " Epoch: 10120 \tValidation Loss: 0.361018 \tR2: 0.236613\n",
            " Epoch: 10121 \tTraining Loss:   0.268866\n",
            " Epoch: 10121 \tValidation Loss: 0.321416 \tR2: 0.236613\n",
            " Epoch: 10122 \tTraining Loss:   0.276341\n",
            " Epoch: 10122 \tValidation Loss: 0.363276 \tR2: 0.236613\n",
            " Epoch: 10123 \tTraining Loss:   0.277152\n",
            " Epoch: 10123 \tValidation Loss: 0.304774 \tR2: 0.236613\n",
            " Epoch: 10124 \tTraining Loss:   0.277658\n",
            " Epoch: 10124 \tValidation Loss: 0.336873 \tR2: 0.236613\n",
            " Epoch: 10125 \tTraining Loss:   0.299440\n",
            " Epoch: 10125 \tValidation Loss: 0.334915 \tR2: 0.236613\n",
            " Epoch: 10126 \tTraining Loss:   0.295335\n",
            " Epoch: 10126 \tValidation Loss: 0.358411 \tR2: 0.236613\n",
            " Epoch: 10127 \tTraining Loss:   0.285498\n",
            " Epoch: 10127 \tValidation Loss: 0.337882 \tR2: 0.236613\n",
            " Epoch: 10128 \tTraining Loss:   0.286854\n",
            " Epoch: 10128 \tValidation Loss: 0.317040 \tR2: 0.236613\n",
            " Epoch: 10129 \tTraining Loss:   0.273314\n",
            " Epoch: 10129 \tValidation Loss: 0.430777 \tR2: 0.236613\n",
            " Epoch: 10130 \tTraining Loss:   0.287079\n",
            " Epoch: 10130 \tValidation Loss: 0.323442 \tR2: 0.236613\n",
            " Epoch: 10131 \tTraining Loss:   0.276007\n",
            " Epoch: 10131 \tValidation Loss: 0.329149 \tR2: 0.236613\n",
            " Epoch: 10132 \tTraining Loss:   0.279410\n",
            " Epoch: 10132 \tValidation Loss: 0.325927 \tR2: 0.236613\n",
            " Epoch: 10133 \tTraining Loss:   0.276627\n",
            " Epoch: 10133 \tValidation Loss: 0.342626 \tR2: 0.236613\n",
            " Epoch: 10134 \tTraining Loss:   0.285689\n",
            " Epoch: 10134 \tValidation Loss: 0.368605 \tR2: 0.236613\n",
            " Epoch: 10135 \tTraining Loss:   0.289645\n",
            " Epoch: 10135 \tValidation Loss: 0.306372 \tR2: 0.236613\n",
            " Epoch: 10136 \tTraining Loss:   0.296003\n",
            " Epoch: 10136 \tValidation Loss: 0.382375 \tR2: 0.236613\n",
            " Epoch: 10137 \tTraining Loss:   0.293858\n",
            " Epoch: 10137 \tValidation Loss: 0.346945 \tR2: 0.236613\n",
            " Epoch: 10138 \tTraining Loss:   0.283985\n",
            " Epoch: 10138 \tValidation Loss: 0.345966 \tR2: 0.236613\n",
            " Epoch: 10139 \tTraining Loss:   0.283178\n",
            " Epoch: 10139 \tValidation Loss: 0.332557 \tR2: 0.236613\n",
            " Epoch: 10140 \tTraining Loss:   0.282378\n",
            " Epoch: 10140 \tValidation Loss: 0.334000 \tR2: 0.236613\n",
            " Epoch: 10141 \tTraining Loss:   0.276948\n",
            " Epoch: 10141 \tValidation Loss: 0.407625 \tR2: 0.236613\n",
            " Epoch: 10142 \tTraining Loss:   0.283232\n",
            " Epoch: 10142 \tValidation Loss: 0.307454 \tR2: 0.236613\n",
            " Epoch: 10143 \tTraining Loss:   0.283541\n",
            " Epoch: 10143 \tValidation Loss: 0.338338 \tR2: 0.236613\n",
            " Epoch: 10144 \tTraining Loss:   0.297248\n",
            " Epoch: 10144 \tValidation Loss: 0.318524 \tR2: 0.236613\n",
            " Epoch: 10145 \tTraining Loss:   0.273917\n",
            " Epoch: 10145 \tValidation Loss: 0.302955 \tR2: 0.236613\n",
            " Epoch: 10146 \tTraining Loss:   0.287086\n",
            " Epoch: 10146 \tValidation Loss: 0.309845 \tR2: 0.236613\n",
            " Epoch: 10147 \tTraining Loss:   0.286156\n",
            " Epoch: 10147 \tValidation Loss: 0.365228 \tR2: 0.236613\n",
            " Epoch: 10148 \tTraining Loss:   0.288513\n",
            " Epoch: 10148 \tValidation Loss: 0.342060 \tR2: 0.236613\n",
            " Epoch: 10149 \tTraining Loss:   0.279012\n",
            " Epoch: 10149 \tValidation Loss: 0.352412 \tR2: 0.236613\n",
            " Epoch: 10150 \tTraining Loss:   0.280351\n",
            " Epoch: 10150 \tValidation Loss: 0.341816 \tR2: 0.236613\n",
            " Epoch: 10151 \tTraining Loss:   0.280972\n",
            " Epoch: 10151 \tValidation Loss: 0.302618 \tR2: 0.236613\n",
            " Epoch: 10152 \tTraining Loss:   0.289752\n",
            " Epoch: 10152 \tValidation Loss: 0.348771 \tR2: 0.236613\n",
            " Epoch: 10153 \tTraining Loss:   0.281380\n",
            " Epoch: 10153 \tValidation Loss: 0.348874 \tR2: 0.236613\n",
            " Epoch: 10154 \tTraining Loss:   0.275623\n",
            " Epoch: 10154 \tValidation Loss: 0.326768 \tR2: 0.236613\n",
            " Epoch: 10155 \tTraining Loss:   0.300084\n",
            " Epoch: 10155 \tValidation Loss: 0.357887 \tR2: 0.236613\n",
            " Epoch: 10156 \tTraining Loss:   0.277448\n",
            " Epoch: 10156 \tValidation Loss: 0.350529 \tR2: 0.236613\n",
            " Epoch: 10157 \tTraining Loss:   0.284215\n",
            " Epoch: 10157 \tValidation Loss: 0.381827 \tR2: 0.236613\n",
            " Epoch: 10158 \tTraining Loss:   0.274498\n",
            " Epoch: 10158 \tValidation Loss: 0.319977 \tR2: 0.236613\n",
            " Epoch: 10159 \tTraining Loss:   0.291582\n",
            " Epoch: 10159 \tValidation Loss: 0.344325 \tR2: 0.236613\n",
            " Epoch: 10160 \tTraining Loss:   0.294936\n",
            " Epoch: 10160 \tValidation Loss: 0.421619 \tR2: 0.236613\n",
            " Epoch: 10161 \tTraining Loss:   0.276103\n",
            " Epoch: 10161 \tValidation Loss: 0.298149 \tR2: 0.236613\n",
            " Epoch: 10162 \tTraining Loss:   0.273243\n",
            " Epoch: 10162 \tValidation Loss: 0.333879 \tR2: 0.236613\n",
            " Epoch: 10163 \tTraining Loss:   0.274400\n",
            " Epoch: 10163 \tValidation Loss: 0.333616 \tR2: 0.236613\n",
            " Epoch: 10164 \tTraining Loss:   0.274644\n",
            " Epoch: 10164 \tValidation Loss: 0.315338 \tR2: 0.236613\n",
            " Epoch: 10165 \tTraining Loss:   0.277930\n",
            " Epoch: 10165 \tValidation Loss: 0.355274 \tR2: 0.236613\n",
            " Epoch: 10166 \tTraining Loss:   0.278387\n",
            " Epoch: 10166 \tValidation Loss: 0.370434 \tR2: 0.236613\n",
            " Epoch: 10167 \tTraining Loss:   0.284438\n",
            " Epoch: 10167 \tValidation Loss: 0.340496 \tR2: 0.236613\n",
            " Epoch: 10168 \tTraining Loss:   0.289801\n",
            " Epoch: 10168 \tValidation Loss: 0.335637 \tR2: 0.236613\n",
            " Epoch: 10169 \tTraining Loss:   0.298757\n",
            " Epoch: 10169 \tValidation Loss: 0.304613 \tR2: 0.236613\n",
            " Epoch: 10170 \tTraining Loss:   0.273885\n",
            " Epoch: 10170 \tValidation Loss: 0.338668 \tR2: 0.236613\n",
            " Epoch: 10171 \tTraining Loss:   0.275665\n",
            " Epoch: 10171 \tValidation Loss: 0.319921 \tR2: 0.236613\n",
            " Epoch: 10172 \tTraining Loss:   0.292205\n",
            " Epoch: 10172 \tValidation Loss: 0.329824 \tR2: 0.236613\n",
            " Epoch: 10173 \tTraining Loss:   0.295038\n",
            " Epoch: 10173 \tValidation Loss: 0.309499 \tR2: 0.236613\n",
            " Epoch: 10174 \tTraining Loss:   0.284611\n",
            " Epoch: 10174 \tValidation Loss: 0.357361 \tR2: 0.236613\n",
            " Epoch: 10175 \tTraining Loss:   0.297687\n",
            " Epoch: 10175 \tValidation Loss: 0.300821 \tR2: 0.236613\n",
            " Epoch: 10176 \tTraining Loss:   0.276115\n",
            " Epoch: 10176 \tValidation Loss: 0.313785 \tR2: 0.236613\n",
            " Epoch: 10177 \tTraining Loss:   0.280856\n",
            " Epoch: 10177 \tValidation Loss: 0.313071 \tR2: 0.236613\n",
            " Epoch: 10178 \tTraining Loss:   0.292774\n",
            " Epoch: 10178 \tValidation Loss: 0.352657 \tR2: 0.236613\n",
            " Epoch: 10179 \tTraining Loss:   0.286426\n",
            " Epoch: 10179 \tValidation Loss: 0.357526 \tR2: 0.236613\n",
            " Epoch: 10180 \tTraining Loss:   0.277058\n",
            " Epoch: 10180 \tValidation Loss: 0.308601 \tR2: 0.236613\n",
            " Epoch: 10181 \tTraining Loss:   0.279504\n",
            " Epoch: 10181 \tValidation Loss: 0.361904 \tR2: 0.236613\n",
            " Epoch: 10182 \tTraining Loss:   0.275484\n",
            " Epoch: 10182 \tValidation Loss: 0.310405 \tR2: 0.236613\n",
            " Epoch: 10183 \tTraining Loss:   0.266756\n",
            " Epoch: 10183 \tValidation Loss: 0.341936 \tR2: 0.236613\n",
            " Epoch: 10184 \tTraining Loss:   0.269458\n",
            " Epoch: 10184 \tValidation Loss: 0.333322 \tR2: 0.236613\n",
            " Epoch: 10185 \tTraining Loss:   0.295521\n",
            " Epoch: 10185 \tValidation Loss: 0.360739 \tR2: 0.236613\n",
            " Epoch: 10186 \tTraining Loss:   0.284327\n",
            " Epoch: 10186 \tValidation Loss: 0.322892 \tR2: 0.236613\n",
            " Epoch: 10187 \tTraining Loss:   0.304535\n",
            " Epoch: 10187 \tValidation Loss: 0.420931 \tR2: 0.236613\n",
            " Epoch: 10188 \tTraining Loss:   0.277949\n",
            " Epoch: 10188 \tValidation Loss: 0.342387 \tR2: 0.236613\n",
            " Epoch: 10189 \tTraining Loss:   0.277984\n",
            " Epoch: 10189 \tValidation Loss: 0.317796 \tR2: 0.236613\n",
            " Epoch: 10190 \tTraining Loss:   0.285337\n",
            " Epoch: 10190 \tValidation Loss: 0.300074 \tR2: 0.236613\n",
            " Epoch: 10191 \tTraining Loss:   0.276373\n",
            " Epoch: 10191 \tValidation Loss: 0.310608 \tR2: 0.236613\n",
            " Epoch: 10192 \tTraining Loss:   0.284421\n",
            " Epoch: 10192 \tValidation Loss: 0.316402 \tR2: 0.236613\n",
            " Epoch: 10193 \tTraining Loss:   0.279268\n",
            " Epoch: 10193 \tValidation Loss: 0.378941 \tR2: 0.236613\n",
            " Epoch: 10194 \tTraining Loss:   0.284158\n",
            " Epoch: 10194 \tValidation Loss: 0.341118 \tR2: 0.236613\n",
            " Epoch: 10195 \tTraining Loss:   0.293587\n",
            " Epoch: 10195 \tValidation Loss: 0.356804 \tR2: 0.236613\n",
            " Epoch: 10196 \tTraining Loss:   0.280799\n",
            " Epoch: 10196 \tValidation Loss: 0.315153 \tR2: 0.236613\n",
            " Epoch: 10197 \tTraining Loss:   0.294120\n",
            " Epoch: 10197 \tValidation Loss: 0.321276 \tR2: 0.236613\n",
            " Epoch: 10198 \tTraining Loss:   0.280721\n",
            " Epoch: 10198 \tValidation Loss: 0.350362 \tR2: 0.236613\n",
            " Epoch: 10199 \tTraining Loss:   0.277710\n",
            " Epoch: 10199 \tValidation Loss: 0.333630 \tR2: 0.236613\n",
            " Epoch: 10200 \tTraining Loss:   0.288744\n",
            " Epoch: 10200 \tValidation Loss: 0.339316 \tR2: 0.521920\n",
            " Epoch: 10201 \tTraining Loss:   0.271014\n",
            " Epoch: 10201 \tValidation Loss: 0.316811 \tR2: 0.521920\n",
            " Epoch: 10202 \tTraining Loss:   0.287618\n",
            " Epoch: 10202 \tValidation Loss: 0.339277 \tR2: 0.521920\n",
            " Epoch: 10203 \tTraining Loss:   0.289114\n",
            " Epoch: 10203 \tValidation Loss: 0.385207 \tR2: 0.521920\n",
            " Epoch: 10204 \tTraining Loss:   0.298326\n",
            " Epoch: 10204 \tValidation Loss: 0.306224 \tR2: 0.521920\n",
            " Epoch: 10205 \tTraining Loss:   0.291179\n",
            " Epoch: 10205 \tValidation Loss: 0.335610 \tR2: 0.521920\n",
            " Epoch: 10206 \tTraining Loss:   0.275928\n",
            " Epoch: 10206 \tValidation Loss: 0.323980 \tR2: 0.521920\n",
            " Epoch: 10207 \tTraining Loss:   0.277226\n",
            " Epoch: 10207 \tValidation Loss: 0.338688 \tR2: 0.521920\n",
            " Epoch: 10208 \tTraining Loss:   0.281944\n",
            " Epoch: 10208 \tValidation Loss: 0.342497 \tR2: 0.521920\n",
            " Epoch: 10209 \tTraining Loss:   0.293525\n",
            " Epoch: 10209 \tValidation Loss: 0.328150 \tR2: 0.521920\n",
            " Epoch: 10210 \tTraining Loss:   0.269227\n",
            " Epoch: 10210 \tValidation Loss: 0.322354 \tR2: 0.521920\n",
            " Epoch: 10211 \tTraining Loss:   0.282627\n",
            " Epoch: 10211 \tValidation Loss: 0.380654 \tR2: 0.521920\n",
            " Epoch: 10212 \tTraining Loss:   0.288130\n",
            " Epoch: 10212 \tValidation Loss: 0.350419 \tR2: 0.521920\n",
            " Epoch: 10213 \tTraining Loss:   0.294246\n",
            " Epoch: 10213 \tValidation Loss: 0.332717 \tR2: 0.521920\n",
            " Epoch: 10214 \tTraining Loss:   0.281391\n",
            " Epoch: 10214 \tValidation Loss: 0.307267 \tR2: 0.521920\n",
            " Epoch: 10215 \tTraining Loss:   0.276361\n",
            " Epoch: 10215 \tValidation Loss: 0.317719 \tR2: 0.521920\n",
            " Epoch: 10216 \tTraining Loss:   0.273256\n",
            " Epoch: 10216 \tValidation Loss: 0.355804 \tR2: 0.521920\n",
            " Epoch: 10217 \tTraining Loss:   0.280626\n",
            " Epoch: 10217 \tValidation Loss: 0.323011 \tR2: 0.521920\n",
            " Epoch: 10218 \tTraining Loss:   0.274709\n",
            " Epoch: 10218 \tValidation Loss: 0.360064 \tR2: 0.521920\n",
            " Epoch: 10219 \tTraining Loss:   0.272334\n",
            " Epoch: 10219 \tValidation Loss: 0.342233 \tR2: 0.521920\n",
            " Epoch: 10220 \tTraining Loss:   0.292863\n",
            " Epoch: 10220 \tValidation Loss: 0.327981 \tR2: 0.521920\n",
            " Epoch: 10221 \tTraining Loss:   0.279787\n",
            " Epoch: 10221 \tValidation Loss: 0.311524 \tR2: 0.521920\n",
            " Epoch: 10222 \tTraining Loss:   0.272167\n",
            " Epoch: 10222 \tValidation Loss: 0.314946 \tR2: 0.521920\n",
            " Epoch: 10223 \tTraining Loss:   0.273852\n",
            " Epoch: 10223 \tValidation Loss: 0.352224 \tR2: 0.521920\n",
            " Epoch: 10224 \tTraining Loss:   0.267793\n",
            " Epoch: 10224 \tValidation Loss: 0.325690 \tR2: 0.521920\n",
            " Epoch: 10225 \tTraining Loss:   0.277554\n",
            " Epoch: 10225 \tValidation Loss: 0.329524 \tR2: 0.521920\n",
            " Epoch: 10226 \tTraining Loss:   0.283887\n",
            " Epoch: 10226 \tValidation Loss: 0.308118 \tR2: 0.521920\n",
            " Epoch: 10227 \tTraining Loss:   0.268305\n",
            " Epoch: 10227 \tValidation Loss: 0.360338 \tR2: 0.521920\n",
            " Epoch: 10228 \tTraining Loss:   0.277701\n",
            " Epoch: 10228 \tValidation Loss: 0.342802 \tR2: 0.521920\n",
            " Epoch: 10229 \tTraining Loss:   0.262193\n",
            " Epoch: 10229 \tValidation Loss: 0.327658 \tR2: 0.521920\n",
            " Epoch: 10230 \tTraining Loss:   0.280863\n",
            " Epoch: 10230 \tValidation Loss: 0.319711 \tR2: 0.521920\n",
            " Epoch: 10231 \tTraining Loss:   0.281290\n",
            " Epoch: 10231 \tValidation Loss: 0.335652 \tR2: 0.521920\n",
            " Epoch: 10232 \tTraining Loss:   0.292123\n",
            " Epoch: 10232 \tValidation Loss: 0.358499 \tR2: 0.521920\n",
            " Epoch: 10233 \tTraining Loss:   0.280359\n",
            " Epoch: 10233 \tValidation Loss: 0.317178 \tR2: 0.521920\n",
            " Epoch: 10234 \tTraining Loss:   0.291394\n",
            " Epoch: 10234 \tValidation Loss: 0.353231 \tR2: 0.521920\n",
            " Epoch: 10235 \tTraining Loss:   0.286990\n",
            " Epoch: 10235 \tValidation Loss: 0.341579 \tR2: 0.521920\n",
            " Epoch: 10236 \tTraining Loss:   0.280947\n",
            " Epoch: 10236 \tValidation Loss: 0.332464 \tR2: 0.521920\n",
            " Epoch: 10237 \tTraining Loss:   0.275220\n",
            " Epoch: 10237 \tValidation Loss: 0.312712 \tR2: 0.521920\n",
            " Epoch: 10238 \tTraining Loss:   0.298015\n",
            " Epoch: 10238 \tValidation Loss: 0.659800 \tR2: 0.521920\n",
            " Epoch: 10239 \tTraining Loss:   0.308682\n",
            " Epoch: 10239 \tValidation Loss: 0.314765 \tR2: 0.521920\n",
            " Epoch: 10240 \tTraining Loss:   0.290454\n",
            " Epoch: 10240 \tValidation Loss: 0.332038 \tR2: 0.521920\n",
            " Epoch: 10241 \tTraining Loss:   0.283362\n",
            " Epoch: 10241 \tValidation Loss: 0.350033 \tR2: 0.521920\n",
            " Epoch: 10242 \tTraining Loss:   0.294561\n",
            " Epoch: 10242 \tValidation Loss: 0.386340 \tR2: 0.521920\n",
            " Epoch: 10243 \tTraining Loss:   0.287602\n",
            " Epoch: 10243 \tValidation Loss: 0.331097 \tR2: 0.521920\n",
            " Epoch: 10244 \tTraining Loss:   0.279115\n",
            " Epoch: 10244 \tValidation Loss: 0.332901 \tR2: 0.521920\n",
            " Epoch: 10245 \tTraining Loss:   0.276438\n",
            " Epoch: 10245 \tValidation Loss: 0.372867 \tR2: 0.521920\n",
            " Epoch: 10246 \tTraining Loss:   0.310283\n",
            " Epoch: 10246 \tValidation Loss: 0.397036 \tR2: 0.521920\n",
            " Epoch: 10247 \tTraining Loss:   0.279509\n",
            " Epoch: 10247 \tValidation Loss: 0.327697 \tR2: 0.521920\n",
            " Epoch: 10248 \tTraining Loss:   0.280304\n",
            " Epoch: 10248 \tValidation Loss: 0.301061 \tR2: 0.521920\n",
            " Epoch: 10249 \tTraining Loss:   0.283043\n",
            " Epoch: 10249 \tValidation Loss: 0.328340 \tR2: 0.521920\n",
            " Epoch: 10250 \tTraining Loss:   0.281652\n",
            " Epoch: 10250 \tValidation Loss: 0.364010 \tR2: 0.521920\n",
            " Epoch: 10251 \tTraining Loss:   0.272475\n",
            " Epoch: 10251 \tValidation Loss: 0.360553 \tR2: 0.521920\n",
            " Epoch: 10252 \tTraining Loss:   0.270628\n",
            " Epoch: 10252 \tValidation Loss: 0.311012 \tR2: 0.521920\n",
            " Epoch: 10253 \tTraining Loss:   0.280585\n",
            " Epoch: 10253 \tValidation Loss: 0.368732 \tR2: 0.521920\n",
            " Epoch: 10254 \tTraining Loss:   0.281459\n",
            " Epoch: 10254 \tValidation Loss: 0.302065 \tR2: 0.521920\n",
            " Epoch: 10255 \tTraining Loss:   0.280721\n",
            " Epoch: 10255 \tValidation Loss: 0.301925 \tR2: 0.521920\n",
            " Epoch: 10256 \tTraining Loss:   0.280724\n",
            " Epoch: 10256 \tValidation Loss: 0.329265 \tR2: 0.521920\n",
            " Epoch: 10257 \tTraining Loss:   0.292930\n",
            " Epoch: 10257 \tValidation Loss: 0.303143 \tR2: 0.521920\n",
            " Epoch: 10258 \tTraining Loss:   0.286115\n",
            " Epoch: 10258 \tValidation Loss: 0.343948 \tR2: 0.521920\n",
            " Epoch: 10259 \tTraining Loss:   0.283327\n",
            " Epoch: 10259 \tValidation Loss: 0.319949 \tR2: 0.521920\n",
            " Epoch: 10260 \tTraining Loss:   0.275900\n",
            " Epoch: 10260 \tValidation Loss: 0.310622 \tR2: 0.521920\n",
            " Epoch: 10261 \tTraining Loss:   0.276629\n",
            " Epoch: 10261 \tValidation Loss: 0.409584 \tR2: 0.521920\n",
            " Epoch: 10262 \tTraining Loss:   0.292843\n",
            " Epoch: 10262 \tValidation Loss: 0.453339 \tR2: 0.521920\n",
            " Epoch: 10263 \tTraining Loss:   0.279373\n",
            " Epoch: 10263 \tValidation Loss: 0.326423 \tR2: 0.521920\n",
            " Epoch: 10264 \tTraining Loss:   0.290553\n",
            " Epoch: 10264 \tValidation Loss: 0.293790 \tR2: 0.521920\n",
            " Epoch: 10265 \tTraining Loss:   0.294390\n",
            " Epoch: 10265 \tValidation Loss: 0.328895 \tR2: 0.521920\n",
            " Epoch: 10266 \tTraining Loss:   0.277380\n",
            " Epoch: 10266 \tValidation Loss: 0.346512 \tR2: 0.521920\n",
            " Epoch: 10267 \tTraining Loss:   0.274176\n",
            " Epoch: 10267 \tValidation Loss: 0.316688 \tR2: 0.521920\n",
            " Epoch: 10268 \tTraining Loss:   0.272264\n",
            " Epoch: 10268 \tValidation Loss: 0.322511 \tR2: 0.521920\n",
            " Epoch: 10269 \tTraining Loss:   0.281592\n",
            " Epoch: 10269 \tValidation Loss: 0.331076 \tR2: 0.521920\n",
            " Epoch: 10270 \tTraining Loss:   0.283280\n",
            " Epoch: 10270 \tValidation Loss: 0.324452 \tR2: 0.521920\n",
            " Epoch: 10271 \tTraining Loss:   0.280173\n",
            " Epoch: 10271 \tValidation Loss: 0.349278 \tR2: 0.521920\n",
            " Epoch: 10272 \tTraining Loss:   0.271631\n",
            " Epoch: 10272 \tValidation Loss: 0.342841 \tR2: 0.521920\n",
            " Epoch: 10273 \tTraining Loss:   0.286408\n",
            " Epoch: 10273 \tValidation Loss: 0.345400 \tR2: 0.521920\n",
            " Epoch: 10274 \tTraining Loss:   0.288924\n",
            " Epoch: 10274 \tValidation Loss: 0.309274 \tR2: 0.521920\n",
            " Epoch: 10275 \tTraining Loss:   0.268238\n",
            " Epoch: 10275 \tValidation Loss: 0.290678 \tR2: 0.521920\n",
            " Epoch: 10276 \tTraining Loss:   0.281479\n",
            " Epoch: 10276 \tValidation Loss: 0.389523 \tR2: 0.521920\n",
            " Epoch: 10277 \tTraining Loss:   0.282502\n",
            " Epoch: 10277 \tValidation Loss: 0.326919 \tR2: 0.521920\n",
            " Epoch: 10278 \tTraining Loss:   0.286248\n",
            " Epoch: 10278 \tValidation Loss: 0.329822 \tR2: 0.521920\n",
            " Epoch: 10279 \tTraining Loss:   0.278431\n",
            " Epoch: 10279 \tValidation Loss: 0.314130 \tR2: 0.521920\n",
            " Epoch: 10280 \tTraining Loss:   0.284348\n",
            " Epoch: 10280 \tValidation Loss: 0.313545 \tR2: 0.521920\n",
            " Epoch: 10281 \tTraining Loss:   0.266685\n",
            " Epoch: 10281 \tValidation Loss: 0.306504 \tR2: 0.521920\n",
            " Epoch: 10282 \tTraining Loss:   0.274792\n",
            " Epoch: 10282 \tValidation Loss: 0.366903 \tR2: 0.521920\n",
            " Epoch: 10283 \tTraining Loss:   0.280048\n",
            " Epoch: 10283 \tValidation Loss: 0.321700 \tR2: 0.521920\n",
            " Epoch: 10284 \tTraining Loss:   0.277313\n",
            " Epoch: 10284 \tValidation Loss: 0.312553 \tR2: 0.521920\n",
            " Epoch: 10285 \tTraining Loss:   0.275800\n",
            " Epoch: 10285 \tValidation Loss: 0.351647 \tR2: 0.521920\n",
            " Epoch: 10286 \tTraining Loss:   0.284013\n",
            " Epoch: 10286 \tValidation Loss: 0.304566 \tR2: 0.521920\n",
            " Epoch: 10287 \tTraining Loss:   0.280029\n",
            " Epoch: 10287 \tValidation Loss: 0.376837 \tR2: 0.521920\n",
            " Epoch: 10288 \tTraining Loss:   0.285453\n",
            " Epoch: 10288 \tValidation Loss: 0.340245 \tR2: 0.521920\n",
            " Epoch: 10289 \tTraining Loss:   0.283012\n",
            " Epoch: 10289 \tValidation Loss: 0.322958 \tR2: 0.521920\n",
            " Epoch: 10290 \tTraining Loss:   0.282371\n",
            " Epoch: 10290 \tValidation Loss: 0.323944 \tR2: 0.521920\n",
            " Epoch: 10291 \tTraining Loss:   0.277089\n",
            " Epoch: 10291 \tValidation Loss: 0.307460 \tR2: 0.521920\n",
            " Epoch: 10292 \tTraining Loss:   0.276225\n",
            " Epoch: 10292 \tValidation Loss: 0.308522 \tR2: 0.521920\n",
            " Epoch: 10293 \tTraining Loss:   0.268814\n",
            " Epoch: 10293 \tValidation Loss: 0.333589 \tR2: 0.521920\n",
            " Epoch: 10294 \tTraining Loss:   0.284584\n",
            " Epoch: 10294 \tValidation Loss: 0.325760 \tR2: 0.521920\n",
            " Epoch: 10295 \tTraining Loss:   0.301469\n",
            " Epoch: 10295 \tValidation Loss: 0.364698 \tR2: 0.521920\n",
            " Epoch: 10296 \tTraining Loss:   0.263762\n",
            " Epoch: 10296 \tValidation Loss: 0.312804 \tR2: 0.521920\n",
            " Epoch: 10297 \tTraining Loss:   0.286927\n",
            " Epoch: 10297 \tValidation Loss: 0.322248 \tR2: 0.521920\n",
            " Epoch: 10298 \tTraining Loss:   0.290611\n",
            " Epoch: 10298 \tValidation Loss: 0.300540 \tR2: 0.521920\n",
            " Epoch: 10299 \tTraining Loss:   0.267009\n",
            " Epoch: 10299 \tValidation Loss: 0.329172 \tR2: 0.521920\n",
            " Epoch: 10300 \tTraining Loss:   0.279482\n",
            " Epoch: 10300 \tValidation Loss: 0.323299 \tR2: 0.509654\n",
            " Epoch: 10301 \tTraining Loss:   0.274535\n",
            " Epoch: 10301 \tValidation Loss: 0.366492 \tR2: 0.509654\n",
            " Epoch: 10302 \tTraining Loss:   0.284343\n",
            " Epoch: 10302 \tValidation Loss: 0.346201 \tR2: 0.509654\n",
            " Epoch: 10303 \tTraining Loss:   0.271305\n",
            " Epoch: 10303 \tValidation Loss: 0.363599 \tR2: 0.509654\n",
            " Epoch: 10304 \tTraining Loss:   0.280452\n",
            " Epoch: 10304 \tValidation Loss: 0.354365 \tR2: 0.509654\n",
            " Epoch: 10305 \tTraining Loss:   0.276443\n",
            " Epoch: 10305 \tValidation Loss: 0.317249 \tR2: 0.509654\n",
            " Epoch: 10306 \tTraining Loss:   0.284504\n",
            " Epoch: 10306 \tValidation Loss: 0.320769 \tR2: 0.509654\n",
            " Epoch: 10307 \tTraining Loss:   0.290231\n",
            " Epoch: 10307 \tValidation Loss: 0.334296 \tR2: 0.509654\n",
            " Epoch: 10308 \tTraining Loss:   0.278732\n",
            " Epoch: 10308 \tValidation Loss: 0.347302 \tR2: 0.509654\n",
            " Epoch: 10309 \tTraining Loss:   0.286116\n",
            " Epoch: 10309 \tValidation Loss: 0.297889 \tR2: 0.509654\n",
            " Epoch: 10310 \tTraining Loss:   0.287420\n",
            " Epoch: 10310 \tValidation Loss: 0.342233 \tR2: 0.509654\n",
            " Epoch: 10311 \tTraining Loss:   0.279669\n",
            " Epoch: 10311 \tValidation Loss: 0.355331 \tR2: 0.509654\n",
            " Epoch: 10312 \tTraining Loss:   0.286452\n",
            " Epoch: 10312 \tValidation Loss: 0.334534 \tR2: 0.509654\n",
            " Epoch: 10313 \tTraining Loss:   0.286124\n",
            " Epoch: 10313 \tValidation Loss: 0.487841 \tR2: 0.509654\n",
            " Epoch: 10314 \tTraining Loss:   0.286840\n",
            " Epoch: 10314 \tValidation Loss: 0.302782 \tR2: 0.509654\n",
            " Epoch: 10315 \tTraining Loss:   0.283962\n",
            " Epoch: 10315 \tValidation Loss: 0.313543 \tR2: 0.509654\n",
            " Epoch: 10316 \tTraining Loss:   0.280863\n",
            " Epoch: 10316 \tValidation Loss: 0.324952 \tR2: 0.509654\n",
            " Epoch: 10317 \tTraining Loss:   0.275452\n",
            " Epoch: 10317 \tValidation Loss: 0.326115 \tR2: 0.509654\n",
            " Epoch: 10318 \tTraining Loss:   0.284258\n",
            " Epoch: 10318 \tValidation Loss: 0.367714 \tR2: 0.509654\n",
            " Epoch: 10319 \tTraining Loss:   0.284558\n",
            " Epoch: 10319 \tValidation Loss: 0.335517 \tR2: 0.509654\n",
            " Epoch: 10320 \tTraining Loss:   0.287386\n",
            " Epoch: 10320 \tValidation Loss: 0.318729 \tR2: 0.509654\n",
            " Epoch: 10321 \tTraining Loss:   0.275502\n",
            " Epoch: 10321 \tValidation Loss: 0.348022 \tR2: 0.509654\n",
            " Epoch: 10322 \tTraining Loss:   0.264284\n",
            " Epoch: 10322 \tValidation Loss: 0.326630 \tR2: 0.509654\n",
            " Epoch: 10323 \tTraining Loss:   0.288533\n",
            " Epoch: 10323 \tValidation Loss: 0.321290 \tR2: 0.509654\n",
            " Epoch: 10324 \tTraining Loss:   0.278930\n",
            " Epoch: 10324 \tValidation Loss: 0.316257 \tR2: 0.509654\n",
            " Epoch: 10325 \tTraining Loss:   0.281085\n",
            " Epoch: 10325 \tValidation Loss: 0.310245 \tR2: 0.509654\n",
            " Epoch: 10326 \tTraining Loss:   0.302881\n",
            " Epoch: 10326 \tValidation Loss: 0.308196 \tR2: 0.509654\n",
            " Epoch: 10327 \tTraining Loss:   0.280959\n",
            " Epoch: 10327 \tValidation Loss: 0.317659 \tR2: 0.509654\n",
            " Epoch: 10328 \tTraining Loss:   0.277911\n",
            " Epoch: 10328 \tValidation Loss: 0.332691 \tR2: 0.509654\n",
            " Epoch: 10329 \tTraining Loss:   0.277616\n",
            " Epoch: 10329 \tValidation Loss: 0.303153 \tR2: 0.509654\n",
            " Epoch: 10330 \tTraining Loss:   0.275706\n",
            " Epoch: 10330 \tValidation Loss: 0.328871 \tR2: 0.509654\n",
            " Epoch: 10331 \tTraining Loss:   0.287103\n",
            " Epoch: 10331 \tValidation Loss: 0.360034 \tR2: 0.509654\n",
            " Epoch: 10332 \tTraining Loss:   0.289077\n",
            " Epoch: 10332 \tValidation Loss: 0.321286 \tR2: 0.509654\n",
            " Epoch: 10333 \tTraining Loss:   0.272925\n",
            " Epoch: 10333 \tValidation Loss: 0.311328 \tR2: 0.509654\n",
            " Epoch: 10334 \tTraining Loss:   0.276952\n",
            " Epoch: 10334 \tValidation Loss: 0.308786 \tR2: 0.509654\n",
            " Epoch: 10335 \tTraining Loss:   0.271719\n",
            " Epoch: 10335 \tValidation Loss: 0.315960 \tR2: 0.509654\n",
            " Epoch: 10336 \tTraining Loss:   0.283815\n",
            " Epoch: 10336 \tValidation Loss: 0.446086 \tR2: 0.509654\n",
            " Epoch: 10337 \tTraining Loss:   0.287101\n",
            " Epoch: 10337 \tValidation Loss: 0.353686 \tR2: 0.509654\n",
            " Epoch: 10338 \tTraining Loss:   0.277007\n",
            " Epoch: 10338 \tValidation Loss: 0.312512 \tR2: 0.509654\n",
            " Epoch: 10339 \tTraining Loss:   0.344644\n",
            " Epoch: 10339 \tValidation Loss: 0.419628 \tR2: 0.509654\n",
            " Epoch: 10340 \tTraining Loss:   0.288740\n",
            " Epoch: 10340 \tValidation Loss: 0.313307 \tR2: 0.509654\n",
            " Epoch: 10341 \tTraining Loss:   0.280573\n",
            " Epoch: 10341 \tValidation Loss: 0.306316 \tR2: 0.509654\n",
            " Epoch: 10342 \tTraining Loss:   0.287940\n",
            " Epoch: 10342 \tValidation Loss: 0.326046 \tR2: 0.509654\n",
            " Epoch: 10343 \tTraining Loss:   0.279306\n",
            " Epoch: 10343 \tValidation Loss: 0.322506 \tR2: 0.509654\n",
            " Epoch: 10344 \tTraining Loss:   0.281587\n",
            " Epoch: 10344 \tValidation Loss: 0.437367 \tR2: 0.509654\n",
            " Epoch: 10345 \tTraining Loss:   0.281000\n",
            " Epoch: 10345 \tValidation Loss: 0.324773 \tR2: 0.509654\n",
            " Epoch: 10346 \tTraining Loss:   0.281956\n",
            " Epoch: 10346 \tValidation Loss: 0.340260 \tR2: 0.509654\n",
            " Epoch: 10347 \tTraining Loss:   0.285242\n",
            " Epoch: 10347 \tValidation Loss: 0.305386 \tR2: 0.509654\n",
            " Epoch: 10348 \tTraining Loss:   0.295846\n",
            " Epoch: 10348 \tValidation Loss: 0.330463 \tR2: 0.509654\n",
            " Epoch: 10349 \tTraining Loss:   0.273604\n",
            " Epoch: 10349 \tValidation Loss: 0.322584 \tR2: 0.509654\n",
            " Epoch: 10350 \tTraining Loss:   0.283195\n",
            " Epoch: 10350 \tValidation Loss: 0.309105 \tR2: 0.509654\n",
            " Epoch: 10351 \tTraining Loss:   0.289101\n",
            " Epoch: 10351 \tValidation Loss: 0.342976 \tR2: 0.509654\n",
            " Epoch: 10352 \tTraining Loss:   0.285197\n",
            " Epoch: 10352 \tValidation Loss: 0.313040 \tR2: 0.509654\n",
            " Epoch: 10353 \tTraining Loss:   0.291356\n",
            " Epoch: 10353 \tValidation Loss: 0.321365 \tR2: 0.509654\n",
            " Epoch: 10354 \tTraining Loss:   0.275440\n",
            " Epoch: 10354 \tValidation Loss: 0.301291 \tR2: 0.509654\n",
            " Epoch: 10355 \tTraining Loss:   0.291878\n",
            " Epoch: 10355 \tValidation Loss: 0.360837 \tR2: 0.509654\n",
            " Epoch: 10356 \tTraining Loss:   0.288060\n",
            " Epoch: 10356 \tValidation Loss: 0.320476 \tR2: 0.509654\n",
            " Epoch: 10357 \tTraining Loss:   0.280850\n",
            " Epoch: 10357 \tValidation Loss: 0.325794 \tR2: 0.509654\n",
            " Epoch: 10358 \tTraining Loss:   0.286599\n",
            " Epoch: 10358 \tValidation Loss: 0.349693 \tR2: 0.509654\n",
            " Epoch: 10359 \tTraining Loss:   0.278960\n",
            " Epoch: 10359 \tValidation Loss: 0.331931 \tR2: 0.509654\n",
            " Epoch: 10360 \tTraining Loss:   0.291491\n",
            " Epoch: 10360 \tValidation Loss: 0.376643 \tR2: 0.509654\n",
            " Epoch: 10361 \tTraining Loss:   0.286688\n",
            " Epoch: 10361 \tValidation Loss: 0.355989 \tR2: 0.509654\n",
            " Epoch: 10362 \tTraining Loss:   0.305426\n",
            " Epoch: 10362 \tValidation Loss: 0.326095 \tR2: 0.509654\n",
            " Epoch: 10363 \tTraining Loss:   0.284176\n",
            " Epoch: 10363 \tValidation Loss: 0.319164 \tR2: 0.509654\n",
            " Epoch: 10364 \tTraining Loss:   0.273919\n",
            " Epoch: 10364 \tValidation Loss: 0.344336 \tR2: 0.509654\n",
            " Epoch: 10365 \tTraining Loss:   0.280765\n",
            " Epoch: 10365 \tValidation Loss: 0.307819 \tR2: 0.509654\n",
            " Epoch: 10366 \tTraining Loss:   0.280077\n",
            " Epoch: 10366 \tValidation Loss: 0.312304 \tR2: 0.509654\n",
            " Epoch: 10367 \tTraining Loss:   0.287590\n",
            " Epoch: 10367 \tValidation Loss: 0.324298 \tR2: 0.509654\n",
            " Epoch: 10368 \tTraining Loss:   0.291891\n",
            " Epoch: 10368 \tValidation Loss: 0.337324 \tR2: 0.509654\n",
            " Epoch: 10369 \tTraining Loss:   0.283597\n",
            " Epoch: 10369 \tValidation Loss: 0.316342 \tR2: 0.509654\n",
            " Epoch: 10370 \tTraining Loss:   0.289496\n",
            " Epoch: 10370 \tValidation Loss: 0.318205 \tR2: 0.509654\n",
            " Epoch: 10371 \tTraining Loss:   0.275312\n",
            " Epoch: 10371 \tValidation Loss: 0.303992 \tR2: 0.509654\n",
            " Epoch: 10372 \tTraining Loss:   0.273659\n",
            " Epoch: 10372 \tValidation Loss: 0.320688 \tR2: 0.509654\n",
            " Epoch: 10373 \tTraining Loss:   0.270992\n",
            " Epoch: 10373 \tValidation Loss: 0.318884 \tR2: 0.509654\n",
            " Epoch: 10374 \tTraining Loss:   0.296279\n",
            " Epoch: 10374 \tValidation Loss: 0.346851 \tR2: 0.509654\n",
            " Epoch: 10375 \tTraining Loss:   0.282752\n",
            " Epoch: 10375 \tValidation Loss: 0.326561 \tR2: 0.509654\n",
            " Epoch: 10376 \tTraining Loss:   0.273691\n",
            " Epoch: 10376 \tValidation Loss: 0.307692 \tR2: 0.509654\n",
            " Epoch: 10377 \tTraining Loss:   0.272847\n",
            " Epoch: 10377 \tValidation Loss: 0.416193 \tR2: 0.509654\n",
            " Epoch: 10378 \tTraining Loss:   0.283938\n",
            " Epoch: 10378 \tValidation Loss: 0.339051 \tR2: 0.509654\n",
            " Epoch: 10379 \tTraining Loss:   0.281454\n",
            " Epoch: 10379 \tValidation Loss: 0.308356 \tR2: 0.509654\n",
            " Epoch: 10380 \tTraining Loss:   0.270088\n",
            " Epoch: 10380 \tValidation Loss: 0.330273 \tR2: 0.509654\n",
            " Epoch: 10381 \tTraining Loss:   0.275812\n",
            " Epoch: 10381 \tValidation Loss: 0.371819 \tR2: 0.509654\n",
            " Epoch: 10382 \tTraining Loss:   0.285382\n",
            " Epoch: 10382 \tValidation Loss: 0.333784 \tR2: 0.509654\n",
            " Epoch: 10383 \tTraining Loss:   0.284734\n",
            " Epoch: 10383 \tValidation Loss: 0.318654 \tR2: 0.509654\n",
            " Epoch: 10384 \tTraining Loss:   0.276399\n",
            " Epoch: 10384 \tValidation Loss: 0.304966 \tR2: 0.509654\n",
            " Epoch: 10385 \tTraining Loss:   0.283639\n",
            " Epoch: 10385 \tValidation Loss: 0.491801 \tR2: 0.509654\n",
            " Epoch: 10386 \tTraining Loss:   0.270708\n",
            " Epoch: 10386 \tValidation Loss: 0.332996 \tR2: 0.509654\n",
            " Epoch: 10387 \tTraining Loss:   0.278149\n",
            " Epoch: 10387 \tValidation Loss: 0.323949 \tR2: 0.509654\n",
            " Epoch: 10388 \tTraining Loss:   0.294018\n",
            " Epoch: 10388 \tValidation Loss: 0.365931 \tR2: 0.509654\n",
            " Epoch: 10389 \tTraining Loss:   0.276460\n",
            " Epoch: 10389 \tValidation Loss: 0.320146 \tR2: 0.509654\n",
            " Epoch: 10390 \tTraining Loss:   0.273582\n",
            " Epoch: 10390 \tValidation Loss: 0.306371 \tR2: 0.509654\n",
            " Epoch: 10391 \tTraining Loss:   0.282596\n",
            " Epoch: 10391 \tValidation Loss: 0.315144 \tR2: 0.509654\n",
            " Epoch: 10392 \tTraining Loss:   0.311626\n",
            " Epoch: 10392 \tValidation Loss: 0.340217 \tR2: 0.509654\n",
            " Epoch: 10393 \tTraining Loss:   0.280082\n",
            " Epoch: 10393 \tValidation Loss: 0.450979 \tR2: 0.509654\n",
            " Epoch: 10394 \tTraining Loss:   0.272787\n",
            " Epoch: 10394 \tValidation Loss: 0.322843 \tR2: 0.509654\n",
            " Epoch: 10395 \tTraining Loss:   0.279236\n",
            " Epoch: 10395 \tValidation Loss: 0.308352 \tR2: 0.509654\n",
            " Epoch: 10396 \tTraining Loss:   0.276029\n",
            " Epoch: 10396 \tValidation Loss: 0.316615 \tR2: 0.509654\n",
            " Epoch: 10397 \tTraining Loss:   0.282503\n",
            " Epoch: 10397 \tValidation Loss: 0.320900 \tR2: 0.509654\n",
            " Epoch: 10398 \tTraining Loss:   0.277690\n",
            " Epoch: 10398 \tValidation Loss: 0.334324 \tR2: 0.509654\n",
            " Epoch: 10399 \tTraining Loss:   0.273334\n",
            " Epoch: 10399 \tValidation Loss: 0.344225 \tR2: 0.509654\n",
            " Epoch: 10400 \tTraining Loss:   0.296782\n",
            " Epoch: 10400 \tValidation Loss: 0.417572 \tR2: 0.630432\n",
            " Epoch: 10401 \tTraining Loss:   0.274800\n",
            " Epoch: 10401 \tValidation Loss: 0.389247 \tR2: 0.630432\n",
            " Epoch: 10402 \tTraining Loss:   0.306476\n",
            " Epoch: 10402 \tValidation Loss: 0.367526 \tR2: 0.630432\n",
            " Epoch: 10403 \tTraining Loss:   0.280605\n",
            " Epoch: 10403 \tValidation Loss: 0.332909 \tR2: 0.630432\n",
            " Epoch: 10404 \tTraining Loss:   0.289475\n",
            " Epoch: 10404 \tValidation Loss: 0.329557 \tR2: 0.630432\n",
            " Epoch: 10405 \tTraining Loss:   0.277933\n",
            " Epoch: 10405 \tValidation Loss: 0.323886 \tR2: 0.630432\n",
            " Epoch: 10406 \tTraining Loss:   0.282665\n",
            " Epoch: 10406 \tValidation Loss: 0.336163 \tR2: 0.630432\n",
            " Epoch: 10407 \tTraining Loss:   0.266552\n",
            " Epoch: 10407 \tValidation Loss: 0.310387 \tR2: 0.630432\n",
            " Epoch: 10408 \tTraining Loss:   0.286560\n",
            " Epoch: 10408 \tValidation Loss: 0.358626 \tR2: 0.630432\n",
            " Epoch: 10409 \tTraining Loss:   0.303975\n",
            " Epoch: 10409 \tValidation Loss: 0.364593 \tR2: 0.630432\n",
            " Epoch: 10410 \tTraining Loss:   0.287246\n",
            " Epoch: 10410 \tValidation Loss: 0.323213 \tR2: 0.630432\n",
            " Epoch: 10411 \tTraining Loss:   0.276083\n",
            " Epoch: 10411 \tValidation Loss: 0.323667 \tR2: 0.630432\n",
            " Epoch: 10412 \tTraining Loss:   0.291616\n",
            " Epoch: 10412 \tValidation Loss: 0.334972 \tR2: 0.630432\n",
            " Epoch: 10413 \tTraining Loss:   0.279139\n",
            " Epoch: 10413 \tValidation Loss: 0.324976 \tR2: 0.630432\n",
            " Epoch: 10414 \tTraining Loss:   0.281151\n",
            " Epoch: 10414 \tValidation Loss: 0.350315 \tR2: 0.630432\n",
            " Epoch: 10415 \tTraining Loss:   0.278623\n",
            " Epoch: 10415 \tValidation Loss: 0.336970 \tR2: 0.630432\n",
            " Epoch: 10416 \tTraining Loss:   0.285116\n",
            " Epoch: 10416 \tValidation Loss: 0.351247 \tR2: 0.630432\n",
            " Epoch: 10417 \tTraining Loss:   0.282138\n",
            " Epoch: 10417 \tValidation Loss: 0.345691 \tR2: 0.630432\n",
            " Epoch: 10418 \tTraining Loss:   0.272591\n",
            " Epoch: 10418 \tValidation Loss: 0.323134 \tR2: 0.630432\n",
            " Epoch: 10419 \tTraining Loss:   0.281777\n",
            " Epoch: 10419 \tValidation Loss: 0.300423 \tR2: 0.630432\n",
            " Epoch: 10420 \tTraining Loss:   0.279115\n",
            " Epoch: 10420 \tValidation Loss: 0.331979 \tR2: 0.630432\n",
            " Epoch: 10421 \tTraining Loss:   0.281408\n",
            " Epoch: 10421 \tValidation Loss: 0.353410 \tR2: 0.630432\n",
            " Epoch: 10422 \tTraining Loss:   0.285436\n",
            " Epoch: 10422 \tValidation Loss: 0.293639 \tR2: 0.630432\n",
            " Epoch: 10423 \tTraining Loss:   0.289048\n",
            " Epoch: 10423 \tValidation Loss: 0.317789 \tR2: 0.630432\n",
            " Epoch: 10424 \tTraining Loss:   0.286193\n",
            " Epoch: 10424 \tValidation Loss: 0.327709 \tR2: 0.630432\n",
            " Epoch: 10425 \tTraining Loss:   0.267834\n",
            " Epoch: 10425 \tValidation Loss: 0.321241 \tR2: 0.630432\n",
            " Epoch: 10426 \tTraining Loss:   0.281201\n",
            " Epoch: 10426 \tValidation Loss: 0.313058 \tR2: 0.630432\n",
            " Epoch: 10427 \tTraining Loss:   0.274995\n",
            " Epoch: 10427 \tValidation Loss: 0.310287 \tR2: 0.630432\n",
            " Epoch: 10428 \tTraining Loss:   0.292795\n",
            " Epoch: 10428 \tValidation Loss: 0.378300 \tR2: 0.630432\n",
            " Epoch: 10429 \tTraining Loss:   0.275507\n",
            " Epoch: 10429 \tValidation Loss: 0.317214 \tR2: 0.630432\n",
            " Epoch: 10430 \tTraining Loss:   0.281247\n",
            " Epoch: 10430 \tValidation Loss: 0.340924 \tR2: 0.630432\n",
            " Epoch: 10431 \tTraining Loss:   0.275482\n",
            " Epoch: 10431 \tValidation Loss: 0.317622 \tR2: 0.630432\n",
            " Epoch: 10432 \tTraining Loss:   0.283121\n",
            " Epoch: 10432 \tValidation Loss: 0.279196 \tR2: 0.630432\n",
            " Epoch: 10433 \tTraining Loss:   0.275497\n",
            " Epoch: 10433 \tValidation Loss: 0.374992 \tR2: 0.630432\n",
            " Epoch: 10434 \tTraining Loss:   0.272556\n",
            " Epoch: 10434 \tValidation Loss: 0.332689 \tR2: 0.630432\n",
            " Epoch: 10435 \tTraining Loss:   0.287706\n",
            " Epoch: 10435 \tValidation Loss: 0.320679 \tR2: 0.630432\n",
            " Epoch: 10436 \tTraining Loss:   0.284583\n",
            " Epoch: 10436 \tValidation Loss: 0.321772 \tR2: 0.630432\n",
            " Epoch: 10437 \tTraining Loss:   0.272121\n",
            " Epoch: 10437 \tValidation Loss: 0.338440 \tR2: 0.630432\n",
            " Epoch: 10438 \tTraining Loss:   0.274030\n",
            " Epoch: 10438 \tValidation Loss: 0.329357 \tR2: 0.630432\n",
            " Epoch: 10439 \tTraining Loss:   0.319081\n",
            " Epoch: 10439 \tValidation Loss: 0.301002 \tR2: 0.630432\n",
            " Epoch: 10440 \tTraining Loss:   0.285302\n",
            " Epoch: 10440 \tValidation Loss: 0.406249 \tR2: 0.630432\n",
            " Epoch: 10441 \tTraining Loss:   0.282864\n",
            " Epoch: 10441 \tValidation Loss: 0.345006 \tR2: 0.630432\n",
            " Epoch: 10442 \tTraining Loss:   0.272555\n",
            " Epoch: 10442 \tValidation Loss: 0.361778 \tR2: 0.630432\n",
            " Epoch: 10443 \tTraining Loss:   0.274155\n",
            " Epoch: 10443 \tValidation Loss: 0.371527 \tR2: 0.630432\n",
            " Epoch: 10444 \tTraining Loss:   0.292176\n",
            " Epoch: 10444 \tValidation Loss: 0.356586 \tR2: 0.630432\n",
            " Epoch: 10445 \tTraining Loss:   0.279819\n",
            " Epoch: 10445 \tValidation Loss: 0.343572 \tR2: 0.630432\n",
            " Epoch: 10446 \tTraining Loss:   0.286650\n",
            " Epoch: 10446 \tValidation Loss: 0.382262 \tR2: 0.630432\n",
            " Epoch: 10447 \tTraining Loss:   0.268736\n",
            " Epoch: 10447 \tValidation Loss: 0.339105 \tR2: 0.630432\n",
            " Epoch: 10448 \tTraining Loss:   0.272324\n",
            " Epoch: 10448 \tValidation Loss: 0.334881 \tR2: 0.630432\n",
            " Epoch: 10449 \tTraining Loss:   0.283477\n",
            " Epoch: 10449 \tValidation Loss: 0.326871 \tR2: 0.630432\n",
            " Epoch: 10450 \tTraining Loss:   0.273891\n",
            " Epoch: 10450 \tValidation Loss: 0.324520 \tR2: 0.630432\n",
            " Epoch: 10451 \tTraining Loss:   0.282564\n",
            " Epoch: 10451 \tValidation Loss: 0.376633 \tR2: 0.630432\n",
            " Epoch: 10452 \tTraining Loss:   0.278735\n",
            " Epoch: 10452 \tValidation Loss: 0.366596 \tR2: 0.630432\n",
            " Epoch: 10453 \tTraining Loss:   0.291010\n",
            " Epoch: 10453 \tValidation Loss: 0.327343 \tR2: 0.630432\n",
            " Epoch: 10454 \tTraining Loss:   0.271110\n",
            " Epoch: 10454 \tValidation Loss: 0.342306 \tR2: 0.630432\n",
            " Epoch: 10455 \tTraining Loss:   0.284056\n",
            " Epoch: 10455 \tValidation Loss: 0.316559 \tR2: 0.630432\n",
            " Epoch: 10456 \tTraining Loss:   0.292478\n",
            " Epoch: 10456 \tValidation Loss: 0.333646 \tR2: 0.630432\n",
            " Epoch: 10457 \tTraining Loss:   0.271072\n",
            " Epoch: 10457 \tValidation Loss: 0.326275 \tR2: 0.630432\n",
            " Epoch: 10458 \tTraining Loss:   0.290202\n",
            " Epoch: 10458 \tValidation Loss: 0.351218 \tR2: 0.630432\n",
            " Epoch: 10459 \tTraining Loss:   0.279688\n",
            " Epoch: 10459 \tValidation Loss: 0.319467 \tR2: 0.630432\n",
            " Epoch: 10460 \tTraining Loss:   0.279927\n",
            " Epoch: 10460 \tValidation Loss: 0.380615 \tR2: 0.630432\n",
            " Epoch: 10461 \tTraining Loss:   0.270798\n",
            " Epoch: 10461 \tValidation Loss: 0.339617 \tR2: 0.630432\n",
            " Epoch: 10462 \tTraining Loss:   0.290576\n",
            " Epoch: 10462 \tValidation Loss: 0.333268 \tR2: 0.630432\n",
            " Epoch: 10463 \tTraining Loss:   0.278304\n",
            " Epoch: 10463 \tValidation Loss: 0.324009 \tR2: 0.630432\n",
            " Epoch: 10464 \tTraining Loss:   0.286821\n",
            " Epoch: 10464 \tValidation Loss: 0.323729 \tR2: 0.630432\n",
            " Epoch: 10465 \tTraining Loss:   0.284480\n",
            " Epoch: 10465 \tValidation Loss: 0.418437 \tR2: 0.630432\n",
            " Epoch: 10466 \tTraining Loss:   0.274168\n",
            " Epoch: 10466 \tValidation Loss: 0.333649 \tR2: 0.630432\n",
            " Epoch: 10467 \tTraining Loss:   0.271756\n",
            " Epoch: 10467 \tValidation Loss: 0.314086 \tR2: 0.630432\n",
            " Epoch: 10468 \tTraining Loss:   0.269909\n",
            " Epoch: 10468 \tValidation Loss: 0.329976 \tR2: 0.630432\n",
            " Epoch: 10469 \tTraining Loss:   0.287394\n",
            " Epoch: 10469 \tValidation Loss: 0.341175 \tR2: 0.630432\n",
            " Epoch: 10470 \tTraining Loss:   0.275567\n",
            " Epoch: 10470 \tValidation Loss: 0.327735 \tR2: 0.630432\n",
            " Epoch: 10471 \tTraining Loss:   0.297589\n",
            " Epoch: 10471 \tValidation Loss: 0.325196 \tR2: 0.630432\n",
            " Epoch: 10472 \tTraining Loss:   0.285079\n",
            " Epoch: 10472 \tValidation Loss: 0.346854 \tR2: 0.630432\n",
            " Epoch: 10473 \tTraining Loss:   0.281397\n",
            " Epoch: 10473 \tValidation Loss: 0.312506 \tR2: 0.630432\n",
            " Epoch: 10474 \tTraining Loss:   0.279267\n",
            " Epoch: 10474 \tValidation Loss: 0.303441 \tR2: 0.630432\n",
            " Epoch: 10475 \tTraining Loss:   0.311829\n",
            " Epoch: 10475 \tValidation Loss: 0.353502 \tR2: 0.630432\n",
            " Epoch: 10476 \tTraining Loss:   0.285766\n",
            " Epoch: 10476 \tValidation Loss: 0.336626 \tR2: 0.630432\n",
            " Epoch: 10477 \tTraining Loss:   0.284338\n",
            " Epoch: 10477 \tValidation Loss: 0.343078 \tR2: 0.630432\n",
            " Epoch: 10478 \tTraining Loss:   0.293681\n",
            " Epoch: 10478 \tValidation Loss: 0.339916 \tR2: 0.630432\n",
            " Epoch: 10479 \tTraining Loss:   0.286789\n",
            " Epoch: 10479 \tValidation Loss: 0.324645 \tR2: 0.630432\n",
            " Epoch: 10480 \tTraining Loss:   0.284589\n",
            " Epoch: 10480 \tValidation Loss: 0.329710 \tR2: 0.630432\n",
            " Epoch: 10481 \tTraining Loss:   0.286424\n",
            " Epoch: 10481 \tValidation Loss: 0.335030 \tR2: 0.630432\n",
            " Epoch: 10482 \tTraining Loss:   0.282033\n",
            " Epoch: 10482 \tValidation Loss: 0.389458 \tR2: 0.630432\n",
            " Epoch: 10483 \tTraining Loss:   0.294212\n",
            " Epoch: 10483 \tValidation Loss: 0.380861 \tR2: 0.630432\n",
            " Epoch: 10484 \tTraining Loss:   0.265205\n",
            " Epoch: 10484 \tValidation Loss: 0.313130 \tR2: 0.630432\n",
            " Epoch: 10485 \tTraining Loss:   0.280509\n",
            " Epoch: 10485 \tValidation Loss: 0.312927 \tR2: 0.630432\n",
            " Epoch: 10486 \tTraining Loss:   0.286724\n",
            " Epoch: 10486 \tValidation Loss: 0.354317 \tR2: 0.630432\n",
            " Epoch: 10487 \tTraining Loss:   0.270780\n",
            " Epoch: 10487 \tValidation Loss: 0.327049 \tR2: 0.630432\n",
            " Epoch: 10488 \tTraining Loss:   0.273623\n",
            " Epoch: 10488 \tValidation Loss: 0.320281 \tR2: 0.630432\n",
            " Epoch: 10489 \tTraining Loss:   0.277343\n",
            " Epoch: 10489 \tValidation Loss: 0.434684 \tR2: 0.630432\n",
            " Epoch: 10490 \tTraining Loss:   0.279237\n",
            " Epoch: 10490 \tValidation Loss: 0.313003 \tR2: 0.630432\n",
            " Epoch: 10491 \tTraining Loss:   0.292347\n",
            " Epoch: 10491 \tValidation Loss: 0.442252 \tR2: 0.630432\n",
            " Epoch: 10492 \tTraining Loss:   0.280143\n",
            " Epoch: 10492 \tValidation Loss: 0.327733 \tR2: 0.630432\n",
            " Epoch: 10493 \tTraining Loss:   0.284054\n",
            " Epoch: 10493 \tValidation Loss: 0.309339 \tR2: 0.630432\n",
            " Epoch: 10494 \tTraining Loss:   0.283533\n",
            " Epoch: 10494 \tValidation Loss: 0.289805 \tR2: 0.630432\n",
            " Epoch: 10495 \tTraining Loss:   0.282940\n",
            " Epoch: 10495 \tValidation Loss: 0.427096 \tR2: 0.630432\n",
            " Epoch: 10496 \tTraining Loss:   0.287324\n",
            " Epoch: 10496 \tValidation Loss: 0.329286 \tR2: 0.630432\n",
            " Epoch: 10497 \tTraining Loss:   0.277643\n",
            " Epoch: 10497 \tValidation Loss: 0.335498 \tR2: 0.630432\n",
            " Epoch: 10498 \tTraining Loss:   0.281061\n",
            " Epoch: 10498 \tValidation Loss: 0.320291 \tR2: 0.630432\n",
            " Epoch: 10499 \tTraining Loss:   0.276150\n",
            " Epoch: 10499 \tValidation Loss: 0.360073 \tR2: 0.630432\n",
            " Epoch: 10500 \tTraining Loss:   0.276860\n",
            " Epoch: 10500 \tValidation Loss: 0.305262 \tR2: 0.539609\n",
            " Epoch: 10501 \tTraining Loss:   0.278771\n",
            " Epoch: 10501 \tValidation Loss: 0.326152 \tR2: 0.539609\n",
            " Epoch: 10502 \tTraining Loss:   0.310749\n",
            " Epoch: 10502 \tValidation Loss: 0.375843 \tR2: 0.539609\n",
            " Epoch: 10503 \tTraining Loss:   0.280775\n",
            " Epoch: 10503 \tValidation Loss: 0.359798 \tR2: 0.539609\n",
            " Epoch: 10504 \tTraining Loss:   0.282772\n",
            " Epoch: 10504 \tValidation Loss: 0.329343 \tR2: 0.539609\n",
            " Epoch: 10505 \tTraining Loss:   0.277419\n",
            " Epoch: 10505 \tValidation Loss: 0.443933 \tR2: 0.539609\n",
            " Epoch: 10506 \tTraining Loss:   0.284059\n",
            " Epoch: 10506 \tValidation Loss: 0.365964 \tR2: 0.539609\n",
            " Epoch: 10507 \tTraining Loss:   0.278542\n",
            " Epoch: 10507 \tValidation Loss: 0.343673 \tR2: 0.539609\n",
            " Epoch: 10508 \tTraining Loss:   0.320544\n",
            " Epoch: 10508 \tValidation Loss: 0.336560 \tR2: 0.539609\n",
            " Epoch: 10509 \tTraining Loss:   0.275741\n",
            " Epoch: 10509 \tValidation Loss: 0.342471 \tR2: 0.539609\n",
            " Epoch: 10510 \tTraining Loss:   0.300528\n",
            " Epoch: 10510 \tValidation Loss: 0.420716 \tR2: 0.539609\n",
            " Epoch: 10511 \tTraining Loss:   0.279944\n",
            " Epoch: 10511 \tValidation Loss: 0.337391 \tR2: 0.539609\n",
            " Epoch: 10512 \tTraining Loss:   0.281883\n",
            " Epoch: 10512 \tValidation Loss: 0.295678 \tR2: 0.539609\n",
            " Epoch: 10513 \tTraining Loss:   0.281299\n",
            " Epoch: 10513 \tValidation Loss: 0.355232 \tR2: 0.539609\n",
            " Epoch: 10514 \tTraining Loss:   0.274280\n",
            " Epoch: 10514 \tValidation Loss: 0.344907 \tR2: 0.539609\n",
            " Epoch: 10515 \tTraining Loss:   0.269890\n",
            " Epoch: 10515 \tValidation Loss: 0.351772 \tR2: 0.539609\n",
            " Epoch: 10516 \tTraining Loss:   0.277877\n",
            " Epoch: 10516 \tValidation Loss: 0.348707 \tR2: 0.539609\n",
            " Epoch: 10517 \tTraining Loss:   0.280586\n",
            " Epoch: 10517 \tValidation Loss: 0.334182 \tR2: 0.539609\n",
            " Epoch: 10518 \tTraining Loss:   0.287172\n",
            " Epoch: 10518 \tValidation Loss: 0.369517 \tR2: 0.539609\n",
            " Epoch: 10519 \tTraining Loss:   0.290660\n",
            " Epoch: 10519 \tValidation Loss: 0.381030 \tR2: 0.539609\n",
            " Epoch: 10520 \tTraining Loss:   0.287165\n",
            " Epoch: 10520 \tValidation Loss: 0.329081 \tR2: 0.539609\n",
            " Epoch: 10521 \tTraining Loss:   0.286025\n",
            " Epoch: 10521 \tValidation Loss: 0.328091 \tR2: 0.539609\n",
            " Epoch: 10522 \tTraining Loss:   0.276743\n",
            " Epoch: 10522 \tValidation Loss: 0.348847 \tR2: 0.539609\n",
            " Epoch: 10523 \tTraining Loss:   0.274050\n",
            " Epoch: 10523 \tValidation Loss: 0.316947 \tR2: 0.539609\n",
            " Epoch: 10524 \tTraining Loss:   0.285559\n",
            " Epoch: 10524 \tValidation Loss: 0.319544 \tR2: 0.539609\n",
            " Epoch: 10525 \tTraining Loss:   0.292061\n",
            " Epoch: 10525 \tValidation Loss: 0.342324 \tR2: 0.539609\n",
            " Epoch: 10526 \tTraining Loss:   0.280463\n",
            " Epoch: 10526 \tValidation Loss: 0.292968 \tR2: 0.539609\n",
            " Epoch: 10527 \tTraining Loss:   0.276272\n",
            " Epoch: 10527 \tValidation Loss: 0.344164 \tR2: 0.539609\n",
            " Epoch: 10528 \tTraining Loss:   0.274944\n",
            " Epoch: 10528 \tValidation Loss: 0.314770 \tR2: 0.539609\n",
            " Epoch: 10529 \tTraining Loss:   0.282579\n",
            " Epoch: 10529 \tValidation Loss: 0.297026 \tR2: 0.539609\n",
            " Epoch: 10530 \tTraining Loss:   0.286842\n",
            " Epoch: 10530 \tValidation Loss: 0.416587 \tR2: 0.539609\n",
            " Epoch: 10531 \tTraining Loss:   0.297028\n",
            " Epoch: 10531 \tValidation Loss: 0.352154 \tR2: 0.539609\n",
            " Epoch: 10532 \tTraining Loss:   0.288636\n",
            " Epoch: 10532 \tValidation Loss: 0.320459 \tR2: 0.539609\n",
            " Epoch: 10533 \tTraining Loss:   0.296323\n",
            " Epoch: 10533 \tValidation Loss: 0.371653 \tR2: 0.539609\n",
            " Epoch: 10534 \tTraining Loss:   0.280029\n",
            " Epoch: 10534 \tValidation Loss: 0.334854 \tR2: 0.539609\n",
            " Epoch: 10535 \tTraining Loss:   0.271483\n",
            " Epoch: 10535 \tValidation Loss: 0.316280 \tR2: 0.539609\n",
            " Epoch: 10536 \tTraining Loss:   0.281900\n",
            " Epoch: 10536 \tValidation Loss: 0.375721 \tR2: 0.539609\n",
            " Epoch: 10537 \tTraining Loss:   0.272268\n",
            " Epoch: 10537 \tValidation Loss: 0.378254 \tR2: 0.539609\n",
            " Epoch: 10538 \tTraining Loss:   0.280594\n",
            " Epoch: 10538 \tValidation Loss: 0.354656 \tR2: 0.539609\n",
            " Epoch: 10539 \tTraining Loss:   0.287669\n",
            " Epoch: 10539 \tValidation Loss: 0.295685 \tR2: 0.539609\n",
            " Epoch: 10540 \tTraining Loss:   0.285349\n",
            " Epoch: 10540 \tValidation Loss: 0.295592 \tR2: 0.539609\n",
            " Epoch: 10541 \tTraining Loss:   0.272806\n",
            " Epoch: 10541 \tValidation Loss: 0.337832 \tR2: 0.539609\n",
            " Epoch: 10542 \tTraining Loss:   0.267992\n",
            " Epoch: 10542 \tValidation Loss: 0.331068 \tR2: 0.539609\n",
            " Epoch: 10543 \tTraining Loss:   0.282478\n",
            " Epoch: 10543 \tValidation Loss: 0.310330 \tR2: 0.539609\n",
            " Epoch: 10544 \tTraining Loss:   0.276062\n",
            " Epoch: 10544 \tValidation Loss: 0.309158 \tR2: 0.539609\n",
            " Epoch: 10545 \tTraining Loss:   0.292155\n",
            " Epoch: 10545 \tValidation Loss: 0.369386 \tR2: 0.539609\n",
            " Epoch: 10546 \tTraining Loss:   0.285447\n",
            " Epoch: 10546 \tValidation Loss: 0.343283 \tR2: 0.539609\n",
            " Epoch: 10547 \tTraining Loss:   0.279800\n",
            " Epoch: 10547 \tValidation Loss: 0.358227 \tR2: 0.539609\n",
            " Epoch: 10548 \tTraining Loss:   0.272478\n",
            " Epoch: 10548 \tValidation Loss: 0.321260 \tR2: 0.539609\n",
            " Epoch: 10549 \tTraining Loss:   0.275878\n",
            " Epoch: 10549 \tValidation Loss: 0.335857 \tR2: 0.539609\n",
            " Epoch: 10550 \tTraining Loss:   0.276664\n",
            " Epoch: 10550 \tValidation Loss: 0.400100 \tR2: 0.539609\n",
            " Epoch: 10551 \tTraining Loss:   0.313731\n",
            " Epoch: 10551 \tValidation Loss: 0.341876 \tR2: 0.539609\n",
            " Epoch: 10552 \tTraining Loss:   0.277689\n",
            " Epoch: 10552 \tValidation Loss: 0.307195 \tR2: 0.539609\n",
            " Epoch: 10553 \tTraining Loss:   0.286429\n",
            " Epoch: 10553 \tValidation Loss: 0.317828 \tR2: 0.539609\n",
            " Epoch: 10554 \tTraining Loss:   0.281599\n",
            " Epoch: 10554 \tValidation Loss: 0.308777 \tR2: 0.539609\n",
            " Epoch: 10555 \tTraining Loss:   0.301021\n",
            " Epoch: 10555 \tValidation Loss: 0.349670 \tR2: 0.539609\n",
            " Epoch: 10556 \tTraining Loss:   0.273701\n",
            " Epoch: 10556 \tValidation Loss: 0.299309 \tR2: 0.539609\n",
            " Epoch: 10557 \tTraining Loss:   0.269946\n",
            " Epoch: 10557 \tValidation Loss: 0.300949 \tR2: 0.539609\n",
            " Epoch: 10558 \tTraining Loss:   0.305932\n",
            " Epoch: 10558 \tValidation Loss: 0.368465 \tR2: 0.539609\n",
            " Epoch: 10559 \tTraining Loss:   0.283613\n",
            " Epoch: 10559 \tValidation Loss: 0.310173 \tR2: 0.539609\n",
            " Epoch: 10560 \tTraining Loss:   0.282553\n",
            " Epoch: 10560 \tValidation Loss: 0.644991 \tR2: 0.539609\n",
            " Epoch: 10561 \tTraining Loss:   0.296602\n",
            " Epoch: 10561 \tValidation Loss: 0.360354 \tR2: 0.539609\n",
            " Epoch: 10562 \tTraining Loss:   0.278356\n",
            " Epoch: 10562 \tValidation Loss: 0.347025 \tR2: 0.539609\n",
            " Epoch: 10563 \tTraining Loss:   0.281352\n",
            " Epoch: 10563 \tValidation Loss: 0.320815 \tR2: 0.539609\n",
            " Epoch: 10564 \tTraining Loss:   0.282204\n",
            " Epoch: 10564 \tValidation Loss: 0.329816 \tR2: 0.539609\n",
            " Epoch: 10565 \tTraining Loss:   0.280345\n",
            " Epoch: 10565 \tValidation Loss: 0.330776 \tR2: 0.539609\n",
            " Epoch: 10566 \tTraining Loss:   0.279386\n",
            " Epoch: 10566 \tValidation Loss: 0.288798 \tR2: 0.539609\n",
            " Epoch: 10567 \tTraining Loss:   0.292945\n",
            " Epoch: 10567 \tValidation Loss: 0.423364 \tR2: 0.539609\n",
            " Epoch: 10568 \tTraining Loss:   0.300670\n",
            " Epoch: 10568 \tValidation Loss: 0.329223 \tR2: 0.539609\n",
            " Epoch: 10569 \tTraining Loss:   0.279495\n",
            " Epoch: 10569 \tValidation Loss: 0.326392 \tR2: 0.539609\n",
            " Epoch: 10570 \tTraining Loss:   0.283577\n",
            " Epoch: 10570 \tValidation Loss: 0.406839 \tR2: 0.539609\n",
            " Epoch: 10571 \tTraining Loss:   0.277855\n",
            " Epoch: 10571 \tValidation Loss: 0.321694 \tR2: 0.539609\n",
            " Epoch: 10572 \tTraining Loss:   0.283610\n",
            " Epoch: 10572 \tValidation Loss: 0.580345 \tR2: 0.539609\n",
            " Epoch: 10573 \tTraining Loss:   0.270193\n",
            " Epoch: 10573 \tValidation Loss: 0.318522 \tR2: 0.539609\n",
            " Epoch: 10574 \tTraining Loss:   0.291071\n",
            " Epoch: 10574 \tValidation Loss: 0.343784 \tR2: 0.539609\n",
            " Epoch: 10575 \tTraining Loss:   0.279666\n",
            " Epoch: 10575 \tValidation Loss: 0.381514 \tR2: 0.539609\n",
            " Epoch: 10576 \tTraining Loss:   0.286898\n",
            " Epoch: 10576 \tValidation Loss: 0.340143 \tR2: 0.539609\n",
            " Epoch: 10577 \tTraining Loss:   0.281399\n",
            " Epoch: 10577 \tValidation Loss: 0.328556 \tR2: 0.539609\n",
            " Epoch: 10578 \tTraining Loss:   0.290757\n",
            " Epoch: 10578 \tValidation Loss: 0.337286 \tR2: 0.539609\n",
            " Epoch: 10579 \tTraining Loss:   0.288734\n",
            " Epoch: 10579 \tValidation Loss: 0.336301 \tR2: 0.539609\n",
            " Epoch: 10580 \tTraining Loss:   0.320469\n",
            " Epoch: 10580 \tValidation Loss: 0.344385 \tR2: 0.539609\n",
            " Epoch: 10581 \tTraining Loss:   0.302661\n",
            " Epoch: 10581 \tValidation Loss: 0.377991 \tR2: 0.539609\n",
            " Epoch: 10582 \tTraining Loss:   0.286244\n",
            " Epoch: 10582 \tValidation Loss: 0.299230 \tR2: 0.539609\n",
            " Epoch: 10583 \tTraining Loss:   0.271756\n",
            " Epoch: 10583 \tValidation Loss: 0.316413 \tR2: 0.539609\n",
            " Epoch: 10584 \tTraining Loss:   0.274677\n",
            " Epoch: 10584 \tValidation Loss: 0.373110 \tR2: 0.539609\n",
            " Epoch: 10585 \tTraining Loss:   0.291057\n",
            " Epoch: 10585 \tValidation Loss: 0.309697 \tR2: 0.539609\n",
            " Epoch: 10586 \tTraining Loss:   0.276761\n",
            " Epoch: 10586 \tValidation Loss: 2.372676 \tR2: 0.539609\n",
            " Epoch: 10587 \tTraining Loss:   0.301987\n",
            " Epoch: 10587 \tValidation Loss: 0.339950 \tR2: 0.539609\n",
            " Epoch: 10588 \tTraining Loss:   0.275689\n",
            " Epoch: 10588 \tValidation Loss: 0.321494 \tR2: 0.539609\n",
            " Epoch: 10589 \tTraining Loss:   0.278356\n",
            " Epoch: 10589 \tValidation Loss: 0.314700 \tR2: 0.539609\n",
            " Epoch: 10590 \tTraining Loss:   0.270587\n",
            " Epoch: 10590 \tValidation Loss: 0.321189 \tR2: 0.539609\n",
            " Epoch: 10591 \tTraining Loss:   0.279196\n",
            " Epoch: 10591 \tValidation Loss: 0.317168 \tR2: 0.539609\n",
            " Epoch: 10592 \tTraining Loss:   0.281541\n",
            " Epoch: 10592 \tValidation Loss: 0.380081 \tR2: 0.539609\n",
            " Epoch: 10593 \tTraining Loss:   0.271873\n",
            " Epoch: 10593 \tValidation Loss: 0.349116 \tR2: 0.539609\n",
            " Epoch: 10594 \tTraining Loss:   0.277311\n",
            " Epoch: 10594 \tValidation Loss: 0.310811 \tR2: 0.539609\n",
            " Epoch: 10595 \tTraining Loss:   0.274685\n",
            " Epoch: 10595 \tValidation Loss: 0.340683 \tR2: 0.539609\n",
            " Epoch: 10596 \tTraining Loss:   0.282100\n",
            " Epoch: 10596 \tValidation Loss: 0.307505 \tR2: 0.539609\n",
            " Epoch: 10597 \tTraining Loss:   0.293449\n",
            " Epoch: 10597 \tValidation Loss: 0.492289 \tR2: 0.539609\n",
            " Epoch: 10598 \tTraining Loss:   0.291385\n",
            " Epoch: 10598 \tValidation Loss: 0.299378 \tR2: 0.539609\n",
            " Epoch: 10599 \tTraining Loss:   0.282495\n",
            " Epoch: 10599 \tValidation Loss: 0.315994 \tR2: 0.539609\n",
            " Epoch: 10600 \tTraining Loss:   0.284799\n",
            " Epoch: 10600 \tValidation Loss: 0.373150 \tR2: -0.288924\n",
            " Epoch: 10601 \tTraining Loss:   0.291060\n",
            " Epoch: 10601 \tValidation Loss: 0.327118 \tR2: -0.288924\n",
            " Epoch: 10602 \tTraining Loss:   0.285731\n",
            " Epoch: 10602 \tValidation Loss: 0.314652 \tR2: -0.288924\n",
            " Epoch: 10603 \tTraining Loss:   0.279038\n",
            " Epoch: 10603 \tValidation Loss: 0.342207 \tR2: -0.288924\n",
            " Epoch: 10604 \tTraining Loss:   0.277040\n",
            " Epoch: 10604 \tValidation Loss: 0.323180 \tR2: -0.288924\n",
            " Epoch: 10605 \tTraining Loss:   0.288713\n",
            " Epoch: 10605 \tValidation Loss: 0.308272 \tR2: -0.288924\n",
            " Epoch: 10606 \tTraining Loss:   0.281567\n",
            " Epoch: 10606 \tValidation Loss: 0.295809 \tR2: -0.288924\n",
            " Epoch: 10607 \tTraining Loss:   0.291283\n",
            " Epoch: 10607 \tValidation Loss: 0.340632 \tR2: -0.288924\n",
            " Epoch: 10608 \tTraining Loss:   0.292210\n",
            " Epoch: 10608 \tValidation Loss: 0.409378 \tR2: -0.288924\n",
            " Epoch: 10609 \tTraining Loss:   0.273734\n",
            " Epoch: 10609 \tValidation Loss: 0.334046 \tR2: -0.288924\n",
            " Epoch: 10610 \tTraining Loss:   0.278689\n",
            " Epoch: 10610 \tValidation Loss: 0.333014 \tR2: -0.288924\n",
            " Epoch: 10611 \tTraining Loss:   0.279914\n",
            " Epoch: 10611 \tValidation Loss: 0.327576 \tR2: -0.288924\n",
            " Epoch: 10612 \tTraining Loss:   0.277094\n",
            " Epoch: 10612 \tValidation Loss: 0.302551 \tR2: -0.288924\n",
            " Epoch: 10613 \tTraining Loss:   0.269917\n",
            " Epoch: 10613 \tValidation Loss: 0.299161 \tR2: -0.288924\n",
            " Epoch: 10614 \tTraining Loss:   0.280220\n",
            " Epoch: 10614 \tValidation Loss: 0.368002 \tR2: -0.288924\n",
            " Epoch: 10615 \tTraining Loss:   0.289123\n",
            " Epoch: 10615 \tValidation Loss: 0.357164 \tR2: -0.288924\n",
            " Epoch: 10616 \tTraining Loss:   0.295161\n",
            " Epoch: 10616 \tValidation Loss: 0.335614 \tR2: -0.288924\n",
            " Epoch: 10617 \tTraining Loss:   0.283595\n",
            " Epoch: 10617 \tValidation Loss: 0.316652 \tR2: -0.288924\n",
            " Epoch: 10618 \tTraining Loss:   0.289629\n",
            " Epoch: 10618 \tValidation Loss: 0.324882 \tR2: -0.288924\n",
            " Epoch: 10619 \tTraining Loss:   0.288758\n",
            " Epoch: 10619 \tValidation Loss: 0.315999 \tR2: -0.288924\n",
            " Epoch: 10620 \tTraining Loss:   0.279404\n",
            " Epoch: 10620 \tValidation Loss: 0.319975 \tR2: -0.288924\n",
            " Epoch: 10621 \tTraining Loss:   0.281341\n",
            " Epoch: 10621 \tValidation Loss: 0.325473 \tR2: -0.288924\n",
            " Epoch: 10622 \tTraining Loss:   0.287740\n",
            " Epoch: 10622 \tValidation Loss: 0.334505 \tR2: -0.288924\n",
            " Epoch: 10623 \tTraining Loss:   0.284624\n",
            " Epoch: 10623 \tValidation Loss: 0.381505 \tR2: -0.288924\n",
            " Epoch: 10624 \tTraining Loss:   0.288782\n",
            " Epoch: 10624 \tValidation Loss: 0.429485 \tR2: -0.288924\n",
            " Epoch: 10625 \tTraining Loss:   0.288693\n",
            " Epoch: 10625 \tValidation Loss: 0.336783 \tR2: -0.288924\n",
            " Epoch: 10626 \tTraining Loss:   0.274671\n",
            " Epoch: 10626 \tValidation Loss: 0.313492 \tR2: -0.288924\n",
            " Epoch: 10627 \tTraining Loss:   0.277256\n",
            " Epoch: 10627 \tValidation Loss: 0.345086 \tR2: -0.288924\n",
            " Epoch: 10628 \tTraining Loss:   0.273951\n",
            " Epoch: 10628 \tValidation Loss: 0.334105 \tR2: -0.288924\n",
            " Epoch: 10629 \tTraining Loss:   0.272232\n",
            " Epoch: 10629 \tValidation Loss: 0.321798 \tR2: -0.288924\n",
            " Epoch: 10630 \tTraining Loss:   0.321669\n",
            " Epoch: 10630 \tValidation Loss: 7.704212 \tR2: -0.288924\n",
            " Epoch: 10631 \tTraining Loss:   0.570854\n",
            " Epoch: 10631 \tValidation Loss: 0.359600 \tR2: -0.288924\n",
            " Epoch: 10632 \tTraining Loss:   0.296788\n",
            " Epoch: 10632 \tValidation Loss: 0.339379 \tR2: -0.288924\n",
            " Epoch: 10633 \tTraining Loss:   0.275966\n",
            " Epoch: 10633 \tValidation Loss: 0.332790 \tR2: -0.288924\n",
            " Epoch: 10634 \tTraining Loss:   0.284031\n",
            " Epoch: 10634 \tValidation Loss: 0.326498 \tR2: -0.288924\n",
            " Epoch: 10635 \tTraining Loss:   0.275801\n",
            " Epoch: 10635 \tValidation Loss: 0.324950 \tR2: -0.288924\n",
            " Epoch: 10636 \tTraining Loss:   0.279962\n",
            " Epoch: 10636 \tValidation Loss: 0.338937 \tR2: -0.288924\n",
            " Epoch: 10637 \tTraining Loss:   0.276286\n",
            " Epoch: 10637 \tValidation Loss: 0.313909 \tR2: -0.288924\n",
            " Epoch: 10638 \tTraining Loss:   0.276523\n",
            " Epoch: 10638 \tValidation Loss: 0.305688 \tR2: -0.288924\n",
            " Epoch: 10639 \tTraining Loss:   0.283454\n",
            " Epoch: 10639 \tValidation Loss: 0.328354 \tR2: -0.288924\n",
            " Epoch: 10640 \tTraining Loss:   0.287978\n",
            " Epoch: 10640 \tValidation Loss: 0.356511 \tR2: -0.288924\n",
            " Epoch: 10641 \tTraining Loss:   0.289320\n",
            " Epoch: 10641 \tValidation Loss: 0.310447 \tR2: -0.288924\n",
            " Epoch: 10642 \tTraining Loss:   0.283308\n",
            " Epoch: 10642 \tValidation Loss: 0.335329 \tR2: -0.288924\n",
            " Epoch: 10643 \tTraining Loss:   0.279461\n",
            " Epoch: 10643 \tValidation Loss: 0.314954 \tR2: -0.288924\n",
            " Epoch: 10644 \tTraining Loss:   0.285704\n",
            " Epoch: 10644 \tValidation Loss: 0.336566 \tR2: -0.288924\n",
            " Epoch: 10645 \tTraining Loss:   0.296688\n",
            " Epoch: 10645 \tValidation Loss: 0.328984 \tR2: -0.288924\n",
            " Epoch: 10646 \tTraining Loss:   0.277926\n",
            " Epoch: 10646 \tValidation Loss: 0.317677 \tR2: -0.288924\n",
            " Epoch: 10647 \tTraining Loss:   0.284228\n",
            " Epoch: 10647 \tValidation Loss: 0.341730 \tR2: -0.288924\n",
            " Epoch: 10648 \tTraining Loss:   0.282035\n",
            " Epoch: 10648 \tValidation Loss: 0.323096 \tR2: -0.288924\n",
            " Epoch: 10649 \tTraining Loss:   0.281005\n",
            " Epoch: 10649 \tValidation Loss: 0.312749 \tR2: -0.288924\n",
            " Epoch: 10650 \tTraining Loss:   0.276558\n",
            " Epoch: 10650 \tValidation Loss: 0.365996 \tR2: -0.288924\n",
            " Epoch: 10651 \tTraining Loss:   0.283144\n",
            " Epoch: 10651 \tValidation Loss: 0.310472 \tR2: -0.288924\n",
            " Epoch: 10652 \tTraining Loss:   0.279670\n",
            " Epoch: 10652 \tValidation Loss: 0.321301 \tR2: -0.288924\n",
            " Epoch: 10653 \tTraining Loss:   0.280742\n",
            " Epoch: 10653 \tValidation Loss: 0.311568 \tR2: -0.288924\n",
            " Epoch: 10654 \tTraining Loss:   0.277744\n",
            " Epoch: 10654 \tValidation Loss: 0.333097 \tR2: -0.288924\n",
            " Epoch: 10655 \tTraining Loss:   0.286271\n",
            " Epoch: 10655 \tValidation Loss: 0.327976 \tR2: -0.288924\n",
            " Epoch: 10656 \tTraining Loss:   0.282132\n",
            " Epoch: 10656 \tValidation Loss: 0.348302 \tR2: -0.288924\n",
            " Epoch: 10657 \tTraining Loss:   0.277935\n",
            " Epoch: 10657 \tValidation Loss: 0.301032 \tR2: -0.288924\n",
            " Epoch: 10658 \tTraining Loss:   0.271989\n",
            " Epoch: 10658 \tValidation Loss: 0.331701 \tR2: -0.288924\n",
            " Epoch: 10659 \tTraining Loss:   0.275477\n",
            " Epoch: 10659 \tValidation Loss: 0.324849 \tR2: -0.288924\n",
            " Epoch: 10660 \tTraining Loss:   0.283659\n",
            " Epoch: 10660 \tValidation Loss: 0.335956 \tR2: -0.288924\n",
            " Epoch: 10661 \tTraining Loss:   0.286997\n",
            " Epoch: 10661 \tValidation Loss: 0.336623 \tR2: -0.288924\n",
            " Epoch: 10662 \tTraining Loss:   0.286607\n",
            " Epoch: 10662 \tValidation Loss: 0.283187 \tR2: -0.288924\n",
            " Epoch: 10663 \tTraining Loss:   0.282571\n",
            " Epoch: 10663 \tValidation Loss: 0.320289 \tR2: -0.288924\n",
            " Epoch: 10664 \tTraining Loss:   0.280337\n",
            " Epoch: 10664 \tValidation Loss: 0.304311 \tR2: -0.288924\n",
            " Epoch: 10665 \tTraining Loss:   0.281893\n",
            " Epoch: 10665 \tValidation Loss: 0.322491 \tR2: -0.288924\n",
            " Epoch: 10666 \tTraining Loss:   0.285799\n",
            " Epoch: 10666 \tValidation Loss: 0.334370 \tR2: -0.288924\n",
            " Epoch: 10667 \tTraining Loss:   0.276067\n",
            " Epoch: 10667 \tValidation Loss: 0.316368 \tR2: -0.288924\n",
            " Epoch: 10668 \tTraining Loss:   0.276769\n",
            " Epoch: 10668 \tValidation Loss: 0.397734 \tR2: -0.288924\n",
            " Epoch: 10669 \tTraining Loss:   0.286821\n",
            " Epoch: 10669 \tValidation Loss: 0.335785 \tR2: -0.288924\n",
            " Epoch: 10670 \tTraining Loss:   0.291205\n",
            " Epoch: 10670 \tValidation Loss: 0.324999 \tR2: -0.288924\n",
            " Epoch: 10671 \tTraining Loss:   0.281694\n",
            " Epoch: 10671 \tValidation Loss: 0.346242 \tR2: -0.288924\n",
            " Epoch: 10672 \tTraining Loss:   0.278894\n",
            " Epoch: 10672 \tValidation Loss: 0.321228 \tR2: -0.288924\n",
            " Epoch: 10673 \tTraining Loss:   0.281276\n",
            " Epoch: 10673 \tValidation Loss: 0.315653 \tR2: -0.288924\n",
            " Epoch: 10674 \tTraining Loss:   0.286987\n",
            " Epoch: 10674 \tValidation Loss: 0.307178 \tR2: -0.288924\n",
            " Epoch: 10675 \tTraining Loss:   0.279010\n",
            " Epoch: 10675 \tValidation Loss: 0.295067 \tR2: -0.288924\n",
            " Epoch: 10676 \tTraining Loss:   0.285385\n",
            " Epoch: 10676 \tValidation Loss: 0.323525 \tR2: -0.288924\n",
            " Epoch: 10677 \tTraining Loss:   0.300375\n",
            " Epoch: 10677 \tValidation Loss: 0.300695 \tR2: -0.288924\n",
            " Epoch: 10678 \tTraining Loss:   0.283963\n",
            " Epoch: 10678 \tValidation Loss: 0.297277 \tR2: -0.288924\n",
            " Epoch: 10679 \tTraining Loss:   0.271555\n",
            " Epoch: 10679 \tValidation Loss: 0.293332 \tR2: -0.288924\n",
            " Epoch: 10680 \tTraining Loss:   0.289130\n",
            " Epoch: 10680 \tValidation Loss: 0.326986 \tR2: -0.288924\n",
            " Epoch: 10681 \tTraining Loss:   0.271922\n",
            " Epoch: 10681 \tValidation Loss: 0.323971 \tR2: -0.288924\n",
            " Epoch: 10682 \tTraining Loss:   0.269360\n",
            " Epoch: 10682 \tValidation Loss: 0.439989 \tR2: -0.288924\n",
            " Epoch: 10683 \tTraining Loss:   0.275241\n",
            " Epoch: 10683 \tValidation Loss: 0.442855 \tR2: -0.288924\n",
            " Epoch: 10684 \tTraining Loss:   0.274578\n",
            " Epoch: 10684 \tValidation Loss: 0.316867 \tR2: -0.288924\n",
            " Epoch: 10685 \tTraining Loss:   0.281899\n",
            " Epoch: 10685 \tValidation Loss: 0.304910 \tR2: -0.288924\n",
            " Epoch: 10686 \tTraining Loss:   0.275623\n",
            " Epoch: 10686 \tValidation Loss: 0.335106 \tR2: -0.288924\n",
            " Epoch: 10687 \tTraining Loss:   0.270746\n",
            " Epoch: 10687 \tValidation Loss: 0.351188 \tR2: -0.288924\n",
            " Epoch: 10688 \tTraining Loss:   0.276194\n",
            " Epoch: 10688 \tValidation Loss: 0.307074 \tR2: -0.288924\n",
            " Epoch: 10689 \tTraining Loss:   0.265061\n",
            " Epoch: 10689 \tValidation Loss: 0.329078 \tR2: -0.288924\n",
            " Epoch: 10690 \tTraining Loss:   0.275393\n",
            " Epoch: 10690 \tValidation Loss: 0.314286 \tR2: -0.288924\n",
            " Epoch: 10691 \tTraining Loss:   0.273353\n",
            " Epoch: 10691 \tValidation Loss: 0.337348 \tR2: -0.288924\n",
            " Epoch: 10692 \tTraining Loss:   0.287150\n",
            " Epoch: 10692 \tValidation Loss: 0.335640 \tR2: -0.288924\n",
            " Epoch: 10693 \tTraining Loss:   0.287050\n",
            " Epoch: 10693 \tValidation Loss: 0.317975 \tR2: -0.288924\n",
            " Epoch: 10694 \tTraining Loss:   0.281046\n",
            " Epoch: 10694 \tValidation Loss: 0.304062 \tR2: -0.288924\n",
            " Epoch: 10695 \tTraining Loss:   0.286801\n",
            " Epoch: 10695 \tValidation Loss: 0.305669 \tR2: -0.288924\n",
            " Epoch: 10696 \tTraining Loss:   0.274994\n",
            " Epoch: 10696 \tValidation Loss: 0.402259 \tR2: -0.288924\n",
            " Epoch: 10697 \tTraining Loss:   0.290354\n",
            " Epoch: 10697 \tValidation Loss: 0.330997 \tR2: -0.288924\n",
            " Epoch: 10698 \tTraining Loss:   0.276963\n",
            " Epoch: 10698 \tValidation Loss: 0.323000 \tR2: -0.288924\n",
            " Epoch: 10699 \tTraining Loss:   0.274438\n",
            " Epoch: 10699 \tValidation Loss: 0.334961 \tR2: -0.288924\n",
            " Epoch: 10700 \tTraining Loss:   0.278683\n",
            " Epoch: 10700 \tValidation Loss: 0.322157 \tR2: 0.518237\n",
            " Epoch: 10701 \tTraining Loss:   0.279601\n",
            " Epoch: 10701 \tValidation Loss: 0.335083 \tR2: 0.518237\n",
            " Epoch: 10702 \tTraining Loss:   0.273977\n",
            " Epoch: 10702 \tValidation Loss: 0.355712 \tR2: 0.518237\n",
            " Epoch: 10703 \tTraining Loss:   0.273977\n",
            " Epoch: 10703 \tValidation Loss: 0.290163 \tR2: 0.518237\n",
            " Epoch: 10704 \tTraining Loss:   0.290191\n",
            " Epoch: 10704 \tValidation Loss: 0.366634 \tR2: 0.518237\n",
            " Epoch: 10705 \tTraining Loss:   0.275550\n",
            " Epoch: 10705 \tValidation Loss: 0.323951 \tR2: 0.518237\n",
            " Epoch: 10706 \tTraining Loss:   0.276272\n",
            " Epoch: 10706 \tValidation Loss: 0.305549 \tR2: 0.518237\n",
            " Epoch: 10707 \tTraining Loss:   0.277188\n",
            " Epoch: 10707 \tValidation Loss: 0.357833 \tR2: 0.518237\n",
            " Epoch: 10708 \tTraining Loss:   0.286831\n",
            " Epoch: 10708 \tValidation Loss: 0.389368 \tR2: 0.518237\n",
            " Epoch: 10709 \tTraining Loss:   0.267806\n",
            " Epoch: 10709 \tValidation Loss: 0.339949 \tR2: 0.518237\n",
            " Epoch: 10710 \tTraining Loss:   0.278152\n",
            " Epoch: 10710 \tValidation Loss: 0.347212 \tR2: 0.518237\n",
            " Epoch: 10711 \tTraining Loss:   0.280370\n",
            " Epoch: 10711 \tValidation Loss: 0.298272 \tR2: 0.518237\n",
            " Epoch: 10712 \tTraining Loss:   0.260061\n",
            " Epoch: 10712 \tValidation Loss: 0.314637 \tR2: 0.518237\n",
            " Epoch: 10713 \tTraining Loss:   0.281460\n",
            " Epoch: 10713 \tValidation Loss: 0.315529 \tR2: 0.518237\n",
            " Epoch: 10714 \tTraining Loss:   0.290342\n",
            " Epoch: 10714 \tValidation Loss: 0.451816 \tR2: 0.518237\n",
            " Epoch: 10715 \tTraining Loss:   0.272837\n",
            " Epoch: 10715 \tValidation Loss: 0.348526 \tR2: 0.518237\n",
            " Epoch: 10716 \tTraining Loss:   0.270814\n",
            " Epoch: 10716 \tValidation Loss: 0.325411 \tR2: 0.518237\n",
            " Epoch: 10717 \tTraining Loss:   0.276881\n",
            " Epoch: 10717 \tValidation Loss: 0.343224 \tR2: 0.518237\n",
            " Epoch: 10718 \tTraining Loss:   0.277614\n",
            " Epoch: 10718 \tValidation Loss: 0.343300 \tR2: 0.518237\n",
            " Epoch: 10719 \tTraining Loss:   0.276153\n",
            " Epoch: 10719 \tValidation Loss: 0.365139 \tR2: 0.518237\n",
            " Epoch: 10720 \tTraining Loss:   0.288892\n",
            " Epoch: 10720 \tValidation Loss: 0.451258 \tR2: 0.518237\n",
            " Epoch: 10721 \tTraining Loss:   0.285792\n",
            " Epoch: 10721 \tValidation Loss: 0.363103 \tR2: 0.518237\n",
            " Epoch: 10722 \tTraining Loss:   0.275741\n",
            " Epoch: 10722 \tValidation Loss: 0.307913 \tR2: 0.518237\n",
            " Epoch: 10723 \tTraining Loss:   0.300058\n",
            " Epoch: 10723 \tValidation Loss: 3.637888 \tR2: 0.518237\n",
            " Epoch: 10724 \tTraining Loss:   0.397599\n",
            " Epoch: 10724 \tValidation Loss: 0.375118 \tR2: 0.518237\n",
            " Epoch: 10725 \tTraining Loss:   0.287027\n",
            " Epoch: 10725 \tValidation Loss: 0.330329 \tR2: 0.518237\n",
            " Epoch: 10726 \tTraining Loss:   0.290145\n",
            " Epoch: 10726 \tValidation Loss: 0.314229 \tR2: 0.518237\n",
            " Epoch: 10727 \tTraining Loss:   0.302161\n",
            " Epoch: 10727 \tValidation Loss: 0.349977 \tR2: 0.518237\n",
            " Epoch: 10728 \tTraining Loss:   0.275537\n",
            " Epoch: 10728 \tValidation Loss: 0.333546 \tR2: 0.518237\n",
            " Epoch: 10729 \tTraining Loss:   0.304238\n",
            " Epoch: 10729 \tValidation Loss: 0.347616 \tR2: 0.518237\n",
            " Epoch: 10730 \tTraining Loss:   0.284087\n",
            " Epoch: 10730 \tValidation Loss: 0.320166 \tR2: 0.518237\n",
            " Epoch: 10731 \tTraining Loss:   0.276906\n",
            " Epoch: 10731 \tValidation Loss: 0.373984 \tR2: 0.518237\n",
            " Epoch: 10732 \tTraining Loss:   0.268809\n",
            " Epoch: 10732 \tValidation Loss: 0.340543 \tR2: 0.518237\n",
            " Epoch: 10733 \tTraining Loss:   0.277039\n",
            " Epoch: 10733 \tValidation Loss: 0.327014 \tR2: 0.518237\n",
            " Epoch: 10734 \tTraining Loss:   0.269694\n",
            " Epoch: 10734 \tValidation Loss: 0.325984 \tR2: 0.518237\n",
            " Epoch: 10735 \tTraining Loss:   0.276077\n",
            " Epoch: 10735 \tValidation Loss: 0.350827 \tR2: 0.518237\n",
            " Epoch: 10736 \tTraining Loss:   0.281837\n",
            " Epoch: 10736 \tValidation Loss: 0.396921 \tR2: 0.518237\n",
            " Epoch: 10737 \tTraining Loss:   0.274336\n",
            " Epoch: 10737 \tValidation Loss: 0.331636 \tR2: 0.518237\n",
            " Epoch: 10738 \tTraining Loss:   0.282009\n",
            " Epoch: 10738 \tValidation Loss: 0.319356 \tR2: 0.518237\n",
            " Epoch: 10739 \tTraining Loss:   0.277732\n",
            " Epoch: 10739 \tValidation Loss: 0.352696 \tR2: 0.518237\n",
            " Epoch: 10740 \tTraining Loss:   0.288183\n",
            " Epoch: 10740 \tValidation Loss: 0.336070 \tR2: 0.518237\n",
            " Epoch: 10741 \tTraining Loss:   0.294127\n",
            " Epoch: 10741 \tValidation Loss: 0.303708 \tR2: 0.518237\n",
            " Epoch: 10742 \tTraining Loss:   0.276847\n",
            " Epoch: 10742 \tValidation Loss: 0.367524 \tR2: 0.518237\n",
            " Epoch: 10743 \tTraining Loss:   0.301107\n",
            " Epoch: 10743 \tValidation Loss: 0.317836 \tR2: 0.518237\n",
            " Epoch: 10744 \tTraining Loss:   0.270400\n",
            " Epoch: 10744 \tValidation Loss: 0.351878 \tR2: 0.518237\n",
            " Epoch: 10745 \tTraining Loss:   0.270326\n",
            " Epoch: 10745 \tValidation Loss: 0.340302 \tR2: 0.518237\n",
            " Epoch: 10746 \tTraining Loss:   0.279627\n",
            " Epoch: 10746 \tValidation Loss: 0.334835 \tR2: 0.518237\n",
            " Epoch: 10747 \tTraining Loss:   0.317163\n",
            " Epoch: 10747 \tValidation Loss: 0.395040 \tR2: 0.518237\n",
            " Epoch: 10748 \tTraining Loss:   0.275222\n",
            " Epoch: 10748 \tValidation Loss: 0.336832 \tR2: 0.518237\n",
            " Epoch: 10749 \tTraining Loss:   0.285134\n",
            " Epoch: 10749 \tValidation Loss: 0.352009 \tR2: 0.518237\n",
            " Epoch: 10750 \tTraining Loss:   0.299662\n",
            " Epoch: 10750 \tValidation Loss: 0.341103 \tR2: 0.518237\n",
            " Epoch: 10751 \tTraining Loss:   0.282839\n",
            " Epoch: 10751 \tValidation Loss: 0.340726 \tR2: 0.518237\n",
            " Epoch: 10752 \tTraining Loss:   0.286974\n",
            " Epoch: 10752 \tValidation Loss: 0.322538 \tR2: 0.518237\n",
            " Epoch: 10753 \tTraining Loss:   0.291116\n",
            " Epoch: 10753 \tValidation Loss: 0.341227 \tR2: 0.518237\n",
            " Epoch: 10754 \tTraining Loss:   0.273622\n",
            " Epoch: 10754 \tValidation Loss: 0.326888 \tR2: 0.518237\n",
            " Epoch: 10755 \tTraining Loss:   0.283273\n",
            " Epoch: 10755 \tValidation Loss: 0.312663 \tR2: 0.518237\n",
            " Epoch: 10756 \tTraining Loss:   0.277633\n",
            " Epoch: 10756 \tValidation Loss: 0.330146 \tR2: 0.518237\n",
            " Epoch: 10757 \tTraining Loss:   0.279462\n",
            " Epoch: 10757 \tValidation Loss: 0.328675 \tR2: 0.518237\n",
            " Epoch: 10758 \tTraining Loss:   0.280149\n",
            " Epoch: 10758 \tValidation Loss: 0.439038 \tR2: 0.518237\n",
            " Epoch: 10759 \tTraining Loss:   0.287882\n",
            " Epoch: 10759 \tValidation Loss: 0.311101 \tR2: 0.518237\n",
            " Epoch: 10760 \tTraining Loss:   0.273580\n",
            " Epoch: 10760 \tValidation Loss: 0.329823 \tR2: 0.518237\n",
            " Epoch: 10761 \tTraining Loss:   0.277995\n",
            " Epoch: 10761 \tValidation Loss: 0.325141 \tR2: 0.518237\n",
            " Epoch: 10762 \tTraining Loss:   0.285741\n",
            " Epoch: 10762 \tValidation Loss: 0.311867 \tR2: 0.518237\n",
            " Epoch: 10763 \tTraining Loss:   0.266329\n",
            " Epoch: 10763 \tValidation Loss: 0.315306 \tR2: 0.518237\n",
            " Epoch: 10764 \tTraining Loss:   0.286729\n",
            " Epoch: 10764 \tValidation Loss: 0.347417 \tR2: 0.518237\n",
            " Epoch: 10765 \tTraining Loss:   0.289352\n",
            " Epoch: 10765 \tValidation Loss: 0.345143 \tR2: 0.518237\n",
            " Epoch: 10766 \tTraining Loss:   0.274646\n",
            " Epoch: 10766 \tValidation Loss: 0.316443 \tR2: 0.518237\n",
            " Epoch: 10767 \tTraining Loss:   0.278851\n",
            " Epoch: 10767 \tValidation Loss: 0.318502 \tR2: 0.518237\n",
            " Epoch: 10768 \tTraining Loss:   0.275509\n",
            " Epoch: 10768 \tValidation Loss: 0.318807 \tR2: 0.518237\n",
            " Epoch: 10769 \tTraining Loss:   0.273223\n",
            " Epoch: 10769 \tValidation Loss: 0.327091 \tR2: 0.518237\n",
            " Epoch: 10770 \tTraining Loss:   0.294644\n",
            " Epoch: 10770 \tValidation Loss: 3.806087 \tR2: 0.518237\n",
            " Epoch: 10771 \tTraining Loss:   0.328809\n",
            " Epoch: 10771 \tValidation Loss: 0.370804 \tR2: 0.518237\n",
            " Epoch: 10772 \tTraining Loss:   0.284912\n",
            " Epoch: 10772 \tValidation Loss: 0.342277 \tR2: 0.518237\n",
            " Epoch: 10773 \tTraining Loss:   0.290313\n",
            " Epoch: 10773 \tValidation Loss: 0.337019 \tR2: 0.518237\n",
            " Epoch: 10774 \tTraining Loss:   0.273105\n",
            " Epoch: 10774 \tValidation Loss: 0.325375 \tR2: 0.518237\n",
            " Epoch: 10775 \tTraining Loss:   0.290730\n",
            " Epoch: 10775 \tValidation Loss: 0.363611 \tR2: 0.518237\n",
            " Epoch: 10776 \tTraining Loss:   0.271380\n",
            " Epoch: 10776 \tValidation Loss: 0.374271 \tR2: 0.518237\n",
            " Epoch: 10777 \tTraining Loss:   0.288837\n",
            " Epoch: 10777 \tValidation Loss: 0.333714 \tR2: 0.518237\n",
            " Epoch: 10778 \tTraining Loss:   0.278015\n",
            " Epoch: 10778 \tValidation Loss: 0.347873 \tR2: 0.518237\n",
            " Epoch: 10779 \tTraining Loss:   0.282665\n",
            " Epoch: 10779 \tValidation Loss: 0.326710 \tR2: 0.518237\n",
            " Epoch: 10780 \tTraining Loss:   0.283995\n",
            " Epoch: 10780 \tValidation Loss: 0.330627 \tR2: 0.518237\n",
            " Epoch: 10781 \tTraining Loss:   0.303019\n",
            " Epoch: 10781 \tValidation Loss: 0.313155 \tR2: 0.518237\n",
            " Epoch: 10782 \tTraining Loss:   0.282666\n",
            " Epoch: 10782 \tValidation Loss: 0.315953 \tR2: 0.518237\n",
            " Epoch: 10783 \tTraining Loss:   0.278953\n",
            " Epoch: 10783 \tValidation Loss: 0.337502 \tR2: 0.518237\n",
            " Epoch: 10784 \tTraining Loss:   0.279633\n",
            " Epoch: 10784 \tValidation Loss: 0.331024 \tR2: 0.518237\n",
            " Epoch: 10785 \tTraining Loss:   0.285151\n",
            " Epoch: 10785 \tValidation Loss: 0.345883 \tR2: 0.518237\n",
            " Epoch: 10786 \tTraining Loss:   0.277069\n",
            " Epoch: 10786 \tValidation Loss: 0.349230 \tR2: 0.518237\n",
            " Epoch: 10787 \tTraining Loss:   0.284947\n",
            " Epoch: 10787 \tValidation Loss: 0.316252 \tR2: 0.518237\n",
            " Epoch: 10788 \tTraining Loss:   0.278031\n",
            " Epoch: 10788 \tValidation Loss: 0.332982 \tR2: 0.518237\n",
            " Epoch: 10789 \tTraining Loss:   0.276884\n",
            " Epoch: 10789 \tValidation Loss: 0.318515 \tR2: 0.518237\n",
            " Epoch: 10790 \tTraining Loss:   0.287593\n",
            " Epoch: 10790 \tValidation Loss: 0.355981 \tR2: 0.518237\n",
            " Epoch: 10791 \tTraining Loss:   0.275706\n",
            " Epoch: 10791 \tValidation Loss: 0.301379 \tR2: 0.518237\n",
            " Epoch: 10792 \tTraining Loss:   0.281345\n",
            " Epoch: 10792 \tValidation Loss: 0.339348 \tR2: 0.518237\n",
            " Epoch: 10793 \tTraining Loss:   0.275662\n",
            " Epoch: 10793 \tValidation Loss: 0.305611 \tR2: 0.518237\n",
            " Epoch: 10794 \tTraining Loss:   0.275766\n",
            " Epoch: 10794 \tValidation Loss: 0.407755 \tR2: 0.518237\n",
            " Epoch: 10795 \tTraining Loss:   0.285097\n",
            " Epoch: 10795 \tValidation Loss: 0.318250 \tR2: 0.518237\n",
            " Epoch: 10796 \tTraining Loss:   0.280432\n",
            " Epoch: 10796 \tValidation Loss: 0.538810 \tR2: 0.518237\n",
            " Epoch: 10797 \tTraining Loss:   0.278621\n",
            " Epoch: 10797 \tValidation Loss: 0.404174 \tR2: 0.518237\n",
            " Epoch: 10798 \tTraining Loss:   0.284748\n",
            " Epoch: 10798 \tValidation Loss: 0.343381 \tR2: 0.518237\n",
            " Epoch: 10799 \tTraining Loss:   0.277302\n",
            " Epoch: 10799 \tValidation Loss: 0.343041 \tR2: 0.518237\n",
            " Epoch: 10800 \tTraining Loss:   0.284385\n",
            " Epoch: 10800 \tValidation Loss: 0.308609 \tR2: 0.430405\n",
            " Epoch: 10801 \tTraining Loss:   0.279009\n",
            " Epoch: 10801 \tValidation Loss: 0.310647 \tR2: 0.430405\n",
            " Epoch: 10802 \tTraining Loss:   0.279302\n",
            " Epoch: 10802 \tValidation Loss: 0.295801 \tR2: 0.430405\n",
            " Epoch: 10803 \tTraining Loss:   0.275787\n",
            " Epoch: 10803 \tValidation Loss: 0.286536 \tR2: 0.430405\n",
            " Epoch: 10804 \tTraining Loss:   0.277304\n",
            " Epoch: 10804 \tValidation Loss: 0.317192 \tR2: 0.430405\n",
            " Epoch: 10805 \tTraining Loss:   0.274643\n",
            " Epoch: 10805 \tValidation Loss: 0.306924 \tR2: 0.430405\n",
            " Epoch: 10806 \tTraining Loss:   0.263062\n",
            " Epoch: 10806 \tValidation Loss: 0.297539 \tR2: 0.430405\n",
            " Epoch: 10807 \tTraining Loss:   0.286683\n",
            " Epoch: 10807 \tValidation Loss: 0.323469 \tR2: 0.430405\n",
            " Epoch: 10808 \tTraining Loss:   0.276172\n",
            " Epoch: 10808 \tValidation Loss: 0.314959 \tR2: 0.430405\n",
            " Epoch: 10809 \tTraining Loss:   0.283023\n",
            " Epoch: 10809 \tValidation Loss: 0.352604 \tR2: 0.430405\n",
            " Epoch: 10810 \tTraining Loss:   0.280531\n",
            " Epoch: 10810 \tValidation Loss: 0.335555 \tR2: 0.430405\n",
            " Epoch: 10811 \tTraining Loss:   0.277452\n",
            " Epoch: 10811 \tValidation Loss: 0.334072 \tR2: 0.430405\n",
            " Epoch: 10812 \tTraining Loss:   0.283157\n",
            " Epoch: 10812 \tValidation Loss: 0.330769 \tR2: 0.430405\n",
            " Epoch: 10813 \tTraining Loss:   0.286319\n",
            " Epoch: 10813 \tValidation Loss: 0.328489 \tR2: 0.430405\n",
            " Epoch: 10814 \tTraining Loss:   0.276121\n",
            " Epoch: 10814 \tValidation Loss: 0.337453 \tR2: 0.430405\n",
            " Epoch: 10815 \tTraining Loss:   0.278509\n",
            " Epoch: 10815 \tValidation Loss: 0.324553 \tR2: 0.430405\n",
            " Epoch: 10816 \tTraining Loss:   0.281172\n",
            " Epoch: 10816 \tValidation Loss: 0.310966 \tR2: 0.430405\n",
            " Epoch: 10817 \tTraining Loss:   0.272765\n",
            " Epoch: 10817 \tValidation Loss: 0.341117 \tR2: 0.430405\n",
            " Epoch: 10818 \tTraining Loss:   0.277435\n",
            " Epoch: 10818 \tValidation Loss: 0.292093 \tR2: 0.430405\n",
            " Epoch: 10819 \tTraining Loss:   0.279656\n",
            " Epoch: 10819 \tValidation Loss: 0.318402 \tR2: 0.430405\n",
            " Epoch: 10820 \tTraining Loss:   0.281977\n",
            " Epoch: 10820 \tValidation Loss: 0.322736 \tR2: 0.430405\n",
            " Epoch: 10821 \tTraining Loss:   0.266320\n",
            " Epoch: 10821 \tValidation Loss: 0.319259 \tR2: 0.430405\n",
            " Epoch: 10822 \tTraining Loss:   0.284922\n",
            " Epoch: 10822 \tValidation Loss: 0.314701 \tR2: 0.430405\n",
            " Epoch: 10823 \tTraining Loss:   0.285840\n",
            " Epoch: 10823 \tValidation Loss: 0.342844 \tR2: 0.430405\n",
            " Epoch: 10824 \tTraining Loss:   0.290754\n",
            " Epoch: 10824 \tValidation Loss: 0.326255 \tR2: 0.430405\n",
            " Epoch: 10825 \tTraining Loss:   0.271171\n",
            " Epoch: 10825 \tValidation Loss: 0.329010 \tR2: 0.430405\n",
            " Epoch: 10826 \tTraining Loss:   0.280572\n",
            " Epoch: 10826 \tValidation Loss: 0.369540 \tR2: 0.430405\n",
            " Epoch: 10827 \tTraining Loss:   0.292584\n",
            " Epoch: 10827 \tValidation Loss: 0.361868 \tR2: 0.430405\n",
            " Epoch: 10828 \tTraining Loss:   0.291567\n",
            " Epoch: 10828 \tValidation Loss: 0.326465 \tR2: 0.430405\n",
            " Epoch: 10829 \tTraining Loss:   0.282480\n",
            " Epoch: 10829 \tValidation Loss: 0.357752 \tR2: 0.430405\n",
            " Epoch: 10830 \tTraining Loss:   0.282786\n",
            " Epoch: 10830 \tValidation Loss: 0.331217 \tR2: 0.430405\n",
            " Epoch: 10831 \tTraining Loss:   0.290827\n",
            " Epoch: 10831 \tValidation Loss: 0.303229 \tR2: 0.430405\n",
            " Epoch: 10832 \tTraining Loss:   0.286172\n",
            " Epoch: 10832 \tValidation Loss: 0.408376 \tR2: 0.430405\n",
            " Epoch: 10833 \tTraining Loss:   0.284695\n",
            " Epoch: 10833 \tValidation Loss: 0.342990 \tR2: 0.430405\n",
            " Epoch: 10834 \tTraining Loss:   0.277972\n",
            " Epoch: 10834 \tValidation Loss: 0.310852 \tR2: 0.430405\n",
            " Epoch: 10835 \tTraining Loss:   0.282260\n",
            " Epoch: 10835 \tValidation Loss: 0.323921 \tR2: 0.430405\n",
            " Epoch: 10836 \tTraining Loss:   0.299779\n",
            " Epoch: 10836 \tValidation Loss: 0.368107 \tR2: 0.430405\n",
            " Epoch: 10837 \tTraining Loss:   0.273305\n",
            " Epoch: 10837 \tValidation Loss: 0.328138 \tR2: 0.430405\n",
            " Epoch: 10838 \tTraining Loss:   0.286109\n",
            " Epoch: 10838 \tValidation Loss: 0.358225 \tR2: 0.430405\n",
            " Epoch: 10839 \tTraining Loss:   0.276972\n",
            " Epoch: 10839 \tValidation Loss: 0.339498 \tR2: 0.430405\n",
            " Epoch: 10840 \tTraining Loss:   0.272068\n",
            " Epoch: 10840 \tValidation Loss: 0.313391 \tR2: 0.430405\n",
            " Epoch: 10841 \tTraining Loss:   0.301430\n",
            " Epoch: 10841 \tValidation Loss: 4.534566 \tR2: 0.430405\n",
            " Epoch: 10842 \tTraining Loss:   0.303559\n",
            " Epoch: 10842 \tValidation Loss: 0.343211 \tR2: 0.430405\n",
            " Epoch: 10843 \tTraining Loss:   0.284300\n",
            " Epoch: 10843 \tValidation Loss: 0.333418 \tR2: 0.430405\n",
            " Epoch: 10844 \tTraining Loss:   0.275898\n",
            " Epoch: 10844 \tValidation Loss: 0.329840 \tR2: 0.430405\n",
            " Epoch: 10845 \tTraining Loss:   0.284179\n",
            " Epoch: 10845 \tValidation Loss: 0.311691 \tR2: 0.430405\n",
            " Epoch: 10846 \tTraining Loss:   0.270396\n",
            " Epoch: 10846 \tValidation Loss: 0.320202 \tR2: 0.430405\n",
            " Epoch: 10847 \tTraining Loss:   0.275343\n",
            " Epoch: 10847 \tValidation Loss: 0.514320 \tR2: 0.430405\n",
            " Epoch: 10848 \tTraining Loss:   0.281664\n",
            " Epoch: 10848 \tValidation Loss: 0.323702 \tR2: 0.430405\n",
            " Epoch: 10849 \tTraining Loss:   0.269934\n",
            " Epoch: 10849 \tValidation Loss: 0.336904 \tR2: 0.430405\n",
            " Epoch: 10850 \tTraining Loss:   0.275959\n",
            " Epoch: 10850 \tValidation Loss: 0.357457 \tR2: 0.430405\n",
            " Epoch: 10851 \tTraining Loss:   0.275214\n",
            " Epoch: 10851 \tValidation Loss: 0.321221 \tR2: 0.430405\n",
            " Epoch: 10852 \tTraining Loss:   0.286587\n",
            " Epoch: 10852 \tValidation Loss: 0.325810 \tR2: 0.430405\n",
            " Epoch: 10853 \tTraining Loss:   0.285864\n",
            " Epoch: 10853 \tValidation Loss: 0.350634 \tR2: 0.430405\n",
            " Epoch: 10854 \tTraining Loss:   0.290446\n",
            " Epoch: 10854 \tValidation Loss: 0.337167 \tR2: 0.430405\n",
            " Epoch: 10855 \tTraining Loss:   0.276748\n",
            " Epoch: 10855 \tValidation Loss: 0.386720 \tR2: 0.430405\n",
            " Epoch: 10856 \tTraining Loss:   0.289077\n",
            " Epoch: 10856 \tValidation Loss: 0.410898 \tR2: 0.430405\n",
            " Epoch: 10857 \tTraining Loss:   0.276057\n",
            " Epoch: 10857 \tValidation Loss: 0.335722 \tR2: 0.430405\n",
            " Epoch: 10858 \tTraining Loss:   0.280326\n",
            " Epoch: 10858 \tValidation Loss: 0.316357 \tR2: 0.430405\n",
            " Epoch: 10859 \tTraining Loss:   0.279921\n",
            " Epoch: 10859 \tValidation Loss: 0.327750 \tR2: 0.430405\n",
            " Epoch: 10860 \tTraining Loss:   0.317211\n",
            " Epoch: 10860 \tValidation Loss: 0.361391 \tR2: 0.430405\n",
            " Epoch: 10861 \tTraining Loss:   0.299580\n",
            " Epoch: 10861 \tValidation Loss: 0.356284 \tR2: 0.430405\n",
            " Epoch: 10862 \tTraining Loss:   0.288675\n",
            " Epoch: 10862 \tValidation Loss: 0.336068 \tR2: 0.430405\n",
            " Epoch: 10863 \tTraining Loss:   0.274749\n",
            " Epoch: 10863 \tValidation Loss: 0.314522 \tR2: 0.430405\n",
            " Epoch: 10864 \tTraining Loss:   0.288930\n",
            " Epoch: 10864 \tValidation Loss: 0.346180 \tR2: 0.430405\n",
            " Epoch: 10865 \tTraining Loss:   0.277490\n",
            " Epoch: 10865 \tValidation Loss: 0.351389 \tR2: 0.430405\n",
            " Epoch: 10866 \tTraining Loss:   0.283544\n",
            " Epoch: 10866 \tValidation Loss: 0.316047 \tR2: 0.430405\n",
            " Epoch: 10867 \tTraining Loss:   0.270805\n",
            " Epoch: 10867 \tValidation Loss: 0.330173 \tR2: 0.430405\n",
            " Epoch: 10868 \tTraining Loss:   0.278216\n",
            " Epoch: 10868 \tValidation Loss: 0.335620 \tR2: 0.430405\n",
            " Epoch: 10869 \tTraining Loss:   0.283253\n",
            " Epoch: 10869 \tValidation Loss: 0.338924 \tR2: 0.430405\n",
            " Epoch: 10870 \tTraining Loss:   0.290642\n",
            " Epoch: 10870 \tValidation Loss: 0.339236 \tR2: 0.430405\n",
            " Epoch: 10871 \tTraining Loss:   0.285164\n",
            " Epoch: 10871 \tValidation Loss: 0.384519 \tR2: 0.430405\n",
            " Epoch: 10872 \tTraining Loss:   0.289000\n",
            " Epoch: 10872 \tValidation Loss: 0.358359 \tR2: 0.430405\n",
            " Epoch: 10873 \tTraining Loss:   0.281962\n",
            " Epoch: 10873 \tValidation Loss: 0.305590 \tR2: 0.430405\n",
            " Epoch: 10874 \tTraining Loss:   0.279129\n",
            " Epoch: 10874 \tValidation Loss: 0.308071 \tR2: 0.430405\n",
            " Epoch: 10875 \tTraining Loss:   0.277770\n",
            " Epoch: 10875 \tValidation Loss: 0.296766 \tR2: 0.430405\n",
            " Epoch: 10876 \tTraining Loss:   0.282163\n",
            " Epoch: 10876 \tValidation Loss: 0.365929 \tR2: 0.430405\n",
            " Epoch: 10877 \tTraining Loss:   0.283067\n",
            " Epoch: 10877 \tValidation Loss: 0.343635 \tR2: 0.430405\n",
            " Epoch: 10878 \tTraining Loss:   0.282565\n",
            " Epoch: 10878 \tValidation Loss: 0.339200 \tR2: 0.430405\n",
            " Epoch: 10879 \tTraining Loss:   0.271774\n",
            " Epoch: 10879 \tValidation Loss: 0.319138 \tR2: 0.430405\n",
            " Epoch: 10880 \tTraining Loss:   0.292225\n",
            " Epoch: 10880 \tValidation Loss: 0.321837 \tR2: 0.430405\n",
            " Epoch: 10881 \tTraining Loss:   0.277131\n",
            " Epoch: 10881 \tValidation Loss: 0.314784 \tR2: 0.430405\n",
            " Epoch: 10882 \tTraining Loss:   0.269639\n",
            " Epoch: 10882 \tValidation Loss: 0.315403 \tR2: 0.430405\n",
            " Epoch: 10883 \tTraining Loss:   0.282428\n",
            " Epoch: 10883 \tValidation Loss: 0.322990 \tR2: 0.430405\n",
            " Epoch: 10884 \tTraining Loss:   0.275757\n",
            " Epoch: 10884 \tValidation Loss: 0.333675 \tR2: 0.430405\n",
            " Epoch: 10885 \tTraining Loss:   0.280795\n",
            " Epoch: 10885 \tValidation Loss: 0.362235 \tR2: 0.430405\n",
            " Epoch: 10886 \tTraining Loss:   0.282227\n",
            " Epoch: 10886 \tValidation Loss: 0.348547 \tR2: 0.430405\n",
            " Epoch: 10887 \tTraining Loss:   0.290678\n",
            " Epoch: 10887 \tValidation Loss: 0.350748 \tR2: 0.430405\n",
            " Epoch: 10888 \tTraining Loss:   0.273420\n",
            " Epoch: 10888 \tValidation Loss: 0.316419 \tR2: 0.430405\n",
            " Epoch: 10889 \tTraining Loss:   0.269307\n",
            " Epoch: 10889 \tValidation Loss: 0.393675 \tR2: 0.430405\n",
            " Epoch: 10890 \tTraining Loss:   0.291298\n",
            " Epoch: 10890 \tValidation Loss: 0.314968 \tR2: 0.430405\n",
            " Epoch: 10891 \tTraining Loss:   0.278757\n",
            " Epoch: 10891 \tValidation Loss: 0.310815 \tR2: 0.430405\n",
            " Epoch: 10892 \tTraining Loss:   0.280984\n",
            " Epoch: 10892 \tValidation Loss: 0.330114 \tR2: 0.430405\n",
            " Epoch: 10893 \tTraining Loss:   0.297171\n",
            " Epoch: 10893 \tValidation Loss: 0.350754 \tR2: 0.430405\n",
            " Epoch: 10894 \tTraining Loss:   0.317028\n",
            " Epoch: 10894 \tValidation Loss: 0.318415 \tR2: 0.430405\n",
            " Epoch: 10895 \tTraining Loss:   0.283059\n",
            " Epoch: 10895 \tValidation Loss: 0.344047 \tR2: 0.430405\n",
            " Epoch: 10896 \tTraining Loss:   0.275482\n",
            " Epoch: 10896 \tValidation Loss: 0.312669 \tR2: 0.430405\n",
            " Epoch: 10897 \tTraining Loss:   0.269724\n",
            " Epoch: 10897 \tValidation Loss: 0.384945 \tR2: 0.430405\n",
            " Epoch: 10898 \tTraining Loss:   0.266629\n",
            " Epoch: 10898 \tValidation Loss: 0.350901 \tR2: 0.430405\n",
            " Epoch: 10899 \tTraining Loss:   0.273722\n",
            " Epoch: 10899 \tValidation Loss: 0.299960 \tR2: 0.430405\n",
            " Epoch: 10900 \tTraining Loss:   0.268101\n",
            " Epoch: 10900 \tValidation Loss: 0.321797 \tR2: 0.553098\n",
            " Epoch: 10901 \tTraining Loss:   0.271486\n",
            " Epoch: 10901 \tValidation Loss: 0.351518 \tR2: 0.553098\n",
            " Epoch: 10902 \tTraining Loss:   0.299271\n",
            " Epoch: 10902 \tValidation Loss: 0.344432 \tR2: 0.553098\n",
            " Epoch: 10903 \tTraining Loss:   0.271455\n",
            " Epoch: 10903 \tValidation Loss: 0.289075 \tR2: 0.553098\n",
            " Epoch: 10904 \tTraining Loss:   0.279424\n",
            " Epoch: 10904 \tValidation Loss: 0.313726 \tR2: 0.553098\n",
            " Epoch: 10905 \tTraining Loss:   0.277192\n",
            " Epoch: 10905 \tValidation Loss: 0.338345 \tR2: 0.553098\n",
            " Epoch: 10906 \tTraining Loss:   0.276241\n",
            " Epoch: 10906 \tValidation Loss: 0.350816 \tR2: 0.553098\n",
            " Epoch: 10907 \tTraining Loss:   0.282042\n",
            " Epoch: 10907 \tValidation Loss: 0.304334 \tR2: 0.553098\n",
            " Epoch: 10908 \tTraining Loss:   0.289458\n",
            " Epoch: 10908 \tValidation Loss: 0.322813 \tR2: 0.553098\n",
            " Epoch: 10909 \tTraining Loss:   0.281454\n",
            " Epoch: 10909 \tValidation Loss: 0.320205 \tR2: 0.553098\n",
            " Epoch: 10910 \tTraining Loss:   0.275752\n",
            " Epoch: 10910 \tValidation Loss: 0.316963 \tR2: 0.553098\n",
            " Epoch: 10911 \tTraining Loss:   0.284118\n",
            " Epoch: 10911 \tValidation Loss: 0.302702 \tR2: 0.553098\n",
            " Epoch: 10912 \tTraining Loss:   0.284550\n",
            " Epoch: 10912 \tValidation Loss: 0.290735 \tR2: 0.553098\n",
            " Epoch: 10913 \tTraining Loss:   0.267111\n",
            " Epoch: 10913 \tValidation Loss: 0.326662 \tR2: 0.553098\n",
            " Epoch: 10914 \tTraining Loss:   0.280055\n",
            " Epoch: 10914 \tValidation Loss: 0.334063 \tR2: 0.553098\n",
            " Epoch: 10915 \tTraining Loss:   0.288021\n",
            " Epoch: 10915 \tValidation Loss: 0.362693 \tR2: 0.553098\n",
            " Epoch: 10916 \tTraining Loss:   0.285789\n",
            " Epoch: 10916 \tValidation Loss: 0.339352 \tR2: 0.553098\n",
            " Epoch: 10917 \tTraining Loss:   0.285888\n",
            " Epoch: 10917 \tValidation Loss: 0.321165 \tR2: 0.553098\n",
            " Epoch: 10918 \tTraining Loss:   0.286288\n",
            " Epoch: 10918 \tValidation Loss: 0.298735 \tR2: 0.553098\n",
            " Epoch: 10919 \tTraining Loss:   0.290271\n",
            " Epoch: 10919 \tValidation Loss: 0.330597 \tR2: 0.553098\n",
            " Epoch: 10920 \tTraining Loss:   0.277997\n",
            " Epoch: 10920 \tValidation Loss: 0.314205 \tR2: 0.553098\n",
            " Epoch: 10921 \tTraining Loss:   0.287281\n",
            " Epoch: 10921 \tValidation Loss: 0.326157 \tR2: 0.553098\n",
            " Epoch: 10922 \tTraining Loss:   0.270859\n",
            " Epoch: 10922 \tValidation Loss: 0.315847 \tR2: 0.553098\n",
            " Epoch: 10923 \tTraining Loss:   0.273678\n",
            " Epoch: 10923 \tValidation Loss: 0.313716 \tR2: 0.553098\n",
            " Epoch: 10924 \tTraining Loss:   0.268797\n",
            " Epoch: 10924 \tValidation Loss: 0.304531 \tR2: 0.553098\n",
            " Epoch: 10925 \tTraining Loss:   0.270513\n",
            " Epoch: 10925 \tValidation Loss: 0.316962 \tR2: 0.553098\n",
            " Epoch: 10926 \tTraining Loss:   0.277741\n",
            " Epoch: 10926 \tValidation Loss: 0.285800 \tR2: 0.553098\n",
            " Epoch: 10927 \tTraining Loss:   0.308190\n",
            " Epoch: 10927 \tValidation Loss: 0.312653 \tR2: 0.553098\n",
            " Epoch: 10928 \tTraining Loss:   0.279195\n",
            " Epoch: 10928 \tValidation Loss: 0.284056 \tR2: 0.553098\n",
            " Epoch: 10929 \tTraining Loss:   0.282452\n",
            " Epoch: 10929 \tValidation Loss: 0.299295 \tR2: 0.553098\n",
            " Epoch: 10930 \tTraining Loss:   0.290558\n",
            " Epoch: 10930 \tValidation Loss: 0.321827 \tR2: 0.553098\n",
            " Epoch: 10931 \tTraining Loss:   0.273806\n",
            " Epoch: 10931 \tValidation Loss: 0.314603 \tR2: 0.553098\n",
            " Epoch: 10932 \tTraining Loss:   0.286909\n",
            " Epoch: 10932 \tValidation Loss: 0.305705 \tR2: 0.553098\n",
            " Epoch: 10933 \tTraining Loss:   0.274216\n",
            " Epoch: 10933 \tValidation Loss: 0.313015 \tR2: 0.553098\n",
            " Epoch: 10934 \tTraining Loss:   0.288115\n",
            " Epoch: 10934 \tValidation Loss: 0.359990 \tR2: 0.553098\n",
            " Epoch: 10935 \tTraining Loss:   0.269076\n",
            " Epoch: 10935 \tValidation Loss: 0.326956 \tR2: 0.553098\n",
            " Epoch: 10936 \tTraining Loss:   0.273564\n",
            " Epoch: 10936 \tValidation Loss: 0.297525 \tR2: 0.553098\n",
            " Epoch: 10937 \tTraining Loss:   0.281088\n",
            " Epoch: 10937 \tValidation Loss: 0.310098 \tR2: 0.553098\n",
            " Epoch: 10938 \tTraining Loss:   0.280234\n",
            " Epoch: 10938 \tValidation Loss: 0.448354 \tR2: 0.553098\n",
            " Epoch: 10939 \tTraining Loss:   0.287449\n",
            " Epoch: 10939 \tValidation Loss: 0.350881 \tR2: 0.553098\n",
            " Epoch: 10940 \tTraining Loss:   0.286519\n",
            " Epoch: 10940 \tValidation Loss: 0.293298 \tR2: 0.553098\n",
            " Epoch: 10941 \tTraining Loss:   0.277694\n",
            " Epoch: 10941 \tValidation Loss: 0.367362 \tR2: 0.553098\n",
            " Epoch: 10942 \tTraining Loss:   0.274847\n",
            " Epoch: 10942 \tValidation Loss: 0.302524 \tR2: 0.553098\n",
            " Epoch: 10943 \tTraining Loss:   0.284437\n",
            " Epoch: 10943 \tValidation Loss: 0.310848 \tR2: 0.553098\n",
            " Epoch: 10944 \tTraining Loss:   0.282291\n",
            " Epoch: 10944 \tValidation Loss: 0.342839 \tR2: 0.553098\n",
            " Epoch: 10945 \tTraining Loss:   0.284234\n",
            " Epoch: 10945 \tValidation Loss: 0.338237 \tR2: 0.553098\n",
            " Epoch: 10946 \tTraining Loss:   0.288082\n",
            " Epoch: 10946 \tValidation Loss: 0.320256 \tR2: 0.553098\n",
            " Epoch: 10947 \tTraining Loss:   0.275150\n",
            " Epoch: 10947 \tValidation Loss: 0.387934 \tR2: 0.553098\n",
            " Epoch: 10948 \tTraining Loss:   0.285232\n",
            " Epoch: 10948 \tValidation Loss: 0.337629 \tR2: 0.553098\n",
            " Epoch: 10949 \tTraining Loss:   0.284549\n",
            " Epoch: 10949 \tValidation Loss: 0.317468 \tR2: 0.553098\n",
            " Epoch: 10950 \tTraining Loss:   0.283125\n",
            " Epoch: 10950 \tValidation Loss: 0.334980 \tR2: 0.553098\n",
            " Epoch: 10951 \tTraining Loss:   0.265194\n",
            " Epoch: 10951 \tValidation Loss: 0.320210 \tR2: 0.553098\n",
            " Epoch: 10952 \tTraining Loss:   0.287072\n",
            " Epoch: 10952 \tValidation Loss: 0.305706 \tR2: 0.553098\n",
            " Epoch: 10953 \tTraining Loss:   0.278205\n",
            " Epoch: 10953 \tValidation Loss: 0.327694 \tR2: 0.553098\n",
            " Epoch: 10954 \tTraining Loss:   0.293580\n",
            " Epoch: 10954 \tValidation Loss: 0.363718 \tR2: 0.553098\n",
            " Epoch: 10955 \tTraining Loss:   0.282314\n",
            " Epoch: 10955 \tValidation Loss: 0.315678 \tR2: 0.553098\n",
            " Epoch: 10956 \tTraining Loss:   0.268620\n",
            " Epoch: 10956 \tValidation Loss: 0.338046 \tR2: 0.553098\n",
            " Epoch: 10957 \tTraining Loss:   0.284148\n",
            " Epoch: 10957 \tValidation Loss: 0.340651 \tR2: 0.553098\n",
            " Epoch: 10958 \tTraining Loss:   0.295396\n",
            " Epoch: 10958 \tValidation Loss: 0.317141 \tR2: 0.553098\n",
            " Epoch: 10959 \tTraining Loss:   0.268283\n",
            " Epoch: 10959 \tValidation Loss: 0.328456 \tR2: 0.553098\n",
            " Epoch: 10960 \tTraining Loss:   0.267570\n",
            " Epoch: 10960 \tValidation Loss: 0.319521 \tR2: 0.553098\n",
            " Epoch: 10961 \tTraining Loss:   0.268720\n",
            " Epoch: 10961 \tValidation Loss: 0.345696 \tR2: 0.553098\n",
            " Epoch: 10962 \tTraining Loss:   0.280475\n",
            " Epoch: 10962 \tValidation Loss: 0.319443 \tR2: 0.553098\n",
            " Epoch: 10963 \tTraining Loss:   0.279141\n",
            " Epoch: 10963 \tValidation Loss: 0.303523 \tR2: 0.553098\n",
            " Epoch: 10964 \tTraining Loss:   0.279630\n",
            " Epoch: 10964 \tValidation Loss: 0.304163 \tR2: 0.553098\n",
            " Epoch: 10965 \tTraining Loss:   0.316274\n",
            " Epoch: 10965 \tValidation Loss: 0.312686 \tR2: 0.553098\n",
            " Epoch: 10966 \tTraining Loss:   0.288162\n",
            " Epoch: 10966 \tValidation Loss: 0.289677 \tR2: 0.553098\n",
            " Epoch: 10967 \tTraining Loss:   0.280916\n",
            " Epoch: 10967 \tValidation Loss: 0.287923 \tR2: 0.553098\n",
            " Epoch: 10968 \tTraining Loss:   0.282897\n",
            " Epoch: 10968 \tValidation Loss: 0.328272 \tR2: 0.553098\n",
            " Epoch: 10969 \tTraining Loss:   0.315295\n",
            " Epoch: 10969 \tValidation Loss: 0.445936 \tR2: 0.553098\n",
            " Epoch: 10970 \tTraining Loss:   0.289509\n",
            " Epoch: 10970 \tValidation Loss: 0.438711 \tR2: 0.553098\n",
            " Epoch: 10971 \tTraining Loss:   0.286114\n",
            " Epoch: 10971 \tValidation Loss: 0.333608 \tR2: 0.553098\n",
            " Epoch: 10972 \tTraining Loss:   0.285235\n",
            " Epoch: 10972 \tValidation Loss: 0.508187 \tR2: 0.553098\n",
            " Epoch: 10973 \tTraining Loss:   0.282996\n",
            " Epoch: 10973 \tValidation Loss: 0.294617 \tR2: 0.553098\n",
            " Epoch: 10974 \tTraining Loss:   0.270958\n",
            " Epoch: 10974 \tValidation Loss: 0.329505 \tR2: 0.553098\n",
            " Epoch: 10975 \tTraining Loss:   0.276669\n",
            " Epoch: 10975 \tValidation Loss: 0.315311 \tR2: 0.553098\n",
            " Epoch: 10976 \tTraining Loss:   0.288887\n",
            " Epoch: 10976 \tValidation Loss: 0.324426 \tR2: 0.553098\n",
            " Epoch: 10977 \tTraining Loss:   0.286707\n",
            " Epoch: 10977 \tValidation Loss: 0.591099 \tR2: 0.553098\n",
            " Epoch: 10978 \tTraining Loss:   0.283533\n",
            " Epoch: 10978 \tValidation Loss: 0.354201 \tR2: 0.553098\n",
            " Epoch: 10979 \tTraining Loss:   0.284824\n",
            " Epoch: 10979 \tValidation Loss: 0.334476 \tR2: 0.553098\n",
            " Epoch: 10980 \tTraining Loss:   0.288230\n",
            " Epoch: 10980 \tValidation Loss: 0.387138 \tR2: 0.553098\n",
            " Epoch: 10981 \tTraining Loss:   0.289361\n",
            " Epoch: 10981 \tValidation Loss: 0.406436 \tR2: 0.553098\n",
            " Epoch: 10982 \tTraining Loss:   0.284463\n",
            " Epoch: 10982 \tValidation Loss: 0.354174 \tR2: 0.553098\n",
            " Epoch: 10983 \tTraining Loss:   0.289358\n",
            " Epoch: 10983 \tValidation Loss: 0.304573 \tR2: 0.553098\n",
            " Epoch: 10984 \tTraining Loss:   0.303556\n",
            " Epoch: 10984 \tValidation Loss: 0.300165 \tR2: 0.553098\n",
            " Epoch: 10985 \tTraining Loss:   0.269860\n",
            " Epoch: 10985 \tValidation Loss: 0.291770 \tR2: 0.553098\n",
            " Epoch: 10986 \tTraining Loss:   0.265827\n",
            " Epoch: 10986 \tValidation Loss: 0.333098 \tR2: 0.553098\n",
            " Epoch: 10987 \tTraining Loss:   0.269956\n",
            " Epoch: 10987 \tValidation Loss: 0.354948 \tR2: 0.553098\n",
            " Epoch: 10988 \tTraining Loss:   0.284831\n",
            " Epoch: 10988 \tValidation Loss: 0.328712 \tR2: 0.553098\n",
            " Epoch: 10989 \tTraining Loss:   0.280182\n",
            " Epoch: 10989 \tValidation Loss: 0.311525 \tR2: 0.553098\n",
            " Epoch: 10990 \tTraining Loss:   0.279723\n",
            " Epoch: 10990 \tValidation Loss: 0.321400 \tR2: 0.553098\n",
            " Epoch: 10991 \tTraining Loss:   0.276795\n",
            " Epoch: 10991 \tValidation Loss: 0.382661 \tR2: 0.553098\n",
            " Epoch: 10992 \tTraining Loss:   0.278780\n",
            " Epoch: 10992 \tValidation Loss: 0.323293 \tR2: 0.553098\n",
            " Epoch: 10993 \tTraining Loss:   0.281859\n",
            " Epoch: 10993 \tValidation Loss: 0.321024 \tR2: 0.553098\n",
            " Epoch: 10994 \tTraining Loss:   0.286690\n",
            " Epoch: 10994 \tValidation Loss: 0.305784 \tR2: 0.553098\n",
            " Epoch: 10995 \tTraining Loss:   0.268848\n",
            " Epoch: 10995 \tValidation Loss: 0.307667 \tR2: 0.553098\n",
            " Epoch: 10996 \tTraining Loss:   0.271276\n",
            " Epoch: 10996 \tValidation Loss: 0.342403 \tR2: 0.553098\n",
            " Epoch: 10997 \tTraining Loss:   0.277892\n",
            " Epoch: 10997 \tValidation Loss: 0.349776 \tR2: 0.553098\n",
            " Epoch: 10998 \tTraining Loss:   0.277598\n",
            " Epoch: 10998 \tValidation Loss: 0.411779 \tR2: 0.553098\n",
            " Epoch: 10999 \tTraining Loss:   0.274224\n",
            " Epoch: 10999 \tValidation Loss: 0.296558 \tR2: 0.553098\n",
            " Epoch: 11000 \tTraining Loss:   0.269152\n",
            " Epoch: 11000 \tValidation Loss: 0.353663 \tR2: 0.193517\n",
            " Epoch: 11001 \tTraining Loss:   0.278984\n",
            " Epoch: 11001 \tValidation Loss: 0.331222 \tR2: 0.193517\n",
            " Epoch: 11002 \tTraining Loss:   0.274648\n",
            " Epoch: 11002 \tValidation Loss: 0.315279 \tR2: 0.193517\n",
            " Epoch: 11003 \tTraining Loss:   0.282480\n",
            " Epoch: 11003 \tValidation Loss: 0.332805 \tR2: 0.193517\n",
            " Epoch: 11004 \tTraining Loss:   0.271523\n",
            " Epoch: 11004 \tValidation Loss: 0.331013 \tR2: 0.193517\n",
            " Epoch: 11005 \tTraining Loss:   0.305331\n",
            " Epoch: 11005 \tValidation Loss: 0.316720 \tR2: 0.193517\n",
            " Epoch: 11006 \tTraining Loss:   0.285503\n",
            " Epoch: 11006 \tValidation Loss: 0.344451 \tR2: 0.193517\n",
            " Epoch: 11007 \tTraining Loss:   0.273694\n",
            " Epoch: 11007 \tValidation Loss: 0.347264 \tR2: 0.193517\n",
            " Epoch: 11008 \tTraining Loss:   0.274642\n",
            " Epoch: 11008 \tValidation Loss: 0.400649 \tR2: 0.193517\n",
            " Epoch: 11009 \tTraining Loss:   0.280441\n",
            " Epoch: 11009 \tValidation Loss: 0.312647 \tR2: 0.193517\n",
            " Epoch: 11010 \tTraining Loss:   0.274020\n",
            " Epoch: 11010 \tValidation Loss: 0.371363 \tR2: 0.193517\n",
            " Epoch: 11011 \tTraining Loss:   0.264511\n",
            " Epoch: 11011 \tValidation Loss: 0.318830 \tR2: 0.193517\n",
            " Epoch: 11012 \tTraining Loss:   0.273897\n",
            " Epoch: 11012 \tValidation Loss: 0.333316 \tR2: 0.193517\n",
            " Epoch: 11013 \tTraining Loss:   0.277400\n",
            " Epoch: 11013 \tValidation Loss: 0.370642 \tR2: 0.193517\n",
            " Epoch: 11014 \tTraining Loss:   0.276534\n",
            " Epoch: 11014 \tValidation Loss: 0.315947 \tR2: 0.193517\n",
            " Epoch: 11015 \tTraining Loss:   0.280971\n",
            " Epoch: 11015 \tValidation Loss: 0.345147 \tR2: 0.193517\n",
            " Epoch: 11016 \tTraining Loss:   0.286913\n",
            " Epoch: 11016 \tValidation Loss: 0.347014 \tR2: 0.193517\n",
            " Epoch: 11017 \tTraining Loss:   0.272924\n",
            " Epoch: 11017 \tValidation Loss: 0.320636 \tR2: 0.193517\n",
            " Epoch: 11018 \tTraining Loss:   0.277370\n",
            " Epoch: 11018 \tValidation Loss: 0.344732 \tR2: 0.193517\n",
            " Epoch: 11019 \tTraining Loss:   0.279711\n",
            " Epoch: 11019 \tValidation Loss: 0.318344 \tR2: 0.193517\n",
            " Epoch: 11020 \tTraining Loss:   0.290546\n",
            " Epoch: 11020 \tValidation Loss: 0.527811 \tR2: 0.193517\n",
            " Epoch: 11021 \tTraining Loss:   0.294308\n",
            " Epoch: 11021 \tValidation Loss: 0.392580 \tR2: 0.193517\n",
            " Epoch: 11022 \tTraining Loss:   0.281501\n",
            " Epoch: 11022 \tValidation Loss: 0.327684 \tR2: 0.193517\n",
            " Epoch: 11023 \tTraining Loss:   0.276693\n",
            " Epoch: 11023 \tValidation Loss: 0.329831 \tR2: 0.193517\n",
            " Epoch: 11024 \tTraining Loss:   0.276746\n",
            " Epoch: 11024 \tValidation Loss: 0.425697 \tR2: 0.193517\n",
            " Epoch: 11025 \tTraining Loss:   0.288678\n",
            " Epoch: 11025 \tValidation Loss: 0.323028 \tR2: 0.193517\n",
            " Epoch: 11026 \tTraining Loss:   0.267481\n",
            " Epoch: 11026 \tValidation Loss: 0.316549 \tR2: 0.193517\n",
            " Epoch: 11027 \tTraining Loss:   0.272335\n",
            " Epoch: 11027 \tValidation Loss: 0.334309 \tR2: 0.193517\n",
            " Epoch: 11028 \tTraining Loss:   0.272100\n",
            " Epoch: 11028 \tValidation Loss: 0.321166 \tR2: 0.193517\n",
            " Epoch: 11029 \tTraining Loss:   0.277065\n",
            " Epoch: 11029 \tValidation Loss: 0.328158 \tR2: 0.193517\n",
            " Epoch: 11030 \tTraining Loss:   0.280219\n",
            " Epoch: 11030 \tValidation Loss: 0.317421 \tR2: 0.193517\n",
            " Epoch: 11031 \tTraining Loss:   0.277612\n",
            " Epoch: 11031 \tValidation Loss: 0.346859 \tR2: 0.193517\n",
            " Epoch: 11032 \tTraining Loss:   0.271306\n",
            " Epoch: 11032 \tValidation Loss: 0.303143 \tR2: 0.193517\n",
            " Epoch: 11033 \tTraining Loss:   0.292893\n",
            " Epoch: 11033 \tValidation Loss: 0.378883 \tR2: 0.193517\n",
            " Epoch: 11034 \tTraining Loss:   0.279092\n",
            " Epoch: 11034 \tValidation Loss: 0.328023 \tR2: 0.193517\n",
            " Epoch: 11035 \tTraining Loss:   0.277775\n",
            " Epoch: 11035 \tValidation Loss: 0.351852 \tR2: 0.193517\n",
            " Epoch: 11036 \tTraining Loss:   0.276160\n",
            " Epoch: 11036 \tValidation Loss: 0.314517 \tR2: 0.193517\n",
            " Epoch: 11037 \tTraining Loss:   0.278184\n",
            " Epoch: 11037 \tValidation Loss: 0.306819 \tR2: 0.193517\n",
            " Epoch: 11038 \tTraining Loss:   0.276160\n",
            " Epoch: 11038 \tValidation Loss: 0.328189 \tR2: 0.193517\n",
            " Epoch: 11039 \tTraining Loss:   0.283333\n",
            " Epoch: 11039 \tValidation Loss: 0.310400 \tR2: 0.193517\n",
            " Epoch: 11040 \tTraining Loss:   0.284913\n",
            " Epoch: 11040 \tValidation Loss: 0.310507 \tR2: 0.193517\n",
            " Epoch: 11041 \tTraining Loss:   0.270502\n",
            " Epoch: 11041 \tValidation Loss: 0.299366 \tR2: 0.193517\n",
            " Epoch: 11042 \tTraining Loss:   0.276860\n",
            " Epoch: 11042 \tValidation Loss: 0.332575 \tR2: 0.193517\n",
            " Epoch: 11043 \tTraining Loss:   0.325360\n",
            " Epoch: 11043 \tValidation Loss: 0.380256 \tR2: 0.193517\n",
            " Epoch: 11044 \tTraining Loss:   0.278056\n",
            " Epoch: 11044 \tValidation Loss: 0.308780 \tR2: 0.193517\n",
            " Epoch: 11045 \tTraining Loss:   0.276966\n",
            " Epoch: 11045 \tValidation Loss: 0.307837 \tR2: 0.193517\n",
            " Epoch: 11046 \tTraining Loss:   0.285042\n",
            " Epoch: 11046 \tValidation Loss: 0.307188 \tR2: 0.193517\n",
            " Epoch: 11047 \tTraining Loss:   0.295935\n",
            " Epoch: 11047 \tValidation Loss: 0.392094 \tR2: 0.193517\n",
            " Epoch: 11048 \tTraining Loss:   0.277357\n",
            " Epoch: 11048 \tValidation Loss: 0.330122 \tR2: 0.193517\n",
            " Epoch: 11049 \tTraining Loss:   0.285577\n",
            " Epoch: 11049 \tValidation Loss: 0.327166 \tR2: 0.193517\n",
            " Epoch: 11050 \tTraining Loss:   0.271610\n",
            " Epoch: 11050 \tValidation Loss: 0.304234 \tR2: 0.193517\n",
            " Epoch: 11051 \tTraining Loss:   0.285513\n",
            " Epoch: 11051 \tValidation Loss: 0.397929 \tR2: 0.193517\n",
            " Epoch: 11052 \tTraining Loss:   0.293924\n",
            " Epoch: 11052 \tValidation Loss: 0.368963 \tR2: 0.193517\n",
            " Epoch: 11053 \tTraining Loss:   0.273268\n",
            " Epoch: 11053 \tValidation Loss: 0.300155 \tR2: 0.193517\n",
            " Epoch: 11054 \tTraining Loss:   0.282701\n",
            " Epoch: 11054 \tValidation Loss: 0.295685 \tR2: 0.193517\n",
            " Epoch: 11055 \tTraining Loss:   0.277401\n",
            " Epoch: 11055 \tValidation Loss: 0.310620 \tR2: 0.193517\n",
            " Epoch: 11056 \tTraining Loss:   0.275699\n",
            " Epoch: 11056 \tValidation Loss: 0.315603 \tR2: 0.193517\n",
            " Epoch: 11057 \tTraining Loss:   0.283372\n",
            " Epoch: 11057 \tValidation Loss: 0.355226 \tR2: 0.193517\n",
            " Epoch: 11058 \tTraining Loss:   0.274609\n",
            " Epoch: 11058 \tValidation Loss: 0.336124 \tR2: 0.193517\n",
            " Epoch: 11059 \tTraining Loss:   0.277061\n",
            " Epoch: 11059 \tValidation Loss: 0.297874 \tR2: 0.193517\n",
            " Epoch: 11060 \tTraining Loss:   0.299098\n",
            " Epoch: 11060 \tValidation Loss: 0.381380 \tR2: 0.193517\n",
            " Epoch: 11061 \tTraining Loss:   0.277969\n",
            " Epoch: 11061 \tValidation Loss: 0.422676 \tR2: 0.193517\n",
            " Epoch: 11062 \tTraining Loss:   0.265493\n",
            " Epoch: 11062 \tValidation Loss: 0.322463 \tR2: 0.193517\n",
            " Epoch: 11063 \tTraining Loss:   0.281099\n",
            " Epoch: 11063 \tValidation Loss: 0.304069 \tR2: 0.193517\n",
            " Epoch: 11064 \tTraining Loss:   0.287492\n",
            " Epoch: 11064 \tValidation Loss: 0.329428 \tR2: 0.193517\n",
            " Epoch: 11065 \tTraining Loss:   0.302804\n",
            " Epoch: 11065 \tValidation Loss: 0.314277 \tR2: 0.193517\n",
            " Epoch: 11066 \tTraining Loss:   0.293657\n",
            " Epoch: 11066 \tValidation Loss: 0.290943 \tR2: 0.193517\n",
            " Epoch: 11067 \tTraining Loss:   0.282736\n",
            " Epoch: 11067 \tValidation Loss: 0.294241 \tR2: 0.193517\n",
            " Epoch: 11068 \tTraining Loss:   0.262057\n",
            " Epoch: 11068 \tValidation Loss: 0.357469 \tR2: 0.193517\n",
            " Epoch: 11069 \tTraining Loss:   0.284112\n",
            " Epoch: 11069 \tValidation Loss: 0.359963 \tR2: 0.193517\n",
            " Epoch: 11070 \tTraining Loss:   0.278178\n",
            " Epoch: 11070 \tValidation Loss: 0.313870 \tR2: 0.193517\n",
            " Epoch: 11071 \tTraining Loss:   0.283595\n",
            " Epoch: 11071 \tValidation Loss: 0.330794 \tR2: 0.193517\n",
            " Epoch: 11072 \tTraining Loss:   0.274770\n",
            " Epoch: 11072 \tValidation Loss: 0.328607 \tR2: 0.193517\n",
            " Epoch: 11073 \tTraining Loss:   0.277173\n",
            " Epoch: 11073 \tValidation Loss: 0.331837 \tR2: 0.193517\n",
            " Epoch: 11074 \tTraining Loss:   0.273021\n",
            " Epoch: 11074 \tValidation Loss: 0.436838 \tR2: 0.193517\n",
            " Epoch: 11075 \tTraining Loss:   0.271396\n",
            " Epoch: 11075 \tValidation Loss: 0.343080 \tR2: 0.193517\n",
            " Epoch: 11076 \tTraining Loss:   0.276188\n",
            " Epoch: 11076 \tValidation Loss: 0.306546 \tR2: 0.193517\n",
            " Epoch: 11077 \tTraining Loss:   0.278457\n",
            " Epoch: 11077 \tValidation Loss: 0.322066 \tR2: 0.193517\n",
            " Epoch: 11078 \tTraining Loss:   0.271612\n",
            " Epoch: 11078 \tValidation Loss: 0.308505 \tR2: 0.193517\n",
            " Epoch: 11079 \tTraining Loss:   0.276169\n",
            " Epoch: 11079 \tValidation Loss: 0.363927 \tR2: 0.193517\n",
            " Epoch: 11080 \tTraining Loss:   0.286759\n",
            " Epoch: 11080 \tValidation Loss: 0.320234 \tR2: 0.193517\n",
            " Epoch: 11081 \tTraining Loss:   0.276709\n",
            " Epoch: 11081 \tValidation Loss: 0.330828 \tR2: 0.193517\n",
            " Epoch: 11082 \tTraining Loss:   0.275475\n",
            " Epoch: 11082 \tValidation Loss: 0.314156 \tR2: 0.193517\n",
            " Epoch: 11083 \tTraining Loss:   0.279660\n",
            " Epoch: 11083 \tValidation Loss: 0.308880 \tR2: 0.193517\n",
            " Epoch: 11084 \tTraining Loss:   0.276903\n",
            " Epoch: 11084 \tValidation Loss: 0.356679 \tR2: 0.193517\n",
            " Epoch: 11085 \tTraining Loss:   0.276428\n",
            " Epoch: 11085 \tValidation Loss: 0.305399 \tR2: 0.193517\n",
            " Epoch: 11086 \tTraining Loss:   0.279982\n",
            " Epoch: 11086 \tValidation Loss: 0.297666 \tR2: 0.193517\n",
            " Epoch: 11087 \tTraining Loss:   0.274888\n",
            " Epoch: 11087 \tValidation Loss: 0.300545 \tR2: 0.193517\n",
            " Epoch: 11088 \tTraining Loss:   0.291557\n",
            " Epoch: 11088 \tValidation Loss: 0.327485 \tR2: 0.193517\n",
            " Epoch: 11089 \tTraining Loss:   0.265919\n",
            " Epoch: 11089 \tValidation Loss: 0.324831 \tR2: 0.193517\n",
            " Epoch: 11090 \tTraining Loss:   0.275876\n",
            " Epoch: 11090 \tValidation Loss: 0.314778 \tR2: 0.193517\n",
            " Epoch: 11091 \tTraining Loss:   0.267539\n",
            " Epoch: 11091 \tValidation Loss: 0.314289 \tR2: 0.193517\n",
            " Epoch: 11092 \tTraining Loss:   0.279869\n",
            " Epoch: 11092 \tValidation Loss: 0.295809 \tR2: 0.193517\n",
            " Epoch: 11093 \tTraining Loss:   0.277814\n",
            " Epoch: 11093 \tValidation Loss: 0.310873 \tR2: 0.193517\n",
            " Epoch: 11094 \tTraining Loss:   0.268552\n",
            " Epoch: 11094 \tValidation Loss: 0.339084 \tR2: 0.193517\n",
            " Epoch: 11095 \tTraining Loss:   0.274308\n",
            " Epoch: 11095 \tValidation Loss: 0.289259 \tR2: 0.193517\n",
            " Epoch: 11096 \tTraining Loss:   0.279031\n",
            " Epoch: 11096 \tValidation Loss: 0.395991 \tR2: 0.193517\n",
            " Epoch: 11097 \tTraining Loss:   0.286936\n",
            " Epoch: 11097 \tValidation Loss: 0.305018 \tR2: 0.193517\n",
            " Epoch: 11098 \tTraining Loss:   0.278811\n",
            " Epoch: 11098 \tValidation Loss: 0.302402 \tR2: 0.193517\n",
            " Epoch: 11099 \tTraining Loss:   0.274750\n",
            " Epoch: 11099 \tValidation Loss: 0.319127 \tR2: 0.193517\n",
            " Epoch: 11100 \tTraining Loss:   0.273554\n",
            " Epoch: 11100 \tValidation Loss: 0.329409 \tR2: 0.336874\n",
            " Epoch: 11101 \tTraining Loss:   0.275182\n",
            " Epoch: 11101 \tValidation Loss: 0.330293 \tR2: 0.336874\n",
            " Epoch: 11102 \tTraining Loss:   0.287258\n",
            " Epoch: 11102 \tValidation Loss: 0.362648 \tR2: 0.336874\n",
            " Epoch: 11103 \tTraining Loss:   0.280767\n",
            " Epoch: 11103 \tValidation Loss: 0.347075 \tR2: 0.336874\n",
            " Epoch: 11104 \tTraining Loss:   0.277024\n",
            " Epoch: 11104 \tValidation Loss: 0.331938 \tR2: 0.336874\n",
            " Epoch: 11105 \tTraining Loss:   0.275354\n",
            " Epoch: 11105 \tValidation Loss: 0.331805 \tR2: 0.336874\n",
            " Epoch: 11106 \tTraining Loss:   0.283508\n",
            " Epoch: 11106 \tValidation Loss: 0.315092 \tR2: 0.336874\n",
            " Epoch: 11107 \tTraining Loss:   0.273163\n",
            " Epoch: 11107 \tValidation Loss: 0.332969 \tR2: 0.336874\n",
            " Epoch: 11108 \tTraining Loss:   0.305255\n",
            " Epoch: 11108 \tValidation Loss: 0.327759 \tR2: 0.336874\n",
            " Epoch: 11109 \tTraining Loss:   0.277446\n",
            " Epoch: 11109 \tValidation Loss: 0.413914 \tR2: 0.336874\n",
            " Epoch: 11110 \tTraining Loss:   0.272433\n",
            " Epoch: 11110 \tValidation Loss: 0.326998 \tR2: 0.336874\n",
            " Epoch: 11111 \tTraining Loss:   0.278606\n",
            " Epoch: 11111 \tValidation Loss: 0.400955 \tR2: 0.336874\n",
            " Epoch: 11112 \tTraining Loss:   0.276718\n",
            " Epoch: 11112 \tValidation Loss: 0.371134 \tR2: 0.336874\n",
            " Epoch: 11113 \tTraining Loss:   0.292320\n",
            " Epoch: 11113 \tValidation Loss: 0.325951 \tR2: 0.336874\n",
            " Epoch: 11114 \tTraining Loss:   0.282594\n",
            " Epoch: 11114 \tValidation Loss: 0.326707 \tR2: 0.336874\n",
            " Epoch: 11115 \tTraining Loss:   0.281565\n",
            " Epoch: 11115 \tValidation Loss: 0.321100 \tR2: 0.336874\n",
            " Epoch: 11116 \tTraining Loss:   0.283318\n",
            " Epoch: 11116 \tValidation Loss: 0.308210 \tR2: 0.336874\n",
            " Epoch: 11117 \tTraining Loss:   0.272594\n",
            " Epoch: 11117 \tValidation Loss: 0.335250 \tR2: 0.336874\n",
            " Epoch: 11118 \tTraining Loss:   0.272915\n",
            " Epoch: 11118 \tValidation Loss: 0.311021 \tR2: 0.336874\n",
            " Epoch: 11119 \tTraining Loss:   0.289711\n",
            " Epoch: 11119 \tValidation Loss: 0.317427 \tR2: 0.336874\n",
            " Epoch: 11120 \tTraining Loss:   0.278705\n",
            " Epoch: 11120 \tValidation Loss: 0.406439 \tR2: 0.336874\n",
            " Epoch: 11121 \tTraining Loss:   0.275552\n",
            " Epoch: 11121 \tValidation Loss: 0.310899 \tR2: 0.336874\n",
            " Epoch: 11122 \tTraining Loss:   0.279703\n",
            " Epoch: 11122 \tValidation Loss: 0.322304 \tR2: 0.336874\n",
            " Epoch: 11123 \tTraining Loss:   0.270982\n",
            " Epoch: 11123 \tValidation Loss: 0.305462 \tR2: 0.336874\n",
            " Epoch: 11124 \tTraining Loss:   0.267118\n",
            " Epoch: 11124 \tValidation Loss: 0.320177 \tR2: 0.336874\n",
            " Epoch: 11125 \tTraining Loss:   0.280699\n",
            " Epoch: 11125 \tValidation Loss: 0.334288 \tR2: 0.336874\n",
            " Epoch: 11126 \tTraining Loss:   0.286196\n",
            " Epoch: 11126 \tValidation Loss: 0.339991 \tR2: 0.336874\n",
            " Epoch: 11127 \tTraining Loss:   0.274690\n",
            " Epoch: 11127 \tValidation Loss: 0.318663 \tR2: 0.336874\n",
            " Epoch: 11128 \tTraining Loss:   0.271977\n",
            " Epoch: 11128 \tValidation Loss: 0.329440 \tR2: 0.336874\n",
            " Epoch: 11129 \tTraining Loss:   0.292279\n",
            " Epoch: 11129 \tValidation Loss: 0.329279 \tR2: 0.336874\n",
            " Epoch: 11130 \tTraining Loss:   0.275568\n",
            " Epoch: 11130 \tValidation Loss: 0.314435 \tR2: 0.336874\n",
            " Epoch: 11131 \tTraining Loss:   0.284117\n",
            " Epoch: 11131 \tValidation Loss: 0.344256 \tR2: 0.336874\n",
            " Epoch: 11132 \tTraining Loss:   0.285405\n",
            " Epoch: 11132 \tValidation Loss: 0.341684 \tR2: 0.336874\n",
            " Epoch: 11133 \tTraining Loss:   0.288765\n",
            " Epoch: 11133 \tValidation Loss: 0.323950 \tR2: 0.336874\n",
            " Epoch: 11134 \tTraining Loss:   0.278224\n",
            " Epoch: 11134 \tValidation Loss: 0.327020 \tR2: 0.336874\n",
            " Epoch: 11135 \tTraining Loss:   0.282039\n",
            " Epoch: 11135 \tValidation Loss: 0.318061 \tR2: 0.336874\n",
            " Epoch: 11136 \tTraining Loss:   0.280561\n",
            " Epoch: 11136 \tValidation Loss: 0.314868 \tR2: 0.336874\n",
            " Epoch: 11137 \tTraining Loss:   0.280337\n",
            " Epoch: 11137 \tValidation Loss: 0.302656 \tR2: 0.336874\n",
            " Epoch: 11138 \tTraining Loss:   0.272499\n",
            " Epoch: 11138 \tValidation Loss: 0.320447 \tR2: 0.336874\n",
            " Epoch: 11139 \tTraining Loss:   0.279355\n",
            " Epoch: 11139 \tValidation Loss: 0.432040 \tR2: 0.336874\n",
            " Epoch: 11140 \tTraining Loss:   0.268692\n",
            " Epoch: 11140 \tValidation Loss: 0.316232 \tR2: 0.336874\n",
            " Epoch: 11141 \tTraining Loss:   0.298616\n",
            " Epoch: 11141 \tValidation Loss: 0.359082 \tR2: 0.336874\n",
            " Epoch: 11142 \tTraining Loss:   0.274971\n",
            " Epoch: 11142 \tValidation Loss: 0.327765 \tR2: 0.336874\n",
            " Epoch: 11143 \tTraining Loss:   0.280871\n",
            " Epoch: 11143 \tValidation Loss: 0.305947 \tR2: 0.336874\n",
            " Epoch: 11144 \tTraining Loss:   0.265387\n",
            " Epoch: 11144 \tValidation Loss: 0.336522 \tR2: 0.336874\n",
            " Epoch: 11145 \tTraining Loss:   0.284259\n",
            " Epoch: 11145 \tValidation Loss: 0.357507 \tR2: 0.336874\n",
            " Epoch: 11146 \tTraining Loss:   0.298429\n",
            " Epoch: 11146 \tValidation Loss: 0.325004 \tR2: 0.336874\n",
            " Epoch: 11147 \tTraining Loss:   0.281473\n",
            " Epoch: 11147 \tValidation Loss: 0.315189 \tR2: 0.336874\n",
            " Epoch: 11148 \tTraining Loss:   0.339236\n",
            " Epoch: 11148 \tValidation Loss: 0.330038 \tR2: 0.336874\n",
            " Epoch: 11149 \tTraining Loss:   0.283881\n",
            " Epoch: 11149 \tValidation Loss: 0.347231 \tR2: 0.336874\n",
            " Epoch: 11150 \tTraining Loss:   0.286338\n",
            " Epoch: 11150 \tValidation Loss: 0.403142 \tR2: 0.336874\n",
            " Epoch: 11151 \tTraining Loss:   0.284146\n",
            " Epoch: 11151 \tValidation Loss: 0.398733 \tR2: 0.336874\n",
            " Epoch: 11152 \tTraining Loss:   0.292343\n",
            " Epoch: 11152 \tValidation Loss: 0.337437 \tR2: 0.336874\n",
            " Epoch: 11153 \tTraining Loss:   0.283877\n",
            " Epoch: 11153 \tValidation Loss: 0.307922 \tR2: 0.336874\n",
            " Epoch: 11154 \tTraining Loss:   0.275775\n",
            " Epoch: 11154 \tValidation Loss: 0.338559 \tR2: 0.336874\n",
            " Epoch: 11155 \tTraining Loss:   0.283984\n",
            " Epoch: 11155 \tValidation Loss: 0.332956 \tR2: 0.336874\n",
            " Epoch: 11156 \tTraining Loss:   0.289669\n",
            " Epoch: 11156 \tValidation Loss: 0.311722 \tR2: 0.336874\n",
            " Epoch: 11157 \tTraining Loss:   0.274292\n",
            " Epoch: 11157 \tValidation Loss: 0.411225 \tR2: 0.336874\n",
            " Epoch: 11158 \tTraining Loss:   0.280326\n",
            " Epoch: 11158 \tValidation Loss: 0.350614 \tR2: 0.336874\n",
            " Epoch: 11159 \tTraining Loss:   0.285245\n",
            " Epoch: 11159 \tValidation Loss: 0.320444 \tR2: 0.336874\n",
            " Epoch: 11160 \tTraining Loss:   0.278633\n",
            " Epoch: 11160 \tValidation Loss: 0.313662 \tR2: 0.336874\n",
            " Epoch: 11161 \tTraining Loss:   0.277969\n",
            " Epoch: 11161 \tValidation Loss: 0.295548 \tR2: 0.336874\n",
            " Epoch: 11162 \tTraining Loss:   0.277282\n",
            " Epoch: 11162 \tValidation Loss: 0.342127 \tR2: 0.336874\n",
            " Epoch: 11163 \tTraining Loss:   0.273538\n",
            " Epoch: 11163 \tValidation Loss: 0.306548 \tR2: 0.336874\n",
            " Epoch: 11164 \tTraining Loss:   0.279736\n",
            " Epoch: 11164 \tValidation Loss: 0.324313 \tR2: 0.336874\n",
            " Epoch: 11165 \tTraining Loss:   0.277419\n",
            " Epoch: 11165 \tValidation Loss: 0.341508 \tR2: 0.336874\n",
            " Epoch: 11166 \tTraining Loss:   0.276775\n",
            " Epoch: 11166 \tValidation Loss: 0.331375 \tR2: 0.336874\n",
            " Epoch: 11167 \tTraining Loss:   0.279227\n",
            " Epoch: 11167 \tValidation Loss: 0.340968 \tR2: 0.336874\n",
            " Epoch: 11168 \tTraining Loss:   0.283067\n",
            " Epoch: 11168 \tValidation Loss: 0.310870 \tR2: 0.336874\n",
            " Epoch: 11169 \tTraining Loss:   0.277317\n",
            " Epoch: 11169 \tValidation Loss: 0.318607 \tR2: 0.336874\n",
            " Epoch: 11170 \tTraining Loss:   0.278537\n",
            " Epoch: 11170 \tValidation Loss: 0.392456 \tR2: 0.336874\n",
            " Epoch: 11171 \tTraining Loss:   0.280324\n",
            " Epoch: 11171 \tValidation Loss: 0.318904 \tR2: 0.336874\n",
            " Epoch: 11172 \tTraining Loss:   0.285124\n",
            " Epoch: 11172 \tValidation Loss: 0.298288 \tR2: 0.336874\n",
            " Epoch: 11173 \tTraining Loss:   0.279731\n",
            " Epoch: 11173 \tValidation Loss: 0.351562 \tR2: 0.336874\n",
            " Epoch: 11174 \tTraining Loss:   0.272902\n",
            " Epoch: 11174 \tValidation Loss: 0.315517 \tR2: 0.336874\n",
            " Epoch: 11175 \tTraining Loss:   0.282751\n",
            " Epoch: 11175 \tValidation Loss: 0.339268 \tR2: 0.336874\n",
            " Epoch: 11176 \tTraining Loss:   0.269746\n",
            " Epoch: 11176 \tValidation Loss: 0.372347 \tR2: 0.336874\n",
            " Epoch: 11177 \tTraining Loss:   0.270002\n",
            " Epoch: 11177 \tValidation Loss: 0.329775 \tR2: 0.336874\n",
            " Epoch: 11178 \tTraining Loss:   0.274200\n",
            " Epoch: 11178 \tValidation Loss: 0.312206 \tR2: 0.336874\n",
            " Epoch: 11179 \tTraining Loss:   0.300384\n",
            " Epoch: 11179 \tValidation Loss: 0.382490 \tR2: 0.336874\n",
            " Epoch: 11180 \tTraining Loss:   0.275648\n",
            " Epoch: 11180 \tValidation Loss: 0.318435 \tR2: 0.336874\n",
            " Epoch: 11181 \tTraining Loss:   0.275191\n",
            " Epoch: 11181 \tValidation Loss: 0.300954 \tR2: 0.336874\n",
            " Epoch: 11182 \tTraining Loss:   0.300522\n",
            " Epoch: 11182 \tValidation Loss: 0.375896 \tR2: 0.336874\n",
            " Epoch: 11183 \tTraining Loss:   0.276638\n",
            " Epoch: 11183 \tValidation Loss: 0.317689 \tR2: 0.336874\n",
            " Epoch: 11184 \tTraining Loss:   0.294100\n",
            " Epoch: 11184 \tValidation Loss: 0.339467 \tR2: 0.336874\n",
            " Epoch: 11185 \tTraining Loss:   0.280464\n",
            " Epoch: 11185 \tValidation Loss: 0.295613 \tR2: 0.336874\n",
            " Epoch: 11186 \tTraining Loss:   0.281253\n",
            " Epoch: 11186 \tValidation Loss: 0.341380 \tR2: 0.336874\n",
            " Epoch: 11187 \tTraining Loss:   0.273416\n",
            " Epoch: 11187 \tValidation Loss: 0.342374 \tR2: 0.336874\n",
            " Epoch: 11188 \tTraining Loss:   0.277739\n",
            " Epoch: 11188 \tValidation Loss: 0.313848 \tR2: 0.336874\n",
            " Epoch: 11189 \tTraining Loss:   0.291725\n",
            " Epoch: 11189 \tValidation Loss: 0.325001 \tR2: 0.336874\n",
            " Epoch: 11190 \tTraining Loss:   0.273549\n",
            " Epoch: 11190 \tValidation Loss: 0.327257 \tR2: 0.336874\n",
            " Epoch: 11191 \tTraining Loss:   0.282238\n",
            " Epoch: 11191 \tValidation Loss: 0.321993 \tR2: 0.336874\n",
            " Epoch: 11192 \tTraining Loss:   0.273341\n",
            " Epoch: 11192 \tValidation Loss: 0.325448 \tR2: 0.336874\n",
            " Epoch: 11193 \tTraining Loss:   0.285222\n",
            " Epoch: 11193 \tValidation Loss: 0.290373 \tR2: 0.336874\n",
            " Epoch: 11194 \tTraining Loss:   0.277640\n",
            " Epoch: 11194 \tValidation Loss: 0.320829 \tR2: 0.336874\n",
            " Epoch: 11195 \tTraining Loss:   0.276144\n",
            " Epoch: 11195 \tValidation Loss: 0.312312 \tR2: 0.336874\n",
            " Epoch: 11196 \tTraining Loss:   0.283519\n",
            " Epoch: 11196 \tValidation Loss: 0.336576 \tR2: 0.336874\n",
            " Epoch: 11197 \tTraining Loss:   0.276112\n",
            " Epoch: 11197 \tValidation Loss: 0.323389 \tR2: 0.336874\n",
            " Epoch: 11198 \tTraining Loss:   0.278411\n",
            " Epoch: 11198 \tValidation Loss: 0.337917 \tR2: 0.336874\n",
            " Epoch: 11199 \tTraining Loss:   0.289359\n",
            " Epoch: 11199 \tValidation Loss: 0.317372 \tR2: 0.336874\n",
            " Epoch: 11200 \tTraining Loss:   0.283367\n",
            " Epoch: 11200 \tValidation Loss: 0.323343 \tR2: 0.596873\n",
            " Epoch: 11201 \tTraining Loss:   0.270477\n",
            " Epoch: 11201 \tValidation Loss: 0.353384 \tR2: 0.596873\n",
            " Epoch: 11202 \tTraining Loss:   0.274487\n",
            " Epoch: 11202 \tValidation Loss: 0.299916 \tR2: 0.596873\n",
            " Epoch: 11203 \tTraining Loss:   0.271624\n",
            " Epoch: 11203 \tValidation Loss: 0.332775 \tR2: 0.596873\n",
            " Epoch: 11204 \tTraining Loss:   0.271395\n",
            " Epoch: 11204 \tValidation Loss: 0.357744 \tR2: 0.596873\n",
            " Epoch: 11205 \tTraining Loss:   0.271760\n",
            " Epoch: 11205 \tValidation Loss: 0.319588 \tR2: 0.596873\n",
            " Epoch: 11206 \tTraining Loss:   0.268609\n",
            " Epoch: 11206 \tValidation Loss: 0.336520 \tR2: 0.596873\n",
            " Epoch: 11207 \tTraining Loss:   0.278389\n",
            " Epoch: 11207 \tValidation Loss: 0.295465 \tR2: 0.596873\n",
            " Epoch: 11208 \tTraining Loss:   0.267931\n",
            " Epoch: 11208 \tValidation Loss: 0.347943 \tR2: 0.596873\n",
            " Epoch: 11209 \tTraining Loss:   0.269823\n",
            " Epoch: 11209 \tValidation Loss: 0.307384 \tR2: 0.596873\n",
            " Epoch: 11210 \tTraining Loss:   0.282476\n",
            " Epoch: 11210 \tValidation Loss: 0.319775 \tR2: 0.596873\n",
            " Epoch: 11211 \tTraining Loss:   0.277641\n",
            " Epoch: 11211 \tValidation Loss: 0.327117 \tR2: 0.596873\n",
            " Epoch: 11212 \tTraining Loss:   0.276927\n",
            " Epoch: 11212 \tValidation Loss: 0.313419 \tR2: 0.596873\n",
            " Epoch: 11213 \tTraining Loss:   0.270424\n",
            " Epoch: 11213 \tValidation Loss: 0.395222 \tR2: 0.596873\n",
            " Epoch: 11214 \tTraining Loss:   0.277522\n",
            " Epoch: 11214 \tValidation Loss: 0.371363 \tR2: 0.596873\n",
            " Epoch: 11215 \tTraining Loss:   0.282804\n",
            " Epoch: 11215 \tValidation Loss: 0.332277 \tR2: 0.596873\n",
            " Epoch: 11216 \tTraining Loss:   0.284319\n",
            " Epoch: 11216 \tValidation Loss: 0.325148 \tR2: 0.596873\n",
            " Epoch: 11217 \tTraining Loss:   0.261807\n",
            " Epoch: 11217 \tValidation Loss: 0.342390 \tR2: 0.596873\n",
            " Epoch: 11218 \tTraining Loss:   0.280238\n",
            " Epoch: 11218 \tValidation Loss: 0.385989 \tR2: 0.596873\n",
            " Epoch: 11219 \tTraining Loss:   0.273568\n",
            " Epoch: 11219 \tValidation Loss: 0.350126 \tR2: 0.596873\n",
            " Epoch: 11220 \tTraining Loss:   0.309054\n",
            " Epoch: 11220 \tValidation Loss: 0.380913 \tR2: 0.596873\n",
            " Epoch: 11221 \tTraining Loss:   0.274185\n",
            " Epoch: 11221 \tValidation Loss: 0.356083 \tR2: 0.596873\n",
            " Epoch: 11222 \tTraining Loss:   0.280090\n",
            " Epoch: 11222 \tValidation Loss: 0.381413 \tR2: 0.596873\n",
            " Epoch: 11223 \tTraining Loss:   0.270896\n",
            " Epoch: 11223 \tValidation Loss: 0.329325 \tR2: 0.596873\n",
            " Epoch: 11224 \tTraining Loss:   0.282273\n",
            " Epoch: 11224 \tValidation Loss: 0.327030 \tR2: 0.596873\n",
            " Epoch: 11225 \tTraining Loss:   0.288238\n",
            " Epoch: 11225 \tValidation Loss: 0.324825 \tR2: 0.596873\n",
            " Epoch: 11226 \tTraining Loss:   0.268039\n",
            " Epoch: 11226 \tValidation Loss: 0.312645 \tR2: 0.596873\n",
            " Epoch: 11227 \tTraining Loss:   0.277739\n",
            " Epoch: 11227 \tValidation Loss: 0.343314 \tR2: 0.596873\n",
            " Epoch: 11228 \tTraining Loss:   0.293785\n",
            " Epoch: 11228 \tValidation Loss: 0.323106 \tR2: 0.596873\n",
            " Epoch: 11229 \tTraining Loss:   0.282573\n",
            " Epoch: 11229 \tValidation Loss: 0.354399 \tR2: 0.596873\n",
            " Epoch: 11230 \tTraining Loss:   0.297188\n",
            " Epoch: 11230 \tValidation Loss: 0.331672 \tR2: 0.596873\n",
            " Epoch: 11231 \tTraining Loss:   0.274288\n",
            " Epoch: 11231 \tValidation Loss: 0.334800 \tR2: 0.596873\n",
            " Epoch: 11232 \tTraining Loss:   0.268208\n",
            " Epoch: 11232 \tValidation Loss: 0.354625 \tR2: 0.596873\n",
            " Epoch: 11233 \tTraining Loss:   0.310327\n",
            " Epoch: 11233 \tValidation Loss: 0.526636 \tR2: 0.596873\n",
            " Epoch: 11234 \tTraining Loss:   0.288942\n",
            " Epoch: 11234 \tValidation Loss: 0.305675 \tR2: 0.596873\n",
            " Epoch: 11235 \tTraining Loss:   0.277331\n",
            " Epoch: 11235 \tValidation Loss: 0.378947 \tR2: 0.596873\n",
            " Epoch: 11236 \tTraining Loss:   0.285522\n",
            " Epoch: 11236 \tValidation Loss: 0.373711 \tR2: 0.596873\n",
            " Epoch: 11237 \tTraining Loss:   0.282895\n",
            " Epoch: 11237 \tValidation Loss: 0.342242 \tR2: 0.596873\n",
            " Epoch: 11238 \tTraining Loss:   0.278771\n",
            " Epoch: 11238 \tValidation Loss: 0.337157 \tR2: 0.596873\n",
            " Epoch: 11239 \tTraining Loss:   0.271422\n",
            " Epoch: 11239 \tValidation Loss: 0.575219 \tR2: 0.596873\n",
            " Epoch: 11240 \tTraining Loss:   0.279149\n",
            " Epoch: 11240 \tValidation Loss: 0.328165 \tR2: 0.596873\n",
            " Epoch: 11241 \tTraining Loss:   0.301448\n",
            " Epoch: 11241 \tValidation Loss: 0.402625 \tR2: 0.596873\n",
            " Epoch: 11242 \tTraining Loss:   0.276628\n",
            " Epoch: 11242 \tValidation Loss: 0.342182 \tR2: 0.596873\n",
            " Epoch: 11243 \tTraining Loss:   0.288841\n",
            " Epoch: 11243 \tValidation Loss: 0.312292 \tR2: 0.596873\n",
            " Epoch: 11244 \tTraining Loss:   0.277782\n",
            " Epoch: 11244 \tValidation Loss: 0.310166 \tR2: 0.596873\n",
            " Epoch: 11245 \tTraining Loss:   0.271675\n",
            " Epoch: 11245 \tValidation Loss: 0.323845 \tR2: 0.596873\n",
            " Epoch: 11246 \tTraining Loss:   0.282907\n",
            " Epoch: 11246 \tValidation Loss: 0.337398 \tR2: 0.596873\n",
            " Epoch: 11247 \tTraining Loss:   0.278859\n",
            " Epoch: 11247 \tValidation Loss: 0.330654 \tR2: 0.596873\n",
            " Epoch: 11248 \tTraining Loss:   0.270542\n",
            " Epoch: 11248 \tValidation Loss: 0.321992 \tR2: 0.596873\n",
            " Epoch: 11249 \tTraining Loss:   0.282817\n",
            " Epoch: 11249 \tValidation Loss: 0.305105 \tR2: 0.596873\n",
            " Epoch: 11250 \tTraining Loss:   0.280152\n",
            " Epoch: 11250 \tValidation Loss: 0.327226 \tR2: 0.596873\n",
            " Epoch: 11251 \tTraining Loss:   0.290145\n",
            " Epoch: 11251 \tValidation Loss: 0.352609 \tR2: 0.596873\n",
            " Epoch: 11252 \tTraining Loss:   0.284819\n",
            " Epoch: 11252 \tValidation Loss: 0.414864 \tR2: 0.596873\n",
            " Epoch: 11253 \tTraining Loss:   0.282003\n",
            " Epoch: 11253 \tValidation Loss: 0.408818 \tR2: 0.596873\n",
            " Epoch: 11254 \tTraining Loss:   0.276365\n",
            " Epoch: 11254 \tValidation Loss: 0.334515 \tR2: 0.596873\n",
            " Epoch: 11255 \tTraining Loss:   0.272517\n",
            " Epoch: 11255 \tValidation Loss: 0.321790 \tR2: 0.596873\n",
            " Epoch: 11256 \tTraining Loss:   0.276331\n",
            " Epoch: 11256 \tValidation Loss: 0.364526 \tR2: 0.596873\n",
            " Epoch: 11257 \tTraining Loss:   0.297883\n",
            " Epoch: 11257 \tValidation Loss: 0.372539 \tR2: 0.596873\n",
            " Epoch: 11258 \tTraining Loss:   0.280995\n",
            " Epoch: 11258 \tValidation Loss: 0.350926 \tR2: 0.596873\n",
            " Epoch: 11259 \tTraining Loss:   0.267609\n",
            " Epoch: 11259 \tValidation Loss: 0.335819 \tR2: 0.596873\n",
            " Epoch: 11260 \tTraining Loss:   0.273341\n",
            " Epoch: 11260 \tValidation Loss: 0.309267 \tR2: 0.596873\n",
            " Epoch: 11261 \tTraining Loss:   0.272912\n",
            " Epoch: 11261 \tValidation Loss: 0.348664 \tR2: 0.596873\n",
            " Epoch: 11262 \tTraining Loss:   0.272803\n",
            " Epoch: 11262 \tValidation Loss: 0.446365 \tR2: 0.596873\n",
            " Epoch: 11263 \tTraining Loss:   0.279529\n",
            " Epoch: 11263 \tValidation Loss: 0.319685 \tR2: 0.596873\n",
            " Epoch: 11264 \tTraining Loss:   0.283224\n",
            " Epoch: 11264 \tValidation Loss: 0.325663 \tR2: 0.596873\n",
            " Epoch: 11265 \tTraining Loss:   0.267089\n",
            " Epoch: 11265 \tValidation Loss: 0.369128 \tR2: 0.596873\n",
            " Epoch: 11266 \tTraining Loss:   0.287361\n",
            " Epoch: 11266 \tValidation Loss: 0.377419 \tR2: 0.596873\n",
            " Epoch: 11267 \tTraining Loss:   0.270740\n",
            " Epoch: 11267 \tValidation Loss: 0.332686 \tR2: 0.596873\n",
            " Epoch: 11268 \tTraining Loss:   0.375977\n",
            " Epoch: 11268 \tValidation Loss: 0.493783 \tR2: 0.596873\n",
            " Epoch: 11269 \tTraining Loss:   0.349608\n",
            " Epoch: 11269 \tValidation Loss: 0.389089 \tR2: 0.596873\n",
            " Epoch: 11270 \tTraining Loss:   0.286861\n",
            " Epoch: 11270 \tValidation Loss: 0.341379 \tR2: 0.596873\n",
            " Epoch: 11271 \tTraining Loss:   0.280315\n",
            " Epoch: 11271 \tValidation Loss: 0.314471 \tR2: 0.596873\n",
            " Epoch: 11272 \tTraining Loss:   0.290941\n",
            " Epoch: 11272 \tValidation Loss: 0.346545 \tR2: 0.596873\n",
            " Epoch: 11273 \tTraining Loss:   0.297991\n",
            " Epoch: 11273 \tValidation Loss: 0.391710 \tR2: 0.596873\n",
            " Epoch: 11274 \tTraining Loss:   0.278330\n",
            " Epoch: 11274 \tValidation Loss: 0.346085 \tR2: 0.596873\n",
            " Epoch: 11275 \tTraining Loss:   0.292784\n",
            " Epoch: 11275 \tValidation Loss: 0.332678 \tR2: 0.596873\n",
            " Epoch: 11276 \tTraining Loss:   0.283382\n",
            " Epoch: 11276 \tValidation Loss: 0.332368 \tR2: 0.596873\n",
            " Epoch: 11277 \tTraining Loss:   0.290026\n",
            " Epoch: 11277 \tValidation Loss: 0.331464 \tR2: 0.596873\n",
            " Epoch: 11278 \tTraining Loss:   0.282050\n",
            " Epoch: 11278 \tValidation Loss: 0.327885 \tR2: 0.596873\n",
            " Epoch: 11279 \tTraining Loss:   0.290706\n",
            " Epoch: 11279 \tValidation Loss: 0.325336 \tR2: 0.596873\n",
            " Epoch: 11280 \tTraining Loss:   0.291388\n",
            " Epoch: 11280 \tValidation Loss: 0.333135 \tR2: 0.596873\n",
            " Epoch: 11281 \tTraining Loss:   0.270948\n",
            " Epoch: 11281 \tValidation Loss: 0.327652 \tR2: 0.596873\n",
            " Epoch: 11282 \tTraining Loss:   0.276911\n",
            " Epoch: 11282 \tValidation Loss: 0.320755 \tR2: 0.596873\n",
            " Epoch: 11283 \tTraining Loss:   0.268897\n",
            " Epoch: 11283 \tValidation Loss: 0.350592 \tR2: 0.596873\n",
            " Epoch: 11284 \tTraining Loss:   0.280332\n",
            " Epoch: 11284 \tValidation Loss: 0.333977 \tR2: 0.596873\n",
            " Epoch: 11285 \tTraining Loss:   0.276396\n",
            " Epoch: 11285 \tValidation Loss: 0.329756 \tR2: 0.596873\n",
            " Epoch: 11286 \tTraining Loss:   0.287677\n",
            " Epoch: 11286 \tValidation Loss: 0.349701 \tR2: 0.596873\n",
            " Epoch: 11287 \tTraining Loss:   0.285383\n",
            " Epoch: 11287 \tValidation Loss: 0.315142 \tR2: 0.596873\n",
            " Epoch: 11288 \tTraining Loss:   0.290390\n",
            " Epoch: 11288 \tValidation Loss: 0.344809 \tR2: 0.596873\n",
            " Epoch: 11289 \tTraining Loss:   0.288948\n",
            " Epoch: 11289 \tValidation Loss: 0.322029 \tR2: 0.596873\n",
            " Epoch: 11290 \tTraining Loss:   0.271200\n",
            " Epoch: 11290 \tValidation Loss: 0.374077 \tR2: 0.596873\n",
            " Epoch: 11291 \tTraining Loss:   0.275148\n",
            " Epoch: 11291 \tValidation Loss: 0.338854 \tR2: 0.596873\n",
            " Epoch: 11292 \tTraining Loss:   0.272259\n",
            " Epoch: 11292 \tValidation Loss: 0.346441 \tR2: 0.596873\n",
            " Epoch: 11293 \tTraining Loss:   0.277517\n",
            " Epoch: 11293 \tValidation Loss: 0.305204 \tR2: 0.596873\n",
            " Epoch: 11294 \tTraining Loss:   0.271374\n",
            " Epoch: 11294 \tValidation Loss: 0.319052 \tR2: 0.596873\n",
            " Epoch: 11295 \tTraining Loss:   0.278133\n",
            " Epoch: 11295 \tValidation Loss: 0.345107 \tR2: 0.596873\n",
            " Epoch: 11296 \tTraining Loss:   0.270983\n",
            " Epoch: 11296 \tValidation Loss: 0.311987 \tR2: 0.596873\n",
            " Epoch: 11297 \tTraining Loss:   0.273144\n",
            " Epoch: 11297 \tValidation Loss: 0.307495 \tR2: 0.596873\n",
            " Epoch: 11298 \tTraining Loss:   0.290225\n",
            " Epoch: 11298 \tValidation Loss: 0.341875 \tR2: 0.596873\n",
            " Epoch: 11299 \tTraining Loss:   0.291592\n",
            " Epoch: 11299 \tValidation Loss: 0.352990 \tR2: 0.596873\n",
            " Epoch: 11300 \tTraining Loss:   0.286722\n",
            " Epoch: 11300 \tValidation Loss: 0.328162 \tR2: 0.314262\n",
            " Epoch: 11301 \tTraining Loss:   0.270872\n",
            " Epoch: 11301 \tValidation Loss: 0.303782 \tR2: 0.314262\n",
            " Epoch: 11302 \tTraining Loss:   0.289530\n",
            " Epoch: 11302 \tValidation Loss: 0.336203 \tR2: 0.314262\n",
            " Epoch: 11303 \tTraining Loss:   0.282026\n",
            " Epoch: 11303 \tValidation Loss: 0.318271 \tR2: 0.314262\n",
            " Epoch: 11304 \tTraining Loss:   0.287239\n",
            " Epoch: 11304 \tValidation Loss: 0.309828 \tR2: 0.314262\n",
            " Epoch: 11305 \tTraining Loss:   0.281224\n",
            " Epoch: 11305 \tValidation Loss: 0.331186 \tR2: 0.314262\n",
            " Epoch: 11306 \tTraining Loss:   0.280621\n",
            " Epoch: 11306 \tValidation Loss: 0.343423 \tR2: 0.314262\n",
            " Epoch: 11307 \tTraining Loss:   0.283646\n",
            " Epoch: 11307 \tValidation Loss: 0.312487 \tR2: 0.314262\n",
            " Epoch: 11308 \tTraining Loss:   0.286760\n",
            " Epoch: 11308 \tValidation Loss: 0.303937 \tR2: 0.314262\n",
            " Epoch: 11309 \tTraining Loss:   0.285819\n",
            " Epoch: 11309 \tValidation Loss: 0.321622 \tR2: 0.314262\n",
            " Epoch: 11310 \tTraining Loss:   0.272619\n",
            " Epoch: 11310 \tValidation Loss: 0.300934 \tR2: 0.314262\n",
            " Epoch: 11311 \tTraining Loss:   0.310070\n",
            " Epoch: 11311 \tValidation Loss: 0.353647 \tR2: 0.314262\n",
            " Epoch: 11312 \tTraining Loss:   0.310081\n",
            " Epoch: 11312 \tValidation Loss: 0.336446 \tR2: 0.314262\n",
            " Epoch: 11313 \tTraining Loss:   0.281538\n",
            " Epoch: 11313 \tValidation Loss: 0.320545 \tR2: 0.314262\n",
            " Epoch: 11314 \tTraining Loss:   0.294362\n",
            " Epoch: 11314 \tValidation Loss: 0.304186 \tR2: 0.314262\n",
            " Epoch: 11315 \tTraining Loss:   0.276243\n",
            " Epoch: 11315 \tValidation Loss: 0.335935 \tR2: 0.314262\n",
            " Epoch: 11316 \tTraining Loss:   0.290280\n",
            " Epoch: 11316 \tValidation Loss: 0.657607 \tR2: 0.314262\n",
            " Epoch: 11317 \tTraining Loss:   0.290237\n",
            " Epoch: 11317 \tValidation Loss: 0.336255 \tR2: 0.314262\n",
            " Epoch: 11318 \tTraining Loss:   0.274067\n",
            " Epoch: 11318 \tValidation Loss: 0.373443 \tR2: 0.314262\n",
            " Epoch: 11319 \tTraining Loss:   0.293242\n",
            " Epoch: 11319 \tValidation Loss: 0.404953 \tR2: 0.314262\n",
            " Epoch: 11320 \tTraining Loss:   0.283700\n",
            " Epoch: 11320 \tValidation Loss: 0.318822 \tR2: 0.314262\n",
            " Epoch: 11321 \tTraining Loss:   0.276395\n",
            " Epoch: 11321 \tValidation Loss: 0.322001 \tR2: 0.314262\n",
            " Epoch: 11322 \tTraining Loss:   0.289988\n",
            " Epoch: 11322 \tValidation Loss: 0.328577 \tR2: 0.314262\n",
            " Epoch: 11323 \tTraining Loss:   0.282794\n",
            " Epoch: 11323 \tValidation Loss: 0.344563 \tR2: 0.314262\n",
            " Epoch: 11324 \tTraining Loss:   0.270772\n",
            " Epoch: 11324 \tValidation Loss: 0.321853 \tR2: 0.314262\n",
            " Epoch: 11325 \tTraining Loss:   0.282078\n",
            " Epoch: 11325 \tValidation Loss: 0.338581 \tR2: 0.314262\n",
            " Epoch: 11326 \tTraining Loss:   0.269739\n",
            " Epoch: 11326 \tValidation Loss: 0.308937 \tR2: 0.314262\n",
            " Epoch: 11327 \tTraining Loss:   0.274358\n",
            " Epoch: 11327 \tValidation Loss: 0.313339 \tR2: 0.314262\n",
            " Epoch: 11328 \tTraining Loss:   0.275099\n",
            " Epoch: 11328 \tValidation Loss: 0.340549 \tR2: 0.314262\n",
            " Epoch: 11329 \tTraining Loss:   0.286021\n",
            " Epoch: 11329 \tValidation Loss: 0.339907 \tR2: 0.314262\n",
            " Epoch: 11330 \tTraining Loss:   0.281308\n",
            " Epoch: 11330 \tValidation Loss: 0.327643 \tR2: 0.314262\n",
            " Epoch: 11331 \tTraining Loss:   0.281249\n",
            " Epoch: 11331 \tValidation Loss: 0.358344 \tR2: 0.314262\n",
            " Epoch: 11332 \tTraining Loss:   0.274662\n",
            " Epoch: 11332 \tValidation Loss: 0.310042 \tR2: 0.314262\n",
            " Epoch: 11333 \tTraining Loss:   0.269465\n",
            " Epoch: 11333 \tValidation Loss: 0.318509 \tR2: 0.314262\n",
            " Epoch: 11334 \tTraining Loss:   0.282132\n",
            " Epoch: 11334 \tValidation Loss: 0.309207 \tR2: 0.314262\n",
            " Epoch: 11335 \tTraining Loss:   0.274614\n",
            " Epoch: 11335 \tValidation Loss: 0.337844 \tR2: 0.314262\n",
            " Epoch: 11336 \tTraining Loss:   0.291711\n",
            " Epoch: 11336 \tValidation Loss: 0.301035 \tR2: 0.314262\n",
            " Epoch: 11337 \tTraining Loss:   0.289949\n",
            " Epoch: 11337 \tValidation Loss: 0.327129 \tR2: 0.314262\n",
            " Epoch: 11338 \tTraining Loss:   0.280650\n",
            " Epoch: 11338 \tValidation Loss: 0.359961 \tR2: 0.314262\n",
            " Epoch: 11339 \tTraining Loss:   0.273011\n",
            " Epoch: 11339 \tValidation Loss: 0.296295 \tR2: 0.314262\n",
            " Epoch: 11340 \tTraining Loss:   0.276798\n",
            " Epoch: 11340 \tValidation Loss: 0.308788 \tR2: 0.314262\n",
            " Epoch: 11341 \tTraining Loss:   0.283344\n",
            " Epoch: 11341 \tValidation Loss: 0.321493 \tR2: 0.314262\n",
            " Epoch: 11342 \tTraining Loss:   0.278062\n",
            " Epoch: 11342 \tValidation Loss: 0.360862 \tR2: 0.314262\n",
            " Epoch: 11343 \tTraining Loss:   0.286445\n",
            " Epoch: 11343 \tValidation Loss: 0.357153 \tR2: 0.314262\n",
            " Epoch: 11344 \tTraining Loss:   0.299301\n",
            " Epoch: 11344 \tValidation Loss: 0.329262 \tR2: 0.314262\n",
            " Epoch: 11345 \tTraining Loss:   0.274927\n",
            " Epoch: 11345 \tValidation Loss: 0.450453 \tR2: 0.314262\n",
            " Epoch: 11346 \tTraining Loss:   0.284107\n",
            " Epoch: 11346 \tValidation Loss: 0.346959 \tR2: 0.314262\n",
            " Epoch: 11347 \tTraining Loss:   0.287461\n",
            " Epoch: 11347 \tValidation Loss: 0.334130 \tR2: 0.314262\n",
            " Epoch: 11348 \tTraining Loss:   0.278025\n",
            " Epoch: 11348 \tValidation Loss: 0.311783 \tR2: 0.314262\n",
            " Epoch: 11349 \tTraining Loss:   0.274199\n",
            " Epoch: 11349 \tValidation Loss: 0.353307 \tR2: 0.314262\n",
            " Epoch: 11350 \tTraining Loss:   0.280576\n",
            " Epoch: 11350 \tValidation Loss: 0.331707 \tR2: 0.314262\n",
            " Epoch: 11351 \tTraining Loss:   0.279616\n",
            " Epoch: 11351 \tValidation Loss: 0.318498 \tR2: 0.314262\n",
            " Epoch: 11352 \tTraining Loss:   0.281335\n",
            " Epoch: 11352 \tValidation Loss: 0.330560 \tR2: 0.314262\n",
            " Epoch: 11353 \tTraining Loss:   0.271238\n",
            " Epoch: 11353 \tValidation Loss: 0.323478 \tR2: 0.314262\n",
            " Epoch: 11354 \tTraining Loss:   0.271751\n",
            " Epoch: 11354 \tValidation Loss: 0.332745 \tR2: 0.314262\n",
            " Epoch: 11355 \tTraining Loss:   0.292897\n",
            " Epoch: 11355 \tValidation Loss: 0.411088 \tR2: 0.314262\n",
            " Epoch: 11356 \tTraining Loss:   0.283574\n",
            " Epoch: 11356 \tValidation Loss: 0.308746 \tR2: 0.314262\n",
            " Epoch: 11357 \tTraining Loss:   0.277585\n",
            " Epoch: 11357 \tValidation Loss: 0.287704 \tR2: 0.314262\n",
            " Epoch: 11358 \tTraining Loss:   0.278399\n",
            " Epoch: 11358 \tValidation Loss: 0.295608 \tR2: 0.314262\n",
            " Epoch: 11359 \tTraining Loss:   0.306129\n",
            " Epoch: 11359 \tValidation Loss: 0.380019 \tR2: 0.314262\n",
            " Epoch: 11360 \tTraining Loss:   0.297213\n",
            " Epoch: 11360 \tValidation Loss: 0.308059 \tR2: 0.314262\n",
            " Epoch: 11361 \tTraining Loss:   0.269599\n",
            " Epoch: 11361 \tValidation Loss: 0.323578 \tR2: 0.314262\n",
            " Epoch: 11362 \tTraining Loss:   0.279045\n",
            " Epoch: 11362 \tValidation Loss: 0.346894 \tR2: 0.314262\n",
            " Epoch: 11363 \tTraining Loss:   0.281929\n",
            " Epoch: 11363 \tValidation Loss: 0.296595 \tR2: 0.314262\n",
            " Epoch: 11364 \tTraining Loss:   0.289301\n",
            " Epoch: 11364 \tValidation Loss: 0.342693 \tR2: 0.314262\n",
            " Epoch: 11365 \tTraining Loss:   0.271392\n",
            " Epoch: 11365 \tValidation Loss: 0.316290 \tR2: 0.314262\n",
            " Epoch: 11366 \tTraining Loss:   0.287394\n",
            " Epoch: 11366 \tValidation Loss: 0.328356 \tR2: 0.314262\n",
            " Epoch: 11367 \tTraining Loss:   0.286444\n",
            " Epoch: 11367 \tValidation Loss: 0.343568 \tR2: 0.314262\n",
            " Epoch: 11368 \tTraining Loss:   0.276139\n",
            " Epoch: 11368 \tValidation Loss: 0.321467 \tR2: 0.314262\n",
            " Epoch: 11369 \tTraining Loss:   0.285423\n",
            " Epoch: 11369 \tValidation Loss: 0.457520 \tR2: 0.314262\n",
            " Epoch: 11370 \tTraining Loss:   0.285679\n",
            " Epoch: 11370 \tValidation Loss: 0.344509 \tR2: 0.314262\n",
            " Epoch: 11371 \tTraining Loss:   0.334390\n",
            " Epoch: 11371 \tValidation Loss: 0.330273 \tR2: 0.314262\n",
            " Epoch: 11372 \tTraining Loss:   0.284641\n",
            " Epoch: 11372 \tValidation Loss: 0.348413 \tR2: 0.314262\n",
            " Epoch: 11373 \tTraining Loss:   0.281910\n",
            " Epoch: 11373 \tValidation Loss: 0.317022 \tR2: 0.314262\n",
            " Epoch: 11374 \tTraining Loss:   0.273684\n",
            " Epoch: 11374 \tValidation Loss: 0.305981 \tR2: 0.314262\n",
            " Epoch: 11375 \tTraining Loss:   0.274158\n",
            " Epoch: 11375 \tValidation Loss: 0.318352 \tR2: 0.314262\n",
            " Epoch: 11376 \tTraining Loss:   0.277393\n",
            " Epoch: 11376 \tValidation Loss: 0.305057 \tR2: 0.314262\n",
            " Epoch: 11377 \tTraining Loss:   0.276827\n",
            " Epoch: 11377 \tValidation Loss: 0.366229 \tR2: 0.314262\n",
            " Epoch: 11378 \tTraining Loss:   0.277153\n",
            " Epoch: 11378 \tValidation Loss: 0.324821 \tR2: 0.314262\n",
            " Epoch: 11379 \tTraining Loss:   0.288218\n",
            " Epoch: 11379 \tValidation Loss: 0.336911 \tR2: 0.314262\n",
            " Epoch: 11380 \tTraining Loss:   0.283873\n",
            " Epoch: 11380 \tValidation Loss: 0.333170 \tR2: 0.314262\n",
            " Epoch: 11381 \tTraining Loss:   0.274524\n",
            " Epoch: 11381 \tValidation Loss: 0.721299 \tR2: 0.314262\n",
            " Epoch: 11382 \tTraining Loss:   0.283457\n",
            " Epoch: 11382 \tValidation Loss: 0.329164 \tR2: 0.314262\n",
            " Epoch: 11383 \tTraining Loss:   0.282001\n",
            " Epoch: 11383 \tValidation Loss: 0.332253 \tR2: 0.314262\n",
            " Epoch: 11384 \tTraining Loss:   0.289503\n",
            " Epoch: 11384 \tValidation Loss: 0.377612 \tR2: 0.314262\n",
            " Epoch: 11385 \tTraining Loss:   0.272469\n",
            " Epoch: 11385 \tValidation Loss: 0.332576 \tR2: 0.314262\n",
            " Epoch: 11386 \tTraining Loss:   0.269800\n",
            " Epoch: 11386 \tValidation Loss: 0.311840 \tR2: 0.314262\n",
            " Epoch: 11387 \tTraining Loss:   0.290792\n",
            " Epoch: 11387 \tValidation Loss: 0.370833 \tR2: 0.314262\n",
            " Epoch: 11388 \tTraining Loss:   0.276551\n",
            " Epoch: 11388 \tValidation Loss: 0.352391 \tR2: 0.314262\n",
            " Epoch: 11389 \tTraining Loss:   0.271718\n",
            " Epoch: 11389 \tValidation Loss: 0.327953 \tR2: 0.314262\n",
            " Epoch: 11390 \tTraining Loss:   0.289808\n",
            " Epoch: 11390 \tValidation Loss: 0.392565 \tR2: 0.314262\n",
            " Epoch: 11391 \tTraining Loss:   0.277727\n",
            " Epoch: 11391 \tValidation Loss: 0.350212 \tR2: 0.314262\n",
            " Epoch: 11392 \tTraining Loss:   0.265174\n",
            " Epoch: 11392 \tValidation Loss: 0.319565 \tR2: 0.314262\n",
            " Epoch: 11393 \tTraining Loss:   0.283615\n",
            " Epoch: 11393 \tValidation Loss: 0.332959 \tR2: 0.314262\n",
            " Epoch: 11394 \tTraining Loss:   0.277968\n",
            " Epoch: 11394 \tValidation Loss: 0.328113 \tR2: 0.314262\n",
            " Epoch: 11395 \tTraining Loss:   0.277678\n",
            " Epoch: 11395 \tValidation Loss: 0.331168 \tR2: 0.314262\n",
            " Epoch: 11396 \tTraining Loss:   0.277357\n",
            " Epoch: 11396 \tValidation Loss: 0.770016 \tR2: 0.314262\n",
            " Epoch: 11397 \tTraining Loss:   0.278836\n",
            " Epoch: 11397 \tValidation Loss: 0.323035 \tR2: 0.314262\n",
            " Epoch: 11398 \tTraining Loss:   0.281237\n",
            " Epoch: 11398 \tValidation Loss: 0.368451 \tR2: 0.314262\n",
            " Epoch: 11399 \tTraining Loss:   0.276671\n",
            " Epoch: 11399 \tValidation Loss: 0.382875 \tR2: 0.314262\n",
            " Epoch: 11400 \tTraining Loss:   0.293713\n",
            " Epoch: 11400 \tValidation Loss: 0.331456 \tR2: 0.406901\n",
            " Epoch: 11401 \tTraining Loss:   0.284477\n",
            " Epoch: 11401 \tValidation Loss: 0.304707 \tR2: 0.406901\n",
            " Epoch: 11402 \tTraining Loss:   0.280613\n",
            " Epoch: 11402 \tValidation Loss: 0.306861 \tR2: 0.406901\n",
            " Epoch: 11403 \tTraining Loss:   0.269448\n",
            " Epoch: 11403 \tValidation Loss: 0.314188 \tR2: 0.406901\n",
            " Epoch: 11404 \tTraining Loss:   0.283148\n",
            " Epoch: 11404 \tValidation Loss: 0.320112 \tR2: 0.406901\n",
            " Epoch: 11405 \tTraining Loss:   0.281877\n",
            " Epoch: 11405 \tValidation Loss: 0.355811 \tR2: 0.406901\n",
            " Epoch: 11406 \tTraining Loss:   0.280054\n",
            " Epoch: 11406 \tValidation Loss: 0.352248 \tR2: 0.406901\n",
            " Epoch: 11407 \tTraining Loss:   0.285356\n",
            " Epoch: 11407 \tValidation Loss: 0.310479 \tR2: 0.406901\n",
            " Epoch: 11408 \tTraining Loss:   0.272269\n",
            " Epoch: 11408 \tValidation Loss: 0.314057 \tR2: 0.406901\n",
            " Epoch: 11409 \tTraining Loss:   0.271407\n",
            " Epoch: 11409 \tValidation Loss: 0.322039 \tR2: 0.406901\n",
            " Epoch: 11410 \tTraining Loss:   0.267099\n",
            " Epoch: 11410 \tValidation Loss: 0.344450 \tR2: 0.406901\n",
            " Epoch: 11411 \tTraining Loss:   0.280772\n",
            " Epoch: 11411 \tValidation Loss: 0.414018 \tR2: 0.406901\n",
            " Epoch: 11412 \tTraining Loss:   0.284578\n",
            " Epoch: 11412 \tValidation Loss: 0.345193 \tR2: 0.406901\n",
            " Epoch: 11413 \tTraining Loss:   0.279882\n",
            " Epoch: 11413 \tValidation Loss: 0.305044 \tR2: 0.406901\n",
            " Epoch: 11414 \tTraining Loss:   0.281911\n",
            " Epoch: 11414 \tValidation Loss: 0.329150 \tR2: 0.406901\n",
            " Epoch: 11415 \tTraining Loss:   0.276330\n",
            " Epoch: 11415 \tValidation Loss: 0.330794 \tR2: 0.406901\n",
            " Epoch: 11416 \tTraining Loss:   0.267483\n",
            " Epoch: 11416 \tValidation Loss: 0.416796 \tR2: 0.406901\n",
            " Epoch: 11417 \tTraining Loss:   0.271621\n",
            " Epoch: 11417 \tValidation Loss: 0.324696 \tR2: 0.406901\n",
            " Epoch: 11418 \tTraining Loss:   0.293937\n",
            " Epoch: 11418 \tValidation Loss: 0.308951 \tR2: 0.406901\n",
            " Epoch: 11419 \tTraining Loss:   0.266250\n",
            " Epoch: 11419 \tValidation Loss: 0.307336 \tR2: 0.406901\n",
            " Epoch: 11420 \tTraining Loss:   0.266176\n",
            " Epoch: 11420 \tValidation Loss: 0.314301 \tR2: 0.406901\n",
            " Epoch: 11421 \tTraining Loss:   0.341054\n",
            " Epoch: 11421 \tValidation Loss: 0.416755 \tR2: 0.406901\n",
            " Epoch: 11422 \tTraining Loss:   0.305543\n",
            " Epoch: 11422 \tValidation Loss: 0.322341 \tR2: 0.406901\n",
            " Epoch: 11423 \tTraining Loss:   0.274013\n",
            " Epoch: 11423 \tValidation Loss: 0.294368 \tR2: 0.406901\n",
            " Epoch: 11424 \tTraining Loss:   0.275879\n",
            " Epoch: 11424 \tValidation Loss: 0.322343 \tR2: 0.406901\n",
            " Epoch: 11425 \tTraining Loss:   0.293043\n",
            " Epoch: 11425 \tValidation Loss: 0.278739 \tR2: 0.406901\n",
            " Epoch: 11426 \tTraining Loss:   0.284821\n",
            " Epoch: 11426 \tValidation Loss: 0.294613 \tR2: 0.406901\n",
            " Epoch: 11427 \tTraining Loss:   0.310682\n",
            " Epoch: 11427 \tValidation Loss: 0.322213 \tR2: 0.406901\n",
            " Epoch: 11428 \tTraining Loss:   0.278726\n",
            " Epoch: 11428 \tValidation Loss: 0.330028 \tR2: 0.406901\n",
            " Epoch: 11429 \tTraining Loss:   0.276944\n",
            " Epoch: 11429 \tValidation Loss: 0.313200 \tR2: 0.406901\n",
            " Epoch: 11430 \tTraining Loss:   0.299557\n",
            " Epoch: 11430 \tValidation Loss: 0.319648 \tR2: 0.406901\n",
            " Epoch: 11431 \tTraining Loss:   0.270571\n",
            " Epoch: 11431 \tValidation Loss: 0.309318 \tR2: 0.406901\n",
            " Epoch: 11432 \tTraining Loss:   0.276007\n",
            " Epoch: 11432 \tValidation Loss: 0.329313 \tR2: 0.406901\n",
            " Epoch: 11433 \tTraining Loss:   0.276733\n",
            " Epoch: 11433 \tValidation Loss: 0.358895 \tR2: 0.406901\n",
            " Epoch: 11434 \tTraining Loss:   0.275543\n",
            " Epoch: 11434 \tValidation Loss: 0.301874 \tR2: 0.406901\n",
            " Epoch: 11435 \tTraining Loss:   0.284322\n",
            " Epoch: 11435 \tValidation Loss: 0.301787 \tR2: 0.406901\n",
            " Epoch: 11436 \tTraining Loss:   0.282745\n",
            " Epoch: 11436 \tValidation Loss: 0.313756 \tR2: 0.406901\n",
            " Epoch: 11437 \tTraining Loss:   0.275764\n",
            " Epoch: 11437 \tValidation Loss: 0.334360 \tR2: 0.406901\n",
            " Epoch: 11438 \tTraining Loss:   0.279840\n",
            " Epoch: 11438 \tValidation Loss: 0.331300 \tR2: 0.406901\n",
            " Epoch: 11439 \tTraining Loss:   0.284729\n",
            " Epoch: 11439 \tValidation Loss: 0.299754 \tR2: 0.406901\n",
            " Epoch: 11440 \tTraining Loss:   0.275878\n",
            " Epoch: 11440 \tValidation Loss: 0.341637 \tR2: 0.406901\n",
            " Epoch: 11441 \tTraining Loss:   0.311986\n",
            " Epoch: 11441 \tValidation Loss: 0.325985 \tR2: 0.406901\n",
            " Epoch: 11442 \tTraining Loss:   0.284827\n",
            " Epoch: 11442 \tValidation Loss: 0.388818 \tR2: 0.406901\n",
            " Epoch: 11443 \tTraining Loss:   0.289306\n",
            " Epoch: 11443 \tValidation Loss: 0.329616 \tR2: 0.406901\n",
            " Epoch: 11444 \tTraining Loss:   0.282005\n",
            " Epoch: 11444 \tValidation Loss: 0.383440 \tR2: 0.406901\n",
            " Epoch: 11445 \tTraining Loss:   0.285484\n",
            " Epoch: 11445 \tValidation Loss: 0.322999 \tR2: 0.406901\n",
            " Epoch: 11446 \tTraining Loss:   0.296744\n",
            " Epoch: 11446 \tValidation Loss: 0.371233 \tR2: 0.406901\n",
            " Epoch: 11447 \tTraining Loss:   0.303580\n",
            " Epoch: 11447 \tValidation Loss: 0.323851 \tR2: 0.406901\n",
            " Epoch: 11448 \tTraining Loss:   0.282528\n",
            " Epoch: 11448 \tValidation Loss: 0.317385 \tR2: 0.406901\n",
            " Epoch: 11449 \tTraining Loss:   0.282800\n",
            " Epoch: 11449 \tValidation Loss: 0.335495 \tR2: 0.406901\n",
            " Epoch: 11450 \tTraining Loss:   0.276878\n",
            " Epoch: 11450 \tValidation Loss: 0.335619 \tR2: 0.406901\n",
            " Epoch: 11451 \tTraining Loss:   0.266780\n",
            " Epoch: 11451 \tValidation Loss: 0.336556 \tR2: 0.406901\n",
            " Epoch: 11452 \tTraining Loss:   0.293347\n",
            " Epoch: 11452 \tValidation Loss: 0.387751 \tR2: 0.406901\n",
            " Epoch: 11453 \tTraining Loss:   0.281231\n",
            " Epoch: 11453 \tValidation Loss: 0.305031 \tR2: 0.406901\n",
            " Epoch: 11454 \tTraining Loss:   0.271958\n",
            " Epoch: 11454 \tValidation Loss: 0.319075 \tR2: 0.406901\n",
            " Epoch: 11455 \tTraining Loss:   0.279301\n",
            " Epoch: 11455 \tValidation Loss: 0.311962 \tR2: 0.406901\n",
            " Epoch: 11456 \tTraining Loss:   0.288030\n",
            " Epoch: 11456 \tValidation Loss: 0.348484 \tR2: 0.406901\n",
            " Epoch: 11457 \tTraining Loss:   0.278544\n",
            " Epoch: 11457 \tValidation Loss: 0.319729 \tR2: 0.406901\n",
            " Epoch: 11458 \tTraining Loss:   0.266059\n",
            " Epoch: 11458 \tValidation Loss: 0.372676 \tR2: 0.406901\n",
            " Epoch: 11459 \tTraining Loss:   0.268230\n",
            " Epoch: 11459 \tValidation Loss: 0.352083 \tR2: 0.406901\n",
            " Epoch: 11460 \tTraining Loss:   0.279478\n",
            " Epoch: 11460 \tValidation Loss: 0.327379 \tR2: 0.406901\n",
            " Epoch: 11461 \tTraining Loss:   0.275767\n",
            " Epoch: 11461 \tValidation Loss: 0.338210 \tR2: 0.406901\n",
            " Epoch: 11462 \tTraining Loss:   0.279159\n",
            " Epoch: 11462 \tValidation Loss: 0.297296 \tR2: 0.406901\n",
            " Epoch: 11463 \tTraining Loss:   0.275622\n",
            " Epoch: 11463 \tValidation Loss: 0.323664 \tR2: 0.406901\n",
            " Epoch: 11464 \tTraining Loss:   0.282737\n",
            " Epoch: 11464 \tValidation Loss: 0.302311 \tR2: 0.406901\n",
            " Epoch: 11465 \tTraining Loss:   0.267391\n",
            " Epoch: 11465 \tValidation Loss: 0.307609 \tR2: 0.406901\n",
            " Epoch: 11466 \tTraining Loss:   0.280733\n",
            " Epoch: 11466 \tValidation Loss: 0.310501 \tR2: 0.406901\n",
            " Epoch: 11467 \tTraining Loss:   0.297910\n",
            " Epoch: 11467 \tValidation Loss: 0.329578 \tR2: 0.406901\n",
            " Epoch: 11468 \tTraining Loss:   0.279381\n",
            " Epoch: 11468 \tValidation Loss: 0.397391 \tR2: 0.406901\n",
            " Epoch: 11469 \tTraining Loss:   0.282965\n",
            " Epoch: 11469 \tValidation Loss: 0.298700 \tR2: 0.406901\n",
            " Epoch: 11470 \tTraining Loss:   0.279876\n",
            " Epoch: 11470 \tValidation Loss: 0.347116 \tR2: 0.406901\n",
            " Epoch: 11471 \tTraining Loss:   0.285557\n",
            " Epoch: 11471 \tValidation Loss: 0.348414 \tR2: 0.406901\n",
            " Epoch: 11472 \tTraining Loss:   0.276244\n",
            " Epoch: 11472 \tValidation Loss: 0.333028 \tR2: 0.406901\n",
            " Epoch: 11473 \tTraining Loss:   0.269825\n",
            " Epoch: 11473 \tValidation Loss: 0.304806 \tR2: 0.406901\n",
            " Epoch: 11474 \tTraining Loss:   0.275732\n",
            " Epoch: 11474 \tValidation Loss: 0.325391 \tR2: 0.406901\n",
            " Epoch: 11475 \tTraining Loss:   0.285310\n",
            " Epoch: 11475 \tValidation Loss: 0.309476 \tR2: 0.406901\n",
            " Epoch: 11476 \tTraining Loss:   0.270899\n",
            " Epoch: 11476 \tValidation Loss: 0.323289 \tR2: 0.406901\n",
            " Epoch: 11477 \tTraining Loss:   0.282385\n",
            " Epoch: 11477 \tValidation Loss: 0.335098 \tR2: 0.406901\n",
            " Epoch: 11478 \tTraining Loss:   0.274787\n",
            " Epoch: 11478 \tValidation Loss: 0.344147 \tR2: 0.406901\n",
            " Epoch: 11479 \tTraining Loss:   0.277409\n",
            " Epoch: 11479 \tValidation Loss: 0.323679 \tR2: 0.406901\n",
            " Epoch: 11480 \tTraining Loss:   0.282121\n",
            " Epoch: 11480 \tValidation Loss: 0.355047 \tR2: 0.406901\n",
            " Epoch: 11481 \tTraining Loss:   0.278768\n",
            " Epoch: 11481 \tValidation Loss: 0.334338 \tR2: 0.406901\n",
            " Epoch: 11482 \tTraining Loss:   0.273841\n",
            " Epoch: 11482 \tValidation Loss: 0.291244 \tR2: 0.406901\n",
            " Epoch: 11483 \tTraining Loss:   0.284006\n",
            " Epoch: 11483 \tValidation Loss: 0.352802 \tR2: 0.406901\n",
            " Epoch: 11484 \tTraining Loss:   0.279238\n",
            " Epoch: 11484 \tValidation Loss: 0.321897 \tR2: 0.406901\n",
            " Epoch: 11485 \tTraining Loss:   0.262412\n",
            " Epoch: 11485 \tValidation Loss: 0.380039 \tR2: 0.406901\n",
            " Epoch: 11486 \tTraining Loss:   0.276173\n",
            " Epoch: 11486 \tValidation Loss: 0.304408 \tR2: 0.406901\n",
            " Epoch: 11487 \tTraining Loss:   0.274263\n",
            " Epoch: 11487 \tValidation Loss: 0.321582 \tR2: 0.406901\n",
            " Epoch: 11488 \tTraining Loss:   0.275276\n",
            " Epoch: 11488 \tValidation Loss: 0.315249 \tR2: 0.406901\n",
            " Epoch: 11489 \tTraining Loss:   0.283900\n",
            " Epoch: 11489 \tValidation Loss: 0.326510 \tR2: 0.406901\n",
            " Epoch: 11490 \tTraining Loss:   0.283792\n",
            " Epoch: 11490 \tValidation Loss: 0.355759 \tR2: 0.406901\n",
            " Epoch: 11491 \tTraining Loss:   0.265050\n",
            " Epoch: 11491 \tValidation Loss: 0.326751 \tR2: 0.406901\n",
            " Epoch: 11492 \tTraining Loss:   0.277649\n",
            " Epoch: 11492 \tValidation Loss: 0.301957 \tR2: 0.406901\n",
            " Epoch: 11493 \tTraining Loss:   0.285577\n",
            " Epoch: 11493 \tValidation Loss: 0.320040 \tR2: 0.406901\n",
            " Epoch: 11494 \tTraining Loss:   0.282595\n",
            " Epoch: 11494 \tValidation Loss: 0.384863 \tR2: 0.406901\n",
            " Epoch: 11495 \tTraining Loss:   0.273591\n",
            " Epoch: 11495 \tValidation Loss: 0.322489 \tR2: 0.406901\n",
            " Epoch: 11496 \tTraining Loss:   0.279087\n",
            " Epoch: 11496 \tValidation Loss: 0.335929 \tR2: 0.406901\n",
            " Epoch: 11497 \tTraining Loss:   0.267116\n",
            " Epoch: 11497 \tValidation Loss: 0.345724 \tR2: 0.406901\n",
            " Epoch: 11498 \tTraining Loss:   0.285774\n",
            " Epoch: 11498 \tValidation Loss: 0.310207 \tR2: 0.406901\n",
            " Epoch: 11499 \tTraining Loss:   0.268778\n",
            " Epoch: 11499 \tValidation Loss: 0.334866 \tR2: 0.406901\n",
            " Epoch: 11500 \tTraining Loss:   0.268414\n",
            " Epoch: 11500 \tValidation Loss: 0.309418 \tR2: 0.499137\n",
            " Epoch: 11501 \tTraining Loss:   0.299177\n",
            " Epoch: 11501 \tValidation Loss: 0.397827 \tR2: 0.499137\n",
            " Epoch: 11502 \tTraining Loss:   0.295483\n",
            " Epoch: 11502 \tValidation Loss: 0.339739 \tR2: 0.499137\n",
            " Epoch: 11503 \tTraining Loss:   0.265090\n",
            " Epoch: 11503 \tValidation Loss: 0.331136 \tR2: 0.499137\n",
            " Epoch: 11504 \tTraining Loss:   0.275926\n",
            " Epoch: 11504 \tValidation Loss: 0.349357 \tR2: 0.499137\n",
            " Epoch: 11505 \tTraining Loss:   0.289522\n",
            " Epoch: 11505 \tValidation Loss: 0.397490 \tR2: 0.499137\n",
            " Epoch: 11506 \tTraining Loss:   0.280927\n",
            " Epoch: 11506 \tValidation Loss: 0.357511 \tR2: 0.499137\n",
            " Epoch: 11507 \tTraining Loss:   0.279706\n",
            " Epoch: 11507 \tValidation Loss: 0.406569 \tR2: 0.499137\n",
            " Epoch: 11508 \tTraining Loss:   0.279135\n",
            " Epoch: 11508 \tValidation Loss: 0.307734 \tR2: 0.499137\n",
            " Epoch: 11509 \tTraining Loss:   0.277148\n",
            " Epoch: 11509 \tValidation Loss: 0.322429 \tR2: 0.499137\n",
            " Epoch: 11510 \tTraining Loss:   0.283843\n",
            " Epoch: 11510 \tValidation Loss: 0.351486 \tR2: 0.499137\n",
            " Epoch: 11511 \tTraining Loss:   0.282454\n",
            " Epoch: 11511 \tValidation Loss: 0.347371 \tR2: 0.499137\n",
            " Epoch: 11512 \tTraining Loss:   0.278161\n",
            " Epoch: 11512 \tValidation Loss: 0.332280 \tR2: 0.499137\n",
            " Epoch: 11513 \tTraining Loss:   0.289096\n",
            " Epoch: 11513 \tValidation Loss: 0.337843 \tR2: 0.499137\n",
            " Epoch: 11514 \tTraining Loss:   0.283239\n",
            " Epoch: 11514 \tValidation Loss: 0.361970 \tR2: 0.499137\n",
            " Epoch: 11515 \tTraining Loss:   0.291606\n",
            " Epoch: 11515 \tValidation Loss: 0.531608 \tR2: 0.499137\n",
            " Epoch: 11516 \tTraining Loss:   0.273036\n",
            " Epoch: 11516 \tValidation Loss: 0.311115 \tR2: 0.499137\n",
            " Epoch: 11517 \tTraining Loss:   0.285071\n",
            " Epoch: 11517 \tValidation Loss: 0.303003 \tR2: 0.499137\n",
            " Epoch: 11518 \tTraining Loss:   0.299466\n",
            " Epoch: 11518 \tValidation Loss: 0.334656 \tR2: 0.499137\n",
            " Epoch: 11519 \tTraining Loss:   0.296420\n",
            " Epoch: 11519 \tValidation Loss: 0.337600 \tR2: 0.499137\n",
            " Epoch: 11520 \tTraining Loss:   0.281187\n",
            " Epoch: 11520 \tValidation Loss: 0.364402 \tR2: 0.499137\n",
            " Epoch: 11521 \tTraining Loss:   0.275845\n",
            " Epoch: 11521 \tValidation Loss: 0.338917 \tR2: 0.499137\n",
            " Epoch: 11522 \tTraining Loss:   0.286923\n",
            " Epoch: 11522 \tValidation Loss: 0.376652 \tR2: 0.499137\n",
            " Epoch: 11523 \tTraining Loss:   0.290049\n",
            " Epoch: 11523 \tValidation Loss: 0.349103 \tR2: 0.499137\n",
            " Epoch: 11524 \tTraining Loss:   0.279858\n",
            " Epoch: 11524 \tValidation Loss: 0.308990 \tR2: 0.499137\n",
            " Epoch: 11525 \tTraining Loss:   0.282788\n",
            " Epoch: 11525 \tValidation Loss: 0.307194 \tR2: 0.499137\n",
            " Epoch: 11526 \tTraining Loss:   0.274821\n",
            " Epoch: 11526 \tValidation Loss: 0.349821 \tR2: 0.499137\n",
            " Epoch: 11527 \tTraining Loss:   0.271660\n",
            " Epoch: 11527 \tValidation Loss: 0.338383 \tR2: 0.499137\n",
            " Epoch: 11528 \tTraining Loss:   0.296037\n",
            " Epoch: 11528 \tValidation Loss: 0.369704 \tR2: 0.499137\n",
            " Epoch: 11529 \tTraining Loss:   0.283339\n",
            " Epoch: 11529 \tValidation Loss: 0.392663 \tR2: 0.499137\n",
            " Epoch: 11530 \tTraining Loss:   0.278417\n",
            " Epoch: 11530 \tValidation Loss: 0.491817 \tR2: 0.499137\n",
            " Epoch: 11531 \tTraining Loss:   0.275093\n",
            " Epoch: 11531 \tValidation Loss: 0.327599 \tR2: 0.499137\n",
            " Epoch: 11532 \tTraining Loss:   0.304602\n",
            " Epoch: 11532 \tValidation Loss: 0.324166 \tR2: 0.499137\n",
            " Epoch: 11533 \tTraining Loss:   0.269717\n",
            " Epoch: 11533 \tValidation Loss: 0.387814 \tR2: 0.499137\n",
            " Epoch: 11534 \tTraining Loss:   0.289061\n",
            " Epoch: 11534 \tValidation Loss: 0.320131 \tR2: 0.499137\n",
            " Epoch: 11535 \tTraining Loss:   0.279109\n",
            " Epoch: 11535 \tValidation Loss: 0.319526 \tR2: 0.499137\n",
            " Epoch: 11536 \tTraining Loss:   0.274841\n",
            " Epoch: 11536 \tValidation Loss: 0.322599 \tR2: 0.499137\n",
            " Epoch: 11537 \tTraining Loss:   0.277399\n",
            " Epoch: 11537 \tValidation Loss: 0.275503 \tR2: 0.499137\n",
            " Epoch: 11538 \tTraining Loss:   0.302445\n",
            " Epoch: 11538 \tValidation Loss: 0.351570 \tR2: 0.499137\n",
            " Epoch: 11539 \tTraining Loss:   0.275508\n",
            " Epoch: 11539 \tValidation Loss: 0.341475 \tR2: 0.499137\n",
            " Epoch: 11540 \tTraining Loss:   0.286844\n",
            " Epoch: 11540 \tValidation Loss: 0.380514 \tR2: 0.499137\n",
            " Epoch: 11541 \tTraining Loss:   0.273339\n",
            " Epoch: 11541 \tValidation Loss: 0.389679 \tR2: 0.499137\n",
            " Epoch: 11542 \tTraining Loss:   0.283650\n",
            " Epoch: 11542 \tValidation Loss: 0.351822 \tR2: 0.499137\n",
            " Epoch: 11543 \tTraining Loss:   0.278381\n",
            " Epoch: 11543 \tValidation Loss: 0.350011 \tR2: 0.499137\n",
            " Epoch: 11544 \tTraining Loss:   0.280845\n",
            " Epoch: 11544 \tValidation Loss: 0.346765 \tR2: 0.499137\n",
            " Epoch: 11545 \tTraining Loss:   0.265635\n",
            " Epoch: 11545 \tValidation Loss: 0.410879 \tR2: 0.499137\n",
            " Epoch: 11546 \tTraining Loss:   0.295759\n",
            " Epoch: 11546 \tValidation Loss: 0.373219 \tR2: 0.499137\n",
            " Epoch: 11547 \tTraining Loss:   0.289725\n",
            " Epoch: 11547 \tValidation Loss: 0.336627 \tR2: 0.499137\n",
            " Epoch: 11548 \tTraining Loss:   0.274151\n",
            " Epoch: 11548 \tValidation Loss: 0.318708 \tR2: 0.499137\n",
            " Epoch: 11549 \tTraining Loss:   0.282184\n",
            " Epoch: 11549 \tValidation Loss: 0.348541 \tR2: 0.499137\n",
            " Epoch: 11550 \tTraining Loss:   0.273810\n",
            " Epoch: 11550 \tValidation Loss: 0.306984 \tR2: 0.499137\n",
            " Epoch: 11551 \tTraining Loss:   0.269234\n",
            " Epoch: 11551 \tValidation Loss: 0.350283 \tR2: 0.499137\n",
            " Epoch: 11552 \tTraining Loss:   0.266727\n",
            " Epoch: 11552 \tValidation Loss: 0.361580 \tR2: 0.499137\n",
            " Epoch: 11553 \tTraining Loss:   0.278760\n",
            " Epoch: 11553 \tValidation Loss: 0.326212 \tR2: 0.499137\n",
            " Epoch: 11554 \tTraining Loss:   0.286097\n",
            " Epoch: 11554 \tValidation Loss: 0.310434 \tR2: 0.499137\n",
            " Epoch: 11555 \tTraining Loss:   0.264584\n",
            " Epoch: 11555 \tValidation Loss: 0.307388 \tR2: 0.499137\n",
            " Epoch: 11556 \tTraining Loss:   0.290301\n",
            " Epoch: 11556 \tValidation Loss: 0.325827 \tR2: 0.499137\n",
            " Epoch: 11557 \tTraining Loss:   0.289768\n",
            " Epoch: 11557 \tValidation Loss: 0.342443 \tR2: 0.499137\n",
            " Epoch: 11558 \tTraining Loss:   0.272310\n",
            " Epoch: 11558 \tValidation Loss: 0.376204 \tR2: 0.499137\n",
            " Epoch: 11559 \tTraining Loss:   0.281402\n",
            " Epoch: 11559 \tValidation Loss: 0.310981 \tR2: 0.499137\n",
            " Epoch: 11560 \tTraining Loss:   0.273777\n",
            " Epoch: 11560 \tValidation Loss: 0.334892 \tR2: 0.499137\n",
            " Epoch: 11561 \tTraining Loss:   0.278913\n",
            " Epoch: 11561 \tValidation Loss: 0.332568 \tR2: 0.499137\n",
            " Epoch: 11562 \tTraining Loss:   0.269400\n",
            " Epoch: 11562 \tValidation Loss: 0.303402 \tR2: 0.499137\n",
            " Epoch: 11563 \tTraining Loss:   0.348369\n",
            " Epoch: 11563 \tValidation Loss: 0.330379 \tR2: 0.499137\n",
            " Epoch: 11564 \tTraining Loss:   0.269174\n",
            " Epoch: 11564 \tValidation Loss: 0.321693 \tR2: 0.499137\n",
            " Epoch: 11565 \tTraining Loss:   0.275168\n",
            " Epoch: 11565 \tValidation Loss: 0.336933 \tR2: 0.499137\n",
            " Epoch: 11566 \tTraining Loss:   0.275910\n",
            " Epoch: 11566 \tValidation Loss: 0.304405 \tR2: 0.499137\n",
            " Epoch: 11567 \tTraining Loss:   0.288511\n",
            " Epoch: 11567 \tValidation Loss: 0.347150 \tR2: 0.499137\n",
            " Epoch: 11568 \tTraining Loss:   0.277868\n",
            " Epoch: 11568 \tValidation Loss: 0.305393 \tR2: 0.499137\n",
            " Epoch: 11569 \tTraining Loss:   0.277224\n",
            " Epoch: 11569 \tValidation Loss: 0.320567 \tR2: 0.499137\n",
            " Epoch: 11570 \tTraining Loss:   0.280182\n",
            " Epoch: 11570 \tValidation Loss: 0.301939 \tR2: 0.499137\n",
            " Epoch: 11571 \tTraining Loss:   0.276493\n",
            " Epoch: 11571 \tValidation Loss: 0.335242 \tR2: 0.499137\n",
            " Epoch: 11572 \tTraining Loss:   0.280031\n",
            " Epoch: 11572 \tValidation Loss: 0.334731 \tR2: 0.499137\n",
            " Epoch: 11573 \tTraining Loss:   0.282710\n",
            " Epoch: 11573 \tValidation Loss: 0.389127 \tR2: 0.499137\n",
            " Epoch: 11574 \tTraining Loss:   0.284626\n",
            " Epoch: 11574 \tValidation Loss: 0.285722 \tR2: 0.499137\n",
            " Epoch: 11575 \tTraining Loss:   0.281119\n",
            " Epoch: 11575 \tValidation Loss: 0.342394 \tR2: 0.499137\n",
            " Epoch: 11576 \tTraining Loss:   0.284073\n",
            " Epoch: 11576 \tValidation Loss: 0.344483 \tR2: 0.499137\n",
            " Epoch: 11577 \tTraining Loss:   0.296461\n",
            " Epoch: 11577 \tValidation Loss: 0.332783 \tR2: 0.499137\n",
            " Epoch: 11578 \tTraining Loss:   0.275768\n",
            " Epoch: 11578 \tValidation Loss: 0.316998 \tR2: 0.499137\n",
            " Epoch: 11579 \tTraining Loss:   0.274563\n",
            " Epoch: 11579 \tValidation Loss: 0.299649 \tR2: 0.499137\n",
            " Epoch: 11580 \tTraining Loss:   0.290981\n",
            " Epoch: 11580 \tValidation Loss: 0.314556 \tR2: 0.499137\n",
            " Epoch: 11581 \tTraining Loss:   0.286549\n",
            " Epoch: 11581 \tValidation Loss: 0.332620 \tR2: 0.499137\n",
            " Epoch: 11582 \tTraining Loss:   0.273127\n",
            " Epoch: 11582 \tValidation Loss: 0.318524 \tR2: 0.499137\n",
            " Epoch: 11583 \tTraining Loss:   0.274774\n",
            " Epoch: 11583 \tValidation Loss: 0.335770 \tR2: 0.499137\n",
            " Epoch: 11584 \tTraining Loss:   0.275461\n",
            " Epoch: 11584 \tValidation Loss: 0.329880 \tR2: 0.499137\n",
            " Epoch: 11585 \tTraining Loss:   0.278547\n",
            " Epoch: 11585 \tValidation Loss: 0.322428 \tR2: 0.499137\n",
            " Epoch: 11586 \tTraining Loss:   0.282886\n",
            " Epoch: 11586 \tValidation Loss: 0.314870 \tR2: 0.499137\n",
            " Epoch: 11587 \tTraining Loss:   0.267564\n",
            " Epoch: 11587 \tValidation Loss: 0.291251 \tR2: 0.499137\n",
            " Epoch: 11588 \tTraining Loss:   0.260658\n",
            " Epoch: 11588 \tValidation Loss: 0.302461 \tR2: 0.499137\n",
            " Epoch: 11589 \tTraining Loss:   0.277008\n",
            " Epoch: 11589 \tValidation Loss: 0.312164 \tR2: 0.499137\n",
            " Epoch: 11590 \tTraining Loss:   0.269230\n",
            " Epoch: 11590 \tValidation Loss: 0.309927 \tR2: 0.499137\n",
            " Epoch: 11591 \tTraining Loss:   0.283017\n",
            " Epoch: 11591 \tValidation Loss: 0.435059 \tR2: 0.499137\n",
            " Epoch: 11592 \tTraining Loss:   0.276938\n",
            " Epoch: 11592 \tValidation Loss: 0.376651 \tR2: 0.499137\n",
            " Epoch: 11593 \tTraining Loss:   0.279393\n",
            " Epoch: 11593 \tValidation Loss: 0.305571 \tR2: 0.499137\n",
            " Epoch: 11594 \tTraining Loss:   0.269747\n",
            " Epoch: 11594 \tValidation Loss: 0.341391 \tR2: 0.499137\n",
            " Epoch: 11595 \tTraining Loss:   0.281082\n",
            " Epoch: 11595 \tValidation Loss: 0.315550 \tR2: 0.499137\n",
            " Epoch: 11596 \tTraining Loss:   0.286008\n",
            " Epoch: 11596 \tValidation Loss: 0.360559 \tR2: 0.499137\n",
            " Epoch: 11597 \tTraining Loss:   0.281610\n",
            " Epoch: 11597 \tValidation Loss: 0.340045 \tR2: 0.499137\n",
            " Epoch: 11598 \tTraining Loss:   0.277691\n",
            " Epoch: 11598 \tValidation Loss: 0.318929 \tR2: 0.499137\n",
            " Epoch: 11599 \tTraining Loss:   0.272189\n",
            " Epoch: 11599 \tValidation Loss: 0.352089 \tR2: 0.499137\n",
            " Epoch: 11600 \tTraining Loss:   0.306383\n",
            " Epoch: 11600 \tValidation Loss: 0.643916 \tR2: -4.042844\n",
            " Epoch: 11601 \tTraining Loss:   0.285192\n",
            " Epoch: 11601 \tValidation Loss: 0.315843 \tR2: -4.042844\n",
            " Epoch: 11602 \tTraining Loss:   0.279637\n",
            " Epoch: 11602 \tValidation Loss: 0.333631 \tR2: -4.042844\n",
            " Epoch: 11603 \tTraining Loss:   0.283570\n",
            " Epoch: 11603 \tValidation Loss: 0.291426 \tR2: -4.042844\n",
            " Epoch: 11604 \tTraining Loss:   0.276551\n",
            " Epoch: 11604 \tValidation Loss: 0.326250 \tR2: -4.042844\n",
            " Epoch: 11605 \tTraining Loss:   0.280095\n",
            " Epoch: 11605 \tValidation Loss: 0.332840 \tR2: -4.042844\n",
            " Epoch: 11606 \tTraining Loss:   0.272811\n",
            " Epoch: 11606 \tValidation Loss: 0.416133 \tR2: -4.042844\n",
            " Epoch: 11607 \tTraining Loss:   0.283731\n",
            " Epoch: 11607 \tValidation Loss: 0.311077 \tR2: -4.042844\n",
            " Epoch: 11608 \tTraining Loss:   0.268322\n",
            " Epoch: 11608 \tValidation Loss: 0.338808 \tR2: -4.042844\n",
            " Epoch: 11609 \tTraining Loss:   0.277641\n",
            " Epoch: 11609 \tValidation Loss: 0.346252 \tR2: -4.042844\n",
            " Epoch: 11610 \tTraining Loss:   0.274174\n",
            " Epoch: 11610 \tValidation Loss: 0.328233 \tR2: -4.042844\n",
            " Epoch: 11611 \tTraining Loss:   0.276869\n",
            " Epoch: 11611 \tValidation Loss: 0.364784 \tR2: -4.042844\n",
            " Epoch: 11612 \tTraining Loss:   0.305331\n",
            " Epoch: 11612 \tValidation Loss: 0.344880 \tR2: -4.042844\n",
            " Epoch: 11613 \tTraining Loss:   0.308839\n",
            " Epoch: 11613 \tValidation Loss: 0.324766 \tR2: -4.042844\n",
            " Epoch: 11614 \tTraining Loss:   0.277666\n",
            " Epoch: 11614 \tValidation Loss: 0.327668 \tR2: -4.042844\n",
            " Epoch: 11615 \tTraining Loss:   0.273130\n",
            " Epoch: 11615 \tValidation Loss: 0.326763 \tR2: -4.042844\n",
            " Epoch: 11616 \tTraining Loss:   0.296282\n",
            " Epoch: 11616 \tValidation Loss: 0.345876 \tR2: -4.042844\n",
            " Epoch: 11617 \tTraining Loss:   0.270372\n",
            " Epoch: 11617 \tValidation Loss: 0.352078 \tR2: -4.042844\n",
            " Epoch: 11618 \tTraining Loss:   0.275999\n",
            " Epoch: 11618 \tValidation Loss: 0.368194 \tR2: -4.042844\n",
            " Epoch: 11619 \tTraining Loss:   0.285827\n",
            " Epoch: 11619 \tValidation Loss: 0.308305 \tR2: -4.042844\n",
            " Epoch: 11620 \tTraining Loss:   0.266500\n",
            " Epoch: 11620 \tValidation Loss: 0.328140 \tR2: -4.042844\n",
            " Epoch: 11621 \tTraining Loss:   0.273693\n",
            " Epoch: 11621 \tValidation Loss: 0.310363 \tR2: -4.042844\n",
            " Epoch: 11622 \tTraining Loss:   0.265747\n",
            " Epoch: 11622 \tValidation Loss: 0.306155 \tR2: -4.042844\n",
            " Epoch: 11623 \tTraining Loss:   0.264549\n",
            " Epoch: 11623 \tValidation Loss: 0.314799 \tR2: -4.042844\n",
            " Epoch: 11624 \tTraining Loss:   0.276914\n",
            " Epoch: 11624 \tValidation Loss: 0.298523 \tR2: -4.042844\n",
            " Epoch: 11625 \tTraining Loss:   0.274055\n",
            " Epoch: 11625 \tValidation Loss: 0.400700 \tR2: -4.042844\n",
            " Epoch: 11626 \tTraining Loss:   0.289396\n",
            " Epoch: 11626 \tValidation Loss: 0.313392 \tR2: -4.042844\n",
            " Epoch: 11627 \tTraining Loss:   0.279305\n",
            " Epoch: 11627 \tValidation Loss: 0.309293 \tR2: -4.042844\n",
            " Epoch: 11628 \tTraining Loss:   0.275676\n",
            " Epoch: 11628 \tValidation Loss: 0.304345 \tR2: -4.042844\n",
            " Epoch: 11629 \tTraining Loss:   0.277734\n",
            " Epoch: 11629 \tValidation Loss: 0.393499 \tR2: -4.042844\n",
            " Epoch: 11630 \tTraining Loss:   0.290158\n",
            " Epoch: 11630 \tValidation Loss: 0.357992 \tR2: -4.042844\n",
            " Epoch: 11631 \tTraining Loss:   0.276700\n",
            " Epoch: 11631 \tValidation Loss: 0.325010 \tR2: -4.042844\n",
            " Epoch: 11632 \tTraining Loss:   0.283827\n",
            " Epoch: 11632 \tValidation Loss: 0.335330 \tR2: -4.042844\n",
            " Epoch: 11633 \tTraining Loss:   0.272588\n",
            " Epoch: 11633 \tValidation Loss: 0.318143 \tR2: -4.042844\n",
            " Epoch: 11634 \tTraining Loss:   0.280577\n",
            " Epoch: 11634 \tValidation Loss: 0.415207 \tR2: -4.042844\n",
            " Epoch: 11635 \tTraining Loss:   0.267310\n",
            " Epoch: 11635 \tValidation Loss: 0.355420 \tR2: -4.042844\n",
            " Epoch: 11636 \tTraining Loss:   0.269948\n",
            " Epoch: 11636 \tValidation Loss: 0.303676 \tR2: -4.042844\n",
            " Epoch: 11637 \tTraining Loss:   0.291491\n",
            " Epoch: 11637 \tValidation Loss: 0.402937 \tR2: -4.042844\n",
            " Epoch: 11638 \tTraining Loss:   0.277234\n",
            " Epoch: 11638 \tValidation Loss: 0.330072 \tR2: -4.042844\n",
            " Epoch: 11639 \tTraining Loss:   0.285721\n",
            " Epoch: 11639 \tValidation Loss: 0.315837 \tR2: -4.042844\n",
            " Epoch: 11640 \tTraining Loss:   0.277871\n",
            " Epoch: 11640 \tValidation Loss: 0.379023 \tR2: -4.042844\n",
            " Epoch: 11641 \tTraining Loss:   0.270751\n",
            " Epoch: 11641 \tValidation Loss: 0.308103 \tR2: -4.042844\n",
            " Epoch: 11642 \tTraining Loss:   0.278897\n",
            " Epoch: 11642 \tValidation Loss: 0.314701 \tR2: -4.042844\n",
            " Epoch: 11643 \tTraining Loss:   0.270869\n",
            " Epoch: 11643 \tValidation Loss: 0.331853 \tR2: -4.042844\n",
            " Epoch: 11644 \tTraining Loss:   0.289445\n",
            " Epoch: 11644 \tValidation Loss: 0.376310 \tR2: -4.042844\n",
            " Epoch: 11645 \tTraining Loss:   0.277840\n",
            " Epoch: 11645 \tValidation Loss: 0.318010 \tR2: -4.042844\n",
            " Epoch: 11646 \tTraining Loss:   0.264746\n",
            " Epoch: 11646 \tValidation Loss: 0.309573 \tR2: -4.042844\n",
            " Epoch: 11647 \tTraining Loss:   0.282635\n",
            " Epoch: 11647 \tValidation Loss: 0.330648 \tR2: -4.042844\n",
            " Epoch: 11648 \tTraining Loss:   0.283553\n",
            " Epoch: 11648 \tValidation Loss: 0.331386 \tR2: -4.042844\n",
            " Epoch: 11649 \tTraining Loss:   0.287930\n",
            " Epoch: 11649 \tValidation Loss: 0.300049 \tR2: -4.042844\n",
            " Epoch: 11650 \tTraining Loss:   0.278447\n",
            " Epoch: 11650 \tValidation Loss: 0.310321 \tR2: -4.042844\n",
            " Epoch: 11651 \tTraining Loss:   0.286417\n",
            " Epoch: 11651 \tValidation Loss: 0.320995 \tR2: -4.042844\n",
            " Epoch: 11652 \tTraining Loss:   0.291099\n",
            " Epoch: 11652 \tValidation Loss: 0.323806 \tR2: -4.042844\n",
            " Epoch: 11653 \tTraining Loss:   0.271806\n",
            " Epoch: 11653 \tValidation Loss: 0.315951 \tR2: -4.042844\n",
            " Epoch: 11654 \tTraining Loss:   0.289550\n",
            " Epoch: 11654 \tValidation Loss: 0.294149 \tR2: -4.042844\n",
            " Epoch: 11655 \tTraining Loss:   0.264215\n",
            " Epoch: 11655 \tValidation Loss: 0.301273 \tR2: -4.042844\n",
            " Epoch: 11656 \tTraining Loss:   0.274134\n",
            " Epoch: 11656 \tValidation Loss: 0.305737 \tR2: -4.042844\n",
            " Epoch: 11657 \tTraining Loss:   0.266571\n",
            " Epoch: 11657 \tValidation Loss: 0.328565 \tR2: -4.042844\n",
            " Epoch: 11658 \tTraining Loss:   0.282403\n",
            " Epoch: 11658 \tValidation Loss: 0.329546 \tR2: -4.042844\n",
            " Epoch: 11659 \tTraining Loss:   0.278503\n",
            " Epoch: 11659 \tValidation Loss: 0.385360 \tR2: -4.042844\n",
            " Epoch: 11660 \tTraining Loss:   0.278468\n",
            " Epoch: 11660 \tValidation Loss: 0.347032 \tR2: -4.042844\n",
            " Epoch: 11661 \tTraining Loss:   0.269398\n",
            " Epoch: 11661 \tValidation Loss: 0.327395 \tR2: -4.042844\n",
            " Epoch: 11662 \tTraining Loss:   0.276647\n",
            " Epoch: 11662 \tValidation Loss: 0.342496 \tR2: -4.042844\n",
            " Epoch: 11663 \tTraining Loss:   0.274920\n",
            " Epoch: 11663 \tValidation Loss: 0.326608 \tR2: -4.042844\n",
            " Epoch: 11664 \tTraining Loss:   0.285660\n",
            " Epoch: 11664 \tValidation Loss: 0.326254 \tR2: -4.042844\n",
            " Epoch: 11665 \tTraining Loss:   0.275014\n",
            " Epoch: 11665 \tValidation Loss: 0.294182 \tR2: -4.042844\n",
            " Epoch: 11666 \tTraining Loss:   0.286376\n",
            " Epoch: 11666 \tValidation Loss: 0.341425 \tR2: -4.042844\n",
            " Epoch: 11667 \tTraining Loss:   0.273160\n",
            " Epoch: 11667 \tValidation Loss: 0.322051 \tR2: -4.042844\n",
            " Epoch: 11668 \tTraining Loss:   0.276594\n",
            " Epoch: 11668 \tValidation Loss: 0.327968 \tR2: -4.042844\n",
            " Epoch: 11669 \tTraining Loss:   0.276017\n",
            " Epoch: 11669 \tValidation Loss: 0.329447 \tR2: -4.042844\n",
            " Epoch: 11670 \tTraining Loss:   0.269566\n",
            " Epoch: 11670 \tValidation Loss: 0.350555 \tR2: -4.042844\n",
            " Epoch: 11671 \tTraining Loss:   0.282072\n",
            " Epoch: 11671 \tValidation Loss: 0.319779 \tR2: -4.042844\n",
            " Epoch: 11672 \tTraining Loss:   0.266995\n",
            " Epoch: 11672 \tValidation Loss: 0.340095 \tR2: -4.042844\n",
            " Epoch: 11673 \tTraining Loss:   0.265918\n",
            " Epoch: 11673 \tValidation Loss: 0.322578 \tR2: -4.042844\n",
            " Epoch: 11674 \tTraining Loss:   0.277914\n",
            " Epoch: 11674 \tValidation Loss: 0.343190 \tR2: -4.042844\n",
            " Epoch: 11675 \tTraining Loss:   0.280050\n",
            " Epoch: 11675 \tValidation Loss: 0.521055 \tR2: -4.042844\n",
            " Epoch: 11676 \tTraining Loss:   0.281623\n",
            " Epoch: 11676 \tValidation Loss: 0.332076 \tR2: -4.042844\n",
            " Epoch: 11677 \tTraining Loss:   0.274027\n",
            " Epoch: 11677 \tValidation Loss: 0.310733 \tR2: -4.042844\n",
            " Epoch: 11678 \tTraining Loss:   0.281805\n",
            " Epoch: 11678 \tValidation Loss: 0.501994 \tR2: -4.042844\n",
            " Epoch: 11679 \tTraining Loss:   0.273875\n",
            " Epoch: 11679 \tValidation Loss: 0.365990 \tR2: -4.042844\n",
            " Epoch: 11680 \tTraining Loss:   0.270143\n",
            " Epoch: 11680 \tValidation Loss: 0.334637 \tR2: -4.042844\n",
            " Epoch: 11681 \tTraining Loss:   0.270002\n",
            " Epoch: 11681 \tValidation Loss: 0.325214 \tR2: -4.042844\n",
            " Epoch: 11682 \tTraining Loss:   0.272778\n",
            " Epoch: 11682 \tValidation Loss: 0.293842 \tR2: -4.042844\n",
            " Epoch: 11683 \tTraining Loss:   0.282165\n",
            " Epoch: 11683 \tValidation Loss: 0.288620 \tR2: -4.042844\n",
            " Epoch: 11684 \tTraining Loss:   0.275766\n",
            " Epoch: 11684 \tValidation Loss: 0.325278 \tR2: -4.042844\n",
            " Epoch: 11685 \tTraining Loss:   0.285057\n",
            " Epoch: 11685 \tValidation Loss: 0.474874 \tR2: -4.042844\n",
            " Epoch: 11686 \tTraining Loss:   0.283182\n",
            " Epoch: 11686 \tValidation Loss: 0.329965 \tR2: -4.042844\n",
            " Epoch: 11687 \tTraining Loss:   0.279379\n",
            " Epoch: 11687 \tValidation Loss: 0.397357 \tR2: -4.042844\n",
            " Epoch: 11688 \tTraining Loss:   0.276531\n",
            " Epoch: 11688 \tValidation Loss: 0.319058 \tR2: -4.042844\n",
            " Epoch: 11689 \tTraining Loss:   0.276182\n",
            " Epoch: 11689 \tValidation Loss: 0.315353 \tR2: -4.042844\n",
            " Epoch: 11690 \tTraining Loss:   0.294774\n",
            " Epoch: 11690 \tValidation Loss: 0.305659 \tR2: -4.042844\n",
            " Epoch: 11691 \tTraining Loss:   0.278504\n",
            " Epoch: 11691 \tValidation Loss: 0.333368 \tR2: -4.042844\n",
            " Epoch: 11692 \tTraining Loss:   0.275989\n",
            " Epoch: 11692 \tValidation Loss: 0.441248 \tR2: -4.042844\n",
            " Epoch: 11693 \tTraining Loss:   0.310792\n",
            " Epoch: 11693 \tValidation Loss: 0.334734 \tR2: -4.042844\n",
            " Epoch: 11694 \tTraining Loss:   0.266839\n",
            " Epoch: 11694 \tValidation Loss: 0.373020 \tR2: -4.042844\n",
            " Epoch: 11695 \tTraining Loss:   0.265734\n",
            " Epoch: 11695 \tValidation Loss: 0.305997 \tR2: -4.042844\n",
            " Epoch: 11696 \tTraining Loss:   0.290956\n",
            " Epoch: 11696 \tValidation Loss: 0.319031 \tR2: -4.042844\n",
            " Epoch: 11697 \tTraining Loss:   0.275621\n",
            " Epoch: 11697 \tValidation Loss: 0.344588 \tR2: -4.042844\n",
            " Epoch: 11698 \tTraining Loss:   0.278335\n",
            " Epoch: 11698 \tValidation Loss: 0.305862 \tR2: -4.042844\n",
            " Epoch: 11699 \tTraining Loss:   0.276893\n",
            " Epoch: 11699 \tValidation Loss: 0.309635 \tR2: -4.042844\n",
            " Epoch: 11700 \tTraining Loss:   0.273184\n",
            " Epoch: 11700 \tValidation Loss: 0.319195 \tR2: 0.628099\n",
            " Epoch: 11701 \tTraining Loss:   0.285188\n",
            " Epoch: 11701 \tValidation Loss: 0.325865 \tR2: 0.628099\n",
            " Epoch: 11702 \tTraining Loss:   0.275670\n",
            " Epoch: 11702 \tValidation Loss: 0.342936 \tR2: 0.628099\n",
            " Epoch: 11703 \tTraining Loss:   0.292516\n",
            " Epoch: 11703 \tValidation Loss: 0.342041 \tR2: 0.628099\n",
            " Epoch: 11704 \tTraining Loss:   0.283206\n",
            " Epoch: 11704 \tValidation Loss: 0.334318 \tR2: 0.628099\n",
            " Epoch: 11705 \tTraining Loss:   0.290455\n",
            " Epoch: 11705 \tValidation Loss: 0.336481 \tR2: 0.628099\n",
            " Epoch: 11706 \tTraining Loss:   0.274125\n",
            " Epoch: 11706 \tValidation Loss: 0.358292 \tR2: 0.628099\n",
            " Epoch: 11707 \tTraining Loss:   0.273042\n",
            " Epoch: 11707 \tValidation Loss: 0.299152 \tR2: 0.628099\n",
            " Epoch: 11708 \tTraining Loss:   0.273057\n",
            " Epoch: 11708 \tValidation Loss: 0.307717 \tR2: 0.628099\n",
            " Epoch: 11709 \tTraining Loss:   0.295139\n",
            " Epoch: 11709 \tValidation Loss: 0.323330 \tR2: 0.628099\n",
            " Epoch: 11710 \tTraining Loss:   0.270741\n",
            " Epoch: 11710 \tValidation Loss: 0.284024 \tR2: 0.628099\n",
            " Epoch: 11711 \tTraining Loss:   0.309520\n",
            " Epoch: 11711 \tValidation Loss: 0.356011 \tR2: 0.628099\n",
            " Epoch: 11712 \tTraining Loss:   0.274083\n",
            " Epoch: 11712 \tValidation Loss: 0.301878 \tR2: 0.628099\n",
            " Epoch: 11713 \tTraining Loss:   0.292300\n",
            " Epoch: 11713 \tValidation Loss: 0.359260 \tR2: 0.628099\n",
            " Epoch: 11714 \tTraining Loss:   0.270964\n",
            " Epoch: 11714 \tValidation Loss: 0.342328 \tR2: 0.628099\n",
            " Epoch: 11715 \tTraining Loss:   0.298181\n",
            " Epoch: 11715 \tValidation Loss: 0.379436 \tR2: 0.628099\n",
            " Epoch: 11716 \tTraining Loss:   0.300725\n",
            " Epoch: 11716 \tValidation Loss: 0.357841 \tR2: 0.628099\n",
            " Epoch: 11717 \tTraining Loss:   0.284710\n",
            " Epoch: 11717 \tValidation Loss: 0.326824 \tR2: 0.628099\n",
            " Epoch: 11718 \tTraining Loss:   0.269693\n",
            " Epoch: 11718 \tValidation Loss: 0.334442 \tR2: 0.628099\n",
            " Epoch: 11719 \tTraining Loss:   0.270739\n",
            " Epoch: 11719 \tValidation Loss: 0.328548 \tR2: 0.628099\n",
            " Epoch: 11720 \tTraining Loss:   0.273918\n",
            " Epoch: 11720 \tValidation Loss: 0.378030 \tR2: 0.628099\n",
            " Epoch: 11721 \tTraining Loss:   0.283756\n",
            " Epoch: 11721 \tValidation Loss: 0.352123 \tR2: 0.628099\n",
            " Epoch: 11722 \tTraining Loss:   0.273376\n",
            " Epoch: 11722 \tValidation Loss: 0.331274 \tR2: 0.628099\n",
            " Epoch: 11723 \tTraining Loss:   0.275930\n",
            " Epoch: 11723 \tValidation Loss: 0.354002 \tR2: 0.628099\n",
            " Epoch: 11724 \tTraining Loss:   0.277106\n",
            " Epoch: 11724 \tValidation Loss: 0.327982 \tR2: 0.628099\n",
            " Epoch: 11725 \tTraining Loss:   0.277272\n",
            " Epoch: 11725 \tValidation Loss: 0.336549 \tR2: 0.628099\n",
            " Epoch: 11726 \tTraining Loss:   0.297056\n",
            " Epoch: 11726 \tValidation Loss: 0.362066 \tR2: 0.628099\n",
            " Epoch: 11727 \tTraining Loss:   0.288903\n",
            " Epoch: 11727 \tValidation Loss: 0.292427 \tR2: 0.628099\n",
            " Epoch: 11728 \tTraining Loss:   0.282816\n",
            " Epoch: 11728 \tValidation Loss: 0.307751 \tR2: 0.628099\n",
            " Epoch: 11729 \tTraining Loss:   0.278735\n",
            " Epoch: 11729 \tValidation Loss: 0.316205 \tR2: 0.628099\n",
            " Epoch: 11730 \tTraining Loss:   0.279180\n",
            " Epoch: 11730 \tValidation Loss: 0.345643 \tR2: 0.628099\n",
            " Epoch: 11731 \tTraining Loss:   0.329164\n",
            " Epoch: 11731 \tValidation Loss: 0.366248 \tR2: 0.628099\n",
            " Epoch: 11732 \tTraining Loss:   0.296053\n",
            " Epoch: 11732 \tValidation Loss: 0.339780 \tR2: 0.628099\n",
            " Epoch: 11733 \tTraining Loss:   0.279921\n",
            " Epoch: 11733 \tValidation Loss: 0.306409 \tR2: 0.628099\n",
            " Epoch: 11734 \tTraining Loss:   0.284381\n",
            " Epoch: 11734 \tValidation Loss: 0.350366 \tR2: 0.628099\n",
            " Epoch: 11735 \tTraining Loss:   0.283242\n",
            " Epoch: 11735 \tValidation Loss: 0.360709 \tR2: 0.628099\n",
            " Epoch: 11736 \tTraining Loss:   0.278224\n",
            " Epoch: 11736 \tValidation Loss: 0.319381 \tR2: 0.628099\n",
            " Epoch: 11737 \tTraining Loss:   0.276793\n",
            " Epoch: 11737 \tValidation Loss: 0.344394 \tR2: 0.628099\n",
            " Epoch: 11738 \tTraining Loss:   0.271681\n",
            " Epoch: 11738 \tValidation Loss: 0.299303 \tR2: 0.628099\n",
            " Epoch: 11739 \tTraining Loss:   0.285324\n",
            " Epoch: 11739 \tValidation Loss: 0.347286 \tR2: 0.628099\n",
            " Epoch: 11740 \tTraining Loss:   0.285713\n",
            " Epoch: 11740 \tValidation Loss: 0.333988 \tR2: 0.628099\n",
            " Epoch: 11741 \tTraining Loss:   0.291854\n",
            " Epoch: 11741 \tValidation Loss: 0.359254 \tR2: 0.628099\n",
            " Epoch: 11742 \tTraining Loss:   0.327828\n",
            " Epoch: 11742 \tValidation Loss: 0.444398 \tR2: 0.628099\n",
            " Epoch: 11743 \tTraining Loss:   0.292377\n",
            " Epoch: 11743 \tValidation Loss: 0.358226 \tR2: 0.628099\n",
            " Epoch: 11744 \tTraining Loss:   0.281562\n",
            " Epoch: 11744 \tValidation Loss: 0.350016 \tR2: 0.628099\n",
            " Epoch: 11745 \tTraining Loss:   0.276009\n",
            " Epoch: 11745 \tValidation Loss: 0.331506 \tR2: 0.628099\n",
            " Epoch: 11746 \tTraining Loss:   0.275422\n",
            " Epoch: 11746 \tValidation Loss: 0.352597 \tR2: 0.628099\n",
            " Epoch: 11747 \tTraining Loss:   0.287738\n",
            " Epoch: 11747 \tValidation Loss: 0.408799 \tR2: 0.628099\n",
            " Epoch: 11748 \tTraining Loss:   0.272654\n",
            " Epoch: 11748 \tValidation Loss: 0.287688 \tR2: 0.628099\n",
            " Epoch: 11749 \tTraining Loss:   0.280345\n",
            " Epoch: 11749 \tValidation Loss: 0.320583 \tR2: 0.628099\n",
            " Epoch: 11750 \tTraining Loss:   0.294605\n",
            " Epoch: 11750 \tValidation Loss: 0.344054 \tR2: 0.628099\n",
            " Epoch: 11751 \tTraining Loss:   0.276915\n",
            " Epoch: 11751 \tValidation Loss: 0.311726 \tR2: 0.628099\n",
            " Epoch: 11752 \tTraining Loss:   0.281144\n",
            " Epoch: 11752 \tValidation Loss: 0.340756 \tR2: 0.628099\n",
            " Epoch: 11753 \tTraining Loss:   0.282313\n",
            " Epoch: 11753 \tValidation Loss: 0.352246 \tR2: 0.628099\n",
            " Epoch: 11754 \tTraining Loss:   0.270709\n",
            " Epoch: 11754 \tValidation Loss: 0.357667 \tR2: 0.628099\n",
            " Epoch: 11755 \tTraining Loss:   0.288981\n",
            " Epoch: 11755 \tValidation Loss: 0.379978 \tR2: 0.628099\n",
            " Epoch: 11756 \tTraining Loss:   0.270583\n",
            " Epoch: 11756 \tValidation Loss: 0.317645 \tR2: 0.628099\n",
            " Epoch: 11757 \tTraining Loss:   0.276902\n",
            " Epoch: 11757 \tValidation Loss: 0.339242 \tR2: 0.628099\n",
            " Epoch: 11758 \tTraining Loss:   0.305428\n",
            " Epoch: 11758 \tValidation Loss: 0.299405 \tR2: 0.628099\n",
            " Epoch: 11759 \tTraining Loss:   0.285629\n",
            " Epoch: 11759 \tValidation Loss: 0.318472 \tR2: 0.628099\n",
            " Epoch: 11760 \tTraining Loss:   0.287436\n",
            " Epoch: 11760 \tValidation Loss: 0.292070 \tR2: 0.628099\n",
            " Epoch: 11761 \tTraining Loss:   0.276008\n",
            " Epoch: 11761 \tValidation Loss: 0.333019 \tR2: 0.628099\n",
            " Epoch: 11762 \tTraining Loss:   0.273065\n",
            " Epoch: 11762 \tValidation Loss: 0.310534 \tR2: 0.628099\n",
            " Epoch: 11763 \tTraining Loss:   0.283244\n",
            " Epoch: 11763 \tValidation Loss: 0.331534 \tR2: 0.628099\n",
            " Epoch: 11764 \tTraining Loss:   0.291606\n",
            " Epoch: 11764 \tValidation Loss: 0.457322 \tR2: 0.628099\n",
            " Epoch: 11765 \tTraining Loss:   0.274674\n",
            " Epoch: 11765 \tValidation Loss: 0.294712 \tR2: 0.628099\n",
            " Epoch: 11766 \tTraining Loss:   0.263933\n",
            " Epoch: 11766 \tValidation Loss: 0.390830 \tR2: 0.628099\n",
            " Epoch: 11767 \tTraining Loss:   0.272011\n",
            " Epoch: 11767 \tValidation Loss: 0.329891 \tR2: 0.628099\n",
            " Epoch: 11768 \tTraining Loss:   0.266297\n",
            " Epoch: 11768 \tValidation Loss: 0.383048 \tR2: 0.628099\n",
            " Epoch: 11769 \tTraining Loss:   0.268212\n",
            " Epoch: 11769 \tValidation Loss: 0.347261 \tR2: 0.628099\n",
            " Epoch: 11770 \tTraining Loss:   0.268105\n",
            " Epoch: 11770 \tValidation Loss: 0.300304 \tR2: 0.628099\n",
            " Epoch: 11771 \tTraining Loss:   0.278996\n",
            " Epoch: 11771 \tValidation Loss: 0.342109 \tR2: 0.628099\n",
            " Epoch: 11772 \tTraining Loss:   0.288205\n",
            " Epoch: 11772 \tValidation Loss: 0.329069 \tR2: 0.628099\n",
            " Epoch: 11773 \tTraining Loss:   0.279232\n",
            " Epoch: 11773 \tValidation Loss: 0.328648 \tR2: 0.628099\n",
            " Epoch: 11774 \tTraining Loss:   0.284721\n",
            " Epoch: 11774 \tValidation Loss: 0.289180 \tR2: 0.628099\n",
            " Epoch: 11775 \tTraining Loss:   0.277570\n",
            " Epoch: 11775 \tValidation Loss: 0.288967 \tR2: 0.628099\n",
            " Epoch: 11776 \tTraining Loss:   0.274325\n",
            " Epoch: 11776 \tValidation Loss: 0.308707 \tR2: 0.628099\n",
            " Epoch: 11777 \tTraining Loss:   0.274258\n",
            " Epoch: 11777 \tValidation Loss: 0.297718 \tR2: 0.628099\n",
            " Epoch: 11778 \tTraining Loss:   0.281490\n",
            " Epoch: 11778 \tValidation Loss: 0.497151 \tR2: 0.628099\n",
            " Epoch: 11779 \tTraining Loss:   0.295079\n",
            " Epoch: 11779 \tValidation Loss: 0.338443 \tR2: 0.628099\n",
            " Epoch: 11780 \tTraining Loss:   0.285359\n",
            " Epoch: 11780 \tValidation Loss: 0.343399 \tR2: 0.628099\n",
            " Epoch: 11781 \tTraining Loss:   0.260883\n",
            " Epoch: 11781 \tValidation Loss: 0.301597 \tR2: 0.628099\n",
            " Epoch: 11782 \tTraining Loss:   0.284938\n",
            " Epoch: 11782 \tValidation Loss: 0.353504 \tR2: 0.628099\n",
            " Epoch: 11783 \tTraining Loss:   0.282480\n",
            " Epoch: 11783 \tValidation Loss: 0.296885 \tR2: 0.628099\n",
            " Epoch: 11784 \tTraining Loss:   0.267677\n",
            " Epoch: 11784 \tValidation Loss: 0.303562 \tR2: 0.628099\n",
            " Epoch: 11785 \tTraining Loss:   0.275860\n",
            " Epoch: 11785 \tValidation Loss: 0.323831 \tR2: 0.628099\n",
            " Epoch: 11786 \tTraining Loss:   0.274081\n",
            " Epoch: 11786 \tValidation Loss: 0.422438 \tR2: 0.628099\n",
            " Epoch: 11787 \tTraining Loss:   0.283671\n",
            " Epoch: 11787 \tValidation Loss: 0.354868 \tR2: 0.628099\n",
            " Epoch: 11788 \tTraining Loss:   0.288910\n",
            " Epoch: 11788 \tValidation Loss: 0.306930 \tR2: 0.628099\n",
            " Epoch: 11789 \tTraining Loss:   0.274270\n",
            " Epoch: 11789 \tValidation Loss: 0.371695 \tR2: 0.628099\n",
            " Epoch: 11790 \tTraining Loss:   0.277325\n",
            " Epoch: 11790 \tValidation Loss: 0.306421 \tR2: 0.628099\n",
            " Epoch: 11791 \tTraining Loss:   0.281698\n",
            " Epoch: 11791 \tValidation Loss: 0.922832 \tR2: 0.628099\n",
            " Epoch: 11792 \tTraining Loss:   0.307760\n",
            " Epoch: 11792 \tValidation Loss: 0.309728 \tR2: 0.628099\n",
            " Epoch: 11793 \tTraining Loss:   0.276800\n",
            " Epoch: 11793 \tValidation Loss: 0.313742 \tR2: 0.628099\n",
            " Epoch: 11794 \tTraining Loss:   0.287506\n",
            " Epoch: 11794 \tValidation Loss: 0.319599 \tR2: 0.628099\n",
            " Epoch: 11795 \tTraining Loss:   0.272552\n",
            " Epoch: 11795 \tValidation Loss: 0.331870 \tR2: 0.628099\n",
            " Epoch: 11796 \tTraining Loss:   0.288400\n",
            " Epoch: 11796 \tValidation Loss: 0.317177 \tR2: 0.628099\n",
            " Epoch: 11797 \tTraining Loss:   0.273944\n",
            " Epoch: 11797 \tValidation Loss: 0.357707 \tR2: 0.628099\n",
            " Epoch: 11798 \tTraining Loss:   0.269063\n",
            " Epoch: 11798 \tValidation Loss: 0.399724 \tR2: 0.628099\n",
            " Epoch: 11799 \tTraining Loss:   0.270767\n",
            " Epoch: 11799 \tValidation Loss: 0.327688 \tR2: 0.628099\n",
            " Epoch: 11800 \tTraining Loss:   0.271248\n",
            " Epoch: 11800 \tValidation Loss: 0.306394 \tR2: 0.523570\n",
            " Epoch: 11801 \tTraining Loss:   0.299433\n",
            " Epoch: 11801 \tValidation Loss: 0.306132 \tR2: 0.523570\n",
            " Epoch: 11802 \tTraining Loss:   0.277331\n",
            " Epoch: 11802 \tValidation Loss: 0.328111 \tR2: 0.523570\n",
            " Epoch: 11803 \tTraining Loss:   0.279055\n",
            " Epoch: 11803 \tValidation Loss: 0.285073 \tR2: 0.523570\n",
            " Epoch: 11804 \tTraining Loss:   0.284404\n",
            " Epoch: 11804 \tValidation Loss: 0.320903 \tR2: 0.523570\n",
            " Epoch: 11805 \tTraining Loss:   0.278078\n",
            " Epoch: 11805 \tValidation Loss: 0.311172 \tR2: 0.523570\n",
            " Epoch: 11806 \tTraining Loss:   0.282258\n",
            " Epoch: 11806 \tValidation Loss: 0.342858 \tR2: 0.523570\n",
            " Epoch: 11807 \tTraining Loss:   0.278802\n",
            " Epoch: 11807 \tValidation Loss: 0.296581 \tR2: 0.523570\n",
            " Epoch: 11808 \tTraining Loss:   0.284477\n",
            " Epoch: 11808 \tValidation Loss: 0.299958 \tR2: 0.523570\n",
            " Epoch: 11809 \tTraining Loss:   0.268664\n",
            " Epoch: 11809 \tValidation Loss: 0.282936 \tR2: 0.523570\n",
            " Epoch: 11810 \tTraining Loss:   0.271136\n",
            " Epoch: 11810 \tValidation Loss: 0.295693 \tR2: 0.523570\n",
            " Epoch: 11811 \tTraining Loss:   0.292089\n",
            " Epoch: 11811 \tValidation Loss: 0.329289 \tR2: 0.523570\n",
            " Epoch: 11812 \tTraining Loss:   0.280046\n",
            " Epoch: 11812 \tValidation Loss: 0.367289 \tR2: 0.523570\n",
            " Epoch: 11813 \tTraining Loss:   0.294562\n",
            " Epoch: 11813 \tValidation Loss: 0.334335 \tR2: 0.523570\n",
            " Epoch: 11814 \tTraining Loss:   0.295256\n",
            " Epoch: 11814 \tValidation Loss: 0.333835 \tR2: 0.523570\n",
            " Epoch: 11815 \tTraining Loss:   0.274928\n",
            " Epoch: 11815 \tValidation Loss: 0.327941 \tR2: 0.523570\n",
            " Epoch: 11816 \tTraining Loss:   0.323174\n",
            " Epoch: 11816 \tValidation Loss: 0.408828 \tR2: 0.523570\n",
            " Epoch: 11817 \tTraining Loss:   0.303254\n",
            " Epoch: 11817 \tValidation Loss: 0.437994 \tR2: 0.523570\n",
            " Epoch: 11818 \tTraining Loss:   0.287574\n",
            " Epoch: 11818 \tValidation Loss: 0.371747 \tR2: 0.523570\n",
            " Epoch: 11819 \tTraining Loss:   0.297072\n",
            " Epoch: 11819 \tValidation Loss: 0.364759 \tR2: 0.523570\n",
            " Epoch: 11820 \tTraining Loss:   0.275343\n",
            " Epoch: 11820 \tValidation Loss: 0.351193 \tR2: 0.523570\n",
            " Epoch: 11821 \tTraining Loss:   0.302189\n",
            " Epoch: 11821 \tValidation Loss: 0.371091 \tR2: 0.523570\n",
            " Epoch: 11822 \tTraining Loss:   0.281366\n",
            " Epoch: 11822 \tValidation Loss: 0.339638 \tR2: 0.523570\n",
            " Epoch: 11823 \tTraining Loss:   0.272663\n",
            " Epoch: 11823 \tValidation Loss: 0.315400 \tR2: 0.523570\n",
            " Epoch: 11824 \tTraining Loss:   0.306061\n",
            " Epoch: 11824 \tValidation Loss: 0.349828 \tR2: 0.523570\n",
            " Epoch: 11825 \tTraining Loss:   0.279805\n",
            " Epoch: 11825 \tValidation Loss: 0.327940 \tR2: 0.523570\n",
            " Epoch: 11826 \tTraining Loss:   0.289037\n",
            " Epoch: 11826 \tValidation Loss: 0.333687 \tR2: 0.523570\n",
            " Epoch: 11827 \tTraining Loss:   0.282242\n",
            " Epoch: 11827 \tValidation Loss: 0.314531 \tR2: 0.523570\n",
            " Epoch: 11828 \tTraining Loss:   0.274551\n",
            " Epoch: 11828 \tValidation Loss: 0.319621 \tR2: 0.523570\n",
            " Epoch: 11829 \tTraining Loss:   0.280643\n",
            " Epoch: 11829 \tValidation Loss: 0.313117 \tR2: 0.523570\n",
            " Epoch: 11830 \tTraining Loss:   0.280790\n",
            " Epoch: 11830 \tValidation Loss: 0.348462 \tR2: 0.523570\n",
            " Epoch: 11831 \tTraining Loss:   0.275089\n",
            " Epoch: 11831 \tValidation Loss: 0.325563 \tR2: 0.523570\n",
            " Epoch: 11832 \tTraining Loss:   0.287647\n",
            " Epoch: 11832 \tValidation Loss: 0.366962 \tR2: 0.523570\n",
            " Epoch: 11833 \tTraining Loss:   0.293740\n",
            " Epoch: 11833 \tValidation Loss: 0.302406 \tR2: 0.523570\n",
            " Epoch: 11834 \tTraining Loss:   0.289887\n",
            " Epoch: 11834 \tValidation Loss: 0.338453 \tR2: 0.523570\n",
            " Epoch: 11835 \tTraining Loss:   0.283989\n",
            " Epoch: 11835 \tValidation Loss: 0.306932 \tR2: 0.523570\n",
            " Epoch: 11836 \tTraining Loss:   0.309683\n",
            " Epoch: 11836 \tValidation Loss: 0.330529 \tR2: 0.523570\n",
            " Epoch: 11837 \tTraining Loss:   0.281603\n",
            " Epoch: 11837 \tValidation Loss: 0.287566 \tR2: 0.523570\n",
            " Epoch: 11838 \tTraining Loss:   0.268723\n",
            " Epoch: 11838 \tValidation Loss: 0.288973 \tR2: 0.523570\n",
            " Epoch: 11839 \tTraining Loss:   0.266124\n",
            " Epoch: 11839 \tValidation Loss: 0.310125 \tR2: 0.523570\n",
            " Epoch: 11840 \tTraining Loss:   0.268233\n",
            " Epoch: 11840 \tValidation Loss: 0.342723 \tR2: 0.523570\n",
            " Epoch: 11841 \tTraining Loss:   0.279608\n",
            " Epoch: 11841 \tValidation Loss: 0.328231 \tR2: 0.523570\n",
            " Epoch: 11842 \tTraining Loss:   0.286607\n",
            " Epoch: 11842 \tValidation Loss: 0.343292 \tR2: 0.523570\n",
            " Epoch: 11843 \tTraining Loss:   0.287025\n",
            " Epoch: 11843 \tValidation Loss: 0.373497 \tR2: 0.523570\n",
            " Epoch: 11844 \tTraining Loss:   0.275954\n",
            " Epoch: 11844 \tValidation Loss: 0.357897 \tR2: 0.523570\n",
            " Epoch: 11845 \tTraining Loss:   0.280417\n",
            " Epoch: 11845 \tValidation Loss: 0.331597 \tR2: 0.523570\n",
            " Epoch: 11846 \tTraining Loss:   0.269601\n",
            " Epoch: 11846 \tValidation Loss: 0.284256 \tR2: 0.523570\n",
            " Epoch: 11847 \tTraining Loss:   0.277201\n",
            " Epoch: 11847 \tValidation Loss: 0.292361 \tR2: 0.523570\n",
            " Epoch: 11848 \tTraining Loss:   0.273756\n",
            " Epoch: 11848 \tValidation Loss: 0.289363 \tR2: 0.523570\n",
            " Epoch: 11849 \tTraining Loss:   0.272008\n",
            " Epoch: 11849 \tValidation Loss: 0.324984 \tR2: 0.523570\n",
            " Epoch: 11850 \tTraining Loss:   0.265558\n",
            " Epoch: 11850 \tValidation Loss: 0.502713 \tR2: 0.523570\n",
            " Epoch: 11851 \tTraining Loss:   0.272955\n",
            " Epoch: 11851 \tValidation Loss: 0.317287 \tR2: 0.523570\n",
            " Epoch: 11852 \tTraining Loss:   0.283911\n",
            " Epoch: 11852 \tValidation Loss: 0.333562 \tR2: 0.523570\n",
            " Epoch: 11853 \tTraining Loss:   0.272256\n",
            " Epoch: 11853 \tValidation Loss: 0.314302 \tR2: 0.523570\n",
            " Epoch: 11854 \tTraining Loss:   0.282423\n",
            " Epoch: 11854 \tValidation Loss: 0.329038 \tR2: 0.523570\n",
            " Epoch: 11855 \tTraining Loss:   0.272088\n",
            " Epoch: 11855 \tValidation Loss: 0.308793 \tR2: 0.523570\n",
            " Epoch: 11856 \tTraining Loss:   0.266732\n",
            " Epoch: 11856 \tValidation Loss: 0.309117 \tR2: 0.523570\n",
            " Epoch: 11857 \tTraining Loss:   0.280870\n",
            " Epoch: 11857 \tValidation Loss: 0.334307 \tR2: 0.523570\n",
            " Epoch: 11858 \tTraining Loss:   0.282449\n",
            " Epoch: 11858 \tValidation Loss: 0.312999 \tR2: 0.523570\n",
            " Epoch: 11859 \tTraining Loss:   0.277569\n",
            " Epoch: 11859 \tValidation Loss: 0.310660 \tR2: 0.523570\n",
            " Epoch: 11860 \tTraining Loss:   0.278143\n",
            " Epoch: 11860 \tValidation Loss: 0.344033 \tR2: 0.523570\n",
            " Epoch: 11861 \tTraining Loss:   0.280764\n",
            " Epoch: 11861 \tValidation Loss: 0.319686 \tR2: 0.523570\n",
            " Epoch: 11862 \tTraining Loss:   0.267607\n",
            " Epoch: 11862 \tValidation Loss: 0.312801 \tR2: 0.523570\n",
            " Epoch: 11863 \tTraining Loss:   0.286597\n",
            " Epoch: 11863 \tValidation Loss: 0.382521 \tR2: 0.523570\n",
            " Epoch: 11864 \tTraining Loss:   0.284354\n",
            " Epoch: 11864 \tValidation Loss: 0.281188 \tR2: 0.523570\n",
            " Epoch: 11865 \tTraining Loss:   0.274804\n",
            " Epoch: 11865 \tValidation Loss: 0.299710 \tR2: 0.523570\n",
            " Epoch: 11866 \tTraining Loss:   0.289473\n",
            " Epoch: 11866 \tValidation Loss: 0.305019 \tR2: 0.523570\n",
            " Epoch: 11867 \tTraining Loss:   0.278281\n",
            " Epoch: 11867 \tValidation Loss: 0.324603 \tR2: 0.523570\n",
            " Epoch: 11868 \tTraining Loss:   0.279016\n",
            " Epoch: 11868 \tValidation Loss: 0.324023 \tR2: 0.523570\n",
            " Epoch: 11869 \tTraining Loss:   0.285901\n",
            " Epoch: 11869 \tValidation Loss: 0.313987 \tR2: 0.523570\n",
            " Epoch: 11870 \tTraining Loss:   0.274270\n",
            " Epoch: 11870 \tValidation Loss: 0.301911 \tR2: 0.523570\n",
            " Epoch: 11871 \tTraining Loss:   0.267700\n",
            " Epoch: 11871 \tValidation Loss: 0.304203 \tR2: 0.523570\n",
            " Epoch: 11872 \tTraining Loss:   0.269587\n",
            " Epoch: 11872 \tValidation Loss: 0.334540 \tR2: 0.523570\n",
            " Epoch: 11873 \tTraining Loss:   0.267998\n",
            " Epoch: 11873 \tValidation Loss: 0.335821 \tR2: 0.523570\n",
            " Epoch: 11874 \tTraining Loss:   0.277687\n",
            " Epoch: 11874 \tValidation Loss: 0.307271 \tR2: 0.523570\n",
            " Epoch: 11875 \tTraining Loss:   0.271089\n",
            " Epoch: 11875 \tValidation Loss: 0.329349 \tR2: 0.523570\n",
            " Epoch: 11876 \tTraining Loss:   0.268967\n",
            " Epoch: 11876 \tValidation Loss: 0.309946 \tR2: 0.523570\n",
            " Epoch: 11877 \tTraining Loss:   0.276722\n",
            " Epoch: 11877 \tValidation Loss: 1.608823 \tR2: 0.523570\n",
            " Epoch: 11878 \tTraining Loss:   0.323319\n",
            " Epoch: 11878 \tValidation Loss: 0.326080 \tR2: 0.523570\n",
            " Epoch: 11879 \tTraining Loss:   0.271999\n",
            " Epoch: 11879 \tValidation Loss: 0.307198 \tR2: 0.523570\n",
            " Epoch: 11880 \tTraining Loss:   0.294782\n",
            " Epoch: 11880 \tValidation Loss: 0.346284 \tR2: 0.523570\n",
            " Epoch: 11881 \tTraining Loss:   0.275102\n",
            " Epoch: 11881 \tValidation Loss: 0.358800 \tR2: 0.523570\n",
            " Epoch: 11882 \tTraining Loss:   0.275610\n",
            " Epoch: 11882 \tValidation Loss: 0.300744 \tR2: 0.523570\n",
            " Epoch: 11883 \tTraining Loss:   0.260337\n",
            " Epoch: 11883 \tValidation Loss: 0.353230 \tR2: 0.523570\n",
            " Epoch: 11884 \tTraining Loss:   0.283232\n",
            " Epoch: 11884 \tValidation Loss: 0.310264 \tR2: 0.523570\n",
            " Epoch: 11885 \tTraining Loss:   0.277087\n",
            " Epoch: 11885 \tValidation Loss: 0.317858 \tR2: 0.523570\n",
            " Epoch: 11886 \tTraining Loss:   0.265524\n",
            " Epoch: 11886 \tValidation Loss: 0.322803 \tR2: 0.523570\n",
            " Epoch: 11887 \tTraining Loss:   0.282675\n",
            " Epoch: 11887 \tValidation Loss: 0.351825 \tR2: 0.523570\n",
            " Epoch: 11888 \tTraining Loss:   0.280268\n",
            " Epoch: 11888 \tValidation Loss: 0.354771 \tR2: 0.523570\n",
            " Epoch: 11889 \tTraining Loss:   0.274697\n",
            " Epoch: 11889 \tValidation Loss: 0.311980 \tR2: 0.523570\n",
            " Epoch: 11890 \tTraining Loss:   0.303120\n",
            " Epoch: 11890 \tValidation Loss: 0.354240 \tR2: 0.523570\n",
            " Epoch: 11891 \tTraining Loss:   0.289969\n",
            " Epoch: 11891 \tValidation Loss: 0.317049 \tR2: 0.523570\n",
            " Epoch: 11892 \tTraining Loss:   0.296819\n",
            " Epoch: 11892 \tValidation Loss: 0.328196 \tR2: 0.523570\n",
            " Epoch: 11893 \tTraining Loss:   0.276442\n",
            " Epoch: 11893 \tValidation Loss: 0.292179 \tR2: 0.523570\n",
            " Epoch: 11894 \tTraining Loss:   0.276587\n",
            " Epoch: 11894 \tValidation Loss: 0.311070 \tR2: 0.523570\n",
            " Epoch: 11895 \tTraining Loss:   0.281845\n",
            " Epoch: 11895 \tValidation Loss: 0.371639 \tR2: 0.523570\n",
            " Epoch: 11896 \tTraining Loss:   0.282689\n",
            " Epoch: 11896 \tValidation Loss: 0.332097 \tR2: 0.523570\n",
            " Epoch: 11897 \tTraining Loss:   0.300292\n",
            " Epoch: 11897 \tValidation Loss: 0.356983 \tR2: 0.523570\n",
            " Epoch: 11898 \tTraining Loss:   0.289871\n",
            " Epoch: 11898 \tValidation Loss: 0.372043 \tR2: 0.523570\n",
            " Epoch: 11899 \tTraining Loss:   0.276045\n",
            " Epoch: 11899 \tValidation Loss: 0.345572 \tR2: 0.523570\n",
            " Epoch: 11900 \tTraining Loss:   0.276290\n",
            " Epoch: 11900 \tValidation Loss: 0.370663 \tR2: 0.466171\n",
            " Epoch: 11901 \tTraining Loss:   0.264936\n",
            " Epoch: 11901 \tValidation Loss: 0.299371 \tR2: 0.466171\n",
            " Epoch: 11902 \tTraining Loss:   0.271051\n",
            " Epoch: 11902 \tValidation Loss: 0.366414 \tR2: 0.466171\n",
            " Epoch: 11903 \tTraining Loss:   0.274541\n",
            " Epoch: 11903 \tValidation Loss: 0.313368 \tR2: 0.466171\n",
            " Epoch: 11904 \tTraining Loss:   0.287597\n",
            " Epoch: 11904 \tValidation Loss: 0.354540 \tR2: 0.466171\n",
            " Epoch: 11905 \tTraining Loss:   0.273538\n",
            " Epoch: 11905 \tValidation Loss: 0.303761 \tR2: 0.466171\n",
            " Epoch: 11906 \tTraining Loss:   0.292765\n",
            " Epoch: 11906 \tValidation Loss: 0.312412 \tR2: 0.466171\n",
            " Epoch: 11907 \tTraining Loss:   0.269818\n",
            " Epoch: 11907 \tValidation Loss: 0.329927 \tR2: 0.466171\n",
            " Epoch: 11908 \tTraining Loss:   0.280451\n",
            " Epoch: 11908 \tValidation Loss: 0.311069 \tR2: 0.466171\n",
            " Epoch: 11909 \tTraining Loss:   0.281083\n",
            " Epoch: 11909 \tValidation Loss: 0.331206 \tR2: 0.466171\n",
            " Epoch: 11910 \tTraining Loss:   0.281597\n",
            " Epoch: 11910 \tValidation Loss: 0.407091 \tR2: 0.466171\n",
            " Epoch: 11911 \tTraining Loss:   0.294862\n",
            " Epoch: 11911 \tValidation Loss: 0.421988 \tR2: 0.466171\n",
            " Epoch: 11912 \tTraining Loss:   0.309639\n",
            " Epoch: 11912 \tValidation Loss: 0.333086 \tR2: 0.466171\n",
            " Epoch: 11913 \tTraining Loss:   0.273225\n",
            " Epoch: 11913 \tValidation Loss: 0.330025 \tR2: 0.466171\n",
            " Epoch: 11914 \tTraining Loss:   0.275067\n",
            " Epoch: 11914 \tValidation Loss: 0.350275 \tR2: 0.466171\n",
            " Epoch: 11915 \tTraining Loss:   0.293293\n",
            " Epoch: 11915 \tValidation Loss: 0.329629 \tR2: 0.466171\n",
            " Epoch: 11916 \tTraining Loss:   0.278589\n",
            " Epoch: 11916 \tValidation Loss: 0.341155 \tR2: 0.466171\n",
            " Epoch: 11917 \tTraining Loss:   0.280387\n",
            " Epoch: 11917 \tValidation Loss: 0.321029 \tR2: 0.466171\n",
            " Epoch: 11918 \tTraining Loss:   0.276179\n",
            " Epoch: 11918 \tValidation Loss: 0.413961 \tR2: 0.466171\n",
            " Epoch: 11919 \tTraining Loss:   0.281685\n",
            " Epoch: 11919 \tValidation Loss: 0.362291 \tR2: 0.466171\n",
            " Epoch: 11920 \tTraining Loss:   0.285516\n",
            " Epoch: 11920 \tValidation Loss: 0.336054 \tR2: 0.466171\n",
            " Epoch: 11921 \tTraining Loss:   0.277734\n",
            " Epoch: 11921 \tValidation Loss: 0.338022 \tR2: 0.466171\n",
            " Epoch: 11922 \tTraining Loss:   0.274502\n",
            " Epoch: 11922 \tValidation Loss: 0.363919 \tR2: 0.466171\n",
            " Epoch: 11923 \tTraining Loss:   0.267354\n",
            " Epoch: 11923 \tValidation Loss: 0.371407 \tR2: 0.466171\n",
            " Epoch: 11924 \tTraining Loss:   0.280207\n",
            " Epoch: 11924 \tValidation Loss: 0.306825 \tR2: 0.466171\n",
            " Epoch: 11925 \tTraining Loss:   0.299306\n",
            " Epoch: 11925 \tValidation Loss: 0.351985 \tR2: 0.466171\n",
            " Epoch: 11926 \tTraining Loss:   0.279047\n",
            " Epoch: 11926 \tValidation Loss: 0.338716 \tR2: 0.466171\n",
            " Epoch: 11927 \tTraining Loss:   0.285636\n",
            " Epoch: 11927 \tValidation Loss: 0.323378 \tR2: 0.466171\n",
            " Epoch: 11928 \tTraining Loss:   0.280559\n",
            " Epoch: 11928 \tValidation Loss: 0.298928 \tR2: 0.466171\n",
            " Epoch: 11929 \tTraining Loss:   0.286830\n",
            " Epoch: 11929 \tValidation Loss: 0.333878 \tR2: 0.466171\n",
            " Epoch: 11930 \tTraining Loss:   0.275888\n",
            " Epoch: 11930 \tValidation Loss: 0.303269 \tR2: 0.466171\n",
            " Epoch: 11931 \tTraining Loss:   0.288844\n",
            " Epoch: 11931 \tValidation Loss: 0.329447 \tR2: 0.466171\n",
            " Epoch: 11932 \tTraining Loss:   0.281765\n",
            " Epoch: 11932 \tValidation Loss: 0.402301 \tR2: 0.466171\n",
            " Epoch: 11933 \tTraining Loss:   0.275144\n",
            " Epoch: 11933 \tValidation Loss: 0.316094 \tR2: 0.466171\n",
            " Epoch: 11934 \tTraining Loss:   0.280449\n",
            " Epoch: 11934 \tValidation Loss: 0.340444 \tR2: 0.466171\n",
            " Epoch: 11935 \tTraining Loss:   0.271463\n",
            " Epoch: 11935 \tValidation Loss: 0.335905 \tR2: 0.466171\n",
            " Epoch: 11936 \tTraining Loss:   0.268897\n",
            " Epoch: 11936 \tValidation Loss: 0.320215 \tR2: 0.466171\n",
            " Epoch: 11937 \tTraining Loss:   0.273416\n",
            " Epoch: 11937 \tValidation Loss: 0.348954 \tR2: 0.466171\n",
            " Epoch: 11938 \tTraining Loss:   0.277251\n",
            " Epoch: 11938 \tValidation Loss: 0.395900 \tR2: 0.466171\n",
            " Epoch: 11939 \tTraining Loss:   0.287627\n",
            " Epoch: 11939 \tValidation Loss: 0.330051 \tR2: 0.466171\n",
            " Epoch: 11940 \tTraining Loss:   0.276620\n",
            " Epoch: 11940 \tValidation Loss: 0.329908 \tR2: 0.466171\n",
            " Epoch: 11941 \tTraining Loss:   0.266282\n",
            " Epoch: 11941 \tValidation Loss: 0.342392 \tR2: 0.466171\n",
            " Epoch: 11942 \tTraining Loss:   0.283786\n",
            " Epoch: 11942 \tValidation Loss: 0.345438 \tR2: 0.466171\n",
            " Epoch: 11943 \tTraining Loss:   0.292880\n",
            " Epoch: 11943 \tValidation Loss: 0.334149 \tR2: 0.466171\n",
            " Epoch: 11944 \tTraining Loss:   0.286650\n",
            " Epoch: 11944 \tValidation Loss: 0.410618 \tR2: 0.466171\n",
            " Epoch: 11945 \tTraining Loss:   0.278855\n",
            " Epoch: 11945 \tValidation Loss: 0.316460 \tR2: 0.466171\n",
            " Epoch: 11946 \tTraining Loss:   0.304513\n",
            " Epoch: 11946 \tValidation Loss: 0.297375 \tR2: 0.466171\n",
            " Epoch: 11947 \tTraining Loss:   0.274428\n",
            " Epoch: 11947 \tValidation Loss: 0.345529 \tR2: 0.466171\n",
            " Epoch: 11948 \tTraining Loss:   0.307456\n",
            " Epoch: 11948 \tValidation Loss: 0.351802 \tR2: 0.466171\n",
            " Epoch: 11949 \tTraining Loss:   0.281468\n",
            " Epoch: 11949 \tValidation Loss: 0.318406 \tR2: 0.466171\n",
            " Epoch: 11950 \tTraining Loss:   0.295805\n",
            " Epoch: 11950 \tValidation Loss: 0.379261 \tR2: 0.466171\n",
            " Epoch: 11951 \tTraining Loss:   0.289823\n",
            " Epoch: 11951 \tValidation Loss: 0.327333 \tR2: 0.466171\n",
            " Epoch: 11952 \tTraining Loss:   0.270404\n",
            " Epoch: 11952 \tValidation Loss: 0.303817 \tR2: 0.466171\n",
            " Epoch: 11953 \tTraining Loss:   0.273904\n",
            " Epoch: 11953 \tValidation Loss: 0.392656 \tR2: 0.466171\n",
            " Epoch: 11954 \tTraining Loss:   0.286009\n",
            " Epoch: 11954 \tValidation Loss: 0.304884 \tR2: 0.466171\n",
            " Epoch: 11955 \tTraining Loss:   0.269235\n",
            " Epoch: 11955 \tValidation Loss: 0.329006 \tR2: 0.466171\n",
            " Epoch: 11956 \tTraining Loss:   0.279547\n",
            " Epoch: 11956 \tValidation Loss: 0.324025 \tR2: 0.466171\n",
            " Epoch: 11957 \tTraining Loss:   0.275453\n",
            " Epoch: 11957 \tValidation Loss: 0.296590 \tR2: 0.466171\n",
            " Epoch: 11958 \tTraining Loss:   0.278258\n",
            " Epoch: 11958 \tValidation Loss: 0.294710 \tR2: 0.466171\n",
            " Epoch: 11959 \tTraining Loss:   0.277945\n",
            " Epoch: 11959 \tValidation Loss: 0.345294 \tR2: 0.466171\n",
            " Epoch: 11960 \tTraining Loss:   0.279663\n",
            " Epoch: 11960 \tValidation Loss: 0.372486 \tR2: 0.466171\n",
            " Epoch: 11961 \tTraining Loss:   0.270316\n",
            " Epoch: 11961 \tValidation Loss: 0.347140 \tR2: 0.466171\n",
            " Epoch: 11962 \tTraining Loss:   0.270944\n",
            " Epoch: 11962 \tValidation Loss: 0.338164 \tR2: 0.466171\n",
            " Epoch: 11963 \tTraining Loss:   0.279769\n",
            " Epoch: 11963 \tValidation Loss: 0.315728 \tR2: 0.466171\n",
            " Epoch: 11964 \tTraining Loss:   0.274786\n",
            " Epoch: 11964 \tValidation Loss: 0.310242 \tR2: 0.466171\n",
            " Epoch: 11965 \tTraining Loss:   0.268575\n",
            " Epoch: 11965 \tValidation Loss: 0.311845 \tR2: 0.466171\n",
            " Epoch: 11966 \tTraining Loss:   0.277694\n",
            " Epoch: 11966 \tValidation Loss: 0.315616 \tR2: 0.466171\n",
            " Epoch: 11967 \tTraining Loss:   0.303099\n",
            " Epoch: 11967 \tValidation Loss: 0.325109 \tR2: 0.466171\n",
            " Epoch: 11968 \tTraining Loss:   0.267064\n",
            " Epoch: 11968 \tValidation Loss: 0.299980 \tR2: 0.466171\n",
            " Epoch: 11969 \tTraining Loss:   0.275583\n",
            " Epoch: 11969 \tValidation Loss: 0.349772 \tR2: 0.466171\n",
            " Epoch: 11970 \tTraining Loss:   0.271989\n",
            " Epoch: 11970 \tValidation Loss: 0.316213 \tR2: 0.466171\n",
            " Epoch: 11971 \tTraining Loss:   0.284292\n",
            " Epoch: 11971 \tValidation Loss: 0.313110 \tR2: 0.466171\n",
            " Epoch: 11972 \tTraining Loss:   0.289327\n",
            " Epoch: 11972 \tValidation Loss: 0.307315 \tR2: 0.466171\n",
            " Epoch: 11973 \tTraining Loss:   0.283176\n",
            " Epoch: 11973 \tValidation Loss: 0.314951 \tR2: 0.466171\n",
            " Epoch: 11974 \tTraining Loss:   0.274665\n",
            " Epoch: 11974 \tValidation Loss: 0.324430 \tR2: 0.466171\n",
            " Epoch: 11975 \tTraining Loss:   0.284020\n",
            " Epoch: 11975 \tValidation Loss: 0.319898 \tR2: 0.466171\n",
            " Epoch: 11976 \tTraining Loss:   0.280476\n",
            " Epoch: 11976 \tValidation Loss: 0.329970 \tR2: 0.466171\n",
            " Epoch: 11977 \tTraining Loss:   0.278851\n",
            " Epoch: 11977 \tValidation Loss: 0.300873 \tR2: 0.466171\n",
            " Epoch: 11978 \tTraining Loss:   0.279460\n",
            " Epoch: 11978 \tValidation Loss: 0.316728 \tR2: 0.466171\n",
            " Epoch: 11979 \tTraining Loss:   0.267349\n",
            " Epoch: 11979 \tValidation Loss: 0.588262 \tR2: 0.466171\n",
            " Epoch: 11980 \tTraining Loss:   0.269850\n",
            " Epoch: 11980 \tValidation Loss: 0.360594 \tR2: 0.466171\n",
            " Epoch: 11981 \tTraining Loss:   0.267330\n",
            " Epoch: 11981 \tValidation Loss: 0.333901 \tR2: 0.466171\n",
            " Epoch: 11982 \tTraining Loss:   0.278901\n",
            " Epoch: 11982 \tValidation Loss: 0.313232 \tR2: 0.466171\n",
            " Epoch: 11983 \tTraining Loss:   0.280235\n",
            " Epoch: 11983 \tValidation Loss: 0.340243 \tR2: 0.466171\n",
            " Epoch: 11984 \tTraining Loss:   0.300474\n",
            " Epoch: 11984 \tValidation Loss: 0.320563 \tR2: 0.466171\n",
            " Epoch: 11985 \tTraining Loss:   0.279019\n",
            " Epoch: 11985 \tValidation Loss: 0.337012 \tR2: 0.466171\n",
            " Epoch: 11986 \tTraining Loss:   0.275383\n",
            " Epoch: 11986 \tValidation Loss: 0.308039 \tR2: 0.466171\n",
            " Epoch: 11987 \tTraining Loss:   0.283369\n",
            " Epoch: 11987 \tValidation Loss: 0.317372 \tR2: 0.466171\n",
            " Epoch: 11988 \tTraining Loss:   0.281429\n",
            " Epoch: 11988 \tValidation Loss: 0.320631 \tR2: 0.466171\n",
            " Epoch: 11989 \tTraining Loss:   0.282682\n",
            " Epoch: 11989 \tValidation Loss: 0.358934 \tR2: 0.466171\n",
            " Epoch: 11990 \tTraining Loss:   0.295040\n",
            " Epoch: 11990 \tValidation Loss: 0.330206 \tR2: 0.466171\n",
            " Epoch: 11991 \tTraining Loss:   0.269787\n",
            " Epoch: 11991 \tValidation Loss: 0.346724 \tR2: 0.466171\n",
            " Epoch: 11992 \tTraining Loss:   0.290099\n",
            " Epoch: 11992 \tValidation Loss: 0.357433 \tR2: 0.466171\n",
            " Epoch: 11993 \tTraining Loss:   0.282063\n",
            " Epoch: 11993 \tValidation Loss: 0.341201 \tR2: 0.466171\n",
            " Epoch: 11994 \tTraining Loss:   0.280129\n",
            " Epoch: 11994 \tValidation Loss: 0.322348 \tR2: 0.466171\n",
            " Epoch: 11995 \tTraining Loss:   0.276339\n",
            " Epoch: 11995 \tValidation Loss: 0.313314 \tR2: 0.466171\n",
            " Epoch: 11996 \tTraining Loss:   0.273980\n",
            " Epoch: 11996 \tValidation Loss: 0.300151 \tR2: 0.466171\n",
            " Epoch: 11997 \tTraining Loss:   0.274353\n",
            " Epoch: 11997 \tValidation Loss: 0.356073 \tR2: 0.466171\n",
            " Epoch: 11998 \tTraining Loss:   0.290178\n",
            " Epoch: 11998 \tValidation Loss: 0.294371 \tR2: 0.466171\n",
            " Epoch: 11999 \tTraining Loss:   0.283348\n",
            " Epoch: 11999 \tValidation Loss: 0.326657 \tR2: 0.466171\n",
            " Epoch: 12000 \tTraining Loss:   0.277240\n",
            " Epoch: 12000 \tValidation Loss: 0.319726 \tR2: 0.318161\n",
            " Epoch: 12001 \tTraining Loss:   0.278667\n",
            " Epoch: 12001 \tValidation Loss: 0.323790 \tR2: 0.318161\n",
            " Epoch: 12002 \tTraining Loss:   0.286058\n",
            " Epoch: 12002 \tValidation Loss: 0.297460 \tR2: 0.318161\n",
            " Epoch: 12003 \tTraining Loss:   0.265085\n",
            " Epoch: 12003 \tValidation Loss: 0.307451 \tR2: 0.318161\n",
            " Epoch: 12004 \tTraining Loss:   0.275559\n",
            " Epoch: 12004 \tValidation Loss: 0.304186 \tR2: 0.318161\n",
            " Epoch: 12005 \tTraining Loss:   0.280789\n",
            " Epoch: 12005 \tValidation Loss: 0.299104 \tR2: 0.318161\n",
            " Epoch: 12006 \tTraining Loss:   0.273904\n",
            " Epoch: 12006 \tValidation Loss: 0.328273 \tR2: 0.318161\n",
            " Epoch: 12007 \tTraining Loss:   0.272385\n",
            " Epoch: 12007 \tValidation Loss: 0.309077 \tR2: 0.318161\n",
            " Epoch: 12008 \tTraining Loss:   0.283061\n",
            " Epoch: 12008 \tValidation Loss: 0.325612 \tR2: 0.318161\n",
            " Epoch: 12009 \tTraining Loss:   0.289346\n",
            " Epoch: 12009 \tValidation Loss: 0.338917 \tR2: 0.318161\n",
            " Epoch: 12010 \tTraining Loss:   0.272901\n",
            " Epoch: 12010 \tValidation Loss: 0.309701 \tR2: 0.318161\n",
            " Epoch: 12011 \tTraining Loss:   0.264691\n",
            " Epoch: 12011 \tValidation Loss: 0.348107 \tR2: 0.318161\n",
            " Epoch: 12012 \tTraining Loss:   0.275996\n",
            " Epoch: 12012 \tValidation Loss: 0.406305 \tR2: 0.318161\n",
            " Epoch: 12013 \tTraining Loss:   0.273193\n",
            " Epoch: 12013 \tValidation Loss: 0.317491 \tR2: 0.318161\n",
            " Epoch: 12014 \tTraining Loss:   0.272834\n",
            " Epoch: 12014 \tValidation Loss: 0.301307 \tR2: 0.318161\n",
            " Epoch: 12015 \tTraining Loss:   0.267749\n",
            " Epoch: 12015 \tValidation Loss: 0.321121 \tR2: 0.318161\n",
            " Epoch: 12016 \tTraining Loss:   0.284424\n",
            " Epoch: 12016 \tValidation Loss: 0.336004 \tR2: 0.318161\n",
            " Epoch: 12017 \tTraining Loss:   0.275616\n",
            " Epoch: 12017 \tValidation Loss: 0.372220 \tR2: 0.318161\n",
            " Epoch: 12018 \tTraining Loss:   0.294777\n",
            " Epoch: 12018 \tValidation Loss: 0.367299 \tR2: 0.318161\n",
            " Epoch: 12019 \tTraining Loss:   0.272273\n",
            " Epoch: 12019 \tValidation Loss: 0.327497 \tR2: 0.318161\n",
            " Epoch: 12020 \tTraining Loss:   0.286796\n",
            " Epoch: 12020 \tValidation Loss: 0.349436 \tR2: 0.318161\n",
            " Epoch: 12021 \tTraining Loss:   0.300148\n",
            " Epoch: 12021 \tValidation Loss: 0.346004 \tR2: 0.318161\n",
            " Epoch: 12022 \tTraining Loss:   0.276468\n",
            " Epoch: 12022 \tValidation Loss: 0.343312 \tR2: 0.318161\n",
            " Epoch: 12023 \tTraining Loss:   0.407159\n",
            " Epoch: 12023 \tValidation Loss: 0.332731 \tR2: 0.318161\n",
            " Epoch: 12024 \tTraining Loss:   0.283453\n",
            " Epoch: 12024 \tValidation Loss: 0.315799 \tR2: 0.318161\n",
            " Epoch: 12025 \tTraining Loss:   0.273876\n",
            " Epoch: 12025 \tValidation Loss: 0.318540 \tR2: 0.318161\n",
            " Epoch: 12026 \tTraining Loss:   0.287552\n",
            " Epoch: 12026 \tValidation Loss: 0.324628 \tR2: 0.318161\n",
            " Epoch: 12027 \tTraining Loss:   0.288086\n",
            " Epoch: 12027 \tValidation Loss: 0.303980 \tR2: 0.318161\n",
            " Epoch: 12028 \tTraining Loss:   0.278516\n",
            " Epoch: 12028 \tValidation Loss: 0.299350 \tR2: 0.318161\n",
            " Epoch: 12029 \tTraining Loss:   0.283711\n",
            " Epoch: 12029 \tValidation Loss: 0.321659 \tR2: 0.318161\n",
            " Epoch: 12030 \tTraining Loss:   0.273805\n",
            " Epoch: 12030 \tValidation Loss: 0.297025 \tR2: 0.318161\n",
            " Epoch: 12031 \tTraining Loss:   0.287973\n",
            " Epoch: 12031 \tValidation Loss: 0.326032 \tR2: 0.318161\n",
            " Epoch: 12032 \tTraining Loss:   0.283899\n",
            " Epoch: 12032 \tValidation Loss: 0.449823 \tR2: 0.318161\n",
            " Epoch: 12033 \tTraining Loss:   0.287968\n",
            " Epoch: 12033 \tValidation Loss: 0.306384 \tR2: 0.318161\n",
            " Epoch: 12034 \tTraining Loss:   0.291137\n",
            " Epoch: 12034 \tValidation Loss: 0.422817 \tR2: 0.318161\n",
            " Epoch: 12035 \tTraining Loss:   0.289901\n",
            " Epoch: 12035 \tValidation Loss: 0.364267 \tR2: 0.318161\n",
            " Epoch: 12036 \tTraining Loss:   0.289771\n",
            " Epoch: 12036 \tValidation Loss: 0.328042 \tR2: 0.318161\n",
            " Epoch: 12037 \tTraining Loss:   0.294376\n",
            " Epoch: 12037 \tValidation Loss: 0.338163 \tR2: 0.318161\n",
            " Epoch: 12038 \tTraining Loss:   0.268258\n",
            " Epoch: 12038 \tValidation Loss: 0.332709 \tR2: 0.318161\n",
            " Epoch: 12039 \tTraining Loss:   0.310111\n",
            " Epoch: 12039 \tValidation Loss: 0.359371 \tR2: 0.318161\n",
            " Epoch: 12040 \tTraining Loss:   0.308931\n",
            " Epoch: 12040 \tValidation Loss: 0.321034 \tR2: 0.318161\n",
            " Epoch: 12041 \tTraining Loss:   0.271573\n",
            " Epoch: 12041 \tValidation Loss: 0.302454 \tR2: 0.318161\n",
            " Epoch: 12042 \tTraining Loss:   0.284702\n",
            " Epoch: 12042 \tValidation Loss: 0.335637 \tR2: 0.318161\n",
            " Epoch: 12043 \tTraining Loss:   0.278822\n",
            " Epoch: 12043 \tValidation Loss: 0.313107 \tR2: 0.318161\n",
            " Epoch: 12044 \tTraining Loss:   0.282369\n",
            " Epoch: 12044 \tValidation Loss: 0.343684 \tR2: 0.318161\n",
            " Epoch: 12045 \tTraining Loss:   0.288707\n",
            " Epoch: 12045 \tValidation Loss: 0.330139 \tR2: 0.318161\n",
            " Epoch: 12046 \tTraining Loss:   0.264796\n",
            " Epoch: 12046 \tValidation Loss: 0.323858 \tR2: 0.318161\n",
            " Epoch: 12047 \tTraining Loss:   0.286361\n",
            " Epoch: 12047 \tValidation Loss: 0.330922 \tR2: 0.318161\n",
            " Epoch: 12048 \tTraining Loss:   0.271899\n",
            " Epoch: 12048 \tValidation Loss: 0.370488 \tR2: 0.318161\n",
            " Epoch: 12049 \tTraining Loss:   0.277645\n",
            " Epoch: 12049 \tValidation Loss: 0.322240 \tR2: 0.318161\n",
            " Epoch: 12050 \tTraining Loss:   0.292038\n",
            " Epoch: 12050 \tValidation Loss: 0.348626 \tR2: 0.318161\n",
            " Epoch: 12051 \tTraining Loss:   0.291053\n",
            " Epoch: 12051 \tValidation Loss: 0.307813 \tR2: 0.318161\n",
            " Epoch: 12052 \tTraining Loss:   0.275600\n",
            " Epoch: 12052 \tValidation Loss: 0.325756 \tR2: 0.318161\n",
            " Epoch: 12053 \tTraining Loss:   0.272816\n",
            " Epoch: 12053 \tValidation Loss: 0.316535 \tR2: 0.318161\n",
            " Epoch: 12054 \tTraining Loss:   0.271997\n",
            " Epoch: 12054 \tValidation Loss: 0.315840 \tR2: 0.318161\n",
            " Epoch: 12055 \tTraining Loss:   0.273825\n",
            " Epoch: 12055 \tValidation Loss: 0.302825 \tR2: 0.318161\n",
            " Epoch: 12056 \tTraining Loss:   0.293175\n",
            " Epoch: 12056 \tValidation Loss: 0.321659 \tR2: 0.318161\n",
            " Epoch: 12057 \tTraining Loss:   0.272438\n",
            " Epoch: 12057 \tValidation Loss: 0.329460 \tR2: 0.318161\n",
            " Epoch: 12058 \tTraining Loss:   0.266412\n",
            " Epoch: 12058 \tValidation Loss: 0.315839 \tR2: 0.318161\n",
            " Epoch: 12059 \tTraining Loss:   0.280344\n",
            " Epoch: 12059 \tValidation Loss: 0.333619 \tR2: 0.318161\n",
            " Epoch: 12060 \tTraining Loss:   0.286337\n",
            " Epoch: 12060 \tValidation Loss: 0.314915 \tR2: 0.318161\n",
            " Epoch: 12061 \tTraining Loss:   0.283912\n",
            " Epoch: 12061 \tValidation Loss: 0.355035 \tR2: 0.318161\n",
            " Epoch: 12062 \tTraining Loss:   0.269066\n",
            " Epoch: 12062 \tValidation Loss: 0.323901 \tR2: 0.318161\n",
            " Epoch: 12063 \tTraining Loss:   0.280367\n",
            " Epoch: 12063 \tValidation Loss: 0.322221 \tR2: 0.318161\n",
            " Epoch: 12064 \tTraining Loss:   0.271744\n",
            " Epoch: 12064 \tValidation Loss: 0.361580 \tR2: 0.318161\n",
            " Epoch: 12065 \tTraining Loss:   0.300700\n",
            " Epoch: 12065 \tValidation Loss: 0.331928 \tR2: 0.318161\n",
            " Epoch: 12066 \tTraining Loss:   0.280441\n",
            " Epoch: 12066 \tValidation Loss: 0.358061 \tR2: 0.318161\n",
            " Epoch: 12067 \tTraining Loss:   0.279279\n",
            " Epoch: 12067 \tValidation Loss: 0.346742 \tR2: 0.318161\n",
            " Epoch: 12068 \tTraining Loss:   0.303181\n",
            " Epoch: 12068 \tValidation Loss: 0.394966 \tR2: 0.318161\n",
            " Epoch: 12069 \tTraining Loss:   0.272728\n",
            " Epoch: 12069 \tValidation Loss: 0.319176 \tR2: 0.318161\n",
            " Epoch: 12070 \tTraining Loss:   0.276733\n",
            " Epoch: 12070 \tValidation Loss: 0.355792 \tR2: 0.318161\n",
            " Epoch: 12071 \tTraining Loss:   0.285369\n",
            " Epoch: 12071 \tValidation Loss: 0.347069 \tR2: 0.318161\n",
            " Epoch: 12072 \tTraining Loss:   0.286034\n",
            " Epoch: 12072 \tValidation Loss: 0.353834 \tR2: 0.318161\n",
            " Epoch: 12073 \tTraining Loss:   0.290557\n",
            " Epoch: 12073 \tValidation Loss: 0.375391 \tR2: 0.318161\n",
            " Epoch: 12074 \tTraining Loss:   0.292970\n",
            " Epoch: 12074 \tValidation Loss: 0.465556 \tR2: 0.318161\n",
            " Epoch: 12075 \tTraining Loss:   0.280510\n",
            " Epoch: 12075 \tValidation Loss: 0.342728 \tR2: 0.318161\n",
            " Epoch: 12076 \tTraining Loss:   0.275232\n",
            " Epoch: 12076 \tValidation Loss: 0.328010 \tR2: 0.318161\n",
            " Epoch: 12077 \tTraining Loss:   0.282057\n",
            " Epoch: 12077 \tValidation Loss: 0.318243 \tR2: 0.318161\n",
            " Epoch: 12078 \tTraining Loss:   0.289890\n",
            " Epoch: 12078 \tValidation Loss: 0.359022 \tR2: 0.318161\n",
            " Epoch: 12079 \tTraining Loss:   0.282050\n",
            " Epoch: 12079 \tValidation Loss: 0.324617 \tR2: 0.318161\n",
            " Epoch: 12080 \tTraining Loss:   0.286067\n",
            " Epoch: 12080 \tValidation Loss: 0.357437 \tR2: 0.318161\n",
            " Epoch: 12081 \tTraining Loss:   0.265281\n",
            " Epoch: 12081 \tValidation Loss: 0.315494 \tR2: 0.318161\n",
            " Epoch: 12082 \tTraining Loss:   0.284656\n",
            " Epoch: 12082 \tValidation Loss: 0.292834 \tR2: 0.318161\n",
            " Epoch: 12083 \tTraining Loss:   0.271141\n",
            " Epoch: 12083 \tValidation Loss: 0.380171 \tR2: 0.318161\n",
            " Epoch: 12084 \tTraining Loss:   0.298978\n",
            " Epoch: 12084 \tValidation Loss: 0.313185 \tR2: 0.318161\n",
            " Epoch: 12085 \tTraining Loss:   0.269010\n",
            " Epoch: 12085 \tValidation Loss: 0.345190 \tR2: 0.318161\n",
            " Epoch: 12086 \tTraining Loss:   0.280737\n",
            " Epoch: 12086 \tValidation Loss: 0.314986 \tR2: 0.318161\n",
            " Epoch: 12087 \tTraining Loss:   0.274733\n",
            " Epoch: 12087 \tValidation Loss: 0.368342 \tR2: 0.318161\n",
            " Epoch: 12088 \tTraining Loss:   0.286088\n",
            " Epoch: 12088 \tValidation Loss: 0.300374 \tR2: 0.318161\n",
            " Epoch: 12089 \tTraining Loss:   0.279154\n",
            " Epoch: 12089 \tValidation Loss: 0.318465 \tR2: 0.318161\n",
            " Epoch: 12090 \tTraining Loss:   0.283757\n",
            " Epoch: 12090 \tValidation Loss: 0.405503 \tR2: 0.318161\n",
            " Epoch: 12091 \tTraining Loss:   0.289134\n",
            " Epoch: 12091 \tValidation Loss: 0.352888 \tR2: 0.318161\n",
            " Epoch: 12092 \tTraining Loss:   0.285473\n",
            " Epoch: 12092 \tValidation Loss: 0.375041 \tR2: 0.318161\n",
            " Epoch: 12093 \tTraining Loss:   0.277034\n",
            " Epoch: 12093 \tValidation Loss: 0.355063 \tR2: 0.318161\n",
            " Epoch: 12094 \tTraining Loss:   0.273116\n",
            " Epoch: 12094 \tValidation Loss: 0.295249 \tR2: 0.318161\n",
            " Epoch: 12095 \tTraining Loss:   0.289627\n",
            " Epoch: 12095 \tValidation Loss: 0.322768 \tR2: 0.318161\n",
            " Epoch: 12096 \tTraining Loss:   0.271050\n",
            " Epoch: 12096 \tValidation Loss: 0.395172 \tR2: 0.318161\n",
            " Epoch: 12097 \tTraining Loss:   0.278548\n",
            " Epoch: 12097 \tValidation Loss: 0.315314 \tR2: 0.318161\n",
            " Epoch: 12098 \tTraining Loss:   0.276993\n",
            " Epoch: 12098 \tValidation Loss: 0.316579 \tR2: 0.318161\n",
            " Epoch: 12099 \tTraining Loss:   0.283905\n",
            " Epoch: 12099 \tValidation Loss: 0.391041 \tR2: 0.318161\n",
            " Epoch: 12100 \tTraining Loss:   0.276126\n",
            " Epoch: 12100 \tValidation Loss: 0.323775 \tR2: 0.362324\n",
            " Epoch: 12101 \tTraining Loss:   0.289973\n",
            " Epoch: 12101 \tValidation Loss: 0.320745 \tR2: 0.362324\n",
            " Epoch: 12102 \tTraining Loss:   0.278447\n",
            " Epoch: 12102 \tValidation Loss: 0.296185 \tR2: 0.362324\n",
            " Epoch: 12103 \tTraining Loss:   0.293810\n",
            " Epoch: 12103 \tValidation Loss: 0.331424 \tR2: 0.362324\n",
            " Epoch: 12104 \tTraining Loss:   0.270625\n",
            " Epoch: 12104 \tValidation Loss: 0.307359 \tR2: 0.362324\n",
            " Epoch: 12105 \tTraining Loss:   0.276380\n",
            " Epoch: 12105 \tValidation Loss: 0.303128 \tR2: 0.362324\n",
            " Epoch: 12106 \tTraining Loss:   0.272204\n",
            " Epoch: 12106 \tValidation Loss: 0.299662 \tR2: 0.362324\n",
            " Epoch: 12107 \tTraining Loss:   0.277206\n",
            " Epoch: 12107 \tValidation Loss: 0.295508 \tR2: 0.362324\n",
            " Epoch: 12108 \tTraining Loss:   0.269911\n",
            " Epoch: 12108 \tValidation Loss: 0.295846 \tR2: 0.362324\n",
            " Epoch: 12109 \tTraining Loss:   0.276909\n",
            " Epoch: 12109 \tValidation Loss: 0.310951 \tR2: 0.362324\n",
            " Epoch: 12110 \tTraining Loss:   0.285182\n",
            " Epoch: 12110 \tValidation Loss: 0.339531 \tR2: 0.362324\n",
            " Epoch: 12111 \tTraining Loss:   0.281085\n",
            " Epoch: 12111 \tValidation Loss: 0.300509 \tR2: 0.362324\n",
            " Epoch: 12112 \tTraining Loss:   0.295377\n",
            " Epoch: 12112 \tValidation Loss: 0.312281 \tR2: 0.362324\n",
            " Epoch: 12113 \tTraining Loss:   0.280532\n",
            " Epoch: 12113 \tValidation Loss: 0.315646 \tR2: 0.362324\n",
            " Epoch: 12114 \tTraining Loss:   0.291689\n",
            " Epoch: 12114 \tValidation Loss: 0.461274 \tR2: 0.362324\n",
            " Epoch: 12115 \tTraining Loss:   0.301653\n",
            " Epoch: 12115 \tValidation Loss: 0.320661 \tR2: 0.362324\n",
            " Epoch: 12116 \tTraining Loss:   0.267119\n",
            " Epoch: 12116 \tValidation Loss: 0.301615 \tR2: 0.362324\n",
            " Epoch: 12117 \tTraining Loss:   0.284449\n",
            " Epoch: 12117 \tValidation Loss: 0.333048 \tR2: 0.362324\n",
            " Epoch: 12118 \tTraining Loss:   0.282863\n",
            " Epoch: 12118 \tValidation Loss: 0.341439 \tR2: 0.362324\n",
            " Epoch: 12119 \tTraining Loss:   0.279276\n",
            " Epoch: 12119 \tValidation Loss: 0.322460 \tR2: 0.362324\n",
            " Epoch: 12120 \tTraining Loss:   0.279375\n",
            " Epoch: 12120 \tValidation Loss: 0.332564 \tR2: 0.362324\n",
            " Epoch: 12121 \tTraining Loss:   0.275072\n",
            " Epoch: 12121 \tValidation Loss: 0.380026 \tR2: 0.362324\n",
            " Epoch: 12122 \tTraining Loss:   0.275666\n",
            " Epoch: 12122 \tValidation Loss: 0.309358 \tR2: 0.362324\n",
            " Epoch: 12123 \tTraining Loss:   0.299374\n",
            " Epoch: 12123 \tValidation Loss: 0.295724 \tR2: 0.362324\n",
            " Epoch: 12124 \tTraining Loss:   0.266952\n",
            " Epoch: 12124 \tValidation Loss: 0.359054 \tR2: 0.362324\n",
            " Epoch: 12125 \tTraining Loss:   0.274081\n",
            " Epoch: 12125 \tValidation Loss: 0.339114 \tR2: 0.362324\n",
            " Epoch: 12126 \tTraining Loss:   0.289767\n",
            " Epoch: 12126 \tValidation Loss: 0.317151 \tR2: 0.362324\n",
            " Epoch: 12127 \tTraining Loss:   0.281268\n",
            " Epoch: 12127 \tValidation Loss: 0.327804 \tR2: 0.362324\n",
            " Epoch: 12128 \tTraining Loss:   0.282786\n",
            " Epoch: 12128 \tValidation Loss: 0.342119 \tR2: 0.362324\n",
            " Epoch: 12129 \tTraining Loss:   0.263056\n",
            " Epoch: 12129 \tValidation Loss: 0.309637 \tR2: 0.362324\n",
            " Epoch: 12130 \tTraining Loss:   0.278979\n",
            " Epoch: 12130 \tValidation Loss: 0.399173 \tR2: 0.362324\n",
            " Epoch: 12131 \tTraining Loss:   0.271590\n",
            " Epoch: 12131 \tValidation Loss: 0.334132 \tR2: 0.362324\n",
            " Epoch: 12132 \tTraining Loss:   0.276289\n",
            " Epoch: 12132 \tValidation Loss: 0.332339 \tR2: 0.362324\n",
            " Epoch: 12133 \tTraining Loss:   0.293936\n",
            " Epoch: 12133 \tValidation Loss: 0.305276 \tR2: 0.362324\n",
            " Epoch: 12134 \tTraining Loss:   0.276959\n",
            " Epoch: 12134 \tValidation Loss: 0.371566 \tR2: 0.362324\n",
            " Epoch: 12135 \tTraining Loss:   0.275902\n",
            " Epoch: 12135 \tValidation Loss: 0.323048 \tR2: 0.362324\n",
            " Epoch: 12136 \tTraining Loss:   0.277024\n",
            " Epoch: 12136 \tValidation Loss: 0.309558 \tR2: 0.362324\n",
            " Epoch: 12137 \tTraining Loss:   0.286169\n",
            " Epoch: 12137 \tValidation Loss: 0.317619 \tR2: 0.362324\n",
            " Epoch: 12138 \tTraining Loss:   0.280078\n",
            " Epoch: 12138 \tValidation Loss: 0.317951 \tR2: 0.362324\n",
            " Epoch: 12139 \tTraining Loss:   0.275783\n",
            " Epoch: 12139 \tValidation Loss: 0.301881 \tR2: 0.362324\n",
            " Epoch: 12140 \tTraining Loss:   0.278460\n",
            " Epoch: 12140 \tValidation Loss: 0.330013 \tR2: 0.362324\n",
            " Epoch: 12141 \tTraining Loss:   0.269020\n",
            " Epoch: 12141 \tValidation Loss: 0.326071 \tR2: 0.362324\n",
            " Epoch: 12142 \tTraining Loss:   0.262495\n",
            " Epoch: 12142 \tValidation Loss: 0.353400 \tR2: 0.362324\n",
            " Epoch: 12143 \tTraining Loss:   0.293484\n",
            " Epoch: 12143 \tValidation Loss: 0.304270 \tR2: 0.362324\n",
            " Epoch: 12144 \tTraining Loss:   0.267513\n",
            " Epoch: 12144 \tValidation Loss: 0.314723 \tR2: 0.362324\n",
            " Epoch: 12145 \tTraining Loss:   0.292309\n",
            " Epoch: 12145 \tValidation Loss: 0.361875 \tR2: 0.362324\n",
            " Epoch: 12146 \tTraining Loss:   0.283225\n",
            " Epoch: 12146 \tValidation Loss: 0.316494 \tR2: 0.362324\n",
            " Epoch: 12147 \tTraining Loss:   0.271548\n",
            " Epoch: 12147 \tValidation Loss: 0.295275 \tR2: 0.362324\n",
            " Epoch: 12148 \tTraining Loss:   0.288811\n",
            " Epoch: 12148 \tValidation Loss: 0.344115 \tR2: 0.362324\n",
            " Epoch: 12149 \tTraining Loss:   0.285832\n",
            " Epoch: 12149 \tValidation Loss: 0.311023 \tR2: 0.362324\n",
            " Epoch: 12150 \tTraining Loss:   0.286242\n",
            " Epoch: 12150 \tValidation Loss: 0.304915 \tR2: 0.362324\n",
            " Epoch: 12151 \tTraining Loss:   0.278338\n",
            " Epoch: 12151 \tValidation Loss: 0.324743 \tR2: 0.362324\n",
            " Epoch: 12152 \tTraining Loss:   0.301447\n",
            " Epoch: 12152 \tValidation Loss: 0.302172 \tR2: 0.362324\n",
            " Epoch: 12153 \tTraining Loss:   0.273793\n",
            " Epoch: 12153 \tValidation Loss: 0.319457 \tR2: 0.362324\n",
            " Epoch: 12154 \tTraining Loss:   0.282383\n",
            " Epoch: 12154 \tValidation Loss: 0.516733 \tR2: 0.362324\n",
            " Epoch: 12155 \tTraining Loss:   0.286433\n",
            " Epoch: 12155 \tValidation Loss: 0.341140 \tR2: 0.362324\n",
            " Epoch: 12156 \tTraining Loss:   0.283730\n",
            " Epoch: 12156 \tValidation Loss: 0.391177 \tR2: 0.362324\n",
            " Epoch: 12157 \tTraining Loss:   0.277851\n",
            " Epoch: 12157 \tValidation Loss: 0.303784 \tR2: 0.362324\n",
            " Epoch: 12158 \tTraining Loss:   0.279868\n",
            " Epoch: 12158 \tValidation Loss: 0.306088 \tR2: 0.362324\n",
            " Epoch: 12159 \tTraining Loss:   0.276238\n",
            " Epoch: 12159 \tValidation Loss: 0.296423 \tR2: 0.362324\n",
            " Epoch: 12160 \tTraining Loss:   0.289338\n",
            " Epoch: 12160 \tValidation Loss: 0.298181 \tR2: 0.362324\n",
            " Epoch: 12161 \tTraining Loss:   0.265938\n",
            " Epoch: 12161 \tValidation Loss: 0.301226 \tR2: 0.362324\n",
            " Epoch: 12162 \tTraining Loss:   0.277667\n",
            " Epoch: 12162 \tValidation Loss: 0.290705 \tR2: 0.362324\n",
            " Epoch: 12163 \tTraining Loss:   0.278138\n",
            " Epoch: 12163 \tValidation Loss: 0.333159 \tR2: 0.362324\n",
            " Epoch: 12164 \tTraining Loss:   0.284546\n",
            " Epoch: 12164 \tValidation Loss: 0.294514 \tR2: 0.362324\n",
            " Epoch: 12165 \tTraining Loss:   0.286855\n",
            " Epoch: 12165 \tValidation Loss: 0.326637 \tR2: 0.362324\n",
            " Epoch: 12166 \tTraining Loss:   0.278393\n",
            " Epoch: 12166 \tValidation Loss: 0.294603 \tR2: 0.362324\n",
            " Epoch: 12167 \tTraining Loss:   0.269721\n",
            " Epoch: 12167 \tValidation Loss: 0.340648 \tR2: 0.362324\n",
            " Epoch: 12168 \tTraining Loss:   0.269987\n",
            " Epoch: 12168 \tValidation Loss: 0.308900 \tR2: 0.362324\n",
            " Epoch: 12169 \tTraining Loss:   0.285776\n",
            " Epoch: 12169 \tValidation Loss: 0.581609 \tR2: 0.362324\n",
            " Epoch: 12170 \tTraining Loss:   0.284871\n",
            " Epoch: 12170 \tValidation Loss: 0.307098 \tR2: 0.362324\n",
            " Epoch: 12171 \tTraining Loss:   0.272610\n",
            " Epoch: 12171 \tValidation Loss: 0.302414 \tR2: 0.362324\n",
            " Epoch: 12172 \tTraining Loss:   0.284430\n",
            " Epoch: 12172 \tValidation Loss: 0.302755 \tR2: 0.362324\n",
            " Epoch: 12173 \tTraining Loss:   0.273915\n",
            " Epoch: 12173 \tValidation Loss: 0.339579 \tR2: 0.362324\n",
            " Epoch: 12174 \tTraining Loss:   0.282279\n",
            " Epoch: 12174 \tValidation Loss: 0.325417 \tR2: 0.362324\n",
            " Epoch: 12175 \tTraining Loss:   0.297309\n",
            " Epoch: 12175 \tValidation Loss: 0.334360 \tR2: 0.362324\n",
            " Epoch: 12176 \tTraining Loss:   0.274738\n",
            " Epoch: 12176 \tValidation Loss: 0.310462 \tR2: 0.362324\n",
            " Epoch: 12177 \tTraining Loss:   0.274379\n",
            " Epoch: 12177 \tValidation Loss: 0.323804 \tR2: 0.362324\n",
            " Epoch: 12178 \tTraining Loss:   0.278626\n",
            " Epoch: 12178 \tValidation Loss: 0.354507 \tR2: 0.362324\n",
            " Epoch: 12179 \tTraining Loss:   0.299661\n",
            " Epoch: 12179 \tValidation Loss: 0.353950 \tR2: 0.362324\n",
            " Epoch: 12180 \tTraining Loss:   0.278917\n",
            " Epoch: 12180 \tValidation Loss: 0.316996 \tR2: 0.362324\n",
            " Epoch: 12181 \tTraining Loss:   0.277636\n",
            " Epoch: 12181 \tValidation Loss: 0.300896 \tR2: 0.362324\n",
            " Epoch: 12182 \tTraining Loss:   0.275292\n",
            " Epoch: 12182 \tValidation Loss: 0.321517 \tR2: 0.362324\n",
            " Epoch: 12183 \tTraining Loss:   0.279775\n",
            " Epoch: 12183 \tValidation Loss: 0.343881 \tR2: 0.362324\n",
            " Epoch: 12184 \tTraining Loss:   0.285184\n",
            " Epoch: 12184 \tValidation Loss: 0.337834 \tR2: 0.362324\n",
            " Epoch: 12185 \tTraining Loss:   0.267145\n",
            " Epoch: 12185 \tValidation Loss: 0.352570 \tR2: 0.362324\n",
            " Epoch: 12186 \tTraining Loss:   0.269493\n",
            " Epoch: 12186 \tValidation Loss: 0.317388 \tR2: 0.362324\n",
            " Epoch: 12187 \tTraining Loss:   0.283702\n",
            " Epoch: 12187 \tValidation Loss: 0.291843 \tR2: 0.362324\n",
            " Epoch: 12188 \tTraining Loss:   0.292453\n",
            " Epoch: 12188 \tValidation Loss: 0.298978 \tR2: 0.362324\n",
            " Epoch: 12189 \tTraining Loss:   0.282862\n",
            " Epoch: 12189 \tValidation Loss: 0.306784 \tR2: 0.362324\n",
            " Epoch: 12190 \tTraining Loss:   0.259481\n",
            " Epoch: 12190 \tValidation Loss: 0.295492 \tR2: 0.362324\n",
            " Epoch: 12191 \tTraining Loss:   0.262731\n",
            " Epoch: 12191 \tValidation Loss: 0.315113 \tR2: 0.362324\n",
            " Epoch: 12192 \tTraining Loss:   0.292050\n",
            " Epoch: 12192 \tValidation Loss: 0.353891 \tR2: 0.362324\n",
            " Epoch: 12193 \tTraining Loss:   0.266746\n",
            " Epoch: 12193 \tValidation Loss: 0.305341 \tR2: 0.362324\n",
            " Epoch: 12194 \tTraining Loss:   0.282722\n",
            " Epoch: 12194 \tValidation Loss: 0.311917 \tR2: 0.362324\n",
            " Epoch: 12195 \tTraining Loss:   0.275255\n",
            " Epoch: 12195 \tValidation Loss: 0.317784 \tR2: 0.362324\n",
            " Epoch: 12196 \tTraining Loss:   0.282319\n",
            " Epoch: 12196 \tValidation Loss: 0.298114 \tR2: 0.362324\n",
            " Epoch: 12197 \tTraining Loss:   0.273442\n",
            " Epoch: 12197 \tValidation Loss: 0.321985 \tR2: 0.362324\n",
            " Epoch: 12198 \tTraining Loss:   0.280650\n",
            " Epoch: 12198 \tValidation Loss: 0.335667 \tR2: 0.362324\n",
            " Epoch: 12199 \tTraining Loss:   0.273824\n",
            " Epoch: 12199 \tValidation Loss: 0.351808 \tR2: 0.362324\n",
            " Epoch: 12200 \tTraining Loss:   0.270357\n",
            " Epoch: 12200 \tValidation Loss: 0.303880 \tR2: 0.514790\n",
            " Epoch: 12201 \tTraining Loss:   0.280109\n",
            " Epoch: 12201 \tValidation Loss: 0.375239 \tR2: 0.514790\n",
            " Epoch: 12202 \tTraining Loss:   0.281800\n",
            " Epoch: 12202 \tValidation Loss: 0.315282 \tR2: 0.514790\n",
            " Epoch: 12203 \tTraining Loss:   0.290230\n",
            " Epoch: 12203 \tValidation Loss: 0.326237 \tR2: 0.514790\n",
            " Epoch: 12204 \tTraining Loss:   0.280497\n",
            " Epoch: 12204 \tValidation Loss: 0.429172 \tR2: 0.514790\n",
            " Epoch: 12205 \tTraining Loss:   0.272285\n",
            " Epoch: 12205 \tValidation Loss: 0.325041 \tR2: 0.514790\n",
            " Epoch: 12206 \tTraining Loss:   0.288249\n",
            " Epoch: 12206 \tValidation Loss: 0.335993 \tR2: 0.514790\n",
            " Epoch: 12207 \tTraining Loss:   0.271499\n",
            " Epoch: 12207 \tValidation Loss: 0.354903 \tR2: 0.514790\n",
            " Epoch: 12208 \tTraining Loss:   0.276597\n",
            " Epoch: 12208 \tValidation Loss: 0.313400 \tR2: 0.514790\n",
            " Epoch: 12209 \tTraining Loss:   0.265334\n",
            " Epoch: 12209 \tValidation Loss: 0.383520 \tR2: 0.514790\n",
            " Epoch: 12210 \tTraining Loss:   0.263393\n",
            " Epoch: 12210 \tValidation Loss: 0.432319 \tR2: 0.514790\n",
            " Epoch: 12211 \tTraining Loss:   0.276362\n",
            " Epoch: 12211 \tValidation Loss: 0.318157 \tR2: 0.514790\n",
            " Epoch: 12212 \tTraining Loss:   0.267529\n",
            " Epoch: 12212 \tValidation Loss: 0.317401 \tR2: 0.514790\n",
            " Epoch: 12213 \tTraining Loss:   0.273044\n",
            " Epoch: 12213 \tValidation Loss: 0.302325 \tR2: 0.514790\n",
            " Epoch: 12214 \tTraining Loss:   0.290182\n",
            " Epoch: 12214 \tValidation Loss: 0.351272 \tR2: 0.514790\n",
            " Epoch: 12215 \tTraining Loss:   0.273298\n",
            " Epoch: 12215 \tValidation Loss: 0.347642 \tR2: 0.514790\n",
            " Epoch: 12216 \tTraining Loss:   0.277362\n",
            " Epoch: 12216 \tValidation Loss: 0.309186 \tR2: 0.514790\n",
            " Epoch: 12217 \tTraining Loss:   0.277605\n",
            " Epoch: 12217 \tValidation Loss: 0.320016 \tR2: 0.514790\n",
            " Epoch: 12218 \tTraining Loss:   0.288856\n",
            " Epoch: 12218 \tValidation Loss: 0.405246 \tR2: 0.514790\n",
            " Epoch: 12219 \tTraining Loss:   0.303345\n",
            " Epoch: 12219 \tValidation Loss: 0.328290 \tR2: 0.514790\n",
            " Epoch: 12220 \tTraining Loss:   0.296036\n",
            " Epoch: 12220 \tValidation Loss: 0.310018 \tR2: 0.514790\n",
            " Epoch: 12221 \tTraining Loss:   0.296028\n",
            " Epoch: 12221 \tValidation Loss: 0.296281 \tR2: 0.514790\n",
            " Epoch: 12222 \tTraining Loss:   0.277306\n",
            " Epoch: 12222 \tValidation Loss: 0.358148 \tR2: 0.514790\n",
            " Epoch: 12223 \tTraining Loss:   0.272592\n",
            " Epoch: 12223 \tValidation Loss: 0.316092 \tR2: 0.514790\n",
            " Epoch: 12224 \tTraining Loss:   0.274509\n",
            " Epoch: 12224 \tValidation Loss: 0.308811 \tR2: 0.514790\n",
            " Epoch: 12225 \tTraining Loss:   0.281088\n",
            " Epoch: 12225 \tValidation Loss: 0.305481 \tR2: 0.514790\n",
            " Epoch: 12226 \tTraining Loss:   0.267743\n",
            " Epoch: 12226 \tValidation Loss: 0.280045 \tR2: 0.514790\n",
            " Epoch: 12227 \tTraining Loss:   0.290179\n",
            " Epoch: 12227 \tValidation Loss: 0.316557 \tR2: 0.514790\n",
            " Epoch: 12228 \tTraining Loss:   0.274664\n",
            " Epoch: 12228 \tValidation Loss: 0.321428 \tR2: 0.514790\n",
            " Epoch: 12229 \tTraining Loss:   0.264352\n",
            " Epoch: 12229 \tValidation Loss: 0.392092 \tR2: 0.514790\n",
            " Epoch: 12230 \tTraining Loss:   0.285759\n",
            " Epoch: 12230 \tValidation Loss: 0.283414 \tR2: 0.514790\n",
            " Epoch: 12231 \tTraining Loss:   0.282943\n",
            " Epoch: 12231 \tValidation Loss: 0.307696 \tR2: 0.514790\n",
            " Epoch: 12232 \tTraining Loss:   0.275402\n",
            " Epoch: 12232 \tValidation Loss: 0.306163 \tR2: 0.514790\n",
            " Epoch: 12233 \tTraining Loss:   0.300647\n",
            " Epoch: 12233 \tValidation Loss: 0.343248 \tR2: 0.514790\n",
            " Epoch: 12234 \tTraining Loss:   0.273620\n",
            " Epoch: 12234 \tValidation Loss: 0.324814 \tR2: 0.514790\n",
            " Epoch: 12235 \tTraining Loss:   0.271840\n",
            " Epoch: 12235 \tValidation Loss: 0.333913 \tR2: 0.514790\n",
            " Epoch: 12236 \tTraining Loss:   0.286018\n",
            " Epoch: 12236 \tValidation Loss: 0.295294 \tR2: 0.514790\n",
            " Epoch: 12237 \tTraining Loss:   0.289404\n",
            " Epoch: 12237 \tValidation Loss: 0.329254 \tR2: 0.514790\n",
            " Epoch: 12238 \tTraining Loss:   0.277391\n",
            " Epoch: 12238 \tValidation Loss: 0.319621 \tR2: 0.514790\n",
            " Epoch: 12239 \tTraining Loss:   0.290330\n",
            " Epoch: 12239 \tValidation Loss: 0.422904 \tR2: 0.514790\n",
            " Epoch: 12240 \tTraining Loss:   0.303507\n",
            " Epoch: 12240 \tValidation Loss: 0.322734 \tR2: 0.514790\n",
            " Epoch: 12241 \tTraining Loss:   0.271466\n",
            " Epoch: 12241 \tValidation Loss: 0.378240 \tR2: 0.514790\n",
            " Epoch: 12242 \tTraining Loss:   0.258958\n",
            " Epoch: 12242 \tValidation Loss: 0.291106 \tR2: 0.514790\n",
            " Epoch: 12243 \tTraining Loss:   0.308195\n",
            " Epoch: 12243 \tValidation Loss: 0.330164 \tR2: 0.514790\n",
            " Epoch: 12244 \tTraining Loss:   0.284486\n",
            " Epoch: 12244 \tValidation Loss: 0.305993 \tR2: 0.514790\n",
            " Epoch: 12245 \tTraining Loss:   0.281744\n",
            " Epoch: 12245 \tValidation Loss: 0.354439 \tR2: 0.514790\n",
            " Epoch: 12246 \tTraining Loss:   0.271574\n",
            " Epoch: 12246 \tValidation Loss: 0.289959 \tR2: 0.514790\n",
            " Epoch: 12247 \tTraining Loss:   0.270853\n",
            " Epoch: 12247 \tValidation Loss: 0.305402 \tR2: 0.514790\n",
            " Epoch: 12248 \tTraining Loss:   0.268811\n",
            " Epoch: 12248 \tValidation Loss: 0.279388 \tR2: 0.514790\n",
            " Epoch: 12249 \tTraining Loss:   0.276472\n",
            " Epoch: 12249 \tValidation Loss: 0.302083 \tR2: 0.514790\n",
            " Epoch: 12250 \tTraining Loss:   0.268896\n",
            " Epoch: 12250 \tValidation Loss: 0.287234 \tR2: 0.514790\n",
            " Epoch: 12251 \tTraining Loss:   0.286305\n",
            " Epoch: 12251 \tValidation Loss: 0.336213 \tR2: 0.514790\n",
            " Epoch: 12252 \tTraining Loss:   0.280959\n",
            " Epoch: 12252 \tValidation Loss: 0.338301 \tR2: 0.514790\n",
            " Epoch: 12253 \tTraining Loss:   0.287451\n",
            " Epoch: 12253 \tValidation Loss: 0.362301 \tR2: 0.514790\n",
            " Epoch: 12254 \tTraining Loss:   0.293081\n",
            " Epoch: 12254 \tValidation Loss: 0.348441 \tR2: 0.514790\n",
            " Epoch: 12255 \tTraining Loss:   0.283190\n",
            " Epoch: 12255 \tValidation Loss: 0.385396 \tR2: 0.514790\n",
            " Epoch: 12256 \tTraining Loss:   0.291898\n",
            " Epoch: 12256 \tValidation Loss: 0.346661 \tR2: 0.514790\n",
            " Epoch: 12257 \tTraining Loss:   0.277966\n",
            " Epoch: 12257 \tValidation Loss: 0.358625 \tR2: 0.514790\n",
            " Epoch: 12258 \tTraining Loss:   0.280401\n",
            " Epoch: 12258 \tValidation Loss: 0.320826 \tR2: 0.514790\n",
            " Epoch: 12259 \tTraining Loss:   0.273185\n",
            " Epoch: 12259 \tValidation Loss: 0.332114 \tR2: 0.514790\n",
            " Epoch: 12260 \tTraining Loss:   0.271380\n",
            " Epoch: 12260 \tValidation Loss: 0.348419 \tR2: 0.514790\n",
            " Epoch: 12261 \tTraining Loss:   0.269230\n",
            " Epoch: 12261 \tValidation Loss: 0.311947 \tR2: 0.514790\n",
            " Epoch: 12262 \tTraining Loss:   0.298809\n",
            " Epoch: 12262 \tValidation Loss: 0.314777 \tR2: 0.514790\n",
            " Epoch: 12263 \tTraining Loss:   0.269833\n",
            " Epoch: 12263 \tValidation Loss: 0.374005 \tR2: 0.514790\n",
            " Epoch: 12264 \tTraining Loss:   0.260532\n",
            " Epoch: 12264 \tValidation Loss: 0.404469 \tR2: 0.514790\n",
            " Epoch: 12265 \tTraining Loss:   0.280495\n",
            " Epoch: 12265 \tValidation Loss: 0.290156 \tR2: 0.514790\n",
            " Epoch: 12266 \tTraining Loss:   0.272293\n",
            " Epoch: 12266 \tValidation Loss: 0.304406 \tR2: 0.514790\n",
            " Epoch: 12267 \tTraining Loss:   0.281353\n",
            " Epoch: 12267 \tValidation Loss: 0.339062 \tR2: 0.514790\n",
            " Epoch: 12268 \tTraining Loss:   0.274326\n",
            " Epoch: 12268 \tValidation Loss: 0.342855 \tR2: 0.514790\n",
            " Epoch: 12269 \tTraining Loss:   0.280357\n",
            " Epoch: 12269 \tValidation Loss: 0.330714 \tR2: 0.514790\n",
            " Epoch: 12270 \tTraining Loss:   0.281461\n",
            " Epoch: 12270 \tValidation Loss: 0.322145 \tR2: 0.514790\n",
            " Epoch: 12271 \tTraining Loss:   0.269830\n",
            " Epoch: 12271 \tValidation Loss: 0.301242 \tR2: 0.514790\n",
            " Epoch: 12272 \tTraining Loss:   0.293816\n",
            " Epoch: 12272 \tValidation Loss: 0.311895 \tR2: 0.514790\n",
            " Epoch: 12273 \tTraining Loss:   0.289915\n",
            " Epoch: 12273 \tValidation Loss: 0.332634 \tR2: 0.514790\n",
            " Epoch: 12274 \tTraining Loss:   0.284341\n",
            " Epoch: 12274 \tValidation Loss: 0.321280 \tR2: 0.514790\n",
            " Epoch: 12275 \tTraining Loss:   0.277652\n",
            " Epoch: 12275 \tValidation Loss: 0.346074 \tR2: 0.514790\n",
            " Epoch: 12276 \tTraining Loss:   0.276947\n",
            " Epoch: 12276 \tValidation Loss: 0.316265 \tR2: 0.514790\n",
            " Epoch: 12277 \tTraining Loss:   0.269071\n",
            " Epoch: 12277 \tValidation Loss: 0.318019 \tR2: 0.514790\n",
            " Epoch: 12278 \tTraining Loss:   0.309353\n",
            " Epoch: 12278 \tValidation Loss: 0.357317 \tR2: 0.514790\n",
            " Epoch: 12279 \tTraining Loss:   0.286814\n",
            " Epoch: 12279 \tValidation Loss: 0.330538 \tR2: 0.514790\n",
            " Epoch: 12280 \tTraining Loss:   0.283652\n",
            " Epoch: 12280 \tValidation Loss: 0.318678 \tR2: 0.514790\n",
            " Epoch: 12281 \tTraining Loss:   0.282409\n",
            " Epoch: 12281 \tValidation Loss: 0.327057 \tR2: 0.514790\n",
            " Epoch: 12282 \tTraining Loss:   0.281720\n",
            " Epoch: 12282 \tValidation Loss: 0.327147 \tR2: 0.514790\n",
            " Epoch: 12283 \tTraining Loss:   0.271940\n",
            " Epoch: 12283 \tValidation Loss: 0.448657 \tR2: 0.514790\n",
            " Epoch: 12284 \tTraining Loss:   0.290699\n",
            " Epoch: 12284 \tValidation Loss: 0.367345 \tR2: 0.514790\n",
            " Epoch: 12285 \tTraining Loss:   0.280200\n",
            " Epoch: 12285 \tValidation Loss: 0.291319 \tR2: 0.514790\n",
            " Epoch: 12286 \tTraining Loss:   0.271989\n",
            " Epoch: 12286 \tValidation Loss: 0.359138 \tR2: 0.514790\n",
            " Epoch: 12287 \tTraining Loss:   0.278568\n",
            " Epoch: 12287 \tValidation Loss: 0.335093 \tR2: 0.514790\n",
            " Epoch: 12288 \tTraining Loss:   0.268273\n",
            " Epoch: 12288 \tValidation Loss: 0.339505 \tR2: 0.514790\n",
            " Epoch: 12289 \tTraining Loss:   0.287982\n",
            " Epoch: 12289 \tValidation Loss: 0.381151 \tR2: 0.514790\n",
            " Epoch: 12290 \tTraining Loss:   0.276587\n",
            " Epoch: 12290 \tValidation Loss: 0.319448 \tR2: 0.514790\n",
            " Epoch: 12291 \tTraining Loss:   0.270687\n",
            " Epoch: 12291 \tValidation Loss: 0.343162 \tR2: 0.514790\n",
            " Epoch: 12292 \tTraining Loss:   0.274197\n",
            " Epoch: 12292 \tValidation Loss: 0.382298 \tR2: 0.514790\n",
            " Epoch: 12293 \tTraining Loss:   0.278394\n",
            " Epoch: 12293 \tValidation Loss: 0.349026 \tR2: 0.514790\n",
            " Epoch: 12294 \tTraining Loss:   0.294400\n",
            " Epoch: 12294 \tValidation Loss: 0.490693 \tR2: 0.514790\n",
            " Epoch: 12295 \tTraining Loss:   0.293161\n",
            " Epoch: 12295 \tValidation Loss: 0.310569 \tR2: 0.514790\n",
            " Epoch: 12296 \tTraining Loss:   0.296558\n",
            " Epoch: 12296 \tValidation Loss: 0.310415 \tR2: 0.514790\n",
            " Epoch: 12297 \tTraining Loss:   0.277848\n",
            " Epoch: 12297 \tValidation Loss: 0.393595 \tR2: 0.514790\n",
            " Epoch: 12298 \tTraining Loss:   0.277979\n",
            " Epoch: 12298 \tValidation Loss: 0.332034 \tR2: 0.514790\n",
            " Epoch: 12299 \tTraining Loss:   0.271379\n",
            " Epoch: 12299 \tValidation Loss: 0.347140 \tR2: 0.514790\n",
            " Epoch: 12300 \tTraining Loss:   0.281408\n",
            " Epoch: 12300 \tValidation Loss: 0.318674 \tR2: 0.598286\n",
            " Epoch: 12301 \tTraining Loss:   0.277065\n",
            " Epoch: 12301 \tValidation Loss: 0.332781 \tR2: 0.598286\n",
            " Epoch: 12302 \tTraining Loss:   0.276986\n",
            " Epoch: 12302 \tValidation Loss: 0.313372 \tR2: 0.598286\n",
            " Epoch: 12303 \tTraining Loss:   0.275106\n",
            " Epoch: 12303 \tValidation Loss: 0.324815 \tR2: 0.598286\n",
            " Epoch: 12304 \tTraining Loss:   0.283419\n",
            " Epoch: 12304 \tValidation Loss: 0.316506 \tR2: 0.598286\n",
            " Epoch: 12305 \tTraining Loss:   0.275551\n",
            " Epoch: 12305 \tValidation Loss: 0.382916 \tR2: 0.598286\n",
            " Epoch: 12306 \tTraining Loss:   0.288383\n",
            " Epoch: 12306 \tValidation Loss: 0.322118 \tR2: 0.598286\n",
            " Epoch: 12307 \tTraining Loss:   0.274847\n",
            " Epoch: 12307 \tValidation Loss: 0.320504 \tR2: 0.598286\n",
            " Epoch: 12308 \tTraining Loss:   0.269974\n",
            " Epoch: 12308 \tValidation Loss: 0.328121 \tR2: 0.598286\n",
            " Epoch: 12309 \tTraining Loss:   0.280610\n",
            " Epoch: 12309 \tValidation Loss: 0.347081 \tR2: 0.598286\n",
            " Epoch: 12310 \tTraining Loss:   0.295112\n",
            " Epoch: 12310 \tValidation Loss: 0.337185 \tR2: 0.598286\n",
            " Epoch: 12311 \tTraining Loss:   0.274729\n",
            " Epoch: 12311 \tValidation Loss: 0.321464 \tR2: 0.598286\n",
            " Epoch: 12312 \tTraining Loss:   0.285461\n",
            " Epoch: 12312 \tValidation Loss: 0.314936 \tR2: 0.598286\n",
            " Epoch: 12313 \tTraining Loss:   0.275712\n",
            " Epoch: 12313 \tValidation Loss: 0.331291 \tR2: 0.598286\n",
            " Epoch: 12314 \tTraining Loss:   0.283676\n",
            " Epoch: 12314 \tValidation Loss: 0.330950 \tR2: 0.598286\n",
            " Epoch: 12315 \tTraining Loss:   0.276062\n",
            " Epoch: 12315 \tValidation Loss: 0.374126 \tR2: 0.598286\n",
            " Epoch: 12316 \tTraining Loss:   0.271163\n",
            " Epoch: 12316 \tValidation Loss: 0.351348 \tR2: 0.598286\n",
            " Epoch: 12317 \tTraining Loss:   0.272478\n",
            " Epoch: 12317 \tValidation Loss: 0.341955 \tR2: 0.598286\n",
            " Epoch: 12318 \tTraining Loss:   0.277252\n",
            " Epoch: 12318 \tValidation Loss: 0.358099 \tR2: 0.598286\n",
            " Epoch: 12319 \tTraining Loss:   0.276540\n",
            " Epoch: 12319 \tValidation Loss: 0.390997 \tR2: 0.598286\n",
            " Epoch: 12320 \tTraining Loss:   0.282807\n",
            " Epoch: 12320 \tValidation Loss: 0.301294 \tR2: 0.598286\n",
            " Epoch: 12321 \tTraining Loss:   0.278836\n",
            " Epoch: 12321 \tValidation Loss: 0.432505 \tR2: 0.598286\n",
            " Epoch: 12322 \tTraining Loss:   0.292379\n",
            " Epoch: 12322 \tValidation Loss: 0.364657 \tR2: 0.598286\n",
            " Epoch: 12323 \tTraining Loss:   0.281069\n",
            " Epoch: 12323 \tValidation Loss: 0.353187 \tR2: 0.598286\n",
            " Epoch: 12324 \tTraining Loss:   0.292460\n",
            " Epoch: 12324 \tValidation Loss: 0.385109 \tR2: 0.598286\n",
            " Epoch: 12325 \tTraining Loss:   0.273024\n",
            " Epoch: 12325 \tValidation Loss: 0.352755 \tR2: 0.598286\n",
            " Epoch: 12326 \tTraining Loss:   0.283225\n",
            " Epoch: 12326 \tValidation Loss: 0.343034 \tR2: 0.598286\n",
            " Epoch: 12327 \tTraining Loss:   0.278333\n",
            " Epoch: 12327 \tValidation Loss: 0.554302 \tR2: 0.598286\n",
            " Epoch: 12328 \tTraining Loss:   0.294509\n",
            " Epoch: 12328 \tValidation Loss: 0.349824 \tR2: 0.598286\n",
            " Epoch: 12329 \tTraining Loss:   0.287721\n",
            " Epoch: 12329 \tValidation Loss: 0.353613 \tR2: 0.598286\n",
            " Epoch: 12330 \tTraining Loss:   0.283026\n",
            " Epoch: 12330 \tValidation Loss: 0.314782 \tR2: 0.598286\n",
            " Epoch: 12331 \tTraining Loss:   0.267085\n",
            " Epoch: 12331 \tValidation Loss: 0.301727 \tR2: 0.598286\n",
            " Epoch: 12332 \tTraining Loss:   0.268890\n",
            " Epoch: 12332 \tValidation Loss: 0.283203 \tR2: 0.598286\n",
            " Epoch: 12333 \tTraining Loss:   0.277321\n",
            " Epoch: 12333 \tValidation Loss: 0.338602 \tR2: 0.598286\n",
            " Epoch: 12334 \tTraining Loss:   0.277453\n",
            " Epoch: 12334 \tValidation Loss: 0.358194 \tR2: 0.598286\n",
            " Epoch: 12335 \tTraining Loss:   0.278473\n",
            " Epoch: 12335 \tValidation Loss: 0.432219 \tR2: 0.598286\n",
            " Epoch: 12336 \tTraining Loss:   0.277485\n",
            " Epoch: 12336 \tValidation Loss: 0.330002 \tR2: 0.598286\n",
            " Epoch: 12337 \tTraining Loss:   0.275652\n",
            " Epoch: 12337 \tValidation Loss: 0.370765 \tR2: 0.598286\n",
            " Epoch: 12338 \tTraining Loss:   0.287423\n",
            " Epoch: 12338 \tValidation Loss: 0.336039 \tR2: 0.598286\n",
            " Epoch: 12339 \tTraining Loss:   0.312449\n",
            " Epoch: 12339 \tValidation Loss: 0.332355 \tR2: 0.598286\n",
            " Epoch: 12340 \tTraining Loss:   0.269240\n",
            " Epoch: 12340 \tValidation Loss: 0.353337 \tR2: 0.598286\n",
            " Epoch: 12341 \tTraining Loss:   0.283502\n",
            " Epoch: 12341 \tValidation Loss: 0.317980 \tR2: 0.598286\n",
            " Epoch: 12342 \tTraining Loss:   0.279022\n",
            " Epoch: 12342 \tValidation Loss: 0.325981 \tR2: 0.598286\n",
            " Epoch: 12343 \tTraining Loss:   0.285220\n",
            " Epoch: 12343 \tValidation Loss: 0.361566 \tR2: 0.598286\n",
            " Epoch: 12344 \tTraining Loss:   0.291018\n",
            " Epoch: 12344 \tValidation Loss: 0.317936 \tR2: 0.598286\n",
            " Epoch: 12345 \tTraining Loss:   0.275491\n",
            " Epoch: 12345 \tValidation Loss: 0.335111 \tR2: 0.598286\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3746d7003e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5e13801a34d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5e13801a34d1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = SWNN(insize=5, outsize=4)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "relation = str()\n",
        "relation = varName[0]+\"~\" + \"+\".join(varName[1:varNum])\n",
        "fit=sm.formula.ols(relation,data=train_set).fit()\n",
        "\n",
        "r2 = 0\n",
        "last_min = 0\n",
        "best_loss = -1\n",
        "weightlist = []\n",
        "temp = []\n",
        "for j in fit.params:\n",
        "    temp.append(j)\n",
        "weightlist.append(temp)\n",
        "out = nn.Linear(4, 1, bias = False)\n",
        "out.weight = nn.Parameter(torch.tensor(weightlist), requires_grad=False)\n",
        "\n",
        "if is_gpu:\n",
        "    model = model.cuda()\n",
        "    out = out.cuda()\n",
        "\n",
        "for epoch in range(1, 200000+1):\n",
        "    train(epoch)\n",
        "    val(epoch)\n",
        "    if last_min >= 2000:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV-v3AVagu9h",
        "outputId": "8e0f37f7-6848-4338-87f3-68da2defa3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r Epoch: 0 \tValidation Loss: 0.275503 \tR2: 0.643757\n"
          ]
        }
      ],
      "source": [
        "model = torch.load(\"model.pkl\")\n",
        "val(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g02G5KT0JFCg"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "861f9c34f7302a1aedb62edfc1533c524ce2793735e6b405602ea89eb9cb2484"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
